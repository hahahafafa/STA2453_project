{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28wzBhPguNHy"
      },
      "source": [
        "# Progress Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9spWiUJuNHz"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "**Recap of Project Overview:**  \n",
        "In this project, I am working to predict patient recovery time using MRI scans combined with demographic data and medical history. The idea is to build a model that can forecast how long patients might take to recover from their injuries, which could be very useful in a clinical setting. By integrating high-dimensional imaging data with information like age, sex, and concussion history, I hope to uncover patterns that relate to recovery outcomes.\n",
        "\n",
        "**Motivation:**  \n",
        "The primary aim of this project is to develop a predictive model that estimates the recovery time (in days) for patients based on their blood flow MRI ( data and demographic/medical features). By achieving this, we aim to provide valuable insights into patient recovery and improve medical decision-making processes.\n",
        "\n",
        "**Research Questions:**  \n",
        "The key question driving this project is: Can we accurately predict recovery time based on blood flow MRI data, demographic features (e.g., age, gender), and medical history (e.g., history of concussion)?\n",
        "This project involves leveraging a deep learning approach to analyze blood flow MRI images and other structured data, enabling the development of a robust predictive tool.\n",
        "\n",
        "Some of the other questions driving me to the key questions include:  \n",
        "- Can MRI scans, along with demographic and medical data, accurately predict patient recovery time?  \n",
        "- Does combining imaging features with demographic data enhance the prediction performance?  \n",
        "- Which specific features are most closely associated with prolonged recovery times?\n",
        "\n",
        "## 2. Data Summary\n",
        "\n",
        "**Datasets:**  \n",
        "For this project, I am using two main datasets. The first dataset, **clean_output.csv**, contains patient metadata such as age, sex, history of concussion (HOC), days since injury (dInj), and days of recovery (dRecov), along with summary statistics for MRI scans and corresponding file names. The second dataset, **cleaned_output.h5**, consists of preprocessed MRI data stored as NumPy arrays, each indexed by a unique subject_id.\n",
        "\n",
        "**Key EDA Findings:**  \n",
        "Through exploratory data analysis (EDA), I have uncovered several important insights. I looked at the variation in recovery time across different categories, including age, HOC, and sex. The recovery time distribution is highly skewed, which may impact model performance. These findings have helped guide my approach to feature engineering and model development, and they underscore the need for further analysis to capture the complex relationships between these variables and recovery time.\n",
        "\n",
        "It also revealed that there are no major issues with missing values or data alignment, providing a solid and consistent foundation for further investigation. The demographic data shows that the age of subjects is concentrated in the late teens to mid-twenties, indicating a relatively homogeneous cohort. Moreover, both sex and history of concussion (HOC) are roughly balanced between their respective categories (male/female, yes/no for concussion), which minimizes concerns related to class imbalance.\n",
        "\n",
        "In terms of recovery time (dRecov), the distribution is notably right-skewed, with a few outliers extending to high values, such as over 300 days. This skewness suggests that while most patients recover within a relatively short period, there are exceptional cases that may need special attention. For the MRI data, applying a brain mask proved to be beneficial as it removed non-brain regions, thereby mitigating skew and reducing the impact of outliers that arise from large areas of zero or near-zero intensities.\n",
        "\n",
        "Dimensionality reduction using PCA further highlighted the benefits of masking. Before masking, the PCA results were dominated by outliers, resulting in a tight cluster of points with a few extreme cases. After masking, however, the points were more evenly distributed in the principal component space, suggesting that this approach captures more meaningful anatomical variation. Additionally, the pairplot analysis confirmed that demographic features such as sex, HOC, and days since injury do not exhibit strong linear correlations with age or each other, which reduces concerns about multicollinearity in subsequent modeling efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIPoAZsJuNH0"
      },
      "source": [
        "## **3 Modeling Approach**\n",
        "\n",
        "In this project, I have implemented multiple models to predict recovery time based on fMRI data and clinical features. My initial models served as baselines, while later models incorporated more advanced techniques, such as K-Fold cross-validation and hyperparameter tuning, to improve performance.\n",
        "\n",
        "#### **Model 1: Neural Network with PyTorch**\n",
        "The first model I implemented was a simple neural network using PyTorch. This model consists of multiple fully connected layers with dropout regularization and ReLU activation. It was trained using the Adam optimizer and Mean Squared Error (MSE) loss function. The training process involved splitting the dataset into training and testing sets, followed by iteratively optimizing the network over 50 epochs.\n",
        "\n",
        "Despite these efforts, Model 1 produced a high RMSE, indicating poor predictive performance. This suggested that the model either lacked the necessary complexity to capture patterns in the data or required more extensive hyperparameter tuning.\n",
        "\n",
        "#### **Model 2: Random Forest**\n",
        "To compare deep learning with traditional machine learning, I implemented a Random Forest model. This ensemble learning method leverages multiple decision trees to improve predictive accuracy. However, like Model 1, this approach resulted in a relatively high RMSE, suggesting that it struggled to fully utilize the complex MRI features.\n",
        "\n",
        "Given the poor performance of these initial models, I shifted my focus toward more advanced deep learning techniques with systematic hyperparameter tuning.\n",
        "\n",
        "#### **Model 3: K-Fold Cross-Validation with GridSearchCV**\n",
        "To improve model reliability and avoid overfitting, I implemented **Model 3**, which uses K-Fold cross-validation. This approach trains the neural network on different subsets of the data and evaluates performance across multiple validation sets. Additionally, I introduced **GridSearchCV**, which allows me to systematically explore different combinations of hyperparameters, including:\n",
        "- **Neurons per layer**: 64 or 128\n",
        "- **Dropout rate**: 0.2 or 0.5\n",
        "- **Learning rate**: 0.001 or 0.01\n",
        "\n",
        "After testing multiple configurations, Model 3 achieved an **RMSE of approximately 58.09 days** with an optimal set of hyperparameters (**64 neurons, 0.5 dropout rate, and a learning rate of 0.001**). While this was an improvement over the earlier models, there was still room for further optimization.\n",
        "\n",
        "#### **Model 4: Hyperparameter Tuning with Additional Regularization**\n",
        "Building on the findings from Model 3, **Model 4** introduced additional hyperparameter tuning, including:\n",
        "- **More fine-tuned neuron values** (64, 80, 96, 128)\n",
        "- **More granular dropout rates** (0.2, 0.3, 0.5)\n",
        "- **Learning rates ranging from 0.0005 to 0.005**\n",
        "- **Weight decay (L2 regularization)**\n",
        "- **Number of layers** (2 or 3)\n",
        "- **Extended training epochs** (100 or 150)\n",
        "\n",
        "Through this refined tuning process, Model 4 achieved a **best RMSE of 55.14 days**, further improving upon the previous models. The optimal configuration included **96 neurons, a dropout rate of 0.3, a learning rate of 0.005, weight decay of 1e-05, and training for 150 epochs**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibTHGxpeuNH1"
      },
      "source": [
        "These results show that our hyperparameter tuning process is working well to reduce the prediction error. In our initial experiment (Model 3), using K-Fold cross-validation, we found that the best configuration with 64 neurons, a dropout rate of 0.5, and a learning rate of 0.001 produced an average RMSE of about 58.09 days. This means that, on average, our modelâ€™s prediction of recovery time was off by roughly 58 days.\n",
        "\n",
        "After further tuning with additional parameters such as weight decay, different numbers of neurons, layers, and epochs, we achieved even better performance. The best result came from a configuration with 96 neurons, a dropout rate of 0.3, a learning rate of 0.005, a weight decay of 1e-05, using 2 layers and training for 150 epochs, which yielded an RMSE of approximately 55.14 days.\n",
        "\n",
        "In summary, by systematically tuning the hyperparameters, we managed to lower our error margin, suggesting that the refined model can predict recovery time more accurately. This improvement demonstrates the value of hyperparameter tuning in capturing the complex relationships in high-dimensional fMRI data combined with clinical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e157n85guNH1"
      },
      "source": [
        "### **Key Takeaways from Model Comparisons**\n",
        "1. **Baseline Models Performed Poorly**  \n",
        "   - Both the initial neural network (Model 1) and the Random Forest model (Model 2) produced high RMSE values, indicating that they were not effectively capturing the complexity of the data.\n",
        "\n",
        "2. **Cross-Validation and Hyperparameter Tuning Improved Performance**  \n",
        "   - Model 3 (K-Fold with GridSearchCV) significantly reduced error by systematically exploring different parameter configurations.\n",
        "   - Model 4 (Further Tuning with Regularization) yielded the best results, demonstrating the importance of weight decay and more refined hyperparameter selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNW_28MmuNH1"
      },
      "source": [
        "## 4. Challenges and Next Steps\n",
        "\n",
        "Although Model 4 demonstrated significant improvements, achieving an RMSE of 55.14 days, this value remains high for clinical applicability. To enhance predictive accuracy and make the model more reliable, I will focus on several key areas, including better handling of outliers, refining feature representations, improving model architecture, and further tuning hyperparameters.\n",
        "\n",
        "One of the biggest challenges in modeling recovery time is the highly skewed distribution of the target variable (dRecov). Extreme outliers, ie patients with recovery times far beyond the majorityâ€”can distort model predictions and increase RMSE. To address this, I will experiment with log transformation or other normalization techniques to reduce the impact of extreme values. Additionally, I plan to test alternative loss functions such as Huber Loss, which is less sensitive to outliers than the commonly used Mean Squared Error (MSE). These approaches should help the model generalize better and make more stable predictions.\n",
        "Currently, the MRI features in my dataset are extracted from whole-brain intensity distributions. However, certain brain regions may be more predictive of recovery time than others. To refine feature extraction, I will apply Region-of-Interest (ROI) segmentation, focusing on specific brain areas, such as the frontal and temporal lobes, that may have stronger clinical relevance. By computing summary statistics (e.g., mean, variance) within each ROI, I aim to enhance the modelâ€™s ability to detect meaningful patterns that contribute to recovery time.\n",
        "\n",
        "While fully connected neural networks (FCNs) can capture complex relationships in the data, they may not fully utilize the spatial information inherent in MRI scans. Convolutional Neural Networks (CNNs), on the other hand, are well-suited for analyzing spatial structures in imaging data. Therefore, I will implement CNN-based models that process 3D fMRI scans directly. By comparing CNN performance with my existing FCN models, I hope to determine whether spatial patterns in MRI scans can provide additional predictive power.\n",
        "\n",
        "Although Principal Component Analysis (PCA) has been useful for dimensionality reduction, it may not be the best method for capturing non-linear relationships in MRI data. As an alternative, I plan to experiment with t-SNE (t-distributed Stochastic Neighbor Embedding) and UMAP (Uniform Manifold Approximation and Projection) to explore whether different recovery profiles emerge when visualizing MRI features in a lower-dimensional space. If clusters corresponding to distinct clinical patterns are found, I will use these features to enhance my predictive models.\n",
        "\n",
        "Hyperparameter tuning played a crucial role in improving Model 4â€™s performance, demonstrating the importance of optimizing parameters like dropout rate, learning rate, and weight decay. While I previously used GridSearchCV, I will now explore Bayesian Optimization, a more efficient technique for navigating high-dimensional parameter spaces. Additionally, I will test adaptive learning rates, which allow the model to adjust its training speed dynamically based on convergence patterns. These optimizations should help the model learn more efficiently while avoiding overfitting.\n",
        "\n",
        "Currently, my models process MRI and demographic features together, but a more structured approach to integrating these data sources may improve performance. One potential solution is to build a multi-input model, where CNNs extract imaging features while a separate neural network processes clinical and demographic data. These two branches would then be merged before the final prediction layer, allowing the model to make better use of both types of information. This fusion approach could improve generalization and predictive accuracy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpCa1jJwuNH0"
      },
      "source": [
        "## 5. Appendix\n",
        "\n",
        "### Snip of the result\n",
        "\n",
        "Here's a snip of the result, full code is avaliable on github:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gophJ96YuNH0",
        "outputId": "a1aaf2e2-f995-446c-fa92-f74e23a56075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Model 3: K-Fold Cross-Validation with PyTorch...\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001\n",
            "Epoch [10/50], Loss: 20276.6414\n",
            "Epoch [20/50], Loss: 16265.1587\n",
            "Epoch [30/50], Loss: 6490.2791\n",
            "Epoch [40/50], Loss: 5100.2111\n",
            "Epoch [50/50], Loss: 3664.1124\n",
            "Fold 1, RMSE: 58.99284744262695\n",
            "Epoch [10/50], Loss: 16862.3271\n",
            "Epoch [20/50], Loss: 19716.2688\n",
            "Epoch [30/50], Loss: 9087.2522\n",
            "Epoch [40/50], Loss: 6594.4599\n",
            "Epoch [50/50], Loss: 6652.3967\n",
            "Fold 2, RMSE: 63.448673248291016\n",
            "Epoch [10/50], Loss: 12629.6711\n",
            "Epoch [20/50], Loss: 6264.5955\n",
            "Epoch [30/50], Loss: 5806.6519\n",
            "Epoch [40/50], Loss: 3285.1958\n",
            "Epoch [50/50], Loss: 2643.7082\n",
            "Fold 3, RMSE: 93.34349060058594\n",
            "Epoch [10/50], Loss: 27332.9478\n",
            "Epoch [20/50], Loss: 17970.0107\n",
            "Epoch [30/50], Loss: 17378.7285\n",
            "Epoch [40/50], Loss: 8668.6591\n",
            "Epoch [50/50], Loss: 10027.6521\n",
            "Fold 4, RMSE: 36.986846923828125\n",
            "Epoch [10/50], Loss: 68122.9775\n",
            "Epoch [20/50], Loss: 12435.8416\n",
            "Epoch [30/50], Loss: 8093.8364\n",
            "Epoch [40/50], Loss: 6136.7468\n",
            "Epoch [50/50], Loss: 7385.2642\n",
            "Fold 5, RMSE: 48.890933990478516\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001: 60.33255844116211\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.01\n",
            "Epoch [10/50], Loss: 42513.7905\n",
            "Epoch [20/50], Loss: 22795.3855\n",
            "Epoch [30/50], Loss: 13326.1963\n",
            "Epoch [40/50], Loss: 16072.4006\n",
            "Epoch [50/50], Loss: 22045.8499\n",
            "Fold 1, RMSE: 48.913021087646484\n",
            "Epoch [10/50], Loss: 95480.2451\n",
            "Epoch [20/50], Loss: 11673.5508\n",
            "Epoch [30/50], Loss: 9050.8478\n",
            "Epoch [40/50], Loss: 12631.6567\n",
            "Epoch [50/50], Loss: 11503.8414\n",
            "Fold 2, RMSE: 73.2386474609375\n",
            "Epoch [10/50], Loss: 258736.9336\n",
            "Epoch [20/50], Loss: 15485.4111\n",
            "Epoch [30/50], Loss: 7152.9576\n",
            "Epoch [40/50], Loss: 6540.6458\n",
            "Epoch [50/50], Loss: 8095.0743\n",
            "Fold 3, RMSE: 86.30178833007812\n",
            "Epoch [10/50], Loss: 390350.6172\n",
            "Epoch [20/50], Loss: 33568.3809\n",
            "Epoch [30/50], Loss: 24154.3047\n",
            "Epoch [40/50], Loss: 16760.2864\n",
            "Epoch [50/50], Loss: 28553.6162\n",
            "Fold 4, RMSE: 46.999755859375\n",
            "Epoch [10/50], Loss: 178706.3867\n",
            "Epoch [20/50], Loss: 19312.6074\n",
            "Epoch [30/50], Loss: 12286.5509\n",
            "Epoch [40/50], Loss: 17501.2158\n",
            "Epoch [50/50], Loss: 13107.5767\n",
            "Fold 5, RMSE: 50.18305206298828\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.01: 61.12725296020508\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001\n",
            "Epoch [10/50], Loss: 19307.6924\n",
            "Epoch [20/50], Loss: 11559.5103\n",
            "Epoch [30/50], Loss: 12722.6880\n",
            "Epoch [40/50], Loss: 10112.9207\n",
            "Epoch [50/50], Loss: 6401.6896\n",
            "Fold 1, RMSE: 50.47276306152344\n",
            "Epoch [10/50], Loss: 11208.5791\n",
            "Epoch [20/50], Loss: 12200.4268\n",
            "Epoch [30/50], Loss: 7908.8508\n",
            "Epoch [40/50], Loss: 9248.3456\n",
            "Epoch [50/50], Loss: 13552.3455\n",
            "Fold 2, RMSE: 73.14628601074219\n",
            "Epoch [10/50], Loss: 11399.3374\n",
            "Epoch [20/50], Loss: 9486.2527\n",
            "Epoch [30/50], Loss: 6807.7844\n",
            "Epoch [40/50], Loss: 6976.0060\n",
            "Epoch [50/50], Loss: 8414.8496\n",
            "Fold 3, RMSE: 89.00782775878906\n",
            "Epoch [10/50], Loss: 11994.3328\n",
            "Epoch [20/50], Loss: 9608.1278\n",
            "Epoch [30/50], Loss: 10112.4530\n",
            "Epoch [40/50], Loss: 10577.9529\n",
            "Epoch [50/50], Loss: 11992.1868\n",
            "Fold 4, RMSE: 34.87022018432617\n",
            "Epoch [10/50], Loss: 16254.5220\n",
            "Epoch [20/50], Loss: 12931.8638\n",
            "Epoch [30/50], Loss: 17310.3162\n",
            "Epoch [40/50], Loss: 7326.6658\n",
            "Epoch [50/50], Loss: 5701.9600\n",
            "Fold 5, RMSE: 42.93119812011719\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001: 58.08565902709961\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.01\n",
            "Epoch [10/50], Loss: 109762.5938\n",
            "Epoch [20/50], Loss: 25515.6836\n",
            "Epoch [30/50], Loss: 12291.1938\n",
            "Epoch [40/50], Loss: 13495.2092\n",
            "Epoch [50/50], Loss: 10483.5176\n",
            "Fold 1, RMSE: 49.95261001586914\n",
            "Epoch [10/50], Loss: 146447.2500\n",
            "Epoch [20/50], Loss: 12670.0457\n",
            "Epoch [30/50], Loss: 11890.8550\n",
            "Epoch [40/50], Loss: 11713.9684\n",
            "Epoch [50/50], Loss: 12090.6300\n",
            "Fold 2, RMSE: 73.89644622802734\n",
            "Epoch [10/50], Loss: 47061.4644\n",
            "Epoch [20/50], Loss: 11335.4507\n",
            "Epoch [30/50], Loss: 14199.0929\n",
            "Epoch [40/50], Loss: 11978.7234\n",
            "Epoch [50/50], Loss: 14957.8809\n",
            "Fold 3, RMSE: 88.75458526611328\n",
            "Epoch [10/50], Loss: 17539.9058\n",
            "Epoch [20/50], Loss: 19670.4626\n",
            "Epoch [30/50], Loss: 17153.8560\n",
            "Epoch [40/50], Loss: 17953.2430\n",
            "Epoch [50/50], Loss: 15219.9146\n",
            "Fold 4, RMSE: 54.724422454833984\n",
            "Epoch [10/50], Loss: 30517.9805\n",
            "Epoch [20/50], Loss: 13603.7048\n",
            "Epoch [30/50], Loss: 14231.0261\n",
            "Epoch [40/50], Loss: 63498.1520\n",
            "Epoch [50/50], Loss: 13082.1697\n",
            "Fold 5, RMSE: 56.790916442871094\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.01: 64.82379608154297\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001\n",
            "Epoch [10/50], Loss: 19904.0181\n",
            "Epoch [20/50], Loss: 11237.4489\n",
            "Epoch [30/50], Loss: 12778.4792\n",
            "Epoch [40/50], Loss: 5909.6158\n",
            "Epoch [50/50], Loss: 3921.9594\n",
            "Fold 1, RMSE: 54.932369232177734\n",
            "Epoch [10/50], Loss: 21800.1245\n",
            "Epoch [20/50], Loss: 11931.7794\n",
            "Epoch [30/50], Loss: 12019.4326\n",
            "Epoch [40/50], Loss: 9460.4282\n",
            "Epoch [50/50], Loss: 10083.4642\n",
            "Fold 2, RMSE: 66.44004821777344\n",
            "Epoch [10/50], Loss: 17597.8206\n",
            "Epoch [20/50], Loss: 9938.3657\n",
            "Epoch [30/50], Loss: 9189.4424\n",
            "Epoch [40/50], Loss: 11704.1003\n",
            "Epoch [50/50], Loss: 10571.5928\n",
            "Fold 3, RMSE: 85.73006439208984\n",
            "Epoch [10/50], Loss: 36032.8232\n",
            "Epoch [20/50], Loss: 19105.4927\n",
            "Epoch [30/50], Loss: 13605.2234\n",
            "Epoch [40/50], Loss: 14356.7612\n",
            "Epoch [50/50], Loss: 8038.2062\n",
            "Fold 4, RMSE: 34.854652404785156\n",
            "Epoch [10/50], Loss: 28895.9468\n",
            "Epoch [20/50], Loss: 10794.9742\n",
            "Epoch [30/50], Loss: 9721.3839\n",
            "Epoch [40/50], Loss: 7565.8835\n",
            "Epoch [50/50], Loss: 5687.9680\n",
            "Fold 5, RMSE: 51.1038818359375\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001: 58.612203216552736\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.01\n",
            "Epoch [10/50], Loss: 186888.6641\n",
            "Epoch [20/50], Loss: 26015.6553\n",
            "Epoch [30/50], Loss: 24590.3418\n",
            "Epoch [40/50], Loss: 19486.8232\n",
            "Epoch [50/50], Loss: 13034.7234\n",
            "Fold 1, RMSE: 44.37154769897461\n",
            "Epoch [10/50], Loss: 705468.5781\n",
            "Epoch [20/50], Loss: 74923.0078\n",
            "Epoch [30/50], Loss: 14000.0054\n",
            "Epoch [40/50], Loss: 15030.3086\n",
            "Epoch [50/50], Loss: 19883.6016\n",
            "Fold 2, RMSE: 67.1455307006836\n",
            "Epoch [10/50], Loss: 133010.6348\n",
            "Epoch [20/50], Loss: 15922.2178\n",
            "Epoch [30/50], Loss: 13565.3560\n",
            "Epoch [40/50], Loss: 7951.0851\n",
            "Epoch [50/50], Loss: 10360.3442\n",
            "Fold 3, RMSE: 90.8171615600586\n",
            "Epoch [10/50], Loss: 36648.6958\n",
            "Epoch [20/50], Loss: 14173.4644\n",
            "Epoch [30/50], Loss: 10984.6754\n",
            "Epoch [40/50], Loss: 13129.3643\n",
            "Epoch [50/50], Loss: 17402.1277\n",
            "Fold 4, RMSE: 34.329830169677734\n",
            "Epoch [10/50], Loss: 50000.2773\n",
            "Epoch [20/50], Loss: 29271.2520\n",
            "Epoch [30/50], Loss: 12697.8870\n",
            "Epoch [40/50], Loss: 15313.2280\n",
            "Epoch [50/50], Loss: 11871.9319\n",
            "Fold 5, RMSE: 58.04029083251953\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.01: 58.94087219238281\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001\n",
            "Epoch [10/50], Loss: 26436.7065\n",
            "Epoch [20/50], Loss: 11003.3347\n",
            "Epoch [30/50], Loss: 7888.7464\n",
            "Epoch [40/50], Loss: 6018.1061\n",
            "Epoch [50/50], Loss: 10324.3517\n",
            "Fold 1, RMSE: 51.87633514404297\n",
            "Epoch [10/50], Loss: 27474.3242\n",
            "Epoch [20/50], Loss: 15774.0378\n",
            "Epoch [30/50], Loss: 10872.5995\n",
            "Epoch [40/50], Loss: 9569.9612\n",
            "Epoch [50/50], Loss: 7134.0474\n",
            "Fold 2, RMSE: 71.35540771484375\n",
            "Epoch [10/50], Loss: 60347.4912\n",
            "Epoch [20/50], Loss: 11117.2034\n",
            "Epoch [30/50], Loss: 11943.7444\n",
            "Epoch [40/50], Loss: 8668.7832\n",
            "Epoch [50/50], Loss: 8652.9778\n",
            "Fold 3, RMSE: 92.23494720458984\n",
            "Epoch [10/50], Loss: 11654.1681\n",
            "Epoch [20/50], Loss: 10611.8788\n",
            "Epoch [30/50], Loss: 10944.1399\n",
            "Epoch [40/50], Loss: 9296.0137\n",
            "Epoch [50/50], Loss: 10985.2687\n",
            "Fold 4, RMSE: 39.85502624511719\n",
            "Epoch [10/50], Loss: 16012.3096\n",
            "Epoch [20/50], Loss: 11558.5645\n",
            "Epoch [30/50], Loss: 10383.8490\n",
            "Epoch [40/50], Loss: 8638.7755\n",
            "Epoch [50/50], Loss: 11950.0488\n",
            "Fold 5, RMSE: 45.19416809082031\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001: 60.10317687988281\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.01\n",
            "Epoch [10/50], Loss: 44496.2070\n",
            "Epoch [20/50], Loss: 21294.1152\n",
            "Epoch [30/50], Loss: 18943.3208\n",
            "Epoch [40/50], Loss: 33250.7006\n",
            "Epoch [50/50], Loss: 36158.1904\n",
            "Fold 1, RMSE: 86.15185546875\n",
            "Epoch [10/50], Loss: 60529.6338\n",
            "Epoch [20/50], Loss: 14266.1077\n",
            "Epoch [30/50], Loss: 15454.1399\n",
            "Epoch [40/50], Loss: 20505.0938\n",
            "Epoch [50/50], Loss: 13548.4719\n",
            "Fold 2, RMSE: 74.46021270751953\n",
            "Epoch [10/50], Loss: 466028.7969\n",
            "Epoch [20/50], Loss: 74473.2031\n",
            "Epoch [30/50], Loss: 21098.0649\n",
            "Epoch [40/50], Loss: 17259.7944\n",
            "Epoch [50/50], Loss: 12687.1321\n",
            "Fold 3, RMSE: 99.93817138671875\n",
            "Epoch [10/50], Loss: 305259.8906\n",
            "Epoch [20/50], Loss: 39019.7256\n",
            "Epoch [30/50], Loss: 32381.0122\n",
            "Epoch [40/50], Loss: 21803.9614\n",
            "Epoch [50/50], Loss: 23396.3892\n",
            "Fold 4, RMSE: 48.389381408691406\n",
            "Epoch [10/50], Loss: 162243.1250\n",
            "Epoch [20/50], Loss: 14137.7793\n",
            "Epoch [30/50], Loss: 10528.5090\n",
            "Epoch [40/50], Loss: 17112.7791\n",
            "Epoch [50/50], Loss: 14510.9946\n",
            "Fold 5, RMSE: 50.113197326660156\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.01: 71.81056365966796\n",
            "Best RMSE: 58.08565902709961\n",
            "Best Params: (64, 0.5, 0.001)\n",
            "Model 3 RMSE: 58.08565902709961\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    X_combined, y = load_and_prepare_data('cleaned_output.csv', 'brain_overlayed_new.h5')\n",
        "\n",
        "    # Run Model 3 (K-Fold CV with GridSearchCV)\n",
        "    model3_rmse = model3(X_combined, y)\n",
        "    print(f\"Model 3 RMSE: {model3_rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ackc69PuNH1",
        "outputId": "2fe226e0-c407-4f4f-d745-7e67162c7372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Model with Further Hyperparameter Tuning...\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 9166.6407\n",
            "Epoch [20/100], Loss: 5711.6373\n",
            "Epoch [30/100], Loss: 6046.6249\n",
            "Epoch [40/100], Loss: 6292.8608\n",
            "Epoch [50/100], Loss: 3266.7817\n",
            "Epoch [60/100], Loss: 3131.8219\n",
            "Epoch [70/100], Loss: 3309.0095\n",
            "Epoch [80/100], Loss: 3796.9489\n",
            "Epoch [90/100], Loss: 4521.7365\n",
            "Epoch [100/100], Loss: 3735.5137\n",
            "Fold 1, RMSE: 63.81948471069336\n",
            "Epoch [10/100], Loss: 12176.0835\n",
            "Epoch [20/100], Loss: 8450.9704\n",
            "Epoch [30/100], Loss: 5848.2982\n",
            "Epoch [40/100], Loss: 4235.7450\n",
            "Epoch [50/100], Loss: 1817.0803\n",
            "Epoch [60/100], Loss: 6538.1494\n",
            "Epoch [70/100], Loss: 1706.3610\n",
            "Epoch [80/100], Loss: 2191.0188\n",
            "Epoch [90/100], Loss: 3514.7665\n",
            "Epoch [100/100], Loss: 3509.8644\n",
            "Fold 2, RMSE: 77.2696762084961\n",
            "Epoch [10/100], Loss: 9744.3691\n",
            "Epoch [20/100], Loss: 5762.5269\n",
            "Epoch [30/100], Loss: 5863.4464\n",
            "Epoch [40/100], Loss: 5965.6083\n",
            "Epoch [50/100], Loss: 3368.4945\n",
            "Epoch [60/100], Loss: 3036.0735\n",
            "Epoch [70/100], Loss: 2172.2342\n",
            "Epoch [80/100], Loss: 1243.2361\n",
            "Epoch [90/100], Loss: 2919.9233\n",
            "Epoch [100/100], Loss: 1941.6379\n",
            "Fold 3, RMSE: 92.50481414794922\n",
            "Epoch [10/100], Loss: 18527.5024\n",
            "Epoch [20/100], Loss: 12260.4685\n",
            "Epoch [30/100], Loss: 7923.8274\n",
            "Epoch [40/100], Loss: 16006.7214\n",
            "Epoch [50/100], Loss: 3268.8077\n",
            "Epoch [60/100], Loss: 2273.7982\n",
            "Epoch [70/100], Loss: 2895.2681\n",
            "Epoch [80/100], Loss: 1069.1440\n",
            "Epoch [90/100], Loss: 4552.7239\n",
            "Epoch [100/100], Loss: 5738.2180\n",
            "Fold 4, RMSE: 41.42780303955078\n",
            "Epoch [10/100], Loss: 12745.8208\n",
            "Epoch [20/100], Loss: 10761.9856\n",
            "Epoch [30/100], Loss: 6322.2347\n",
            "Epoch [40/100], Loss: 4491.1542\n",
            "Epoch [50/100], Loss: 4806.7697\n",
            "Epoch [60/100], Loss: 3351.1547\n",
            "Epoch [70/100], Loss: 4922.5372\n",
            "Epoch [80/100], Loss: 2607.3788\n",
            "Epoch [90/100], Loss: 2965.4514\n",
            "Epoch [100/100], Loss: 2422.7382\n",
            "Fold 5, RMSE: 48.1499137878418\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 64.63433837890625\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 12249.2122\n",
            "Epoch [20/150], Loss: 7018.7654\n",
            "Epoch [30/150], Loss: 6882.9404\n",
            "Epoch [40/150], Loss: 6008.5365\n",
            "Epoch [50/150], Loss: 2957.4179\n",
            "Epoch [60/150], Loss: 3022.2490\n",
            "Epoch [70/150], Loss: 2823.8532\n",
            "Epoch [80/150], Loss: 3232.4258\n",
            "Epoch [90/150], Loss: 5372.9303\n",
            "Epoch [100/150], Loss: 2198.1589\n",
            "Epoch [110/150], Loss: 2732.7863\n",
            "Epoch [120/150], Loss: 1754.1355\n",
            "Epoch [130/150], Loss: 1718.5268\n",
            "Epoch [140/150], Loss: 2670.5137\n",
            "Epoch [150/150], Loss: 830.4523\n",
            "Fold 1, RMSE: 55.93073272705078\n",
            "Epoch [10/150], Loss: 12999.0344\n",
            "Epoch [20/150], Loss: 8066.3165\n",
            "Epoch [30/150], Loss: 9049.1414\n",
            "Epoch [40/150], Loss: 5247.7960\n",
            "Epoch [50/150], Loss: 12372.6077\n",
            "Epoch [60/150], Loss: 3571.0855\n",
            "Epoch [70/150], Loss: 4071.6665\n",
            "Epoch [80/150], Loss: 2197.1169\n",
            "Epoch [90/150], Loss: 1569.8165\n",
            "Epoch [100/150], Loss: 2118.1800\n",
            "Epoch [110/150], Loss: 1193.9405\n",
            "Epoch [120/150], Loss: 1489.0859\n",
            "Epoch [130/150], Loss: 1626.2284\n",
            "Epoch [140/150], Loss: 1053.1640\n",
            "Epoch [150/150], Loss: 2915.9452\n",
            "Fold 2, RMSE: 64.76522064208984\n",
            "Epoch [10/150], Loss: 12731.3274\n",
            "Epoch [20/150], Loss: 5986.7313\n",
            "Epoch [30/150], Loss: 4044.4969\n",
            "Epoch [40/150], Loss: 2787.4396\n",
            "Epoch [50/150], Loss: 1774.6940\n",
            "Epoch [60/150], Loss: 2694.6922\n",
            "Epoch [70/150], Loss: 2042.0349\n",
            "Epoch [80/150], Loss: 3068.9693\n",
            "Epoch [90/150], Loss: 4409.2862\n",
            "Epoch [100/150], Loss: 2214.6205\n",
            "Epoch [110/150], Loss: 2132.5875\n",
            "Epoch [120/150], Loss: 1832.9748\n",
            "Epoch [130/150], Loss: 1769.4623\n",
            "Epoch [140/150], Loss: 2346.3324\n",
            "Epoch [150/150], Loss: 1069.0140\n",
            "Fold 3, RMSE: 88.43041229248047\n",
            "Epoch [10/150], Loss: 10444.7614\n",
            "Epoch [20/150], Loss: 12337.0280\n",
            "Epoch [30/150], Loss: 9767.6521\n",
            "Epoch [40/150], Loss: 3895.0314\n",
            "Epoch [50/150], Loss: 4487.1560\n",
            "Epoch [60/150], Loss: 6459.4272\n",
            "Epoch [70/150], Loss: 2870.7612\n",
            "Epoch [80/150], Loss: 1918.1678\n",
            "Epoch [90/150], Loss: 2299.1359\n",
            "Epoch [100/150], Loss: 2278.8490\n",
            "Epoch [110/150], Loss: 3416.5215\n",
            "Epoch [120/150], Loss: 4250.0521\n",
            "Epoch [130/150], Loss: 2697.0474\n",
            "Epoch [140/150], Loss: 2135.2065\n",
            "Epoch [150/150], Loss: 1884.4691\n",
            "Fold 4, RMSE: 44.136680603027344\n",
            "Epoch [10/150], Loss: 13166.3140\n",
            "Epoch [20/150], Loss: 9560.5082\n",
            "Epoch [30/150], Loss: 3976.5346\n",
            "Epoch [40/150], Loss: 4182.6150\n",
            "Epoch [50/150], Loss: 4936.4237\n",
            "Epoch [60/150], Loss: 3962.9453\n",
            "Epoch [70/150], Loss: 3586.5642\n",
            "Epoch [80/150], Loss: 2610.9035\n",
            "Epoch [90/150], Loss: 3651.7514\n",
            "Epoch [100/150], Loss: 2309.9012\n",
            "Epoch [110/150], Loss: 1952.1529\n",
            "Epoch [120/150], Loss: 2287.6462\n",
            "Epoch [130/150], Loss: 1921.6674\n",
            "Epoch [140/150], Loss: 2422.3538\n",
            "Epoch [150/150], Loss: 3643.8284\n",
            "Fold 5, RMSE: 45.585968017578125\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 59.76980285644531\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16326.4233\n",
            "Epoch [20/100], Loss: 8600.3484\n",
            "Epoch [30/100], Loss: 5915.7827\n",
            "Epoch [40/100], Loss: 4223.6360\n",
            "Epoch [50/100], Loss: 3290.2365\n",
            "Epoch [60/100], Loss: 4262.3537\n",
            "Epoch [70/100], Loss: 2031.8872\n",
            "Epoch [80/100], Loss: 2151.1962\n",
            "Epoch [90/100], Loss: 1221.2389\n",
            "Epoch [100/100], Loss: 1752.2953\n",
            "Fold 1, RMSE: 58.10365676879883\n",
            "Epoch [10/100], Loss: 8232.6301\n",
            "Epoch [20/100], Loss: 7564.3944\n",
            "Epoch [30/100], Loss: 4572.7330\n",
            "Epoch [40/100], Loss: 1685.8293\n",
            "Epoch [50/100], Loss: 3214.6445\n",
            "Epoch [60/100], Loss: 3230.0108\n",
            "Epoch [70/100], Loss: 1731.0261\n",
            "Epoch [80/100], Loss: 3449.5193\n",
            "Epoch [90/100], Loss: 2124.6676\n",
            "Epoch [100/100], Loss: 2205.4088\n",
            "Fold 2, RMSE: 67.31101989746094\n",
            "Epoch [10/100], Loss: 11294.9344\n",
            "Epoch [20/100], Loss: 4439.4690\n",
            "Epoch [30/100], Loss: 4853.6021\n",
            "Epoch [40/100], Loss: 3831.9078\n",
            "Epoch [50/100], Loss: 1240.5331\n",
            "Epoch [60/100], Loss: 1723.9785\n",
            "Epoch [70/100], Loss: 2785.3296\n",
            "Epoch [80/100], Loss: 2100.2233\n",
            "Epoch [90/100], Loss: 1144.3986\n",
            "Epoch [100/100], Loss: 1488.0472\n",
            "Fold 3, RMSE: 89.82537078857422\n",
            "Epoch [10/100], Loss: 10760.7441\n",
            "Epoch [20/100], Loss: 8079.9323\n",
            "Epoch [30/100], Loss: 7169.1544\n",
            "Epoch [40/100], Loss: 3049.6242\n",
            "Epoch [50/100], Loss: 2104.0681\n",
            "Epoch [60/100], Loss: 2336.7579\n",
            "Epoch [70/100], Loss: 1714.1566\n",
            "Epoch [80/100], Loss: 3726.9779\n",
            "Epoch [90/100], Loss: 1435.2389\n",
            "Epoch [100/100], Loss: 3764.2070\n",
            "Fold 4, RMSE: 45.898765563964844\n",
            "Epoch [10/100], Loss: 12600.3862\n",
            "Epoch [20/100], Loss: 14188.5059\n",
            "Epoch [30/100], Loss: 3620.8310\n",
            "Epoch [40/100], Loss: 3813.3492\n",
            "Epoch [50/100], Loss: 2515.5842\n",
            "Epoch [60/100], Loss: 2649.4428\n",
            "Epoch [70/100], Loss: 4893.0708\n",
            "Epoch [80/100], Loss: 2011.4630\n",
            "Epoch [90/100], Loss: 2926.8651\n",
            "Epoch [100/100], Loss: 1821.1383\n",
            "Fold 5, RMSE: 44.57670593261719\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 61.143103790283206\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 9840.5465\n",
            "Epoch [20/150], Loss: 7071.2127\n",
            "Epoch [30/150], Loss: 4395.0964\n",
            "Epoch [40/150], Loss: 3919.5406\n",
            "Epoch [50/150], Loss: 3158.3665\n",
            "Epoch [60/150], Loss: 2063.3447\n",
            "Epoch [70/150], Loss: 2876.7422\n",
            "Epoch [80/150], Loss: 1859.8631\n",
            "Epoch [90/150], Loss: 2083.8525\n",
            "Epoch [100/150], Loss: 1092.1930\n",
            "Epoch [110/150], Loss: 1365.6772\n",
            "Epoch [120/150], Loss: 1824.9605\n",
            "Epoch [130/150], Loss: 3161.4142\n",
            "Epoch [140/150], Loss: 1179.5544\n",
            "Epoch [150/150], Loss: 1181.0400\n",
            "Fold 1, RMSE: 55.665008544921875\n",
            "Epoch [10/150], Loss: 10073.7231\n",
            "Epoch [20/150], Loss: 8862.9362\n",
            "Epoch [30/150], Loss: 6473.3487\n",
            "Epoch [40/150], Loss: 3825.7623\n",
            "Epoch [50/150], Loss: 1855.7287\n",
            "Epoch [60/150], Loss: 1915.7143\n",
            "Epoch [70/150], Loss: 1706.5005\n",
            "Epoch [80/150], Loss: 3296.5742\n",
            "Epoch [90/150], Loss: 4110.7429\n",
            "Epoch [100/150], Loss: 4477.5401\n",
            "Epoch [110/150], Loss: 3055.0557\n",
            "Epoch [120/150], Loss: 2285.3977\n",
            "Epoch [130/150], Loss: 1756.8709\n",
            "Epoch [140/150], Loss: 1319.3116\n",
            "Epoch [150/150], Loss: 1961.7108\n",
            "Fold 2, RMSE: 65.52823638916016\n",
            "Epoch [10/150], Loss: 7536.6537\n",
            "Epoch [20/150], Loss: 5298.4719\n",
            "Epoch [30/150], Loss: 2922.4607\n",
            "Epoch [40/150], Loss: 1679.5470\n",
            "Epoch [50/150], Loss: 1772.4478\n",
            "Epoch [60/150], Loss: 2509.6500\n",
            "Epoch [70/150], Loss: 1735.8084\n",
            "Epoch [80/150], Loss: 2964.7983\n",
            "Epoch [90/150], Loss: 4295.0482\n",
            "Epoch [100/150], Loss: 2053.3245\n",
            "Epoch [110/150], Loss: 2634.1549\n",
            "Epoch [120/150], Loss: 1756.8227\n",
            "Epoch [130/150], Loss: 1953.5855\n",
            "Epoch [140/150], Loss: 1548.6118\n",
            "Epoch [150/150], Loss: 2335.8285\n",
            "Fold 3, RMSE: 91.69096374511719\n",
            "Epoch [10/150], Loss: 12501.3372\n",
            "Epoch [20/150], Loss: 8129.8113\n",
            "Epoch [30/150], Loss: 6225.5840\n",
            "Epoch [40/150], Loss: 3292.5951\n",
            "Epoch [50/150], Loss: 4009.5202\n",
            "Epoch [60/150], Loss: 3225.6857\n",
            "Epoch [70/150], Loss: 3872.2625\n",
            "Epoch [80/150], Loss: 4625.5358\n",
            "Epoch [90/150], Loss: 2223.9328\n",
            "Epoch [100/150], Loss: 1767.0308\n",
            "Epoch [110/150], Loss: 4544.9056\n",
            "Epoch [120/150], Loss: 665.4696\n",
            "Epoch [130/150], Loss: 1118.1503\n",
            "Epoch [140/150], Loss: 3112.5783\n",
            "Epoch [150/150], Loss: 1110.4808\n",
            "Fold 4, RMSE: 41.2122802734375\n",
            "Epoch [10/150], Loss: 11120.8239\n",
            "Epoch [20/150], Loss: 7680.0109\n",
            "Epoch [30/150], Loss: 3089.9897\n",
            "Epoch [40/150], Loss: 2020.4933\n",
            "Epoch [50/150], Loss: 2525.0499\n",
            "Epoch [60/150], Loss: 2070.2134\n",
            "Epoch [70/150], Loss: 2618.3055\n",
            "Epoch [80/150], Loss: 2599.5769\n",
            "Epoch [90/150], Loss: 1445.2556\n",
            "Epoch [100/150], Loss: 1635.2615\n",
            "Epoch [110/150], Loss: 2477.7377\n",
            "Epoch [120/150], Loss: 2042.6382\n",
            "Epoch [130/150], Loss: 1523.6707\n",
            "Epoch [140/150], Loss: 1238.2903\n",
            "Epoch [150/150], Loss: 1263.4812\n",
            "Fold 5, RMSE: 50.41178512573242\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 60.90165481567383\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 10075.2811\n",
            "Epoch [20/100], Loss: 6530.6671\n",
            "Epoch [30/100], Loss: 3408.8139\n",
            "Epoch [40/100], Loss: 3156.2410\n",
            "Epoch [50/100], Loss: 3687.2771\n",
            "Epoch [60/100], Loss: 2596.2256\n",
            "Epoch [70/100], Loss: 1127.8435\n",
            "Epoch [80/100], Loss: 1549.7914\n",
            "Epoch [90/100], Loss: 1604.2161\n",
            "Epoch [100/100], Loss: 1587.3376\n",
            "Fold 1, RMSE: 59.49959945678711\n",
            "Epoch [10/100], Loss: 19204.8513\n",
            "Epoch [20/100], Loss: 8282.5961\n",
            "Epoch [30/100], Loss: 7127.7535\n",
            "Epoch [40/100], Loss: 5930.1541\n",
            "Epoch [50/100], Loss: 3329.6416\n",
            "Epoch [60/100], Loss: 2553.5000\n",
            "Epoch [70/100], Loss: 3145.6190\n",
            "Epoch [80/100], Loss: 6739.3003\n",
            "Epoch [90/100], Loss: 5433.5192\n",
            "Epoch [100/100], Loss: 7780.8139\n",
            "Fold 2, RMSE: 66.73128509521484\n",
            "Epoch [10/100], Loss: 7269.9153\n",
            "Epoch [20/100], Loss: 10542.6713\n",
            "Epoch [30/100], Loss: 5140.9778\n",
            "Epoch [40/100], Loss: 5268.7150\n",
            "Epoch [50/100], Loss: 9825.1215\n",
            "Epoch [60/100], Loss: 2827.6382\n",
            "Epoch [70/100], Loss: 4345.3536\n",
            "Epoch [80/100], Loss: 3053.8384\n",
            "Epoch [90/100], Loss: 1734.7230\n",
            "Epoch [100/100], Loss: 6651.1910\n",
            "Fold 3, RMSE: 94.66959381103516\n",
            "Epoch [10/100], Loss: 21406.6108\n",
            "Epoch [20/100], Loss: 13498.1189\n",
            "Epoch [30/100], Loss: 14163.8655\n",
            "Epoch [40/100], Loss: 6705.8400\n",
            "Epoch [50/100], Loss: 2107.1147\n",
            "Epoch [60/100], Loss: 3042.6317\n",
            "Epoch [70/100], Loss: 1729.8851\n",
            "Epoch [80/100], Loss: 3493.0068\n",
            "Epoch [90/100], Loss: 3467.0838\n",
            "Epoch [100/100], Loss: 3404.3334\n",
            "Fold 4, RMSE: 38.92407989501953\n",
            "Epoch [10/100], Loss: 9347.7571\n",
            "Epoch [20/100], Loss: 8432.9302\n",
            "Epoch [30/100], Loss: 6214.7735\n",
            "Epoch [40/100], Loss: 5038.5029\n",
            "Epoch [50/100], Loss: 1820.5625\n",
            "Epoch [60/100], Loss: 2783.3359\n",
            "Epoch [70/100], Loss: 2193.7906\n",
            "Epoch [80/100], Loss: 3424.4305\n",
            "Epoch [90/100], Loss: 2741.8755\n",
            "Epoch [100/100], Loss: 3889.9163\n",
            "Fold 5, RMSE: 46.19368362426758\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 61.20364837646484\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16091.0439\n",
            "Epoch [20/150], Loss: 10180.3667\n",
            "Epoch [30/150], Loss: 9033.7459\n",
            "Epoch [40/150], Loss: 7913.7952\n",
            "Epoch [50/150], Loss: 3728.1984\n",
            "Epoch [60/150], Loss: 2481.5926\n",
            "Epoch [70/150], Loss: 5037.1255\n",
            "Epoch [80/150], Loss: 2925.9347\n",
            "Epoch [90/150], Loss: 3602.9332\n",
            "Epoch [100/150], Loss: 5054.7167\n",
            "Epoch [110/150], Loss: 4850.5948\n",
            "Epoch [120/150], Loss: 4977.7331\n",
            "Epoch [130/150], Loss: 6378.0906\n",
            "Epoch [140/150], Loss: 8872.7457\n",
            "Epoch [150/150], Loss: 2065.2888\n",
            "Fold 1, RMSE: 60.18851852416992\n",
            "Epoch [10/150], Loss: 12210.7949\n",
            "Epoch [20/150], Loss: 10809.1830\n",
            "Epoch [30/150], Loss: 10592.8484\n",
            "Epoch [40/150], Loss: 11490.8582\n",
            "Epoch [50/150], Loss: 5064.4529\n",
            "Epoch [60/150], Loss: 3252.7183\n",
            "Epoch [70/150], Loss: 3401.5706\n",
            "Epoch [80/150], Loss: 2289.6272\n",
            "Epoch [90/150], Loss: 1536.9176\n",
            "Epoch [100/150], Loss: 2872.1672\n",
            "Epoch [110/150], Loss: 1817.2539\n",
            "Epoch [120/150], Loss: 2006.0444\n",
            "Epoch [130/150], Loss: 2126.2904\n",
            "Epoch [140/150], Loss: 1105.5129\n",
            "Epoch [150/150], Loss: 2559.2706\n",
            "Fold 2, RMSE: 66.99191284179688\n",
            "Epoch [10/150], Loss: 13019.9724\n",
            "Epoch [20/150], Loss: 8312.5308\n",
            "Epoch [30/150], Loss: 5104.6292\n",
            "Epoch [40/150], Loss: 6737.7562\n",
            "Epoch [50/150], Loss: 2797.1602\n",
            "Epoch [60/150], Loss: 4795.7073\n",
            "Epoch [70/150], Loss: 2237.2399\n",
            "Epoch [80/150], Loss: 3252.5350\n",
            "Epoch [90/150], Loss: 3240.7660\n",
            "Epoch [100/150], Loss: 2033.4438\n",
            "Epoch [110/150], Loss: 5718.2640\n",
            "Epoch [120/150], Loss: 2123.3173\n",
            "Epoch [130/150], Loss: 2334.9124\n",
            "Epoch [140/150], Loss: 4373.0463\n",
            "Epoch [150/150], Loss: 1851.8638\n",
            "Fold 3, RMSE: 89.10282135009766\n",
            "Epoch [10/150], Loss: 12100.1863\n",
            "Epoch [20/150], Loss: 8260.2957\n",
            "Epoch [30/150], Loss: 3771.0531\n",
            "Epoch [40/150], Loss: 5953.2576\n",
            "Epoch [50/150], Loss: 1817.3618\n",
            "Epoch [60/150], Loss: 2557.7437\n",
            "Epoch [70/150], Loss: 1842.2458\n",
            "Epoch [80/150], Loss: 1139.3338\n",
            "Epoch [90/150], Loss: 1832.6450\n",
            "Epoch [100/150], Loss: 3819.2196\n",
            "Epoch [110/150], Loss: 1188.3883\n",
            "Epoch [120/150], Loss: 3140.8049\n",
            "Epoch [130/150], Loss: 1950.0187\n",
            "Epoch [140/150], Loss: 4559.1980\n",
            "Epoch [150/150], Loss: 2014.7988\n",
            "Fold 4, RMSE: 44.69891357421875\n",
            "Epoch [10/150], Loss: 12484.0171\n",
            "Epoch [20/150], Loss: 9383.2529\n",
            "Epoch [30/150], Loss: 10646.2067\n",
            "Epoch [40/150], Loss: 6808.6783\n",
            "Epoch [50/150], Loss: 3861.3283\n",
            "Epoch [60/150], Loss: 4742.3550\n",
            "Epoch [70/150], Loss: 5666.1107\n",
            "Epoch [80/150], Loss: 2410.5865\n",
            "Epoch [90/150], Loss: 3915.6655\n",
            "Epoch [100/150], Loss: 2928.8739\n",
            "Epoch [110/150], Loss: 5148.5986\n",
            "Epoch [120/150], Loss: 1979.0353\n",
            "Epoch [130/150], Loss: 1173.4817\n",
            "Epoch [140/150], Loss: 2770.0629\n",
            "Epoch [150/150], Loss: 2141.3803\n",
            "Fold 5, RMSE: 44.01205062866211\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 60.998843383789065\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 11150.2244\n",
            "Epoch [20/100], Loss: 6486.8192\n",
            "Epoch [30/100], Loss: 3918.5532\n",
            "Epoch [40/100], Loss: 2280.8269\n",
            "Epoch [50/100], Loss: 2143.9333\n",
            "Epoch [60/100], Loss: 2761.3763\n",
            "Epoch [70/100], Loss: 3536.2415\n",
            "Epoch [80/100], Loss: 2645.7419\n",
            "Epoch [90/100], Loss: 4952.4854\n",
            "Epoch [100/100], Loss: 2536.9688\n",
            "Fold 1, RMSE: 59.96074295043945\n",
            "Epoch [10/100], Loss: 9661.7847\n",
            "Epoch [20/100], Loss: 5724.5990\n",
            "Epoch [30/100], Loss: 4085.2378\n",
            "Epoch [40/100], Loss: 1985.1757\n",
            "Epoch [50/100], Loss: 3150.3846\n",
            "Epoch [60/100], Loss: 4828.1630\n",
            "Epoch [70/100], Loss: 2540.3315\n",
            "Epoch [80/100], Loss: 5345.6924\n",
            "Epoch [90/100], Loss: 2083.7367\n",
            "Epoch [100/100], Loss: 2833.3665\n",
            "Fold 2, RMSE: 65.04336547851562\n",
            "Epoch [10/100], Loss: 6404.8214\n",
            "Epoch [20/100], Loss: 4018.8627\n",
            "Epoch [30/100], Loss: 2992.9338\n",
            "Epoch [40/100], Loss: 2052.6797\n",
            "Epoch [50/100], Loss: 2415.6143\n",
            "Epoch [60/100], Loss: 1834.4474\n",
            "Epoch [70/100], Loss: 6583.7152\n",
            "Epoch [80/100], Loss: 4705.9222\n",
            "Epoch [90/100], Loss: 3301.4174\n",
            "Epoch [100/100], Loss: 3659.2482\n",
            "Fold 3, RMSE: 92.13031768798828\n",
            "Epoch [10/100], Loss: 15215.8818\n",
            "Epoch [20/100], Loss: 8515.7172\n",
            "Epoch [30/100], Loss: 5342.1890\n",
            "Epoch [40/100], Loss: 2315.5929\n",
            "Epoch [50/100], Loss: 2579.3220\n",
            "Epoch [60/100], Loss: 3623.6692\n",
            "Epoch [70/100], Loss: 2704.3321\n",
            "Epoch [80/100], Loss: 5516.4929\n",
            "Epoch [90/100], Loss: 2007.0124\n",
            "Epoch [100/100], Loss: 1541.7239\n",
            "Fold 4, RMSE: 44.66087341308594\n",
            "Epoch [10/100], Loss: 10503.2893\n",
            "Epoch [20/100], Loss: 8757.3108\n",
            "Epoch [30/100], Loss: 3790.8174\n",
            "Epoch [40/100], Loss: 3017.9266\n",
            "Epoch [50/100], Loss: 5479.3085\n",
            "Epoch [60/100], Loss: 3732.2051\n",
            "Epoch [70/100], Loss: 1977.0644\n",
            "Epoch [80/100], Loss: 2731.7554\n",
            "Epoch [90/100], Loss: 1672.8645\n",
            "Epoch [100/100], Loss: 848.9448\n",
            "Fold 5, RMSE: 44.94061279296875\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 61.34718246459961\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 12743.9624\n",
            "Epoch [20/150], Loss: 9419.4551\n",
            "Epoch [30/150], Loss: 7743.1316\n",
            "Epoch [40/150], Loss: 5188.4677\n",
            "Epoch [50/150], Loss: 4906.6942\n",
            "Epoch [60/150], Loss: 5420.7632\n",
            "Epoch [70/150], Loss: 3117.7254\n",
            "Epoch [80/150], Loss: 1573.1606\n",
            "Epoch [90/150], Loss: 1127.2212\n",
            "Epoch [100/150], Loss: 2807.6912\n",
            "Epoch [110/150], Loss: 1439.7965\n",
            "Epoch [120/150], Loss: 1993.6007\n",
            "Epoch [130/150], Loss: 667.8540\n",
            "Epoch [140/150], Loss: 2467.4496\n",
            "Epoch [150/150], Loss: 499.8056\n",
            "Fold 1, RMSE: 58.537376403808594\n",
            "Epoch [10/150], Loss: 10139.2368\n",
            "Epoch [20/150], Loss: 6795.6258\n",
            "Epoch [30/150], Loss: 4336.9752\n",
            "Epoch [40/150], Loss: 3728.7391\n",
            "Epoch [50/150], Loss: 2232.6261\n",
            "Epoch [60/150], Loss: 3660.5210\n",
            "Epoch [70/150], Loss: 1698.6243\n",
            "Epoch [80/150], Loss: 1273.5716\n",
            "Epoch [90/150], Loss: 1586.2176\n",
            "Epoch [100/150], Loss: 1323.2726\n",
            "Epoch [110/150], Loss: 2498.5081\n",
            "Epoch [120/150], Loss: 4852.3976\n",
            "Epoch [130/150], Loss: 1059.9068\n",
            "Epoch [140/150], Loss: 842.8467\n",
            "Epoch [150/150], Loss: 1655.9051\n",
            "Fold 2, RMSE: 67.19449615478516\n",
            "Epoch [10/150], Loss: 6197.2265\n",
            "Epoch [20/150], Loss: 3908.4376\n",
            "Epoch [30/150], Loss: 5860.6932\n",
            "Epoch [40/150], Loss: 3447.2800\n",
            "Epoch [50/150], Loss: 5060.1571\n",
            "Epoch [60/150], Loss: 1747.6310\n",
            "Epoch [70/150], Loss: 1696.6692\n",
            "Epoch [80/150], Loss: 1296.3211\n",
            "Epoch [90/150], Loss: 4605.8199\n",
            "Epoch [100/150], Loss: 2190.0446\n",
            "Epoch [110/150], Loss: 906.2328\n",
            "Epoch [120/150], Loss: 2357.6975\n",
            "Epoch [130/150], Loss: 1004.1916\n",
            "Epoch [140/150], Loss: 1237.2766\n",
            "Epoch [150/150], Loss: 3225.4522\n",
            "Fold 3, RMSE: 91.44597625732422\n",
            "Epoch [10/150], Loss: 15657.3833\n",
            "Epoch [20/150], Loss: 13086.1145\n",
            "Epoch [30/150], Loss: 5655.3387\n",
            "Epoch [40/150], Loss: 7720.7632\n",
            "Epoch [50/150], Loss: 3305.3809\n",
            "Epoch [60/150], Loss: 3642.8439\n",
            "Epoch [70/150], Loss: 1616.7706\n",
            "Epoch [80/150], Loss: 4255.3453\n",
            "Epoch [90/150], Loss: 3448.1875\n",
            "Epoch [100/150], Loss: 1195.6917\n",
            "Epoch [110/150], Loss: 1594.5575\n",
            "Epoch [120/150], Loss: 2622.4568\n",
            "Epoch [130/150], Loss: 1067.4584\n",
            "Epoch [140/150], Loss: 1994.2973\n",
            "Epoch [150/150], Loss: 960.1184\n",
            "Fold 4, RMSE: 40.75679397583008\n",
            "Epoch [10/150], Loss: 14369.2537\n",
            "Epoch [20/150], Loss: 6514.0612\n",
            "Epoch [30/150], Loss: 4707.7477\n",
            "Epoch [40/150], Loss: 3557.9320\n",
            "Epoch [50/150], Loss: 3796.7358\n",
            "Epoch [60/150], Loss: 3241.9579\n",
            "Epoch [70/150], Loss: 3686.9769\n",
            "Epoch [80/150], Loss: 1710.6660\n",
            "Epoch [90/150], Loss: 3158.9142\n",
            "Epoch [100/150], Loss: 1535.8643\n",
            "Epoch [110/150], Loss: 2216.3809\n",
            "Epoch [120/150], Loss: 972.2802\n",
            "Epoch [130/150], Loss: 1324.1413\n",
            "Epoch [140/150], Loss: 1341.7417\n",
            "Epoch [150/150], Loss: 1741.3947\n",
            "Fold 5, RMSE: 45.334537506103516\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 60.65383605957031\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14361.3826\n",
            "Epoch [20/100], Loss: 8382.2000\n",
            "Epoch [30/100], Loss: 4942.0807\n",
            "Epoch [40/100], Loss: 4234.1771\n",
            "Epoch [50/100], Loss: 4019.7863\n",
            "Epoch [60/100], Loss: 3838.7894\n",
            "Epoch [70/100], Loss: 6051.9271\n",
            "Epoch [80/100], Loss: 2747.4297\n",
            "Epoch [90/100], Loss: 3049.6779\n",
            "Epoch [100/100], Loss: 2690.5178\n",
            "Fold 1, RMSE: 62.10852813720703\n",
            "Epoch [10/100], Loss: 11348.0776\n",
            "Epoch [20/100], Loss: 10042.6157\n",
            "Epoch [30/100], Loss: 6479.5560\n",
            "Epoch [40/100], Loss: 6022.6970\n",
            "Epoch [50/100], Loss: 2285.8893\n",
            "Epoch [60/100], Loss: 3606.4596\n",
            "Epoch [70/100], Loss: 4424.6190\n",
            "Epoch [80/100], Loss: 4089.0216\n",
            "Epoch [90/100], Loss: 2987.6699\n",
            "Epoch [100/100], Loss: 4003.7136\n",
            "Fold 2, RMSE: 75.0772705078125\n",
            "Epoch [10/100], Loss: 7110.7179\n",
            "Epoch [20/100], Loss: 4971.8962\n",
            "Epoch [30/100], Loss: 3859.6282\n",
            "Epoch [40/100], Loss: 3303.3252\n",
            "Epoch [50/100], Loss: 2386.0009\n",
            "Epoch [60/100], Loss: 1520.3609\n",
            "Epoch [70/100], Loss: 1429.5646\n",
            "Epoch [80/100], Loss: 979.8613\n",
            "Epoch [90/100], Loss: 1118.4233\n",
            "Epoch [100/100], Loss: 983.7535\n",
            "Fold 3, RMSE: 94.81999969482422\n",
            "Epoch [10/100], Loss: 15985.0839\n",
            "Epoch [20/100], Loss: 9021.4645\n",
            "Epoch [30/100], Loss: 10081.6266\n",
            "Epoch [40/100], Loss: 5887.9169\n",
            "Epoch [50/100], Loss: 4522.9338\n",
            "Epoch [60/100], Loss: 4987.4790\n",
            "Epoch [70/100], Loss: 1637.6854\n",
            "Epoch [80/100], Loss: 4409.9956\n",
            "Epoch [90/100], Loss: 3691.3191\n",
            "Epoch [100/100], Loss: 2719.5667\n",
            "Fold 4, RMSE: 37.05778121948242\n",
            "Epoch [10/100], Loss: 11101.1995\n",
            "Epoch [20/100], Loss: 10890.9200\n",
            "Epoch [30/100], Loss: 8189.5306\n",
            "Epoch [40/100], Loss: 2704.7337\n",
            "Epoch [50/100], Loss: 4265.1851\n",
            "Epoch [60/100], Loss: 1730.8431\n",
            "Epoch [70/100], Loss: 2559.2696\n",
            "Epoch [80/100], Loss: 2352.6438\n",
            "Epoch [90/100], Loss: 3492.7797\n",
            "Epoch [100/100], Loss: 2206.3977\n",
            "Fold 5, RMSE: 45.97554016113281\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 63.0078239440918\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 15017.1567\n",
            "Epoch [20/150], Loss: 6997.5348\n",
            "Epoch [30/150], Loss: 6820.7163\n",
            "Epoch [40/150], Loss: 5310.7246\n",
            "Epoch [50/150], Loss: 5038.2075\n",
            "Epoch [60/150], Loss: 4447.8368\n",
            "Epoch [70/150], Loss: 5284.7590\n",
            "Epoch [80/150], Loss: 1915.8518\n",
            "Epoch [90/150], Loss: 2366.5241\n",
            "Epoch [100/150], Loss: 1195.1497\n",
            "Epoch [110/150], Loss: 769.7785\n",
            "Epoch [120/150], Loss: 863.4351\n",
            "Epoch [130/150], Loss: 1512.0750\n",
            "Epoch [140/150], Loss: 3378.7877\n",
            "Epoch [150/150], Loss: 1491.9241\n",
            "Fold 1, RMSE: 61.47281265258789\n",
            "Epoch [10/150], Loss: 15173.3613\n",
            "Epoch [20/150], Loss: 5265.4909\n",
            "Epoch [30/150], Loss: 6229.5393\n",
            "Epoch [40/150], Loss: 2933.5651\n",
            "Epoch [50/150], Loss: 1595.7541\n",
            "Epoch [60/150], Loss: 2857.0183\n",
            "Epoch [70/150], Loss: 1725.6380\n",
            "Epoch [80/150], Loss: 2219.5768\n",
            "Epoch [90/150], Loss: 1288.3344\n",
            "Epoch [100/150], Loss: 2556.8175\n",
            "Epoch [110/150], Loss: 1841.8981\n",
            "Epoch [120/150], Loss: 2959.3102\n",
            "Epoch [130/150], Loss: 2822.2210\n",
            "Epoch [140/150], Loss: 2337.9319\n",
            "Epoch [150/150], Loss: 720.9896\n",
            "Fold 2, RMSE: 63.31076431274414\n",
            "Epoch [10/150], Loss: 8936.5717\n",
            "Epoch [20/150], Loss: 7099.0413\n",
            "Epoch [30/150], Loss: 7268.0348\n",
            "Epoch [40/150], Loss: 5027.5552\n",
            "Epoch [50/150], Loss: 3155.5256\n",
            "Epoch [60/150], Loss: 1407.2916\n",
            "Epoch [70/150], Loss: 1607.3090\n",
            "Epoch [80/150], Loss: 1825.9549\n",
            "Epoch [90/150], Loss: 1454.7372\n",
            "Epoch [100/150], Loss: 1713.5890\n",
            "Epoch [110/150], Loss: 1441.9462\n",
            "Epoch [120/150], Loss: 946.7736\n",
            "Epoch [130/150], Loss: 1607.0971\n",
            "Epoch [140/150], Loss: 1555.5901\n",
            "Epoch [150/150], Loss: 721.8111\n",
            "Fold 3, RMSE: 92.69171142578125\n",
            "Epoch [10/150], Loss: 15409.4795\n",
            "Epoch [20/150], Loss: 9830.3248\n",
            "Epoch [30/150], Loss: 4494.9122\n",
            "Epoch [40/150], Loss: 6999.9082\n",
            "Epoch [50/150], Loss: 4022.6447\n",
            "Epoch [60/150], Loss: 3345.6326\n",
            "Epoch [70/150], Loss: 5881.0788\n",
            "Epoch [80/150], Loss: 3270.4725\n",
            "Epoch [90/150], Loss: 1597.8453\n",
            "Epoch [100/150], Loss: 3048.0047\n",
            "Epoch [110/150], Loss: 1447.4906\n",
            "Epoch [120/150], Loss: 2109.6559\n",
            "Epoch [130/150], Loss: 3017.0131\n",
            "Epoch [140/150], Loss: 901.2813\n",
            "Epoch [150/150], Loss: 1995.9697\n",
            "Fold 4, RMSE: 40.923240661621094\n",
            "Epoch [10/150], Loss: 16058.7803\n",
            "Epoch [20/150], Loss: 10202.1338\n",
            "Epoch [30/150], Loss: 7676.2559\n",
            "Epoch [40/150], Loss: 4918.4013\n",
            "Epoch [50/150], Loss: 4678.5647\n",
            "Epoch [60/150], Loss: 4165.7764\n",
            "Epoch [70/150], Loss: 5940.7558\n",
            "Epoch [80/150], Loss: 2567.4492\n",
            "Epoch [90/150], Loss: 4868.6166\n",
            "Epoch [100/150], Loss: 2469.4211\n",
            "Epoch [110/150], Loss: 3857.3942\n",
            "Epoch [120/150], Loss: 1602.2047\n",
            "Epoch [130/150], Loss: 1970.6647\n",
            "Epoch [140/150], Loss: 1593.9481\n",
            "Epoch [150/150], Loss: 1578.5115\n",
            "Fold 5, RMSE: 56.922550201416016\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 63.06421585083008\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18707.6827\n",
            "Epoch [20/100], Loss: 6134.1037\n",
            "Epoch [30/100], Loss: 5872.4858\n",
            "Epoch [40/100], Loss: 4065.8033\n",
            "Epoch [50/100], Loss: 4168.3117\n",
            "Epoch [60/100], Loss: 2527.6165\n",
            "Epoch [70/100], Loss: 3512.9134\n",
            "Epoch [80/100], Loss: 2607.3995\n",
            "Epoch [90/100], Loss: 1773.4023\n",
            "Epoch [100/100], Loss: 3291.2417\n",
            "Fold 1, RMSE: 62.19602966308594\n",
            "Epoch [10/100], Loss: 12124.4612\n",
            "Epoch [20/100], Loss: 8667.2006\n",
            "Epoch [30/100], Loss: 4727.2445\n",
            "Epoch [40/100], Loss: 6128.4824\n",
            "Epoch [50/100], Loss: 4500.6079\n",
            "Epoch [60/100], Loss: 2709.8540\n",
            "Epoch [70/100], Loss: 1941.8324\n",
            "Epoch [80/100], Loss: 2205.8405\n",
            "Epoch [90/100], Loss: 2428.5037\n",
            "Epoch [100/100], Loss: 1436.4108\n",
            "Fold 2, RMSE: 63.57196044921875\n",
            "Epoch [10/100], Loss: 7789.0801\n",
            "Epoch [20/100], Loss: 4764.7291\n",
            "Epoch [30/100], Loss: 3182.3278\n",
            "Epoch [40/100], Loss: 3882.5488\n",
            "Epoch [50/100], Loss: 3889.7045\n",
            "Epoch [60/100], Loss: 1468.9849\n",
            "Epoch [70/100], Loss: 3863.2036\n",
            "Epoch [80/100], Loss: 1328.4447\n",
            "Epoch [90/100], Loss: 1500.6782\n",
            "Epoch [100/100], Loss: 1469.1397\n",
            "Fold 3, RMSE: 93.9410171508789\n",
            "Epoch [10/100], Loss: 13552.4832\n",
            "Epoch [20/100], Loss: 11018.3679\n",
            "Epoch [30/100], Loss: 7213.0784\n",
            "Epoch [40/100], Loss: 5446.0963\n",
            "Epoch [50/100], Loss: 2771.0422\n",
            "Epoch [60/100], Loss: 1947.5841\n",
            "Epoch [70/100], Loss: 5108.8998\n",
            "Epoch [80/100], Loss: 1374.8890\n",
            "Epoch [90/100], Loss: 3653.1443\n",
            "Epoch [100/100], Loss: 1114.7962\n",
            "Fold 4, RMSE: 40.62751007080078\n",
            "Epoch [10/100], Loss: 18495.6104\n",
            "Epoch [20/100], Loss: 10857.6426\n",
            "Epoch [30/100], Loss: 5128.3851\n",
            "Epoch [40/100], Loss: 5478.9558\n",
            "Epoch [50/100], Loss: 2449.3335\n",
            "Epoch [60/100], Loss: 3265.7955\n",
            "Epoch [70/100], Loss: 3026.5413\n",
            "Epoch [80/100], Loss: 1999.2999\n",
            "Epoch [90/100], Loss: 2392.7843\n",
            "Epoch [100/100], Loss: 1486.0778\n",
            "Fold 5, RMSE: 46.273929595947266\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 61.32208938598633\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13306.6974\n",
            "Epoch [20/150], Loss: 7768.3121\n",
            "Epoch [30/150], Loss: 5443.5204\n",
            "Epoch [40/150], Loss: 2002.8386\n",
            "Epoch [50/150], Loss: 3063.7547\n",
            "Epoch [60/150], Loss: 2422.8349\n",
            "Epoch [70/150], Loss: 2993.3629\n",
            "Epoch [80/150], Loss: 2145.8212\n",
            "Epoch [90/150], Loss: 2286.6506\n",
            "Epoch [100/150], Loss: 1041.6704\n",
            "Epoch [110/150], Loss: 1276.4644\n",
            "Epoch [120/150], Loss: 1328.7388\n",
            "Epoch [130/150], Loss: 1682.8665\n",
            "Epoch [140/150], Loss: 2816.1040\n",
            "Epoch [150/150], Loss: 1799.7050\n",
            "Fold 1, RMSE: 55.55497360229492\n",
            "Epoch [10/150], Loss: 12411.6741\n",
            "Epoch [20/150], Loss: 9330.0969\n",
            "Epoch [30/150], Loss: 7211.7596\n",
            "Epoch [40/150], Loss: 4616.0519\n",
            "Epoch [50/150], Loss: 3072.4553\n",
            "Epoch [60/150], Loss: 1611.4083\n",
            "Epoch [70/150], Loss: 1731.8593\n",
            "Epoch [80/150], Loss: 3034.7484\n",
            "Epoch [90/150], Loss: 3535.8143\n",
            "Epoch [100/150], Loss: 3569.9794\n",
            "Epoch [110/150], Loss: 2479.4510\n",
            "Epoch [120/150], Loss: 3132.5765\n",
            "Epoch [130/150], Loss: 1760.3384\n",
            "Epoch [140/150], Loss: 1257.3442\n",
            "Epoch [150/150], Loss: 803.8524\n",
            "Fold 2, RMSE: 62.10441589355469\n",
            "Epoch [10/150], Loss: 11493.5778\n",
            "Epoch [20/150], Loss: 5730.6578\n",
            "Epoch [30/150], Loss: 4137.7271\n",
            "Epoch [40/150], Loss: 2147.9879\n",
            "Epoch [50/150], Loss: 2558.4385\n",
            "Epoch [60/150], Loss: 2908.1345\n",
            "Epoch [70/150], Loss: 1496.8470\n",
            "Epoch [80/150], Loss: 7164.2182\n",
            "Epoch [90/150], Loss: 1497.8153\n",
            "Epoch [100/150], Loss: 1285.0749\n",
            "Epoch [110/150], Loss: 2630.0529\n",
            "Epoch [120/150], Loss: 1937.4795\n",
            "Epoch [130/150], Loss: 1881.4628\n",
            "Epoch [140/150], Loss: 1214.4813\n",
            "Epoch [150/150], Loss: 744.7972\n",
            "Fold 3, RMSE: 96.0293960571289\n",
            "Epoch [10/150], Loss: 12193.2295\n",
            "Epoch [20/150], Loss: 8085.9225\n",
            "Epoch [30/150], Loss: 5300.2066\n",
            "Epoch [40/150], Loss: 3308.1347\n",
            "Epoch [50/150], Loss: 4865.1200\n",
            "Epoch [60/150], Loss: 2018.7336\n",
            "Epoch [70/150], Loss: 2275.1707\n",
            "Epoch [80/150], Loss: 1991.0292\n",
            "Epoch [90/150], Loss: 2557.1766\n",
            "Epoch [100/150], Loss: 1907.3394\n",
            "Epoch [110/150], Loss: 1613.7879\n",
            "Epoch [120/150], Loss: 2236.5391\n",
            "Epoch [130/150], Loss: 3102.0531\n",
            "Epoch [140/150], Loss: 3722.3259\n",
            "Epoch [150/150], Loss: 4942.5630\n",
            "Fold 4, RMSE: 39.88920593261719\n",
            "Epoch [10/150], Loss: 8896.1718\n",
            "Epoch [20/150], Loss: 7296.0125\n",
            "Epoch [30/150], Loss: 9794.1818\n",
            "Epoch [40/150], Loss: 3234.2032\n",
            "Epoch [50/150], Loss: 1837.3395\n",
            "Epoch [60/150], Loss: 2428.6343\n",
            "Epoch [70/150], Loss: 1366.3511\n",
            "Epoch [80/150], Loss: 3878.2632\n",
            "Epoch [90/150], Loss: 1940.6015\n",
            "Epoch [100/150], Loss: 1860.0227\n",
            "Epoch [110/150], Loss: 1343.1468\n",
            "Epoch [120/150], Loss: 4868.6178\n",
            "Epoch [130/150], Loss: 1266.9244\n",
            "Epoch [140/150], Loss: 3618.7817\n",
            "Epoch [150/150], Loss: 1533.4750\n",
            "Fold 5, RMSE: 44.75498962402344\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 59.666596221923825\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 25228.3345\n",
            "Epoch [20/100], Loss: 15360.4658\n",
            "Epoch [30/100], Loss: 7577.2516\n",
            "Epoch [40/100], Loss: 5920.6159\n",
            "Epoch [50/100], Loss: 6805.1027\n",
            "Epoch [60/100], Loss: 3015.1903\n",
            "Epoch [70/100], Loss: 1950.0684\n",
            "Epoch [80/100], Loss: 2219.9532\n",
            "Epoch [90/100], Loss: 2250.6129\n",
            "Epoch [100/100], Loss: 3452.1414\n",
            "Fold 1, RMSE: 56.57933044433594\n",
            "Epoch [10/100], Loss: 10269.7394\n",
            "Epoch [20/100], Loss: 8977.8221\n",
            "Epoch [30/100], Loss: 12465.9891\n",
            "Epoch [40/100], Loss: 4296.3721\n",
            "Epoch [50/100], Loss: 4218.5521\n",
            "Epoch [60/100], Loss: 3294.8064\n",
            "Epoch [70/100], Loss: 3375.9702\n",
            "Epoch [80/100], Loss: 4273.7115\n",
            "Epoch [90/100], Loss: 4757.9406\n",
            "Epoch [100/100], Loss: 9755.3668\n",
            "Fold 2, RMSE: 76.26641082763672\n",
            "Epoch [10/100], Loss: 12968.9990\n",
            "Epoch [20/100], Loss: 5589.6213\n",
            "Epoch [30/100], Loss: 6897.4351\n",
            "Epoch [40/100], Loss: 4217.0006\n",
            "Epoch [50/100], Loss: 7205.8243\n",
            "Epoch [60/100], Loss: 3606.0461\n",
            "Epoch [70/100], Loss: 8970.4167\n",
            "Epoch [80/100], Loss: 2310.5995\n",
            "Epoch [90/100], Loss: 6134.7911\n",
            "Epoch [100/100], Loss: 3302.6141\n",
            "Fold 3, RMSE: 96.52672576904297\n",
            "Epoch [10/100], Loss: 17567.1892\n",
            "Epoch [20/100], Loss: 12317.1707\n",
            "Epoch [30/100], Loss: 9450.3186\n",
            "Epoch [40/100], Loss: 8818.7444\n",
            "Epoch [50/100], Loss: 2966.8692\n",
            "Epoch [60/100], Loss: 3607.0773\n",
            "Epoch [70/100], Loss: 2571.3553\n",
            "Epoch [80/100], Loss: 4245.3095\n",
            "Epoch [90/100], Loss: 5348.3696\n",
            "Epoch [100/100], Loss: 4774.5372\n",
            "Fold 4, RMSE: 36.10906219482422\n",
            "Epoch [10/100], Loss: 23535.4741\n",
            "Epoch [20/100], Loss: 12021.6328\n",
            "Epoch [30/100], Loss: 11572.3223\n",
            "Epoch [40/100], Loss: 12381.6685\n",
            "Epoch [50/100], Loss: 7320.1287\n",
            "Epoch [60/100], Loss: 8411.1986\n",
            "Epoch [70/100], Loss: 7882.2494\n",
            "Epoch [80/100], Loss: 11676.2241\n",
            "Epoch [90/100], Loss: 2206.7199\n",
            "Epoch [100/100], Loss: 3211.3105\n",
            "Fold 5, RMSE: 44.60076141357422\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 62.016458129882814\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16027.6929\n",
            "Epoch [20/150], Loss: 13255.1255\n",
            "Epoch [30/150], Loss: 14641.5388\n",
            "Epoch [40/150], Loss: 4830.9131\n",
            "Epoch [50/150], Loss: 5715.6536\n",
            "Epoch [60/150], Loss: 1975.1946\n",
            "Epoch [70/150], Loss: 2900.1580\n",
            "Epoch [80/150], Loss: 2214.4305\n",
            "Epoch [90/150], Loss: 3135.2898\n",
            "Epoch [100/150], Loss: 2617.8311\n",
            "Epoch [110/150], Loss: 5808.9916\n",
            "Epoch [120/150], Loss: 1731.3025\n",
            "Epoch [130/150], Loss: 3800.3224\n",
            "Epoch [140/150], Loss: 2411.7242\n",
            "Epoch [150/150], Loss: 1245.5886\n",
            "Fold 1, RMSE: 59.76953887939453\n",
            "Epoch [10/150], Loss: 36646.9287\n",
            "Epoch [20/150], Loss: 9997.8220\n",
            "Epoch [30/150], Loss: 11371.6282\n",
            "Epoch [40/150], Loss: 7102.1711\n",
            "Epoch [50/150], Loss: 10758.8667\n",
            "Epoch [60/150], Loss: 4216.8212\n",
            "Epoch [70/150], Loss: 2473.2651\n",
            "Epoch [80/150], Loss: 2794.6797\n",
            "Epoch [90/150], Loss: 3082.9630\n",
            "Epoch [100/150], Loss: 2052.4647\n",
            "Epoch [110/150], Loss: 2587.7957\n",
            "Epoch [120/150], Loss: 2156.9749\n",
            "Epoch [130/150], Loss: 2216.1362\n",
            "Epoch [140/150], Loss: 2807.1645\n",
            "Epoch [150/150], Loss: 2717.4841\n",
            "Fold 2, RMSE: 66.70771026611328\n",
            "Epoch [10/150], Loss: 10173.6145\n",
            "Epoch [20/150], Loss: 5270.9252\n",
            "Epoch [30/150], Loss: 3409.7053\n",
            "Epoch [40/150], Loss: 5176.2213\n",
            "Epoch [50/150], Loss: 2228.7010\n",
            "Epoch [60/150], Loss: 5369.6747\n",
            "Epoch [70/150], Loss: 4011.7971\n",
            "Epoch [80/150], Loss: 1582.8148\n",
            "Epoch [90/150], Loss: 4736.8979\n",
            "Epoch [100/150], Loss: 4486.3768\n",
            "Epoch [110/150], Loss: 2760.4078\n",
            "Epoch [120/150], Loss: 5618.3950\n",
            "Epoch [130/150], Loss: 2888.6468\n",
            "Epoch [140/150], Loss: 5638.5717\n",
            "Epoch [150/150], Loss: 4760.7008\n",
            "Fold 3, RMSE: 94.2434310913086\n",
            "Epoch [10/150], Loss: 16070.8149\n",
            "Epoch [20/150], Loss: 12236.6040\n",
            "Epoch [30/150], Loss: 12395.9395\n",
            "Epoch [40/150], Loss: 8730.0395\n",
            "Epoch [50/150], Loss: 2095.4840\n",
            "Epoch [60/150], Loss: 3315.3979\n",
            "Epoch [70/150], Loss: 3688.0213\n",
            "Epoch [80/150], Loss: 6183.7946\n",
            "Epoch [90/150], Loss: 2489.1444\n",
            "Epoch [100/150], Loss: 9067.5366\n",
            "Epoch [110/150], Loss: 3856.8826\n",
            "Epoch [120/150], Loss: 5048.2183\n",
            "Epoch [130/150], Loss: 3269.8554\n",
            "Epoch [140/150], Loss: 5763.7245\n",
            "Epoch [150/150], Loss: 2245.8683\n",
            "Fold 4, RMSE: 44.00453567504883\n",
            "Epoch [10/150], Loss: 12494.9377\n",
            "Epoch [20/150], Loss: 17835.1619\n",
            "Epoch [30/150], Loss: 10726.1299\n",
            "Epoch [40/150], Loss: 9308.6224\n",
            "Epoch [50/150], Loss: 9932.3264\n",
            "Epoch [60/150], Loss: 8441.7512\n",
            "Epoch [70/150], Loss: 5253.7661\n",
            "Epoch [80/150], Loss: 8580.6853\n",
            "Epoch [90/150], Loss: 3311.9774\n",
            "Epoch [100/150], Loss: 8091.8292\n",
            "Epoch [110/150], Loss: 6171.2056\n",
            "Epoch [120/150], Loss: 6982.2268\n",
            "Epoch [130/150], Loss: 3300.5328\n",
            "Epoch [140/150], Loss: 5900.7289\n",
            "Epoch [150/150], Loss: 2784.0209\n",
            "Fold 5, RMSE: 45.09825897216797\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 61.96469497680664\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 13052.6404\n",
            "Epoch [20/100], Loss: 7004.8722\n",
            "Epoch [30/100], Loss: 4208.9586\n",
            "Epoch [40/100], Loss: 5866.1418\n",
            "Epoch [50/100], Loss: 4283.9000\n",
            "Epoch [60/100], Loss: 1971.8118\n",
            "Epoch [70/100], Loss: 1169.5184\n",
            "Epoch [80/100], Loss: 3448.6657\n",
            "Epoch [90/100], Loss: 1508.6476\n",
            "Epoch [100/100], Loss: 1526.3010\n",
            "Fold 1, RMSE: 58.488468170166016\n",
            "Epoch [10/100], Loss: 13791.5498\n",
            "Epoch [20/100], Loss: 10280.2288\n",
            "Epoch [30/100], Loss: 8755.9828\n",
            "Epoch [40/100], Loss: 13438.2690\n",
            "Epoch [50/100], Loss: 12103.7439\n",
            "Epoch [60/100], Loss: 3460.1895\n",
            "Epoch [70/100], Loss: 4660.7544\n",
            "Epoch [80/100], Loss: 6188.2020\n",
            "Epoch [90/100], Loss: 4086.0627\n",
            "Epoch [100/100], Loss: 3410.1041\n",
            "Fold 2, RMSE: 70.7691650390625\n",
            "Epoch [10/100], Loss: 7112.8989\n",
            "Epoch [20/100], Loss: 2619.7419\n",
            "Epoch [30/100], Loss: 1745.7287\n",
            "Epoch [40/100], Loss: 1485.7018\n",
            "Epoch [50/100], Loss: 1322.9895\n",
            "Epoch [60/100], Loss: 1959.9015\n",
            "Epoch [70/100], Loss: 1547.8486\n",
            "Epoch [80/100], Loss: 8095.6569\n",
            "Epoch [90/100], Loss: 3109.6683\n",
            "Epoch [100/100], Loss: 1189.3293\n",
            "Fold 3, RMSE: 92.40542602539062\n",
            "Epoch [10/100], Loss: 19734.5835\n",
            "Epoch [20/100], Loss: 11510.5796\n",
            "Epoch [30/100], Loss: 14164.5240\n",
            "Epoch [40/100], Loss: 6552.7844\n",
            "Epoch [50/100], Loss: 4076.8018\n",
            "Epoch [60/100], Loss: 6261.5749\n",
            "Epoch [70/100], Loss: 5577.1710\n",
            "Epoch [80/100], Loss: 4842.4581\n",
            "Epoch [90/100], Loss: 1049.4811\n",
            "Epoch [100/100], Loss: 2567.5743\n",
            "Fold 4, RMSE: 40.52906036376953\n",
            "Epoch [10/100], Loss: 12406.3713\n",
            "Epoch [20/100], Loss: 6257.4779\n",
            "Epoch [30/100], Loss: 6504.1812\n",
            "Epoch [40/100], Loss: 6184.7555\n",
            "Epoch [50/100], Loss: 3863.4282\n",
            "Epoch [60/100], Loss: 2801.5948\n",
            "Epoch [70/100], Loss: 3044.0191\n",
            "Epoch [80/100], Loss: 2753.2670\n",
            "Epoch [90/100], Loss: 3913.6605\n",
            "Epoch [100/100], Loss: 1476.8820\n",
            "Fold 5, RMSE: 46.680389404296875\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 61.77450180053711\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 9402.8915\n",
            "Epoch [20/150], Loss: 7939.4614\n",
            "Epoch [30/150], Loss: 3853.6265\n",
            "Epoch [40/150], Loss: 2742.8735\n",
            "Epoch [50/150], Loss: 3882.1103\n",
            "Epoch [60/150], Loss: 2787.7122\n",
            "Epoch [70/150], Loss: 4772.3280\n",
            "Epoch [80/150], Loss: 1265.0854\n",
            "Epoch [90/150], Loss: 3560.3819\n",
            "Epoch [100/150], Loss: 1722.3976\n",
            "Epoch [110/150], Loss: 664.7500\n",
            "Epoch [120/150], Loss: 2064.7008\n",
            "Epoch [130/150], Loss: 1458.9552\n",
            "Epoch [140/150], Loss: 1677.5634\n",
            "Epoch [150/150], Loss: 12417.4778\n",
            "Fold 1, RMSE: 58.340572357177734\n",
            "Epoch [10/150], Loss: 15266.5847\n",
            "Epoch [20/150], Loss: 9401.3372\n",
            "Epoch [30/150], Loss: 7032.1945\n",
            "Epoch [40/150], Loss: 2216.4147\n",
            "Epoch [50/150], Loss: 4325.8800\n",
            "Epoch [60/150], Loss: 3088.2675\n",
            "Epoch [70/150], Loss: 2453.2575\n",
            "Epoch [80/150], Loss: 6713.5750\n",
            "Epoch [90/150], Loss: 7929.1677\n",
            "Epoch [100/150], Loss: 1998.9816\n",
            "Epoch [110/150], Loss: 5419.2454\n",
            "Epoch [120/150], Loss: 1359.8334\n",
            "Epoch [130/150], Loss: 2590.4675\n",
            "Epoch [140/150], Loss: 1534.2721\n",
            "Epoch [150/150], Loss: 2307.3375\n",
            "Fold 2, RMSE: 70.08104705810547\n",
            "Epoch [10/150], Loss: 10807.3788\n",
            "Epoch [20/150], Loss: 7738.5609\n",
            "Epoch [30/150], Loss: 6001.9415\n",
            "Epoch [40/150], Loss: 4199.1912\n",
            "Epoch [50/150], Loss: 1503.2571\n",
            "Epoch [60/150], Loss: 5841.0131\n",
            "Epoch [70/150], Loss: 1744.5120\n",
            "Epoch [80/150], Loss: 3906.5994\n",
            "Epoch [90/150], Loss: 1498.1099\n",
            "Epoch [100/150], Loss: 1723.7989\n",
            "Epoch [110/150], Loss: 1695.9952\n",
            "Epoch [120/150], Loss: 3185.7042\n",
            "Epoch [130/150], Loss: 1670.5346\n",
            "Epoch [140/150], Loss: 1073.5103\n",
            "Epoch [150/150], Loss: 3293.1062\n",
            "Fold 3, RMSE: 92.21758270263672\n",
            "Epoch [10/150], Loss: 10029.3936\n",
            "Epoch [20/150], Loss: 9250.8545\n",
            "Epoch [30/150], Loss: 5463.8361\n",
            "Epoch [40/150], Loss: 4597.3627\n",
            "Epoch [50/150], Loss: 2979.4775\n",
            "Epoch [60/150], Loss: 3327.4916\n",
            "Epoch [70/150], Loss: 5167.5922\n",
            "Epoch [80/150], Loss: 7958.5050\n",
            "Epoch [90/150], Loss: 3944.0347\n",
            "Epoch [100/150], Loss: 2567.9798\n",
            "Epoch [110/150], Loss: 4571.5155\n",
            "Epoch [120/150], Loss: 1581.4046\n",
            "Epoch [130/150], Loss: 1036.8214\n",
            "Epoch [140/150], Loss: 2190.6524\n",
            "Epoch [150/150], Loss: 789.2459\n",
            "Fold 4, RMSE: 43.624324798583984\n",
            "Epoch [10/150], Loss: 18204.2678\n",
            "Epoch [20/150], Loss: 13677.9443\n",
            "Epoch [30/150], Loss: 7854.3281\n",
            "Epoch [40/150], Loss: 3439.4309\n",
            "Epoch [50/150], Loss: 2271.5190\n",
            "Epoch [60/150], Loss: 6881.8217\n",
            "Epoch [70/150], Loss: 6127.8935\n",
            "Epoch [80/150], Loss: 7793.2417\n",
            "Epoch [90/150], Loss: 2201.2544\n",
            "Epoch [100/150], Loss: 3459.1631\n",
            "Epoch [110/150], Loss: 2192.5089\n",
            "Epoch [120/150], Loss: 2965.0139\n",
            "Epoch [130/150], Loss: 1494.0477\n",
            "Epoch [140/150], Loss: 3176.0067\n",
            "Epoch [150/150], Loss: 1611.8611\n",
            "Fold 5, RMSE: 44.19972610473633\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 61.692650604248044\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14425.2917\n",
            "Epoch [20/100], Loss: 11400.6321\n",
            "Epoch [30/100], Loss: 12636.8357\n",
            "Epoch [40/100], Loss: 9185.6370\n",
            "Epoch [50/100], Loss: 16664.0760\n",
            "Epoch [60/100], Loss: 10700.1692\n",
            "Epoch [70/100], Loss: 5893.6603\n",
            "Epoch [80/100], Loss: 4508.8910\n",
            "Epoch [90/100], Loss: 4929.6693\n",
            "Epoch [100/100], Loss: 2668.5187\n",
            "Fold 1, RMSE: 58.6191291809082\n",
            "Epoch [10/100], Loss: 21904.6836\n",
            "Epoch [20/100], Loss: 10150.8442\n",
            "Epoch [30/100], Loss: 8037.2688\n",
            "Epoch [40/100], Loss: 6516.3389\n",
            "Epoch [50/100], Loss: 3815.8832\n",
            "Epoch [60/100], Loss: 3704.6198\n",
            "Epoch [70/100], Loss: 1881.0769\n",
            "Epoch [80/100], Loss: 2946.4799\n",
            "Epoch [90/100], Loss: 6079.8693\n",
            "Epoch [100/100], Loss: 1424.8627\n",
            "Fold 2, RMSE: 63.92787170410156\n",
            "Epoch [10/100], Loss: 24856.1230\n",
            "Epoch [20/100], Loss: 8674.1156\n",
            "Epoch [30/100], Loss: 9792.2906\n",
            "Epoch [40/100], Loss: 6266.5836\n",
            "Epoch [50/100], Loss: 3788.0651\n",
            "Epoch [60/100], Loss: 4128.5326\n",
            "Epoch [70/100], Loss: 2596.4466\n",
            "Epoch [80/100], Loss: 2437.4751\n",
            "Epoch [90/100], Loss: 1870.9042\n",
            "Epoch [100/100], Loss: 1439.0065\n",
            "Fold 3, RMSE: 92.14346313476562\n",
            "Epoch [10/100], Loss: 12310.6401\n",
            "Epoch [20/100], Loss: 9690.7510\n",
            "Epoch [30/100], Loss: 5429.4415\n",
            "Epoch [40/100], Loss: 3519.3577\n",
            "Epoch [50/100], Loss: 3209.5822\n",
            "Epoch [60/100], Loss: 1740.4486\n",
            "Epoch [70/100], Loss: 2251.0287\n",
            "Epoch [80/100], Loss: 1790.7035\n",
            "Epoch [90/100], Loss: 2605.1481\n",
            "Epoch [100/100], Loss: 2612.1137\n",
            "Fold 4, RMSE: 43.94862747192383\n",
            "Epoch [10/100], Loss: 27116.4878\n",
            "Epoch [20/100], Loss: 11682.4218\n",
            "Epoch [30/100], Loss: 9532.0311\n",
            "Epoch [40/100], Loss: 4595.6927\n",
            "Epoch [50/100], Loss: 5155.3167\n",
            "Epoch [60/100], Loss: 4910.6982\n",
            "Epoch [70/100], Loss: 3258.8539\n",
            "Epoch [80/100], Loss: 2985.1617\n",
            "Epoch [90/100], Loss: 1908.5858\n",
            "Epoch [100/100], Loss: 3798.2901\n",
            "Fold 5, RMSE: 47.26224899291992\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 61.180268096923825\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 17007.2190\n",
            "Epoch [20/150], Loss: 9022.1002\n",
            "Epoch [30/150], Loss: 6169.6594\n",
            "Epoch [40/150], Loss: 4498.4590\n",
            "Epoch [50/150], Loss: 6185.2987\n",
            "Epoch [60/150], Loss: 4533.4214\n",
            "Epoch [70/150], Loss: 7347.1211\n",
            "Epoch [80/150], Loss: 12483.3582\n",
            "Epoch [90/150], Loss: 2736.6692\n",
            "Epoch [100/150], Loss: 4457.1380\n",
            "Epoch [110/150], Loss: 2015.4611\n",
            "Epoch [120/150], Loss: 2506.5421\n",
            "Epoch [130/150], Loss: 2847.3187\n",
            "Epoch [140/150], Loss: 5694.4094\n",
            "Epoch [150/150], Loss: 4622.8951\n",
            "Fold 1, RMSE: 61.20091247558594\n",
            "Epoch [10/150], Loss: 33934.6299\n",
            "Epoch [20/150], Loss: 11262.1841\n",
            "Epoch [30/150], Loss: 9275.8003\n",
            "Epoch [40/150], Loss: 7540.5741\n",
            "Epoch [50/150], Loss: 9234.3298\n",
            "Epoch [60/150], Loss: 3567.3626\n",
            "Epoch [70/150], Loss: 4048.5075\n",
            "Epoch [80/150], Loss: 2521.0840\n",
            "Epoch [90/150], Loss: 2581.3461\n",
            "Epoch [100/150], Loss: 4144.5283\n",
            "Epoch [110/150], Loss: 2929.7369\n",
            "Epoch [120/150], Loss: 2431.4179\n",
            "Epoch [130/150], Loss: 3002.0908\n",
            "Epoch [140/150], Loss: 2434.6080\n",
            "Epoch [150/150], Loss: 1615.7411\n",
            "Fold 2, RMSE: 68.43902587890625\n",
            "Epoch [10/150], Loss: 12766.2493\n",
            "Epoch [20/150], Loss: 14469.7505\n",
            "Epoch [30/150], Loss: 11055.1770\n",
            "Epoch [40/150], Loss: 10514.5271\n",
            "Epoch [50/150], Loss: 8890.5852\n",
            "Epoch [60/150], Loss: 6409.5220\n",
            "Epoch [70/150], Loss: 3260.0891\n",
            "Epoch [80/150], Loss: 8598.4399\n",
            "Epoch [90/150], Loss: 3922.0725\n",
            "Epoch [100/150], Loss: 4687.7585\n",
            "Epoch [110/150], Loss: 2902.2858\n",
            "Epoch [120/150], Loss: 3294.2884\n",
            "Epoch [130/150], Loss: 2126.0892\n",
            "Epoch [140/150], Loss: 3513.1561\n",
            "Epoch [150/150], Loss: 5236.7401\n",
            "Fold 3, RMSE: 94.70838928222656\n",
            "Epoch [10/150], Loss: 41248.1091\n",
            "Epoch [20/150], Loss: 12805.3779\n",
            "Epoch [30/150], Loss: 11425.1477\n",
            "Epoch [40/150], Loss: 14409.7029\n",
            "Epoch [50/150], Loss: 11758.9009\n",
            "Epoch [60/150], Loss: 6773.7157\n",
            "Epoch [70/150], Loss: 5491.3226\n",
            "Epoch [80/150], Loss: 1929.2399\n",
            "Epoch [90/150], Loss: 1845.3947\n",
            "Epoch [100/150], Loss: 1869.2064\n",
            "Epoch [110/150], Loss: 1812.2046\n",
            "Epoch [120/150], Loss: 1466.6817\n",
            "Epoch [130/150], Loss: 1621.7734\n",
            "Epoch [140/150], Loss: 2537.2014\n",
            "Epoch [150/150], Loss: 3166.2650\n",
            "Fold 4, RMSE: 46.708892822265625\n",
            "Epoch [10/150], Loss: 26878.0840\n",
            "Epoch [20/150], Loss: 12571.7788\n",
            "Epoch [30/150], Loss: 11102.7058\n",
            "Epoch [40/150], Loss: 7321.1005\n",
            "Epoch [50/150], Loss: 3865.4099\n",
            "Epoch [60/150], Loss: 3299.1738\n",
            "Epoch [70/150], Loss: 1764.4698\n",
            "Epoch [80/150], Loss: 875.1502\n",
            "Epoch [90/150], Loss: 1784.3563\n",
            "Epoch [100/150], Loss: 2530.4806\n",
            "Epoch [110/150], Loss: 1808.9660\n",
            "Epoch [120/150], Loss: 2962.7342\n",
            "Epoch [130/150], Loss: 5320.5380\n",
            "Epoch [140/150], Loss: 2529.0754\n",
            "Epoch [150/150], Loss: 7996.0420\n",
            "Fold 5, RMSE: 47.06232452392578\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 63.62390899658203\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 12206.6691\n",
            "Epoch [20/100], Loss: 8630.2862\n",
            "Epoch [30/100], Loss: 6878.4662\n",
            "Epoch [40/100], Loss: 3086.1633\n",
            "Epoch [50/100], Loss: 4220.1956\n",
            "Epoch [60/100], Loss: 1795.5181\n",
            "Epoch [70/100], Loss: 2009.8859\n",
            "Epoch [80/100], Loss: 1680.5450\n",
            "Epoch [90/100], Loss: 1205.7052\n",
            "Epoch [100/100], Loss: 1017.5507\n",
            "Fold 1, RMSE: 56.76940155029297\n",
            "Epoch [10/100], Loss: 12867.3765\n",
            "Epoch [20/100], Loss: 9309.8694\n",
            "Epoch [30/100], Loss: 8780.0653\n",
            "Epoch [40/100], Loss: 7601.9849\n",
            "Epoch [50/100], Loss: 7175.2733\n",
            "Epoch [60/100], Loss: 2381.3590\n",
            "Epoch [70/100], Loss: 2302.7184\n",
            "Epoch [80/100], Loss: 4115.2418\n",
            "Epoch [90/100], Loss: 4815.7363\n",
            "Epoch [100/100], Loss: 1632.4605\n",
            "Fold 2, RMSE: 56.40281677246094\n",
            "Epoch [10/100], Loss: 10101.8794\n",
            "Epoch [20/100], Loss: 4959.0708\n",
            "Epoch [30/100], Loss: 2511.8351\n",
            "Epoch [40/100], Loss: 2623.7279\n",
            "Epoch [50/100], Loss: 2245.3134\n",
            "Epoch [60/100], Loss: 2232.3320\n",
            "Epoch [70/100], Loss: 3120.6221\n",
            "Epoch [80/100], Loss: 1685.3864\n",
            "Epoch [90/100], Loss: 1648.2357\n",
            "Epoch [100/100], Loss: 1241.4241\n",
            "Fold 3, RMSE: 89.76544189453125\n",
            "Epoch [10/100], Loss: 13628.4543\n",
            "Epoch [20/100], Loss: 14539.3936\n",
            "Epoch [30/100], Loss: 5105.2950\n",
            "Epoch [40/100], Loss: 2518.2736\n",
            "Epoch [50/100], Loss: 5299.4935\n",
            "Epoch [60/100], Loss: 6212.8923\n",
            "Epoch [70/100], Loss: 2991.8705\n",
            "Epoch [80/100], Loss: 3102.2411\n",
            "Epoch [90/100], Loss: 1990.5845\n",
            "Epoch [100/100], Loss: 1299.3342\n",
            "Fold 4, RMSE: 41.61420440673828\n",
            "Epoch [10/100], Loss: 10752.6681\n",
            "Epoch [20/100], Loss: 7872.8722\n",
            "Epoch [30/100], Loss: 4653.9081\n",
            "Epoch [40/100], Loss: 4266.2963\n",
            "Epoch [50/100], Loss: 2235.3658\n",
            "Epoch [60/100], Loss: 3127.2503\n",
            "Epoch [70/100], Loss: 1061.6288\n",
            "Epoch [80/100], Loss: 3735.5262\n",
            "Epoch [90/100], Loss: 1482.7038\n",
            "Epoch [100/100], Loss: 2562.3668\n",
            "Fold 5, RMSE: 48.26469039916992\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 58.56331100463867\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13699.0627\n",
            "Epoch [20/150], Loss: 8857.8298\n",
            "Epoch [30/150], Loss: 4386.7427\n",
            "Epoch [40/150], Loss: 4192.1892\n",
            "Epoch [50/150], Loss: 2066.9751\n",
            "Epoch [60/150], Loss: 3046.3654\n",
            "Epoch [70/150], Loss: 5978.2778\n",
            "Epoch [80/150], Loss: 2635.8428\n",
            "Epoch [90/150], Loss: 2259.4052\n",
            "Epoch [100/150], Loss: 2453.4801\n",
            "Epoch [110/150], Loss: 3096.1070\n",
            "Epoch [120/150], Loss: 2332.6223\n",
            "Epoch [130/150], Loss: 1058.4909\n",
            "Epoch [140/150], Loss: 2493.6568\n",
            "Epoch [150/150], Loss: 1487.0291\n",
            "Fold 1, RMSE: 55.96436309814453\n",
            "Epoch [10/150], Loss: 8016.3249\n",
            "Epoch [20/150], Loss: 6841.6495\n",
            "Epoch [30/150], Loss: 3357.9083\n",
            "Epoch [40/150], Loss: 2650.9772\n",
            "Epoch [50/150], Loss: 3375.7723\n",
            "Epoch [60/150], Loss: 972.4644\n",
            "Epoch [70/150], Loss: 3521.9020\n",
            "Epoch [80/150], Loss: 1801.9072\n",
            "Epoch [90/150], Loss: 1969.4034\n",
            "Epoch [100/150], Loss: 850.5302\n",
            "Epoch [110/150], Loss: 1328.9421\n",
            "Epoch [120/150], Loss: 1057.0981\n",
            "Epoch [130/150], Loss: 2842.7262\n",
            "Epoch [140/150], Loss: 2558.5620\n",
            "Epoch [150/150], Loss: 5421.2405\n",
            "Fold 2, RMSE: 61.691009521484375\n",
            "Epoch [10/150], Loss: 17952.4219\n",
            "Epoch [20/150], Loss: 7567.4440\n",
            "Epoch [30/150], Loss: 4676.5447\n",
            "Epoch [40/150], Loss: 5306.3616\n",
            "Epoch [50/150], Loss: 3831.5922\n",
            "Epoch [60/150], Loss: 1696.6363\n",
            "Epoch [70/150], Loss: 2197.8814\n",
            "Epoch [80/150], Loss: 3283.7008\n",
            "Epoch [90/150], Loss: 2515.3657\n",
            "Epoch [100/150], Loss: 6970.0345\n",
            "Epoch [110/150], Loss: 1474.7934\n",
            "Epoch [120/150], Loss: 1626.4240\n",
            "Epoch [130/150], Loss: 1238.7765\n",
            "Epoch [140/150], Loss: 2167.3260\n",
            "Epoch [150/150], Loss: 2013.1867\n",
            "Fold 3, RMSE: 95.46641540527344\n",
            "Epoch [10/150], Loss: 15078.1663\n",
            "Epoch [20/150], Loss: 9493.7672\n",
            "Epoch [30/150], Loss: 8040.2786\n",
            "Epoch [40/150], Loss: 3455.6226\n",
            "Epoch [50/150], Loss: 1930.0008\n",
            "Epoch [60/150], Loss: 3031.5928\n",
            "Epoch [70/150], Loss: 2145.5035\n",
            "Epoch [80/150], Loss: 1767.0737\n",
            "Epoch [90/150], Loss: 3115.7421\n",
            "Epoch [100/150], Loss: 2221.8757\n",
            "Epoch [110/150], Loss: 2527.5743\n",
            "Epoch [120/150], Loss: 1814.6981\n",
            "Epoch [130/150], Loss: 2762.4572\n",
            "Epoch [140/150], Loss: 1568.0473\n",
            "Epoch [150/150], Loss: 1300.2466\n",
            "Fold 4, RMSE: 43.654693603515625\n",
            "Epoch [10/150], Loss: 19641.2988\n",
            "Epoch [20/150], Loss: 10346.7031\n",
            "Epoch [30/150], Loss: 6658.7104\n",
            "Epoch [40/150], Loss: 3246.0590\n",
            "Epoch [50/150], Loss: 3732.4614\n",
            "Epoch [60/150], Loss: 3830.7499\n",
            "Epoch [70/150], Loss: 2081.3374\n",
            "Epoch [80/150], Loss: 2342.3531\n",
            "Epoch [90/150], Loss: 3145.5847\n",
            "Epoch [100/150], Loss: 2021.8664\n",
            "Epoch [110/150], Loss: 3065.2902\n",
            "Epoch [120/150], Loss: 2546.4863\n",
            "Epoch [130/150], Loss: 740.7156\n",
            "Epoch [140/150], Loss: 746.5747\n",
            "Epoch [150/150], Loss: 1905.5469\n",
            "Fold 5, RMSE: 44.49833679199219\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 60.25496368408203\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 50055.2627\n",
            "Epoch [20/100], Loss: 12166.7311\n",
            "Epoch [30/100], Loss: 13356.6392\n",
            "Epoch [40/100], Loss: 9344.5082\n",
            "Epoch [50/100], Loss: 12390.6063\n",
            "Epoch [60/100], Loss: 8386.3020\n",
            "Epoch [70/100], Loss: 6019.4049\n",
            "Epoch [80/100], Loss: 4897.4396\n",
            "Epoch [90/100], Loss: 2328.1445\n",
            "Epoch [100/100], Loss: 4277.5404\n",
            "Fold 1, RMSE: 57.773094177246094\n",
            "Epoch [10/100], Loss: 16716.0199\n",
            "Epoch [20/100], Loss: 7804.7756\n",
            "Epoch [30/100], Loss: 9420.7390\n",
            "Epoch [40/100], Loss: 5548.9117\n",
            "Epoch [50/100], Loss: 4652.2130\n",
            "Epoch [60/100], Loss: 4623.1044\n",
            "Epoch [70/100], Loss: 2791.2570\n",
            "Epoch [80/100], Loss: 3913.6841\n",
            "Epoch [90/100], Loss: 3181.9815\n",
            "Epoch [100/100], Loss: 1764.5607\n",
            "Fold 2, RMSE: 69.54077911376953\n",
            "Epoch [10/100], Loss: 70249.1729\n",
            "Epoch [20/100], Loss: 13771.0371\n",
            "Epoch [30/100], Loss: 7139.8730\n",
            "Epoch [40/100], Loss: 3657.9977\n",
            "Epoch [50/100], Loss: 5244.6640\n",
            "Epoch [60/100], Loss: 3482.6476\n",
            "Epoch [70/100], Loss: 2828.2833\n",
            "Epoch [80/100], Loss: 3724.0764\n",
            "Epoch [90/100], Loss: 4111.2822\n",
            "Epoch [100/100], Loss: 2004.2549\n",
            "Fold 3, RMSE: 91.76283264160156\n",
            "Epoch [10/100], Loss: 30409.8408\n",
            "Epoch [20/100], Loss: 12662.9089\n",
            "Epoch [30/100], Loss: 13894.5588\n",
            "Epoch [40/100], Loss: 8129.0677\n",
            "Epoch [50/100], Loss: 7153.4022\n",
            "Epoch [60/100], Loss: 3855.2548\n",
            "Epoch [70/100], Loss: 2688.2091\n",
            "Epoch [80/100], Loss: 3147.8271\n",
            "Epoch [90/100], Loss: 5171.1716\n",
            "Epoch [100/100], Loss: 3394.2855\n",
            "Fold 4, RMSE: 42.47776794433594\n",
            "Epoch [10/100], Loss: 20596.5532\n",
            "Epoch [20/100], Loss: 20843.7710\n",
            "Epoch [30/100], Loss: 11552.0829\n",
            "Epoch [40/100], Loss: 8184.0165\n",
            "Epoch [50/100], Loss: 6829.9099\n",
            "Epoch [60/100], Loss: 5479.2534\n",
            "Epoch [70/100], Loss: 8276.9293\n",
            "Epoch [80/100], Loss: 2235.7352\n",
            "Epoch [90/100], Loss: 4353.9993\n",
            "Epoch [100/100], Loss: 6046.8016\n",
            "Fold 5, RMSE: 49.85261535644531\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 62.28141784667969\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16510.7458\n",
            "Epoch [20/150], Loss: 18152.9062\n",
            "Epoch [30/150], Loss: 28194.3022\n",
            "Epoch [40/150], Loss: 16850.7715\n",
            "Epoch [50/150], Loss: 20950.0005\n",
            "Epoch [60/150], Loss: 18310.7505\n",
            "Epoch [70/150], Loss: 18201.1709\n",
            "Epoch [80/150], Loss: 17631.9109\n",
            "Epoch [90/150], Loss: 19231.4282\n",
            "Epoch [100/150], Loss: 19981.3818\n",
            "Epoch [110/150], Loss: 15683.2767\n",
            "Epoch [120/150], Loss: 16410.9399\n",
            "Epoch [130/150], Loss: 16196.7583\n",
            "Epoch [140/150], Loss: 19905.1387\n",
            "Epoch [150/150], Loss: 15562.7546\n",
            "Fold 1, RMSE: 67.25213623046875\n",
            "Epoch [10/150], Loss: 14680.3269\n",
            "Epoch [20/150], Loss: 7785.2267\n",
            "Epoch [30/150], Loss: 12108.4746\n",
            "Epoch [40/150], Loss: 6035.4392\n",
            "Epoch [50/150], Loss: 5792.4049\n",
            "Epoch [60/150], Loss: 4475.6867\n",
            "Epoch [70/150], Loss: 2877.1049\n",
            "Epoch [80/150], Loss: 5275.7181\n",
            "Epoch [90/150], Loss: 2505.9862\n",
            "Epoch [100/150], Loss: 6159.3816\n",
            "Epoch [110/150], Loss: 5749.7710\n",
            "Epoch [120/150], Loss: 1848.4242\n",
            "Epoch [130/150], Loss: 7172.8431\n",
            "Epoch [140/150], Loss: 3772.2893\n",
            "Epoch [150/150], Loss: 1953.4268\n",
            "Fold 2, RMSE: 69.03630065917969\n",
            "Epoch [10/150], Loss: 23246.4453\n",
            "Epoch [20/150], Loss: 8891.3660\n",
            "Epoch [30/150], Loss: 11478.0288\n",
            "Epoch [40/150], Loss: 7219.5789\n",
            "Epoch [50/150], Loss: 7704.2568\n",
            "Epoch [60/150], Loss: 4927.3559\n",
            "Epoch [70/150], Loss: 4373.3082\n",
            "Epoch [80/150], Loss: 2173.1979\n",
            "Epoch [90/150], Loss: 2026.5984\n",
            "Epoch [100/150], Loss: 2755.6605\n",
            "Epoch [110/150], Loss: 1620.5804\n",
            "Epoch [120/150], Loss: 2354.6459\n",
            "Epoch [130/150], Loss: 2179.6597\n",
            "Epoch [140/150], Loss: 3212.7583\n",
            "Epoch [150/150], Loss: 591.3818\n",
            "Fold 3, RMSE: 95.23201751708984\n",
            "Epoch [10/150], Loss: 17557.8594\n",
            "Epoch [20/150], Loss: 13679.0879\n",
            "Epoch [30/150], Loss: 13146.2302\n",
            "Epoch [40/150], Loss: 8608.1544\n",
            "Epoch [50/150], Loss: 5089.9927\n",
            "Epoch [60/150], Loss: 4943.2706\n",
            "Epoch [70/150], Loss: 3034.8093\n",
            "Epoch [80/150], Loss: 3344.0349\n",
            "Epoch [90/150], Loss: 2656.1328\n",
            "Epoch [100/150], Loss: 5314.7781\n",
            "Epoch [110/150], Loss: 1739.0040\n",
            "Epoch [120/150], Loss: 3473.5148\n",
            "Epoch [130/150], Loss: 1882.8514\n",
            "Epoch [140/150], Loss: 1093.7703\n",
            "Epoch [150/150], Loss: 2347.1574\n",
            "Fold 4, RMSE: 45.79521179199219\n",
            "Epoch [10/150], Loss: 23303.9668\n",
            "Epoch [20/150], Loss: 14642.2410\n",
            "Epoch [30/150], Loss: 10093.3706\n",
            "Epoch [40/150], Loss: 15484.1091\n",
            "Epoch [50/150], Loss: 7137.7229\n",
            "Epoch [60/150], Loss: 4588.5117\n",
            "Epoch [70/150], Loss: 2618.6365\n",
            "Epoch [80/150], Loss: 3254.3922\n",
            "Epoch [90/150], Loss: 6619.2427\n",
            "Epoch [100/150], Loss: 4364.1588\n",
            "Epoch [110/150], Loss: 6453.7031\n",
            "Epoch [120/150], Loss: 3366.5079\n",
            "Epoch [130/150], Loss: 7421.2278\n",
            "Epoch [140/150], Loss: 2539.5081\n",
            "Epoch [150/150], Loss: 4097.2414\n",
            "Fold 5, RMSE: 49.31279373168945\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 65.32569198608398\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 12580.9253\n",
            "Epoch [20/100], Loss: 8055.7466\n",
            "Epoch [30/100], Loss: 7651.8387\n",
            "Epoch [40/100], Loss: 4164.7964\n",
            "Epoch [50/100], Loss: 8674.1543\n",
            "Epoch [60/100], Loss: 4610.1241\n",
            "Epoch [70/100], Loss: 5596.8185\n",
            "Epoch [80/100], Loss: 4923.6729\n",
            "Epoch [90/100], Loss: 6594.0530\n",
            "Epoch [100/100], Loss: 7218.7882\n",
            "Fold 1, RMSE: 59.850433349609375\n",
            "Epoch [10/100], Loss: 11472.7770\n",
            "Epoch [20/100], Loss: 9293.0052\n",
            "Epoch [30/100], Loss: 6310.7791\n",
            "Epoch [40/100], Loss: 5568.2618\n",
            "Epoch [50/100], Loss: 3075.9291\n",
            "Epoch [60/100], Loss: 7463.9216\n",
            "Epoch [70/100], Loss: 4532.6936\n",
            "Epoch [80/100], Loss: 3446.7386\n",
            "Epoch [90/100], Loss: 3202.3808\n",
            "Epoch [100/100], Loss: 2227.0105\n",
            "Fold 2, RMSE: 64.93517303466797\n",
            "Epoch [10/100], Loss: 16107.0103\n",
            "Epoch [20/100], Loss: 8410.8398\n",
            "Epoch [30/100], Loss: 5860.4395\n",
            "Epoch [40/100], Loss: 5202.4727\n",
            "Epoch [50/100], Loss: 5481.4069\n",
            "Epoch [60/100], Loss: 1815.1170\n",
            "Epoch [70/100], Loss: 4696.2201\n",
            "Epoch [80/100], Loss: 2497.8645\n",
            "Epoch [90/100], Loss: 3545.3394\n",
            "Epoch [100/100], Loss: 707.0819\n",
            "Fold 3, RMSE: 95.91310119628906\n",
            "Epoch [10/100], Loss: 12321.5369\n",
            "Epoch [20/100], Loss: 6853.2126\n",
            "Epoch [30/100], Loss: 5307.5980\n",
            "Epoch [40/100], Loss: 4986.4982\n",
            "Epoch [50/100], Loss: 2609.5502\n",
            "Epoch [60/100], Loss: 4947.9029\n",
            "Epoch [70/100], Loss: 1871.6700\n",
            "Epoch [80/100], Loss: 1069.2734\n",
            "Epoch [90/100], Loss: 11719.4232\n",
            "Epoch [100/100], Loss: 1194.3530\n",
            "Fold 4, RMSE: 36.84482955932617\n",
            "Epoch [10/100], Loss: 12563.6853\n",
            "Epoch [20/100], Loss: 7439.1937\n",
            "Epoch [30/100], Loss: 2439.5650\n",
            "Epoch [40/100], Loss: 1640.0836\n",
            "Epoch [50/100], Loss: 2503.4669\n",
            "Epoch [60/100], Loss: 3438.7682\n",
            "Epoch [70/100], Loss: 1441.6332\n",
            "Epoch [80/100], Loss: 1458.6146\n",
            "Epoch [90/100], Loss: 3102.2739\n",
            "Epoch [100/100], Loss: 6370.5377\n",
            "Fold 5, RMSE: 47.2975959777832\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 60.96822662353516\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 17809.4556\n",
            "Epoch [20/150], Loss: 14837.6671\n",
            "Epoch [30/150], Loss: 8132.8989\n",
            "Epoch [40/150], Loss: 5655.4550\n",
            "Epoch [50/150], Loss: 3132.4545\n",
            "Epoch [60/150], Loss: 3113.4462\n",
            "Epoch [70/150], Loss: 1553.9524\n",
            "Epoch [80/150], Loss: 1845.8456\n",
            "Epoch [90/150], Loss: 1728.2663\n",
            "Epoch [100/150], Loss: 2254.8638\n",
            "Epoch [110/150], Loss: 1422.2554\n",
            "Epoch [120/150], Loss: 5465.1697\n",
            "Epoch [130/150], Loss: 1677.0724\n",
            "Epoch [140/150], Loss: 1643.7278\n",
            "Epoch [150/150], Loss: 2872.2526\n",
            "Fold 1, RMSE: 58.7684326171875\n",
            "Epoch [10/150], Loss: 12201.9517\n",
            "Epoch [20/150], Loss: 6559.9236\n",
            "Epoch [30/150], Loss: 3863.8937\n",
            "Epoch [40/150], Loss: 2402.4239\n",
            "Epoch [50/150], Loss: 3729.0551\n",
            "Epoch [60/150], Loss: 2391.5527\n",
            "Epoch [70/150], Loss: 2258.4543\n",
            "Epoch [80/150], Loss: 1071.0405\n",
            "Epoch [90/150], Loss: 1412.5081\n",
            "Epoch [100/150], Loss: 3250.2399\n",
            "Epoch [110/150], Loss: 3975.3366\n",
            "Epoch [120/150], Loss: 1404.1048\n",
            "Epoch [130/150], Loss: 2779.8647\n",
            "Epoch [140/150], Loss: 1103.8026\n",
            "Epoch [150/150], Loss: 1479.2121\n",
            "Fold 2, RMSE: 67.79443359375\n",
            "Epoch [10/150], Loss: 7780.9639\n",
            "Epoch [20/150], Loss: 5731.3395\n",
            "Epoch [30/150], Loss: 1965.1417\n",
            "Epoch [40/150], Loss: 1556.7706\n",
            "Epoch [50/150], Loss: 5249.3751\n",
            "Epoch [60/150], Loss: 4309.7326\n",
            "Epoch [70/150], Loss: 1859.3760\n",
            "Epoch [80/150], Loss: 823.9373\n",
            "Epoch [90/150], Loss: 3907.0794\n",
            "Epoch [100/150], Loss: 2318.1581\n",
            "Epoch [110/150], Loss: 1009.9596\n",
            "Epoch [120/150], Loss: 2254.7376\n",
            "Epoch [130/150], Loss: 1423.0304\n",
            "Epoch [140/150], Loss: 836.3269\n",
            "Epoch [150/150], Loss: 6136.4683\n",
            "Fold 3, RMSE: 91.67240905761719\n",
            "Epoch [10/150], Loss: 24615.7090\n",
            "Epoch [20/150], Loss: 9867.3489\n",
            "Epoch [30/150], Loss: 7643.4788\n",
            "Epoch [40/150], Loss: 5404.5430\n",
            "Epoch [50/150], Loss: 4088.0563\n",
            "Epoch [60/150], Loss: 2118.3672\n",
            "Epoch [70/150], Loss: 3306.8282\n",
            "Epoch [80/150], Loss: 3040.3098\n",
            "Epoch [90/150], Loss: 2345.0406\n",
            "Epoch [100/150], Loss: 2889.4746\n",
            "Epoch [110/150], Loss: 1517.6911\n",
            "Epoch [120/150], Loss: 1622.3906\n",
            "Epoch [130/150], Loss: 2064.9364\n",
            "Epoch [140/150], Loss: 2337.0546\n",
            "Epoch [150/150], Loss: 1709.5018\n",
            "Fold 4, RMSE: 40.34125518798828\n",
            "Epoch [10/150], Loss: 11109.9553\n",
            "Epoch [20/150], Loss: 9867.0051\n",
            "Epoch [30/150], Loss: 5874.7083\n",
            "Epoch [40/150], Loss: 3245.3032\n",
            "Epoch [50/150], Loss: 1806.5357\n",
            "Epoch [60/150], Loss: 4184.5261\n",
            "Epoch [70/150], Loss: 1821.4495\n",
            "Epoch [80/150], Loss: 1624.0702\n",
            "Epoch [90/150], Loss: 3259.9806\n",
            "Epoch [100/150], Loss: 2080.3164\n",
            "Epoch [110/150], Loss: 2033.8995\n",
            "Epoch [120/150], Loss: 2860.6371\n",
            "Epoch [130/150], Loss: 1465.8029\n",
            "Epoch [140/150], Loss: 1802.1667\n",
            "Epoch [150/150], Loss: 1324.2119\n",
            "Fold 5, RMSE: 45.212303161621094\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 60.75776672363281\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 16620.5859\n",
            "Epoch [20/100], Loss: 25054.6040\n",
            "Epoch [30/100], Loss: 19208.8066\n",
            "Epoch [40/100], Loss: 17376.9043\n",
            "Epoch [50/100], Loss: 21324.1846\n",
            "Epoch [60/100], Loss: 22718.2202\n",
            "Epoch [70/100], Loss: 26289.8745\n",
            "Epoch [80/100], Loss: 17628.9863\n",
            "Epoch [90/100], Loss: 17832.8149\n",
            "Epoch [100/100], Loss: 16668.7529\n",
            "Fold 1, RMSE: 67.53311920166016\n",
            "Epoch [10/100], Loss: 166718.8633\n",
            "Epoch [20/100], Loss: 33171.2480\n",
            "Epoch [30/100], Loss: 14793.5496\n",
            "Epoch [40/100], Loss: 11431.9622\n",
            "Epoch [50/100], Loss: 15346.3896\n",
            "Epoch [60/100], Loss: 10888.0571\n",
            "Epoch [70/100], Loss: 10157.9224\n",
            "Epoch [80/100], Loss: 8429.9258\n",
            "Epoch [90/100], Loss: 7875.6628\n",
            "Epoch [100/100], Loss: 8768.1597\n",
            "Fold 2, RMSE: 65.73574829101562\n",
            "Epoch [10/100], Loss: 149098.3125\n",
            "Epoch [20/100], Loss: 19584.2354\n",
            "Epoch [30/100], Loss: 10064.9821\n",
            "Epoch [40/100], Loss: 7302.1830\n",
            "Epoch [50/100], Loss: 8555.9307\n",
            "Epoch [60/100], Loss: 7796.0974\n",
            "Epoch [70/100], Loss: 7372.6052\n",
            "Epoch [80/100], Loss: 6845.5822\n",
            "Epoch [90/100], Loss: 8983.6027\n",
            "Epoch [100/100], Loss: 7328.8291\n",
            "Fold 3, RMSE: 86.98204040527344\n",
            "Epoch [10/100], Loss: 407428.8789\n",
            "Epoch [20/100], Loss: 89611.2070\n",
            "Epoch [30/100], Loss: 33808.9531\n",
            "Epoch [40/100], Loss: 37640.6104\n",
            "Epoch [50/100], Loss: 28299.2417\n",
            "Epoch [60/100], Loss: 31988.3926\n",
            "Epoch [70/100], Loss: 11689.4231\n",
            "Epoch [80/100], Loss: 11371.6305\n",
            "Epoch [90/100], Loss: 11035.9781\n",
            "Epoch [100/100], Loss: 20151.9031\n",
            "Fold 4, RMSE: 34.762481689453125\n",
            "Epoch [10/100], Loss: 18218.6790\n",
            "Epoch [20/100], Loss: 19149.5234\n",
            "Epoch [30/100], Loss: 18181.7258\n",
            "Epoch [40/100], Loss: 27724.2383\n",
            "Epoch [50/100], Loss: 18790.7883\n",
            "Epoch [60/100], Loss: 20098.0508\n",
            "Epoch [70/100], Loss: 21522.2842\n",
            "Epoch [80/100], Loss: 18426.6272\n",
            "Epoch [90/100], Loss: 17807.8828\n",
            "Epoch [100/100], Loss: 21558.3342\n",
            "Fold 5, RMSE: 57.921531677246094\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 62.58698425292969\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 138003.3574\n",
            "Epoch [20/150], Loss: 22269.7891\n",
            "Epoch [30/150], Loss: 19033.4731\n",
            "Epoch [40/150], Loss: 12166.8022\n",
            "Epoch [50/150], Loss: 14251.2205\n",
            "Epoch [60/150], Loss: 10694.9495\n",
            "Epoch [70/150], Loss: 9133.4718\n",
            "Epoch [80/150], Loss: 8738.3988\n",
            "Epoch [90/150], Loss: 11410.2009\n",
            "Epoch [100/150], Loss: 9723.8092\n",
            "Epoch [110/150], Loss: 8063.7341\n",
            "Epoch [120/150], Loss: 6824.7184\n",
            "Epoch [130/150], Loss: 6935.0684\n",
            "Epoch [140/150], Loss: 2364.6288\n",
            "Epoch [150/150], Loss: 3422.2025\n",
            "Fold 1, RMSE: 61.704246520996094\n",
            "Epoch [10/150], Loss: 20366.1676\n",
            "Epoch [20/150], Loss: 17252.9438\n",
            "Epoch [30/150], Loss: 27110.6047\n",
            "Epoch [40/150], Loss: 18651.1860\n",
            "Epoch [50/150], Loss: 17559.1973\n",
            "Epoch [60/150], Loss: 15761.5244\n",
            "Epoch [70/150], Loss: 16084.0066\n",
            "Epoch [80/150], Loss: 13727.0285\n",
            "Epoch [90/150], Loss: 25071.3049\n",
            "Epoch [100/150], Loss: 16839.9341\n",
            "Epoch [110/150], Loss: 13769.2252\n",
            "Epoch [120/150], Loss: 15099.8459\n",
            "Epoch [130/150], Loss: 15574.2061\n",
            "Epoch [140/150], Loss: 17246.5640\n",
            "Epoch [150/150], Loss: 17403.7937\n",
            "Fold 2, RMSE: 87.51116180419922\n",
            "Epoch [10/150], Loss: 14314.2598\n",
            "Epoch [20/150], Loss: 15504.8448\n",
            "Epoch [30/150], Loss: 11622.0659\n",
            "Epoch [40/150], Loss: 12594.9446\n",
            "Epoch [50/150], Loss: 14160.7837\n",
            "Epoch [60/150], Loss: 14261.1289\n",
            "Epoch [70/150], Loss: 12802.0876\n",
            "Epoch [80/150], Loss: 12236.7473\n",
            "Epoch [90/150], Loss: 14408.4736\n",
            "Epoch [100/150], Loss: 13618.7507\n",
            "Epoch [110/150], Loss: 12378.6235\n",
            "Epoch [120/150], Loss: 16276.4841\n",
            "Epoch [130/150], Loss: 15569.4353\n",
            "Epoch [140/150], Loss: 15833.7722\n",
            "Epoch [150/150], Loss: 11981.9501\n",
            "Fold 3, RMSE: 109.48526000976562\n",
            "Epoch [10/150], Loss: 135454.1348\n",
            "Epoch [20/150], Loss: 20712.8013\n",
            "Epoch [30/150], Loss: 16544.2437\n",
            "Epoch [40/150], Loss: 11685.8757\n",
            "Epoch [50/150], Loss: 15206.9299\n",
            "Epoch [60/150], Loss: 12523.9922\n",
            "Epoch [70/150], Loss: 11166.1414\n",
            "Epoch [80/150], Loss: 9191.1217\n",
            "Epoch [90/150], Loss: 8779.4098\n",
            "Epoch [100/150], Loss: 8176.6831\n",
            "Epoch [110/150], Loss: 8997.9370\n",
            "Epoch [120/150], Loss: 3684.3434\n",
            "Epoch [130/150], Loss: 3971.3996\n",
            "Epoch [140/150], Loss: 5475.2291\n",
            "Epoch [150/150], Loss: 4723.7332\n",
            "Fold 4, RMSE: 36.32645034790039\n",
            "Epoch [10/150], Loss: 320720.9688\n",
            "Epoch [20/150], Loss: 68486.0117\n",
            "Epoch [30/150], Loss: 27939.9600\n",
            "Epoch [40/150], Loss: 20970.1689\n",
            "Epoch [50/150], Loss: 17419.4121\n",
            "Epoch [60/150], Loss: 14949.9961\n",
            "Epoch [70/150], Loss: 15229.1440\n",
            "Epoch [80/150], Loss: 30651.0750\n",
            "Epoch [90/150], Loss: 14358.0291\n",
            "Epoch [100/150], Loss: 11069.3873\n",
            "Epoch [110/150], Loss: 12207.5947\n",
            "Epoch [120/150], Loss: 10618.8110\n",
            "Epoch [130/150], Loss: 10710.0862\n",
            "Epoch [140/150], Loss: 10829.8473\n",
            "Epoch [150/150], Loss: 10659.8596\n",
            "Fold 5, RMSE: 46.03077697753906\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 68.21157913208008\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 15023.7461\n",
            "Epoch [20/100], Loss: 8317.2673\n",
            "Epoch [30/100], Loss: 8375.7737\n",
            "Epoch [40/100], Loss: 10460.7593\n",
            "Epoch [50/100], Loss: 2362.2489\n",
            "Epoch [60/100], Loss: 5932.7629\n",
            "Epoch [70/100], Loss: 5651.9921\n",
            "Epoch [80/100], Loss: 1640.1226\n",
            "Epoch [90/100], Loss: 14575.8967\n",
            "Epoch [100/100], Loss: 8703.8636\n",
            "Fold 1, RMSE: 52.90664291381836\n",
            "Epoch [10/100], Loss: 58378.7334\n",
            "Epoch [20/100], Loss: 15476.1350\n",
            "Epoch [30/100], Loss: 9660.2260\n",
            "Epoch [40/100], Loss: 11257.7490\n",
            "Epoch [50/100], Loss: 10209.5865\n",
            "Epoch [60/100], Loss: 10640.4333\n",
            "Epoch [70/100], Loss: 9460.3704\n",
            "Epoch [80/100], Loss: 11114.3733\n",
            "Epoch [90/100], Loss: 10799.5198\n",
            "Epoch [100/100], Loss: 12166.9175\n",
            "Fold 2, RMSE: 65.93311309814453\n",
            "Epoch [10/100], Loss: 14615.9829\n",
            "Epoch [20/100], Loss: 10315.8403\n",
            "Epoch [30/100], Loss: 7207.6172\n",
            "Epoch [40/100], Loss: 8039.2791\n",
            "Epoch [50/100], Loss: 7667.0549\n",
            "Epoch [60/100], Loss: 5696.2665\n",
            "Epoch [70/100], Loss: 4398.3855\n",
            "Epoch [80/100], Loss: 7701.3298\n",
            "Epoch [90/100], Loss: 5682.8717\n",
            "Epoch [100/100], Loss: 5681.1827\n",
            "Fold 3, RMSE: 93.06889343261719\n",
            "Epoch [10/100], Loss: 22512.7957\n",
            "Epoch [20/100], Loss: 13815.3564\n",
            "Epoch [30/100], Loss: 11382.9958\n",
            "Epoch [40/100], Loss: 9102.6979\n",
            "Epoch [50/100], Loss: 8474.7808\n",
            "Epoch [60/100], Loss: 8383.7266\n",
            "Epoch [70/100], Loss: 4955.5816\n",
            "Epoch [80/100], Loss: 11194.5339\n",
            "Epoch [90/100], Loss: 7552.8932\n",
            "Epoch [100/100], Loss: 8608.2358\n",
            "Fold 4, RMSE: 43.29374313354492\n",
            "Epoch [10/100], Loss: 40436.1104\n",
            "Epoch [20/100], Loss: 11555.3762\n",
            "Epoch [30/100], Loss: 11127.9207\n",
            "Epoch [40/100], Loss: 8765.1998\n",
            "Epoch [50/100], Loss: 11250.1245\n",
            "Epoch [60/100], Loss: 7501.9246\n",
            "Epoch [70/100], Loss: 8427.8324\n",
            "Epoch [80/100], Loss: 9264.1577\n",
            "Epoch [90/100], Loss: 5082.9842\n",
            "Epoch [100/100], Loss: 5042.6331\n",
            "Fold 5, RMSE: 50.86993408203125\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 61.21446533203125\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 24203.6382\n",
            "Epoch [20/150], Loss: 14960.2566\n",
            "Epoch [30/150], Loss: 9411.8887\n",
            "Epoch [40/150], Loss: 7851.7639\n",
            "Epoch [50/150], Loss: 5583.7191\n",
            "Epoch [60/150], Loss: 4914.2661\n",
            "Epoch [70/150], Loss: 3452.5057\n",
            "Epoch [80/150], Loss: 7440.0948\n",
            "Epoch [90/150], Loss: 7684.8604\n",
            "Epoch [100/150], Loss: 9420.1855\n",
            "Epoch [110/150], Loss: 5932.2156\n",
            "Epoch [120/150], Loss: 13193.4695\n",
            "Epoch [130/150], Loss: 12269.5029\n",
            "Epoch [140/150], Loss: 7444.3601\n",
            "Epoch [150/150], Loss: 6067.8215\n",
            "Fold 1, RMSE: 62.69381332397461\n",
            "Epoch [10/150], Loss: 20647.0479\n",
            "Epoch [20/150], Loss: 9482.0156\n",
            "Epoch [30/150], Loss: 9898.4127\n",
            "Epoch [40/150], Loss: 7943.2411\n",
            "Epoch [50/150], Loss: 8858.6354\n",
            "Epoch [60/150], Loss: 4406.9626\n",
            "Epoch [70/150], Loss: 6557.4436\n",
            "Epoch [80/150], Loss: 5333.0885\n",
            "Epoch [90/150], Loss: 2781.0862\n",
            "Epoch [100/150], Loss: 3038.2417\n",
            "Epoch [110/150], Loss: 3146.5224\n",
            "Epoch [120/150], Loss: 2621.8937\n",
            "Epoch [130/150], Loss: 849.3116\n",
            "Epoch [140/150], Loss: 1839.3911\n",
            "Epoch [150/150], Loss: 4919.8674\n",
            "Fold 2, RMSE: 63.744544982910156\n",
            "Epoch [10/150], Loss: 34062.4771\n",
            "Epoch [20/150], Loss: 12391.0605\n",
            "Epoch [30/150], Loss: 7398.0089\n",
            "Epoch [40/150], Loss: 9741.7988\n",
            "Epoch [50/150], Loss: 6721.5365\n",
            "Epoch [60/150], Loss: 12120.1647\n",
            "Epoch [70/150], Loss: 7062.7147\n",
            "Epoch [80/150], Loss: 9102.3782\n",
            "Epoch [90/150], Loss: 6189.9487\n",
            "Epoch [100/150], Loss: 7073.1350\n",
            "Epoch [110/150], Loss: 5658.0636\n",
            "Epoch [120/150], Loss: 6237.5555\n",
            "Epoch [130/150], Loss: 2984.2808\n",
            "Epoch [140/150], Loss: 3423.0682\n",
            "Epoch [150/150], Loss: 4096.7825\n",
            "Fold 3, RMSE: 91.53626251220703\n",
            "Epoch [10/150], Loss: 65641.7539\n",
            "Epoch [20/150], Loss: 20125.1687\n",
            "Epoch [30/150], Loss: 14202.9084\n",
            "Epoch [40/150], Loss: 15520.6562\n",
            "Epoch [50/150], Loss: 10135.7406\n",
            "Epoch [60/150], Loss: 24177.9844\n",
            "Epoch [70/150], Loss: 16072.5862\n",
            "Epoch [80/150], Loss: 13179.6604\n",
            "Epoch [90/150], Loss: 12731.2957\n",
            "Epoch [100/150], Loss: 11830.3462\n",
            "Epoch [110/150], Loss: 14385.3286\n",
            "Epoch [120/150], Loss: 12406.1423\n",
            "Epoch [130/150], Loss: 10322.7916\n",
            "Epoch [140/150], Loss: 14897.5220\n",
            "Epoch [150/150], Loss: 9201.6798\n",
            "Fold 4, RMSE: 37.23958969116211\n",
            "Epoch [10/150], Loss: 89369.6680\n",
            "Epoch [20/150], Loss: 18188.2598\n",
            "Epoch [30/150], Loss: 12471.0251\n",
            "Epoch [40/150], Loss: 10428.9990\n",
            "Epoch [50/150], Loss: 14193.6567\n",
            "Epoch [60/150], Loss: 13554.7512\n",
            "Epoch [70/150], Loss: 11741.5670\n",
            "Epoch [80/150], Loss: 11720.6560\n",
            "Epoch [90/150], Loss: 11871.0271\n",
            "Epoch [100/150], Loss: 16165.4138\n",
            "Epoch [110/150], Loss: 10241.0961\n",
            "Epoch [120/150], Loss: 10986.5903\n",
            "Epoch [130/150], Loss: 10425.3375\n",
            "Epoch [140/150], Loss: 11295.7694\n",
            "Epoch [150/150], Loss: 14862.2417\n",
            "Fold 5, RMSE: 46.68946075439453\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 60.380734252929685\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 172135.4238\n",
            "Epoch [20/100], Loss: 27764.3418\n",
            "Epoch [30/100], Loss: 18123.3560\n",
            "Epoch [40/100], Loss: 18518.8643\n",
            "Epoch [50/100], Loss: 23688.9695\n",
            "Epoch [60/100], Loss: 14706.9073\n",
            "Epoch [70/100], Loss: 17111.6179\n",
            "Epoch [80/100], Loss: 11258.6802\n",
            "Epoch [90/100], Loss: 12774.5808\n",
            "Epoch [100/100], Loss: 12410.5378\n",
            "Fold 1, RMSE: 48.79964828491211\n",
            "Epoch [10/100], Loss: 177803.6875\n",
            "Epoch [20/100], Loss: 39677.8877\n",
            "Epoch [30/100], Loss: 15030.9302\n",
            "Epoch [40/100], Loss: 13440.5100\n",
            "Epoch [50/100], Loss: 16527.1042\n",
            "Epoch [60/100], Loss: 10755.6287\n",
            "Epoch [70/100], Loss: 8806.7085\n",
            "Epoch [80/100], Loss: 8598.2938\n",
            "Epoch [90/100], Loss: 10207.7146\n",
            "Epoch [100/100], Loss: 11809.3276\n",
            "Fold 2, RMSE: 67.6901626586914\n",
            "Epoch [10/100], Loss: 40287.7773\n",
            "Epoch [20/100], Loss: 9914.6747\n",
            "Epoch [30/100], Loss: 13272.6729\n",
            "Epoch [40/100], Loss: 6152.6505\n",
            "Epoch [50/100], Loss: 8764.6721\n",
            "Epoch [60/100], Loss: 7445.2579\n",
            "Epoch [70/100], Loss: 7149.7917\n",
            "Epoch [80/100], Loss: 4038.4494\n",
            "Epoch [90/100], Loss: 3648.8195\n",
            "Epoch [100/100], Loss: 2534.0615\n",
            "Fold 3, RMSE: 90.70757293701172\n",
            "Epoch [10/100], Loss: 194159.5898\n",
            "Epoch [20/100], Loss: 25411.4888\n",
            "Epoch [30/100], Loss: 19841.0793\n",
            "Epoch [40/100], Loss: 12450.5837\n",
            "Epoch [50/100], Loss: 14567.2993\n",
            "Epoch [60/100], Loss: 10551.7737\n",
            "Epoch [70/100], Loss: 8619.6359\n",
            "Epoch [80/100], Loss: 9371.0228\n",
            "Epoch [90/100], Loss: 9892.0258\n",
            "Epoch [100/100], Loss: 5302.8228\n",
            "Fold 4, RMSE: 39.264278411865234\n",
            "Epoch [10/100], Loss: 198024.5352\n",
            "Epoch [20/100], Loss: 37903.6033\n",
            "Epoch [30/100], Loss: 22575.7070\n",
            "Epoch [40/100], Loss: 13759.4200\n",
            "Epoch [50/100], Loss: 15216.0542\n",
            "Epoch [60/100], Loss: 22162.8481\n",
            "Epoch [70/100], Loss: 12828.1973\n",
            "Epoch [80/100], Loss: 15915.2449\n",
            "Epoch [90/100], Loss: 13669.5647\n",
            "Epoch [100/100], Loss: 11110.9304\n",
            "Fold 5, RMSE: 48.11345291137695\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 58.91502304077149\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 343798.5195\n",
            "Epoch [20/150], Loss: 76310.5684\n",
            "Epoch [30/150], Loss: 22550.1201\n",
            "Epoch [40/150], Loss: 25755.1953\n",
            "Epoch [50/150], Loss: 18672.7158\n",
            "Epoch [60/150], Loss: 17652.2229\n",
            "Epoch [70/150], Loss: 16010.6106\n",
            "Epoch [80/150], Loss: 12553.2209\n",
            "Epoch [90/150], Loss: 8060.2460\n",
            "Epoch [100/150], Loss: 10546.4158\n",
            "Epoch [110/150], Loss: 11323.8403\n",
            "Epoch [120/150], Loss: 7194.9801\n",
            "Epoch [130/150], Loss: 6263.0016\n",
            "Epoch [140/150], Loss: 4551.8428\n",
            "Epoch [150/150], Loss: 6272.6816\n",
            "Fold 1, RMSE: 52.86407470703125\n",
            "Epoch [10/150], Loss: 236513.7617\n",
            "Epoch [20/150], Loss: 48513.4844\n",
            "Epoch [30/150], Loss: 32974.3008\n",
            "Epoch [40/150], Loss: 16738.6223\n",
            "Epoch [50/150], Loss: 14172.3372\n",
            "Epoch [60/150], Loss: 13341.1331\n",
            "Epoch [70/150], Loss: 18942.9843\n",
            "Epoch [80/150], Loss: 10861.9512\n",
            "Epoch [90/150], Loss: 13742.4132\n",
            "Epoch [100/150], Loss: 9936.4553\n",
            "Epoch [110/150], Loss: 7428.6930\n",
            "Epoch [120/150], Loss: 11412.3503\n",
            "Epoch [130/150], Loss: 9266.6665\n",
            "Epoch [140/150], Loss: 8614.8613\n",
            "Epoch [150/150], Loss: 17013.2262\n",
            "Fold 2, RMSE: 67.51900482177734\n",
            "Epoch [10/150], Loss: 308441.2344\n",
            "Epoch [20/150], Loss: 37393.2168\n",
            "Epoch [30/150], Loss: 23454.9502\n",
            "Epoch [40/150], Loss: 18184.7102\n",
            "Epoch [50/150], Loss: 16574.6621\n",
            "Epoch [60/150], Loss: 12211.6947\n",
            "Epoch [70/150], Loss: 12259.7761\n",
            "Epoch [80/150], Loss: 11827.4062\n",
            "Epoch [90/150], Loss: 7447.1947\n",
            "Epoch [100/150], Loss: 8729.5078\n",
            "Epoch [110/150], Loss: 6492.9769\n",
            "Epoch [120/150], Loss: 8461.1448\n",
            "Epoch [130/150], Loss: 5355.4459\n",
            "Epoch [140/150], Loss: 5908.2725\n",
            "Epoch [150/150], Loss: 5318.4277\n",
            "Fold 3, RMSE: 92.0091323852539\n",
            "Epoch [10/150], Loss: 207690.8164\n",
            "Epoch [20/150], Loss: 45696.4541\n",
            "Epoch [30/150], Loss: 39511.3457\n",
            "Epoch [40/150], Loss: 32017.1245\n",
            "Epoch [50/150], Loss: 26835.1646\n",
            "Epoch [60/150], Loss: 20597.9795\n",
            "Epoch [70/150], Loss: 18761.9761\n",
            "Epoch [80/150], Loss: 16817.0457\n",
            "Epoch [90/150], Loss: 14642.4099\n",
            "Epoch [100/150], Loss: 19090.1582\n",
            "Epoch [110/150], Loss: 16046.3809\n",
            "Epoch [120/150], Loss: 20063.8467\n",
            "Epoch [130/150], Loss: 10488.0441\n",
            "Epoch [140/150], Loss: 15523.8403\n",
            "Epoch [150/150], Loss: 13524.2710\n",
            "Fold 4, RMSE: 34.560699462890625\n",
            "Epoch [10/150], Loss: 491694.6250\n",
            "Epoch [20/150], Loss: 64047.2397\n",
            "Epoch [30/150], Loss: 29058.9775\n",
            "Epoch [40/150], Loss: 29677.4170\n",
            "Epoch [50/150], Loss: 20075.7590\n",
            "Epoch [60/150], Loss: 17088.6519\n",
            "Epoch [70/150], Loss: 15271.1025\n",
            "Epoch [80/150], Loss: 13332.7234\n",
            "Epoch [90/150], Loss: 11646.2762\n",
            "Epoch [100/150], Loss: 15220.3982\n",
            "Epoch [110/150], Loss: 14406.0400\n",
            "Epoch [120/150], Loss: 16099.9910\n",
            "Epoch [130/150], Loss: 10526.6133\n",
            "Epoch [140/150], Loss: 12884.4104\n",
            "Epoch [150/150], Loss: 9312.4879\n",
            "Fold 5, RMSE: 47.812225341796875\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 58.95302734375\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 19991.9231\n",
            "Epoch [20/100], Loss: 12054.6448\n",
            "Epoch [30/100], Loss: 10545.9382\n",
            "Epoch [40/100], Loss: 11161.3193\n",
            "Epoch [50/100], Loss: 9287.9548\n",
            "Epoch [60/100], Loss: 8712.8738\n",
            "Epoch [70/100], Loss: 7857.6134\n",
            "Epoch [80/100], Loss: 8153.0637\n",
            "Epoch [90/100], Loss: 8212.2914\n",
            "Epoch [100/100], Loss: 6764.3096\n",
            "Fold 1, RMSE: 54.61018753051758\n",
            "Epoch [10/100], Loss: 22263.5776\n",
            "Epoch [20/100], Loss: 15115.0425\n",
            "Epoch [30/100], Loss: 10404.0537\n",
            "Epoch [40/100], Loss: 11330.5125\n",
            "Epoch [50/100], Loss: 10341.2054\n",
            "Epoch [60/100], Loss: 12574.5371\n",
            "Epoch [70/100], Loss: 10180.2339\n",
            "Epoch [80/100], Loss: 8358.2155\n",
            "Epoch [90/100], Loss: 8853.4911\n",
            "Epoch [100/100], Loss: 7463.8769\n",
            "Fold 2, RMSE: 68.64514923095703\n",
            "Epoch [10/100], Loss: 38348.2139\n",
            "Epoch [20/100], Loss: 7760.8932\n",
            "Epoch [30/100], Loss: 11659.4763\n",
            "Epoch [40/100], Loss: 12622.9773\n",
            "Epoch [50/100], Loss: 7847.4504\n",
            "Epoch [60/100], Loss: 9546.8567\n",
            "Epoch [70/100], Loss: 7545.1351\n",
            "Epoch [80/100], Loss: 7810.7036\n",
            "Epoch [90/100], Loss: 8972.2463\n",
            "Epoch [100/100], Loss: 7849.0051\n",
            "Fold 3, RMSE: 89.35520935058594\n",
            "Epoch [10/100], Loss: 36723.1367\n",
            "Epoch [20/100], Loss: 18965.1230\n",
            "Epoch [30/100], Loss: 12402.5386\n",
            "Epoch [40/100], Loss: 14775.9214\n",
            "Epoch [50/100], Loss: 12745.4827\n",
            "Epoch [60/100], Loss: 16022.8213\n",
            "Epoch [70/100], Loss: 13258.1316\n",
            "Epoch [80/100], Loss: 14531.6792\n",
            "Epoch [90/100], Loss: 13166.7783\n",
            "Epoch [100/100], Loss: 12270.3152\n",
            "Fold 4, RMSE: 36.21934127807617\n",
            "Epoch [10/100], Loss: 23686.2959\n",
            "Epoch [20/100], Loss: 12726.3821\n",
            "Epoch [30/100], Loss: 15956.2457\n",
            "Epoch [40/100], Loss: 13308.8064\n",
            "Epoch [50/100], Loss: 13385.4055\n",
            "Epoch [60/100], Loss: 9847.9192\n",
            "Epoch [70/100], Loss: 7746.1898\n",
            "Epoch [80/100], Loss: 11381.5144\n",
            "Epoch [90/100], Loss: 7075.3765\n",
            "Epoch [100/100], Loss: 6693.6581\n",
            "Fold 5, RMSE: 47.02986145019531\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 59.171949768066405\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 44095.1270\n",
            "Epoch [20/150], Loss: 20088.0259\n",
            "Epoch [30/150], Loss: 13912.6704\n",
            "Epoch [40/150], Loss: 12129.6978\n",
            "Epoch [50/150], Loss: 10684.6259\n",
            "Epoch [60/150], Loss: 16180.5447\n",
            "Epoch [70/150], Loss: 14029.4736\n",
            "Epoch [80/150], Loss: 10454.6848\n",
            "Epoch [90/150], Loss: 10739.9341\n",
            "Epoch [100/150], Loss: 12915.4276\n",
            "Epoch [110/150], Loss: 9430.9465\n",
            "Epoch [120/150], Loss: 7094.0653\n",
            "Epoch [130/150], Loss: 8760.8074\n",
            "Epoch [140/150], Loss: 9349.9436\n",
            "Epoch [150/150], Loss: 6212.8893\n",
            "Fold 1, RMSE: 57.68429946899414\n",
            "Epoch [10/150], Loss: 12163.7045\n",
            "Epoch [20/150], Loss: 9066.3792\n",
            "Epoch [30/150], Loss: 6599.4742\n",
            "Epoch [40/150], Loss: 2832.4857\n",
            "Epoch [50/150], Loss: 8546.9031\n",
            "Epoch [60/150], Loss: 7802.1683\n",
            "Epoch [70/150], Loss: 8698.8561\n",
            "Epoch [80/150], Loss: 12161.1724\n",
            "Epoch [90/150], Loss: 5768.6252\n",
            "Epoch [100/150], Loss: 5097.2060\n",
            "Epoch [110/150], Loss: 4949.4851\n",
            "Epoch [120/150], Loss: 4821.9246\n",
            "Epoch [130/150], Loss: 3884.1495\n",
            "Epoch [140/150], Loss: 4730.5390\n",
            "Epoch [150/150], Loss: 2271.8516\n",
            "Fold 2, RMSE: 63.189453125\n",
            "Epoch [10/150], Loss: 19924.1060\n",
            "Epoch [20/150], Loss: 13000.3516\n",
            "Epoch [30/150], Loss: 13930.7966\n",
            "Epoch [40/150], Loss: 8568.7830\n",
            "Epoch [50/150], Loss: 9623.1616\n",
            "Epoch [60/150], Loss: 9434.5637\n",
            "Epoch [70/150], Loss: 9104.1797\n",
            "Epoch [80/150], Loss: 10524.2358\n",
            "Epoch [90/150], Loss: 7927.7599\n",
            "Epoch [100/150], Loss: 8822.8857\n",
            "Epoch [110/150], Loss: 6437.8277\n",
            "Epoch [120/150], Loss: 6786.4143\n",
            "Epoch [130/150], Loss: 8172.4321\n",
            "Epoch [140/150], Loss: 7064.7733\n",
            "Epoch [150/150], Loss: 8334.1951\n",
            "Fold 3, RMSE: 91.76586151123047\n",
            "Epoch [10/150], Loss: 14343.3943\n",
            "Epoch [20/150], Loss: 11591.4734\n",
            "Epoch [30/150], Loss: 11967.4873\n",
            "Epoch [40/150], Loss: 12618.3772\n",
            "Epoch [50/150], Loss: 8158.5864\n",
            "Epoch [60/150], Loss: 6260.8730\n",
            "Epoch [70/150], Loss: 5294.1033\n",
            "Epoch [80/150], Loss: 3685.9919\n",
            "Epoch [90/150], Loss: 2913.6219\n",
            "Epoch [100/150], Loss: 2560.7928\n",
            "Epoch [110/150], Loss: 4425.8135\n",
            "Epoch [120/150], Loss: 2874.4745\n",
            "Epoch [130/150], Loss: 1689.7735\n",
            "Epoch [140/150], Loss: 2693.6429\n",
            "Epoch [150/150], Loss: 2087.6669\n",
            "Fold 4, RMSE: 36.68360137939453\n",
            "Epoch [10/150], Loss: 55387.6084\n",
            "Epoch [20/150], Loss: 14694.0386\n",
            "Epoch [30/150], Loss: 14977.9854\n",
            "Epoch [40/150], Loss: 11320.4182\n",
            "Epoch [50/150], Loss: 13356.9504\n",
            "Epoch [60/150], Loss: 11406.4229\n",
            "Epoch [70/150], Loss: 9909.9843\n",
            "Epoch [80/150], Loss: 13392.5646\n",
            "Epoch [90/150], Loss: 8467.0894\n",
            "Epoch [100/150], Loss: 11518.1843\n",
            "Epoch [110/150], Loss: 6822.4761\n",
            "Epoch [120/150], Loss: 8495.0923\n",
            "Epoch [130/150], Loss: 5043.4798\n",
            "Epoch [140/150], Loss: 5703.5916\n",
            "Epoch [150/150], Loss: 6585.9291\n",
            "Fold 5, RMSE: 43.962711334228516\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 58.65718536376953\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 227249.7578\n",
            "Epoch [20/100], Loss: 35344.4575\n",
            "Epoch [30/100], Loss: 63028.9727\n",
            "Epoch [40/100], Loss: 20589.7900\n",
            "Epoch [50/100], Loss: 26472.3726\n",
            "Epoch [60/100], Loss: 15442.6553\n",
            "Epoch [70/100], Loss: 17814.0178\n",
            "Epoch [80/100], Loss: 12687.8960\n",
            "Epoch [90/100], Loss: 20996.9766\n",
            "Epoch [100/100], Loss: 13512.6064\n",
            "Fold 1, RMSE: 48.08700180053711\n",
            "Epoch [10/100], Loss: 196048.5938\n",
            "Epoch [20/100], Loss: 31787.9131\n",
            "Epoch [30/100], Loss: 22861.7573\n",
            "Epoch [40/100], Loss: 18664.7874\n",
            "Epoch [50/100], Loss: 21381.2578\n",
            "Epoch [60/100], Loss: 11011.2888\n",
            "Epoch [70/100], Loss: 16208.9480\n",
            "Epoch [80/100], Loss: 12268.8710\n",
            "Epoch [90/100], Loss: 14474.0081\n",
            "Epoch [100/100], Loss: 11432.1646\n",
            "Fold 2, RMSE: 67.1336669921875\n",
            "Epoch [10/100], Loss: 343028.9062\n",
            "Epoch [20/100], Loss: 36611.1865\n",
            "Epoch [30/100], Loss: 15068.2056\n",
            "Epoch [40/100], Loss: 16042.2542\n",
            "Epoch [50/100], Loss: 7554.3381\n",
            "Epoch [60/100], Loss: 10819.1853\n",
            "Epoch [70/100], Loss: 10923.1116\n",
            "Epoch [80/100], Loss: 9461.8949\n",
            "Epoch [90/100], Loss: 7709.8312\n",
            "Epoch [100/100], Loss: 5554.1222\n",
            "Fold 3, RMSE: 89.65196990966797\n",
            "Epoch [10/100], Loss: 191999.4922\n",
            "Epoch [20/100], Loss: 40386.6523\n",
            "Epoch [30/100], Loss: 23642.6409\n",
            "Epoch [40/100], Loss: 19468.1523\n",
            "Epoch [50/100], Loss: 18458.9534\n",
            "Epoch [60/100], Loss: 13909.9250\n",
            "Epoch [70/100], Loss: 15994.8560\n",
            "Epoch [80/100], Loss: 10424.1060\n",
            "Epoch [90/100], Loss: 16689.5168\n",
            "Epoch [100/100], Loss: 12907.3899\n",
            "Fold 4, RMSE: 36.488765716552734\n",
            "Epoch [10/100], Loss: 172891.2656\n",
            "Epoch [20/100], Loss: 94348.1250\n",
            "Epoch [30/100], Loss: 33636.3535\n",
            "Epoch [40/100], Loss: 21474.8623\n",
            "Epoch [50/100], Loss: 17511.9512\n",
            "Epoch [60/100], Loss: 12740.7273\n",
            "Epoch [70/100], Loss: 10760.2098\n",
            "Epoch [80/100], Loss: 12622.3286\n",
            "Epoch [90/100], Loss: 14768.1104\n",
            "Epoch [100/100], Loss: 16059.3601\n",
            "Fold 5, RMSE: 46.59756851196289\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 57.59179458618164\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 27440.3516\n",
            "Epoch [20/150], Loss: 16344.7139\n",
            "Epoch [30/150], Loss: 17559.6934\n",
            "Epoch [40/150], Loss: 9434.1423\n",
            "Epoch [50/150], Loss: 12973.0186\n",
            "Epoch [60/150], Loss: 10582.7437\n",
            "Epoch [70/150], Loss: 8652.6094\n",
            "Epoch [80/150], Loss: 5262.3506\n",
            "Epoch [90/150], Loss: 5551.8641\n",
            "Epoch [100/150], Loss: 2023.0640\n",
            "Epoch [110/150], Loss: 3418.1013\n",
            "Epoch [120/150], Loss: 2526.0782\n",
            "Epoch [130/150], Loss: 3420.5270\n",
            "Epoch [140/150], Loss: 1747.8778\n",
            "Epoch [150/150], Loss: 3634.5906\n",
            "Fold 1, RMSE: 61.297420501708984\n",
            "Epoch [10/150], Loss: 85878.3105\n",
            "Epoch [20/150], Loss: 30254.1934\n",
            "Epoch [30/150], Loss: 14632.4497\n",
            "Epoch [40/150], Loss: 15060.6841\n",
            "Epoch [50/150], Loss: 17013.7854\n",
            "Epoch [60/150], Loss: 15763.9385\n",
            "Epoch [70/150], Loss: 12503.8176\n",
            "Epoch [80/150], Loss: 12900.8829\n",
            "Epoch [90/150], Loss: 13324.9592\n",
            "Epoch [100/150], Loss: 15295.8555\n",
            "Epoch [110/150], Loss: 12128.7722\n",
            "Epoch [120/150], Loss: 12796.4695\n",
            "Epoch [130/150], Loss: 9103.3580\n",
            "Epoch [140/150], Loss: 12318.5327\n",
            "Epoch [150/150], Loss: 11013.9404\n",
            "Fold 2, RMSE: 66.40483093261719\n",
            "Epoch [10/150], Loss: 841574.8359\n",
            "Epoch [20/150], Loss: 49498.5215\n",
            "Epoch [30/150], Loss: 22612.7363\n",
            "Epoch [40/150], Loss: 13619.0696\n",
            "Epoch [50/150], Loss: 19755.2410\n",
            "Epoch [60/150], Loss: 13651.8967\n",
            "Epoch [70/150], Loss: 16050.6553\n",
            "Epoch [80/150], Loss: 7820.8756\n",
            "Epoch [90/150], Loss: 6801.8845\n",
            "Epoch [100/150], Loss: 9619.0569\n",
            "Epoch [110/150], Loss: 12154.4961\n",
            "Epoch [120/150], Loss: 9491.6306\n",
            "Epoch [130/150], Loss: 8111.8732\n",
            "Epoch [140/150], Loss: 12206.8184\n",
            "Epoch [150/150], Loss: 6169.2733\n",
            "Fold 3, RMSE: 91.105712890625\n",
            "Epoch [10/150], Loss: 383002.8984\n",
            "Epoch [20/150], Loss: 28134.0225\n",
            "Epoch [30/150], Loss: 17850.9160\n",
            "Epoch [40/150], Loss: 10761.0359\n",
            "Epoch [50/150], Loss: 17509.2542\n",
            "Epoch [60/150], Loss: 17929.0015\n",
            "Epoch [70/150], Loss: 18170.7427\n",
            "Epoch [80/150], Loss: 7344.5886\n",
            "Epoch [90/150], Loss: 15113.5933\n",
            "Epoch [100/150], Loss: 4982.1084\n",
            "Epoch [110/150], Loss: 5151.9726\n",
            "Epoch [120/150], Loss: 6045.3566\n",
            "Epoch [130/150], Loss: 2690.6290\n",
            "Epoch [140/150], Loss: 2572.5448\n",
            "Epoch [150/150], Loss: 3644.1240\n",
            "Fold 4, RMSE: 40.84209060668945\n",
            "Epoch [10/150], Loss: 345773.1016\n",
            "Epoch [20/150], Loss: 74592.7441\n",
            "Epoch [30/150], Loss: 34041.1411\n",
            "Epoch [40/150], Loss: 26750.5474\n",
            "Epoch [50/150], Loss: 24794.4800\n",
            "Epoch [60/150], Loss: 13981.4902\n",
            "Epoch [70/150], Loss: 16416.4731\n",
            "Epoch [80/150], Loss: 18050.9802\n",
            "Epoch [90/150], Loss: 16628.0742\n",
            "Epoch [100/150], Loss: 15261.9526\n",
            "Epoch [110/150], Loss: 10451.3115\n",
            "Epoch [120/150], Loss: 11384.7119\n",
            "Epoch [130/150], Loss: 13567.6553\n",
            "Epoch [140/150], Loss: 15963.0483\n",
            "Epoch [150/150], Loss: 8578.9164\n",
            "Fold 5, RMSE: 46.512027740478516\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 61.23241653442383\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 62468.3936\n",
            "Epoch [20/100], Loss: 13131.1166\n",
            "Epoch [30/100], Loss: 21170.0227\n",
            "Epoch [40/100], Loss: 15422.8477\n",
            "Epoch [50/100], Loss: 16324.7800\n",
            "Epoch [60/100], Loss: 17929.5991\n",
            "Epoch [70/100], Loss: 14344.1663\n",
            "Epoch [80/100], Loss: 18541.2535\n",
            "Epoch [90/100], Loss: 9806.0525\n",
            "Epoch [100/100], Loss: 10021.4117\n",
            "Fold 1, RMSE: 49.23544692993164\n",
            "Epoch [10/100], Loss: 51728.8652\n",
            "Epoch [20/100], Loss: 16589.0469\n",
            "Epoch [30/100], Loss: 15675.8696\n",
            "Epoch [40/100], Loss: 10335.7878\n",
            "Epoch [50/100], Loss: 8679.4825\n",
            "Epoch [60/100], Loss: 12817.0076\n",
            "Epoch [70/100], Loss: 10282.4182\n",
            "Epoch [80/100], Loss: 10156.5261\n",
            "Epoch [90/100], Loss: 16863.6108\n",
            "Epoch [100/100], Loss: 10852.6984\n",
            "Fold 2, RMSE: 66.75887298583984\n",
            "Epoch [10/100], Loss: 23511.3643\n",
            "Epoch [20/100], Loss: 10320.6030\n",
            "Epoch [30/100], Loss: 7411.5087\n",
            "Epoch [40/100], Loss: 8485.0605\n",
            "Epoch [50/100], Loss: 8035.6054\n",
            "Epoch [60/100], Loss: 6764.8738\n",
            "Epoch [70/100], Loss: 7297.4226\n",
            "Epoch [80/100], Loss: 7503.3130\n",
            "Epoch [90/100], Loss: 8121.9933\n",
            "Epoch [100/100], Loss: 8554.0081\n",
            "Fold 3, RMSE: 92.54196166992188\n",
            "Epoch [10/100], Loss: 72846.6982\n",
            "Epoch [20/100], Loss: 22316.4849\n",
            "Epoch [30/100], Loss: 14260.8232\n",
            "Epoch [40/100], Loss: 12847.4949\n",
            "Epoch [50/100], Loss: 13028.1897\n",
            "Epoch [60/100], Loss: 12152.7251\n",
            "Epoch [70/100], Loss: 14019.6929\n",
            "Epoch [80/100], Loss: 16825.7395\n",
            "Epoch [90/100], Loss: 12372.2625\n",
            "Epoch [100/100], Loss: 20389.2959\n",
            "Fold 4, RMSE: 35.39240646362305\n",
            "Epoch [10/100], Loss: 38809.3271\n",
            "Epoch [20/100], Loss: 14406.7456\n",
            "Epoch [30/100], Loss: 12341.6033\n",
            "Epoch [40/100], Loss: 10607.0697\n",
            "Epoch [50/100], Loss: 10734.5602\n",
            "Epoch [60/100], Loss: 10827.9963\n",
            "Epoch [70/100], Loss: 14561.5808\n",
            "Epoch [80/100], Loss: 11033.1123\n",
            "Epoch [90/100], Loss: 10553.8114\n",
            "Epoch [100/100], Loss: 10382.9941\n",
            "Fold 5, RMSE: 47.7806396484375\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 58.34186553955078\n",
            "Training with neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 12246.8259\n",
            "Epoch [20/150], Loss: 15270.9829\n",
            "Epoch [30/150], Loss: 12027.7100\n",
            "Epoch [40/150], Loss: 8000.7829\n",
            "Epoch [50/150], Loss: 13507.3712\n",
            "Epoch [60/150], Loss: 8505.9065\n",
            "Epoch [70/150], Loss: 3466.5706\n",
            "Epoch [80/150], Loss: 3363.8196\n",
            "Epoch [90/150], Loss: 4187.9600\n",
            "Epoch [100/150], Loss: 3172.9749\n",
            "Epoch [110/150], Loss: 4306.2794\n",
            "Epoch [120/150], Loss: 2952.6431\n",
            "Epoch [130/150], Loss: 2778.5754\n",
            "Epoch [140/150], Loss: 2423.4096\n",
            "Epoch [150/150], Loss: 2726.0217\n",
            "Fold 1, RMSE: 54.77567672729492\n",
            "Epoch [10/150], Loss: 22561.4355\n",
            "Epoch [20/150], Loss: 15616.5791\n",
            "Epoch [30/150], Loss: 9606.3242\n",
            "Epoch [40/150], Loss: 11838.5972\n",
            "Epoch [50/150], Loss: 6778.2796\n",
            "Epoch [60/150], Loss: 8528.0167\n",
            "Epoch [70/150], Loss: 5219.9983\n",
            "Epoch [80/150], Loss: 3169.4832\n",
            "Epoch [90/150], Loss: 6000.2149\n",
            "Epoch [100/150], Loss: 4262.5209\n",
            "Epoch [110/150], Loss: 3448.9335\n",
            "Epoch [120/150], Loss: 1919.9441\n",
            "Epoch [130/150], Loss: 2024.1204\n",
            "Epoch [140/150], Loss: 2292.3737\n",
            "Epoch [150/150], Loss: 1843.6610\n",
            "Fold 2, RMSE: 59.48977279663086\n",
            "Epoch [10/150], Loss: 29366.3945\n",
            "Epoch [20/150], Loss: 9356.6455\n",
            "Epoch [30/150], Loss: 9610.7372\n",
            "Epoch [40/150], Loss: 8939.5990\n",
            "Epoch [50/150], Loss: 9542.4421\n",
            "Epoch [60/150], Loss: 7147.3240\n",
            "Epoch [70/150], Loss: 9386.2343\n",
            "Epoch [80/150], Loss: 10052.4663\n",
            "Epoch [90/150], Loss: 9285.1185\n",
            "Epoch [100/150], Loss: 9763.0659\n",
            "Epoch [110/150], Loss: 13002.2219\n",
            "Epoch [120/150], Loss: 8487.4828\n",
            "Epoch [130/150], Loss: 6414.2960\n",
            "Epoch [140/150], Loss: 9075.5750\n",
            "Epoch [150/150], Loss: 7199.0438\n",
            "Fold 3, RMSE: 92.38268280029297\n",
            "Epoch [10/150], Loss: 125019.4531\n",
            "Epoch [20/150], Loss: 23048.4600\n",
            "Epoch [30/150], Loss: 17775.0947\n",
            "Epoch [40/150], Loss: 18081.9570\n",
            "Epoch [50/150], Loss: 15119.9108\n",
            "Epoch [60/150], Loss: 14881.8525\n",
            "Epoch [70/150], Loss: 21764.2812\n",
            "Epoch [80/150], Loss: 18067.5159\n",
            "Epoch [90/150], Loss: 12058.1213\n",
            "Epoch [100/150], Loss: 11097.9634\n",
            "Epoch [110/150], Loss: 10344.9193\n",
            "Epoch [120/150], Loss: 11679.1045\n",
            "Epoch [130/150], Loss: 14642.2961\n",
            "Epoch [140/150], Loss: 10789.9854\n",
            "Epoch [150/150], Loss: 12005.8191\n",
            "Fold 4, RMSE: 35.89047622680664\n",
            "Epoch [10/150], Loss: 69133.0381\n",
            "Epoch [20/150], Loss: 18114.7178\n",
            "Epoch [30/150], Loss: 10705.8035\n",
            "Epoch [40/150], Loss: 13490.1389\n",
            "Epoch [50/150], Loss: 11518.1290\n",
            "Epoch [60/150], Loss: 11357.5654\n",
            "Epoch [70/150], Loss: 10919.0427\n",
            "Epoch [80/150], Loss: 9752.3284\n",
            "Epoch [90/150], Loss: 13073.8157\n",
            "Epoch [100/150], Loss: 10542.0247\n",
            "Epoch [110/150], Loss: 10357.8340\n",
            "Epoch [120/150], Loss: 10090.7273\n",
            "Epoch [130/150], Loss: 10609.2146\n",
            "Epoch [140/150], Loss: 10430.7126\n",
            "Epoch [150/150], Loss: 10927.3828\n",
            "Fold 5, RMSE: 48.318450927734375\n",
            "Avg RMSE for neurons=64, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 58.17141189575195\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14359.0039\n",
            "Epoch [20/100], Loss: 8660.2225\n",
            "Epoch [30/100], Loss: 9248.5321\n",
            "Epoch [40/100], Loss: 4966.6354\n",
            "Epoch [50/100], Loss: 3482.2388\n",
            "Epoch [60/100], Loss: 5832.3015\n",
            "Epoch [70/100], Loss: 6173.9475\n",
            "Epoch [80/100], Loss: 6883.2253\n",
            "Epoch [90/100], Loss: 3825.9684\n",
            "Epoch [100/100], Loss: 3629.1550\n",
            "Fold 1, RMSE: 58.89862060546875\n",
            "Epoch [10/100], Loss: 12143.2153\n",
            "Epoch [20/100], Loss: 8738.7943\n",
            "Epoch [30/100], Loss: 7889.9062\n",
            "Epoch [40/100], Loss: 4852.8781\n",
            "Epoch [50/100], Loss: 3545.4318\n",
            "Epoch [60/100], Loss: 5048.9684\n",
            "Epoch [70/100], Loss: 4302.2759\n",
            "Epoch [80/100], Loss: 3071.6464\n",
            "Epoch [90/100], Loss: 2712.1124\n",
            "Epoch [100/100], Loss: 7235.4260\n",
            "Fold 2, RMSE: 65.67333221435547\n",
            "Epoch [10/100], Loss: 13970.9595\n",
            "Epoch [20/100], Loss: 11649.1935\n",
            "Epoch [30/100], Loss: 15874.8811\n",
            "Epoch [40/100], Loss: 13297.9094\n",
            "Epoch [50/100], Loss: 13242.1843\n",
            "Epoch [60/100], Loss: 16530.0908\n",
            "Epoch [70/100], Loss: 12864.4600\n",
            "Epoch [80/100], Loss: 11715.6807\n",
            "Epoch [90/100], Loss: 14393.4141\n",
            "Epoch [100/100], Loss: 13826.2705\n",
            "Fold 3, RMSE: 109.74539947509766\n",
            "Epoch [10/100], Loss: 21973.0271\n",
            "Epoch [20/100], Loss: 13825.3179\n",
            "Epoch [30/100], Loss: 14392.5323\n",
            "Epoch [40/100], Loss: 11685.7610\n",
            "Epoch [50/100], Loss: 11187.5562\n",
            "Epoch [60/100], Loss: 21310.3237\n",
            "Epoch [70/100], Loss: 11089.7822\n",
            "Epoch [80/100], Loss: 8278.0850\n",
            "Epoch [90/100], Loss: 6593.6942\n",
            "Epoch [100/100], Loss: 3978.7754\n",
            "Fold 4, RMSE: 41.492332458496094\n",
            "Epoch [10/100], Loss: 13263.9398\n",
            "Epoch [20/100], Loss: 10740.0906\n",
            "Epoch [30/100], Loss: 6157.1785\n",
            "Epoch [40/100], Loss: 6174.6733\n",
            "Epoch [50/100], Loss: 4452.8521\n",
            "Epoch [60/100], Loss: 11408.0040\n",
            "Epoch [70/100], Loss: 7885.4335\n",
            "Epoch [80/100], Loss: 4764.9231\n",
            "Epoch [90/100], Loss: 5204.2568\n",
            "Epoch [100/100], Loss: 5458.7001\n",
            "Fold 5, RMSE: 46.8262939453125\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 64.52719573974609\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 17180.0215\n",
            "Epoch [20/150], Loss: 8577.3055\n",
            "Epoch [30/150], Loss: 8144.8167\n",
            "Epoch [40/150], Loss: 5011.7119\n",
            "Epoch [50/150], Loss: 4730.5009\n",
            "Epoch [60/150], Loss: 5714.5536\n",
            "Epoch [70/150], Loss: 5832.0177\n",
            "Epoch [80/150], Loss: 7305.0295\n",
            "Epoch [90/150], Loss: 3206.5848\n",
            "Epoch [100/150], Loss: 4363.7041\n",
            "Epoch [110/150], Loss: 2631.1966\n",
            "Epoch [120/150], Loss: 3299.9562\n",
            "Epoch [130/150], Loss: 3150.6145\n",
            "Epoch [140/150], Loss: 3460.7966\n",
            "Epoch [150/150], Loss: 3366.0638\n",
            "Fold 1, RMSE: 57.43964767456055\n",
            "Epoch [10/150], Loss: 15283.5906\n",
            "Epoch [20/150], Loss: 7737.5101\n",
            "Epoch [30/150], Loss: 6786.1293\n",
            "Epoch [40/150], Loss: 7731.5997\n",
            "Epoch [50/150], Loss: 6377.3217\n",
            "Epoch [60/150], Loss: 6365.1793\n",
            "Epoch [70/150], Loss: 4560.8531\n",
            "Epoch [80/150], Loss: 6345.3582\n",
            "Epoch [90/150], Loss: 4865.5337\n",
            "Epoch [100/150], Loss: 5176.6907\n",
            "Epoch [110/150], Loss: 2580.1444\n",
            "Epoch [120/150], Loss: 3664.2661\n",
            "Epoch [130/150], Loss: 2990.4044\n",
            "Epoch [140/150], Loss: 3634.6912\n",
            "Epoch [150/150], Loss: 5149.1267\n",
            "Fold 2, RMSE: 68.85392761230469\n",
            "Epoch [10/150], Loss: 10078.5035\n",
            "Epoch [20/150], Loss: 5321.5975\n",
            "Epoch [30/150], Loss: 4629.4276\n",
            "Epoch [40/150], Loss: 4430.6995\n",
            "Epoch [50/150], Loss: 4723.6099\n",
            "Epoch [60/150], Loss: 3057.5140\n",
            "Epoch [70/150], Loss: 4007.6511\n",
            "Epoch [80/150], Loss: 6042.5891\n",
            "Epoch [90/150], Loss: 3929.8672\n",
            "Epoch [100/150], Loss: 6739.4816\n",
            "Epoch [110/150], Loss: 5623.0787\n",
            "Epoch [120/150], Loss: 7683.7760\n",
            "Epoch [130/150], Loss: 8250.9277\n",
            "Epoch [140/150], Loss: 6716.7182\n",
            "Epoch [150/150], Loss: 4566.1089\n",
            "Fold 3, RMSE: 94.18981170654297\n",
            "Epoch [10/150], Loss: 13735.8755\n",
            "Epoch [20/150], Loss: 13100.6729\n",
            "Epoch [30/150], Loss: 7196.7578\n",
            "Epoch [40/150], Loss: 6664.0854\n",
            "Epoch [50/150], Loss: 8127.0001\n",
            "Epoch [60/150], Loss: 2347.7064\n",
            "Epoch [70/150], Loss: 5678.3143\n",
            "Epoch [80/150], Loss: 4582.3565\n",
            "Epoch [90/150], Loss: 4048.0861\n",
            "Epoch [100/150], Loss: 6875.1816\n",
            "Epoch [110/150], Loss: 5425.6106\n",
            "Epoch [120/150], Loss: 8012.6689\n",
            "Epoch [130/150], Loss: 6443.3789\n",
            "Epoch [140/150], Loss: 2582.2027\n",
            "Epoch [150/150], Loss: 4787.3684\n",
            "Fold 4, RMSE: 43.13922882080078\n",
            "Epoch [10/150], Loss: 16199.0442\n",
            "Epoch [20/150], Loss: 8008.3878\n",
            "Epoch [30/150], Loss: 14677.8390\n",
            "Epoch [40/150], Loss: 8028.5060\n",
            "Epoch [50/150], Loss: 9352.8135\n",
            "Epoch [60/150], Loss: 5905.8281\n",
            "Epoch [70/150], Loss: 2795.1051\n",
            "Epoch [80/150], Loss: 5892.3650\n",
            "Epoch [90/150], Loss: 13808.3721\n",
            "Epoch [100/150], Loss: 5010.1011\n",
            "Epoch [110/150], Loss: 5449.1506\n",
            "Epoch [120/150], Loss: 5692.5067\n",
            "Epoch [130/150], Loss: 2881.2808\n",
            "Epoch [140/150], Loss: 5625.4874\n",
            "Epoch [150/150], Loss: 5916.2418\n",
            "Fold 5, RMSE: 54.420379638671875\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 63.60859909057617\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 9501.3508\n",
            "Epoch [20/100], Loss: 8266.4326\n",
            "Epoch [30/100], Loss: 8053.6409\n",
            "Epoch [40/100], Loss: 5094.7794\n",
            "Epoch [50/100], Loss: 5252.1336\n",
            "Epoch [60/100], Loss: 8296.7422\n",
            "Epoch [70/100], Loss: 3937.7914\n",
            "Epoch [80/100], Loss: 3646.5896\n",
            "Epoch [90/100], Loss: 2102.7128\n",
            "Epoch [100/100], Loss: 1856.7411\n",
            "Fold 1, RMSE: 55.92341232299805\n",
            "Epoch [10/100], Loss: 18151.7200\n",
            "Epoch [20/100], Loss: 6713.2772\n",
            "Epoch [30/100], Loss: 11919.6616\n",
            "Epoch [40/100], Loss: 6982.0101\n",
            "Epoch [50/100], Loss: 6572.1714\n",
            "Epoch [60/100], Loss: 5006.5019\n",
            "Epoch [70/100], Loss: 5808.4417\n",
            "Epoch [80/100], Loss: 7559.8220\n",
            "Epoch [90/100], Loss: 2225.8094\n",
            "Epoch [100/100], Loss: 4853.3146\n",
            "Fold 2, RMSE: 67.72219848632812\n",
            "Epoch [10/100], Loss: 9596.4795\n",
            "Epoch [20/100], Loss: 5578.4434\n",
            "Epoch [30/100], Loss: 6281.0057\n",
            "Epoch [40/100], Loss: 5449.0538\n",
            "Epoch [50/100], Loss: 4824.6057\n",
            "Epoch [60/100], Loss: 4885.6538\n",
            "Epoch [70/100], Loss: 1814.3766\n",
            "Epoch [80/100], Loss: 2670.5650\n",
            "Epoch [90/100], Loss: 1147.4304\n",
            "Epoch [100/100], Loss: 1015.8445\n",
            "Fold 3, RMSE: 98.59236145019531\n",
            "Epoch [10/100], Loss: 11847.6887\n",
            "Epoch [20/100], Loss: 13365.2302\n",
            "Epoch [30/100], Loss: 4022.7712\n",
            "Epoch [40/100], Loss: 3550.0333\n",
            "Epoch [50/100], Loss: 5650.2665\n",
            "Epoch [60/100], Loss: 5078.5344\n",
            "Epoch [70/100], Loss: 5522.4016\n",
            "Epoch [80/100], Loss: 9402.3645\n",
            "Epoch [90/100], Loss: 6772.5616\n",
            "Epoch [100/100], Loss: 5153.0776\n",
            "Fold 4, RMSE: 38.812564849853516\n",
            "Epoch [10/100], Loss: 15596.3513\n",
            "Epoch [20/100], Loss: 9636.9370\n",
            "Epoch [30/100], Loss: 7282.1544\n",
            "Epoch [40/100], Loss: 5282.3904\n",
            "Epoch [50/100], Loss: 4757.3672\n",
            "Epoch [60/100], Loss: 2047.0684\n",
            "Epoch [70/100], Loss: 4889.5582\n",
            "Epoch [80/100], Loss: 6431.6104\n",
            "Epoch [90/100], Loss: 2715.9524\n",
            "Epoch [100/100], Loss: 4333.8661\n",
            "Fold 5, RMSE: 44.10688781738281\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 61.03148498535156\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 9424.5162\n",
            "Epoch [20/150], Loss: 8415.7563\n",
            "Epoch [30/150], Loss: 5893.3832\n",
            "Epoch [40/150], Loss: 2623.6346\n",
            "Epoch [50/150], Loss: 3680.4171\n",
            "Epoch [60/150], Loss: 2152.7734\n",
            "Epoch [70/150], Loss: 4691.0858\n",
            "Epoch [80/150], Loss: 7342.1796\n",
            "Epoch [90/150], Loss: 3792.6118\n",
            "Epoch [100/150], Loss: 4935.6105\n",
            "Epoch [110/150], Loss: 4536.6909\n",
            "Epoch [120/150], Loss: 2685.0398\n",
            "Epoch [130/150], Loss: 849.2643\n",
            "Epoch [140/150], Loss: 2213.4427\n",
            "Epoch [150/150], Loss: 2086.4818\n",
            "Fold 1, RMSE: 55.36712646484375\n",
            "Epoch [10/150], Loss: 11509.1689\n",
            "Epoch [20/150], Loss: 7745.2910\n",
            "Epoch [30/150], Loss: 7533.8801\n",
            "Epoch [40/150], Loss: 8185.7610\n",
            "Epoch [50/150], Loss: 2412.6462\n",
            "Epoch [60/150], Loss: 3528.7088\n",
            "Epoch [70/150], Loss: 2432.1926\n",
            "Epoch [80/150], Loss: 2375.1605\n",
            "Epoch [90/150], Loss: 5841.2433\n",
            "Epoch [100/150], Loss: 1427.9487\n",
            "Epoch [110/150], Loss: 1947.9415\n",
            "Epoch [120/150], Loss: 1732.6707\n",
            "Epoch [130/150], Loss: 1395.8657\n",
            "Epoch [140/150], Loss: 4923.2831\n",
            "Epoch [150/150], Loss: 1601.2239\n",
            "Fold 2, RMSE: 68.170166015625\n",
            "Epoch [10/150], Loss: 12106.7766\n",
            "Epoch [20/150], Loss: 4514.8282\n",
            "Epoch [30/150], Loss: 4518.1651\n",
            "Epoch [40/150], Loss: 3909.8616\n",
            "Epoch [50/150], Loss: 4349.2338\n",
            "Epoch [60/150], Loss: 3652.9178\n",
            "Epoch [70/150], Loss: 3937.6199\n",
            "Epoch [80/150], Loss: 2794.5857\n",
            "Epoch [90/150], Loss: 4527.6454\n",
            "Epoch [100/150], Loss: 5039.6641\n",
            "Epoch [110/150], Loss: 7553.4955\n",
            "Epoch [120/150], Loss: 5471.2889\n",
            "Epoch [130/150], Loss: 4180.4334\n",
            "Epoch [140/150], Loss: 6482.5477\n",
            "Epoch [150/150], Loss: 2721.7022\n",
            "Fold 3, RMSE: 95.24095916748047\n",
            "Epoch [10/150], Loss: 22158.9604\n",
            "Epoch [20/150], Loss: 11452.8818\n",
            "Epoch [30/150], Loss: 4825.8998\n",
            "Epoch [40/150], Loss: 9261.6605\n",
            "Epoch [50/150], Loss: 8286.6986\n",
            "Epoch [60/150], Loss: 2292.8655\n",
            "Epoch [70/150], Loss: 8659.8495\n",
            "Epoch [80/150], Loss: 3049.6328\n",
            "Epoch [90/150], Loss: 2579.9073\n",
            "Epoch [100/150], Loss: 6085.5851\n",
            "Epoch [110/150], Loss: 4375.5055\n",
            "Epoch [120/150], Loss: 4328.0359\n",
            "Epoch [130/150], Loss: 3697.5657\n",
            "Epoch [140/150], Loss: 2226.4517\n",
            "Epoch [150/150], Loss: 4321.2206\n",
            "Fold 4, RMSE: 42.461177825927734\n",
            "Epoch [10/150], Loss: 10884.6038\n",
            "Epoch [20/150], Loss: 12567.9363\n",
            "Epoch [30/150], Loss: 6065.0662\n",
            "Epoch [40/150], Loss: 6373.3806\n",
            "Epoch [50/150], Loss: 11730.1367\n",
            "Epoch [60/150], Loss: 4688.6373\n",
            "Epoch [70/150], Loss: 3005.3667\n",
            "Epoch [80/150], Loss: 3213.2899\n",
            "Epoch [90/150], Loss: 2686.8280\n",
            "Epoch [100/150], Loss: 3290.3874\n",
            "Epoch [110/150], Loss: 2858.2462\n",
            "Epoch [120/150], Loss: 2582.0420\n",
            "Epoch [130/150], Loss: 2739.8401\n",
            "Epoch [140/150], Loss: 4000.8950\n",
            "Epoch [150/150], Loss: 2362.0093\n",
            "Fold 5, RMSE: 44.10733413696289\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 61.06935272216797\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 13620.0774\n",
            "Epoch [20/100], Loss: 15799.4543\n",
            "Epoch [30/100], Loss: 9210.3453\n",
            "Epoch [40/100], Loss: 7093.0851\n",
            "Epoch [50/100], Loss: 5594.4301\n",
            "Epoch [60/100], Loss: 4261.7379\n",
            "Epoch [70/100], Loss: 3392.6270\n",
            "Epoch [80/100], Loss: 5595.0463\n",
            "Epoch [90/100], Loss: 9450.2126\n",
            "Epoch [100/100], Loss: 4010.3776\n",
            "Fold 1, RMSE: 58.7354621887207\n",
            "Epoch [10/100], Loss: 21426.3630\n",
            "Epoch [20/100], Loss: 16667.7395\n",
            "Epoch [30/100], Loss: 8509.7655\n",
            "Epoch [40/100], Loss: 9166.6339\n",
            "Epoch [50/100], Loss: 7175.3314\n",
            "Epoch [60/100], Loss: 4239.8119\n",
            "Epoch [70/100], Loss: 2455.3167\n",
            "Epoch [80/100], Loss: 2301.4448\n",
            "Epoch [90/100], Loss: 5057.7858\n",
            "Epoch [100/100], Loss: 3160.2405\n",
            "Fold 2, RMSE: 68.57976531982422\n",
            "Epoch [10/100], Loss: 9772.2993\n",
            "Epoch [20/100], Loss: 9657.7852\n",
            "Epoch [30/100], Loss: 7075.5835\n",
            "Epoch [40/100], Loss: 7012.7494\n",
            "Epoch [50/100], Loss: 8421.2648\n",
            "Epoch [60/100], Loss: 8785.0508\n",
            "Epoch [70/100], Loss: 6670.8507\n",
            "Epoch [80/100], Loss: 5807.7764\n",
            "Epoch [90/100], Loss: 8209.6705\n",
            "Epoch [100/100], Loss: 3773.6193\n",
            "Fold 3, RMSE: 91.58526611328125\n",
            "Epoch [10/100], Loss: 12005.7355\n",
            "Epoch [20/100], Loss: 13488.7498\n",
            "Epoch [30/100], Loss: 10429.2615\n",
            "Epoch [40/100], Loss: 10299.3743\n",
            "Epoch [50/100], Loss: 4355.0999\n",
            "Epoch [60/100], Loss: 2784.2936\n",
            "Epoch [70/100], Loss: 3899.6193\n",
            "Epoch [80/100], Loss: 15383.6453\n",
            "Epoch [90/100], Loss: 9560.6653\n",
            "Epoch [100/100], Loss: 2741.0768\n",
            "Fold 4, RMSE: 45.189361572265625\n",
            "Epoch [10/100], Loss: 14685.5432\n",
            "Epoch [20/100], Loss: 10880.8521\n",
            "Epoch [30/100], Loss: 13772.2810\n",
            "Epoch [40/100], Loss: 4671.4103\n",
            "Epoch [50/100], Loss: 5057.7684\n",
            "Epoch [60/100], Loss: 4977.7635\n",
            "Epoch [70/100], Loss: 6526.4115\n",
            "Epoch [80/100], Loss: 2863.7667\n",
            "Epoch [90/100], Loss: 4386.0679\n",
            "Epoch [100/100], Loss: 3927.7222\n",
            "Fold 5, RMSE: 45.79956817626953\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 61.977884674072264\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 12640.3350\n",
            "Epoch [20/150], Loss: 12110.7036\n",
            "Epoch [30/150], Loss: 8418.2771\n",
            "Epoch [40/150], Loss: 6722.2283\n",
            "Epoch [50/150], Loss: 4460.6375\n",
            "Epoch [60/150], Loss: 3686.1770\n",
            "Epoch [70/150], Loss: 6231.2949\n",
            "Epoch [80/150], Loss: 2664.2075\n",
            "Epoch [90/150], Loss: 3223.3058\n",
            "Epoch [100/150], Loss: 2269.8398\n",
            "Epoch [110/150], Loss: 1809.7445\n",
            "Epoch [120/150], Loss: 2956.7558\n",
            "Epoch [130/150], Loss: 1577.1196\n",
            "Epoch [140/150], Loss: 2050.0005\n",
            "Epoch [150/150], Loss: 2904.2722\n",
            "Fold 1, RMSE: 57.70402908325195\n",
            "Epoch [10/150], Loss: 14024.9113\n",
            "Epoch [20/150], Loss: 10337.5306\n",
            "Epoch [30/150], Loss: 6906.6932\n",
            "Epoch [40/150], Loss: 5878.2999\n",
            "Epoch [50/150], Loss: 4391.7732\n",
            "Epoch [60/150], Loss: 2829.3890\n",
            "Epoch [70/150], Loss: 2658.5545\n",
            "Epoch [80/150], Loss: 2516.1801\n",
            "Epoch [90/150], Loss: 2919.6806\n",
            "Epoch [100/150], Loss: 3387.6745\n",
            "Epoch [110/150], Loss: 5270.6557\n",
            "Epoch [120/150], Loss: 3091.7110\n",
            "Epoch [130/150], Loss: 3306.7757\n",
            "Epoch [140/150], Loss: 5745.1558\n",
            "Epoch [150/150], Loss: 1356.5398\n",
            "Fold 2, RMSE: 65.47677612304688\n",
            "Epoch [10/150], Loss: 9413.6108\n",
            "Epoch [20/150], Loss: 7706.2300\n",
            "Epoch [30/150], Loss: 5915.1460\n",
            "Epoch [40/150], Loss: 6119.3413\n",
            "Epoch [50/150], Loss: 4355.7970\n",
            "Epoch [60/150], Loss: 6670.2985\n",
            "Epoch [70/150], Loss: 4458.7595\n",
            "Epoch [80/150], Loss: 3746.9340\n",
            "Epoch [90/150], Loss: 1641.3517\n",
            "Epoch [100/150], Loss: 4967.3179\n",
            "Epoch [110/150], Loss: 3432.5894\n",
            "Epoch [120/150], Loss: 5760.0198\n",
            "Epoch [130/150], Loss: 5971.0544\n",
            "Epoch [140/150], Loss: 3566.4273\n",
            "Epoch [150/150], Loss: 1456.2611\n",
            "Fold 3, RMSE: 94.51004791259766\n",
            "Epoch [10/150], Loss: 20338.5817\n",
            "Epoch [20/150], Loss: 10003.8599\n",
            "Epoch [30/150], Loss: 8550.2957\n",
            "Epoch [40/150], Loss: 8151.2173\n",
            "Epoch [50/150], Loss: 7223.1235\n",
            "Epoch [60/150], Loss: 8313.5854\n",
            "Epoch [70/150], Loss: 7240.8140\n",
            "Epoch [80/150], Loss: 13742.3779\n",
            "Epoch [90/150], Loss: 3579.7534\n",
            "Epoch [100/150], Loss: 2236.8757\n",
            "Epoch [110/150], Loss: 2785.3943\n",
            "Epoch [120/150], Loss: 1812.8951\n",
            "Epoch [130/150], Loss: 1070.7327\n",
            "Epoch [140/150], Loss: 5375.0442\n",
            "Epoch [150/150], Loss: 3306.6315\n",
            "Fold 4, RMSE: 40.98091506958008\n",
            "Epoch [10/150], Loss: 19331.5996\n",
            "Epoch [20/150], Loss: 11710.3284\n",
            "Epoch [30/150], Loss: 12579.6130\n",
            "Epoch [40/150], Loss: 6139.9602\n",
            "Epoch [50/150], Loss: 2844.7417\n",
            "Epoch [60/150], Loss: 3588.1718\n",
            "Epoch [70/150], Loss: 3668.3920\n",
            "Epoch [80/150], Loss: 7061.3467\n",
            "Epoch [90/150], Loss: 4539.2609\n",
            "Epoch [100/150], Loss: 1946.1649\n",
            "Epoch [110/150], Loss: 3693.7558\n",
            "Epoch [120/150], Loss: 3537.6783\n",
            "Epoch [130/150], Loss: 4173.3479\n",
            "Epoch [140/150], Loss: 980.4392\n",
            "Epoch [150/150], Loss: 3002.8532\n",
            "Fold 5, RMSE: 44.89383316040039\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 60.71312026977539\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 17970.1528\n",
            "Epoch [20/100], Loss: 7223.9081\n",
            "Epoch [30/100], Loss: 4569.4857\n",
            "Epoch [40/100], Loss: 2509.8360\n",
            "Epoch [50/100], Loss: 4178.0642\n",
            "Epoch [60/100], Loss: 3900.2125\n",
            "Epoch [70/100], Loss: 5975.1884\n",
            "Epoch [80/100], Loss: 1978.6053\n",
            "Epoch [90/100], Loss: 1573.1667\n",
            "Epoch [100/100], Loss: 2006.2572\n",
            "Fold 1, RMSE: 57.69709014892578\n",
            "Epoch [10/100], Loss: 10088.1174\n",
            "Epoch [20/100], Loss: 11745.8400\n",
            "Epoch [30/100], Loss: 5511.9780\n",
            "Epoch [40/100], Loss: 7383.0223\n",
            "Epoch [50/100], Loss: 4180.6351\n",
            "Epoch [60/100], Loss: 3149.6020\n",
            "Epoch [70/100], Loss: 1414.8675\n",
            "Epoch [80/100], Loss: 9916.6318\n",
            "Epoch [90/100], Loss: 4684.2660\n",
            "Epoch [100/100], Loss: 7070.4399\n",
            "Fold 2, RMSE: 68.0003890991211\n",
            "Epoch [10/100], Loss: 9923.7781\n",
            "Epoch [20/100], Loss: 10565.0205\n",
            "Epoch [30/100], Loss: 8200.4326\n",
            "Epoch [40/100], Loss: 5253.2524\n",
            "Epoch [50/100], Loss: 5437.4851\n",
            "Epoch [60/100], Loss: 7459.7972\n",
            "Epoch [70/100], Loss: 3778.2583\n",
            "Epoch [80/100], Loss: 3556.0942\n",
            "Epoch [90/100], Loss: 3448.3851\n",
            "Epoch [100/100], Loss: 3966.4912\n",
            "Fold 3, RMSE: 93.68793487548828\n",
            "Epoch [10/100], Loss: 17805.2158\n",
            "Epoch [20/100], Loss: 13154.9492\n",
            "Epoch [30/100], Loss: 12951.1787\n",
            "Epoch [40/100], Loss: 8396.6082\n",
            "Epoch [50/100], Loss: 4047.4394\n",
            "Epoch [60/100], Loss: 5930.3600\n",
            "Epoch [70/100], Loss: 3323.9092\n",
            "Epoch [80/100], Loss: 3231.7831\n",
            "Epoch [90/100], Loss: 3356.2172\n",
            "Epoch [100/100], Loss: 5947.1866\n",
            "Fold 4, RMSE: 35.10070037841797\n",
            "Epoch [10/100], Loss: 17750.1721\n",
            "Epoch [20/100], Loss: 8395.4049\n",
            "Epoch [30/100], Loss: 11372.0499\n",
            "Epoch [40/100], Loss: 9859.2991\n",
            "Epoch [50/100], Loss: 4629.5065\n",
            "Epoch [60/100], Loss: 10046.6667\n",
            "Epoch [70/100], Loss: 5396.0123\n",
            "Epoch [80/100], Loss: 19934.9785\n",
            "Epoch [90/100], Loss: 7450.0235\n",
            "Epoch [100/100], Loss: 7505.7197\n",
            "Fold 5, RMSE: 44.93712615966797\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 59.884648132324216\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 10857.3323\n",
            "Epoch [20/150], Loss: 7248.4352\n",
            "Epoch [30/150], Loss: 4353.8497\n",
            "Epoch [40/150], Loss: 10468.0782\n",
            "Epoch [50/150], Loss: 3265.5417\n",
            "Epoch [60/150], Loss: 6906.9927\n",
            "Epoch [70/150], Loss: 5638.9325\n",
            "Epoch [80/150], Loss: 7674.0355\n",
            "Epoch [90/150], Loss: 5780.7916\n",
            "Epoch [100/150], Loss: 2309.3304\n",
            "Epoch [110/150], Loss: 2724.0167\n",
            "Epoch [120/150], Loss: 2001.7819\n",
            "Epoch [130/150], Loss: 1262.4996\n",
            "Epoch [140/150], Loss: 1862.3776\n",
            "Epoch [150/150], Loss: 1789.6455\n",
            "Fold 1, RMSE: 55.73744201660156\n",
            "Epoch [10/150], Loss: 14047.5205\n",
            "Epoch [20/150], Loss: 8807.7250\n",
            "Epoch [30/150], Loss: 10546.7721\n",
            "Epoch [40/150], Loss: 4706.7241\n",
            "Epoch [50/150], Loss: 5685.6996\n",
            "Epoch [60/150], Loss: 4540.2082\n",
            "Epoch [70/150], Loss: 3693.8284\n",
            "Epoch [80/150], Loss: 5309.1967\n",
            "Epoch [90/150], Loss: 6442.1694\n",
            "Epoch [100/150], Loss: 4070.8251\n",
            "Epoch [110/150], Loss: 3885.8241\n",
            "Epoch [120/150], Loss: 7603.7272\n",
            "Epoch [130/150], Loss: 2381.3409\n",
            "Epoch [140/150], Loss: 3968.0813\n",
            "Epoch [150/150], Loss: 3017.9313\n",
            "Fold 2, RMSE: 68.75996398925781\n",
            "Epoch [10/150], Loss: 11543.5764\n",
            "Epoch [20/150], Loss: 7513.2869\n",
            "Epoch [30/150], Loss: 6436.5962\n",
            "Epoch [40/150], Loss: 5055.4395\n",
            "Epoch [50/150], Loss: 4084.2274\n",
            "Epoch [60/150], Loss: 3410.1539\n",
            "Epoch [70/150], Loss: 2506.2114\n",
            "Epoch [80/150], Loss: 3331.4243\n",
            "Epoch [90/150], Loss: 4397.3153\n",
            "Epoch [100/150], Loss: 2442.6498\n",
            "Epoch [110/150], Loss: 2915.8554\n",
            "Epoch [120/150], Loss: 2192.4144\n",
            "Epoch [130/150], Loss: 1569.7543\n",
            "Epoch [140/150], Loss: 4307.0194\n",
            "Epoch [150/150], Loss: 1407.5197\n",
            "Fold 3, RMSE: 94.70023345947266\n",
            "Epoch [10/150], Loss: 11327.6976\n",
            "Epoch [20/150], Loss: 6007.7108\n",
            "Epoch [30/150], Loss: 5372.2402\n",
            "Epoch [40/150], Loss: 8259.5970\n",
            "Epoch [50/150], Loss: 4864.0466\n",
            "Epoch [60/150], Loss: 8736.9004\n",
            "Epoch [70/150], Loss: 4052.2656\n",
            "Epoch [80/150], Loss: 3805.4688\n",
            "Epoch [90/150], Loss: 2880.7859\n",
            "Epoch [100/150], Loss: 2099.1800\n",
            "Epoch [110/150], Loss: 2638.4223\n",
            "Epoch [120/150], Loss: 2907.8108\n",
            "Epoch [130/150], Loss: 5036.3971\n",
            "Epoch [140/150], Loss: 12402.2517\n",
            "Epoch [150/150], Loss: 4308.0553\n",
            "Fold 4, RMSE: 39.606971740722656\n",
            "Epoch [10/150], Loss: 13202.2903\n",
            "Epoch [20/150], Loss: 13877.3599\n",
            "Epoch [30/150], Loss: 10947.1626\n",
            "Epoch [40/150], Loss: 5689.2675\n",
            "Epoch [50/150], Loss: 5018.4615\n",
            "Epoch [60/150], Loss: 9004.0321\n",
            "Epoch [70/150], Loss: 4731.3436\n",
            "Epoch [80/150], Loss: 7428.5054\n",
            "Epoch [90/150], Loss: 4174.8668\n",
            "Epoch [100/150], Loss: 4993.0898\n",
            "Epoch [110/150], Loss: 4461.8069\n",
            "Epoch [120/150], Loss: 2917.2416\n",
            "Epoch [130/150], Loss: 2463.2566\n",
            "Epoch [140/150], Loss: 2526.6915\n",
            "Epoch [150/150], Loss: 2343.0841\n",
            "Fold 5, RMSE: 44.498233795166016\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 60.66056900024414\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 13825.6011\n",
            "Epoch [20/100], Loss: 10581.4021\n",
            "Epoch [30/100], Loss: 9234.8384\n",
            "Epoch [40/100], Loss: 10874.1885\n",
            "Epoch [50/100], Loss: 4499.4816\n",
            "Epoch [60/100], Loss: 2796.6542\n",
            "Epoch [70/100], Loss: 3730.6860\n",
            "Epoch [80/100], Loss: 2937.2969\n",
            "Epoch [90/100], Loss: 2875.6455\n",
            "Epoch [100/100], Loss: 3846.8873\n",
            "Fold 1, RMSE: 55.87114334106445\n",
            "Epoch [10/100], Loss: 17079.7959\n",
            "Epoch [20/100], Loss: 13564.9871\n",
            "Epoch [30/100], Loss: 10398.9177\n",
            "Epoch [40/100], Loss: 10876.9594\n",
            "Epoch [50/100], Loss: 10165.6400\n",
            "Epoch [60/100], Loss: 10335.0171\n",
            "Epoch [70/100], Loss: 7104.0654\n",
            "Epoch [80/100], Loss: 3545.8309\n",
            "Epoch [90/100], Loss: 4119.0265\n",
            "Epoch [100/100], Loss: 3832.9401\n",
            "Fold 2, RMSE: 66.07486724853516\n",
            "Epoch [10/100], Loss: 9055.6288\n",
            "Epoch [20/100], Loss: 8427.7424\n",
            "Epoch [30/100], Loss: 9704.2864\n",
            "Epoch [40/100], Loss: 11089.5283\n",
            "Epoch [50/100], Loss: 6965.4397\n",
            "Epoch [60/100], Loss: 4831.5038\n",
            "Epoch [70/100], Loss: 4951.2517\n",
            "Epoch [80/100], Loss: 4746.4975\n",
            "Epoch [90/100], Loss: 5754.6182\n",
            "Epoch [100/100], Loss: 6843.1205\n",
            "Fold 3, RMSE: 91.7236328125\n",
            "Epoch [10/100], Loss: 14074.7148\n",
            "Epoch [20/100], Loss: 9562.4282\n",
            "Epoch [30/100], Loss: 10146.1938\n",
            "Epoch [40/100], Loss: 7862.7278\n",
            "Epoch [50/100], Loss: 4349.9739\n",
            "Epoch [60/100], Loss: 11998.5056\n",
            "Epoch [70/100], Loss: 8178.9221\n",
            "Epoch [80/100], Loss: 2911.6563\n",
            "Epoch [90/100], Loss: 3927.3898\n",
            "Epoch [100/100], Loss: 6690.4622\n",
            "Fold 4, RMSE: 43.983760833740234\n",
            "Epoch [10/100], Loss: 12985.8210\n",
            "Epoch [20/100], Loss: 14771.7830\n",
            "Epoch [30/100], Loss: 16608.8851\n",
            "Epoch [40/100], Loss: 6384.1309\n",
            "Epoch [50/100], Loss: 3499.3503\n",
            "Epoch [60/100], Loss: 4224.9550\n",
            "Epoch [70/100], Loss: 2136.2093\n",
            "Epoch [80/100], Loss: 2498.6462\n",
            "Epoch [90/100], Loss: 5364.8757\n",
            "Epoch [100/100], Loss: 3004.2523\n",
            "Fold 5, RMSE: 44.91255187988281\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 60.51319122314453\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 11505.7424\n",
            "Epoch [20/150], Loss: 9512.5867\n",
            "Epoch [30/150], Loss: 6121.4777\n",
            "Epoch [40/150], Loss: 3545.7281\n",
            "Epoch [50/150], Loss: 6881.5962\n",
            "Epoch [60/150], Loss: 4595.6964\n",
            "Epoch [70/150], Loss: 7957.5676\n",
            "Epoch [80/150], Loss: 7044.6865\n",
            "Epoch [90/150], Loss: 7612.8865\n",
            "Epoch [100/150], Loss: 8816.1418\n",
            "Epoch [110/150], Loss: 5239.3163\n",
            "Epoch [120/150], Loss: 6853.9197\n",
            "Epoch [130/150], Loss: 5315.6393\n",
            "Epoch [140/150], Loss: 9156.9507\n",
            "Epoch [150/150], Loss: 4929.3746\n",
            "Fold 1, RMSE: 58.48341369628906\n",
            "Epoch [10/150], Loss: 13013.4353\n",
            "Epoch [20/150], Loss: 13767.8514\n",
            "Epoch [30/150], Loss: 9395.7380\n",
            "Epoch [40/150], Loss: 9620.1182\n",
            "Epoch [50/150], Loss: 5815.8038\n",
            "Epoch [60/150], Loss: 5514.8290\n",
            "Epoch [70/150], Loss: 10750.6826\n",
            "Epoch [80/150], Loss: 5011.9178\n",
            "Epoch [90/150], Loss: 4155.1464\n",
            "Epoch [100/150], Loss: 5487.9929\n",
            "Epoch [110/150], Loss: 3764.4768\n",
            "Epoch [120/150], Loss: 4247.0131\n",
            "Epoch [130/150], Loss: 4510.4582\n",
            "Epoch [140/150], Loss: 5884.3689\n",
            "Epoch [150/150], Loss: 3272.4829\n",
            "Fold 2, RMSE: 72.81678771972656\n",
            "Epoch [10/150], Loss: 11991.0896\n",
            "Epoch [20/150], Loss: 11676.6392\n",
            "Epoch [30/150], Loss: 10913.3384\n",
            "Epoch [40/150], Loss: 6665.0415\n",
            "Epoch [50/150], Loss: 6328.2559\n",
            "Epoch [60/150], Loss: 3716.6365\n",
            "Epoch [70/150], Loss: 4666.6023\n",
            "Epoch [80/150], Loss: 5816.8556\n",
            "Epoch [90/150], Loss: 5009.1149\n",
            "Epoch [100/150], Loss: 8762.1111\n",
            "Epoch [110/150], Loss: 3758.4248\n",
            "Epoch [120/150], Loss: 6942.9905\n",
            "Epoch [130/150], Loss: 3110.3137\n",
            "Epoch [140/150], Loss: 3104.5125\n",
            "Epoch [150/150], Loss: 3874.8592\n",
            "Fold 3, RMSE: 99.37301635742188\n",
            "Epoch [10/150], Loss: 19784.0635\n",
            "Epoch [20/150], Loss: 15505.2268\n",
            "Epoch [30/150], Loss: 11446.5461\n",
            "Epoch [40/150], Loss: 8443.6836\n",
            "Epoch [50/150], Loss: 4624.7164\n",
            "Epoch [60/150], Loss: 4401.4597\n",
            "Epoch [70/150], Loss: 4403.6959\n",
            "Epoch [80/150], Loss: 5747.0731\n",
            "Epoch [90/150], Loss: 3369.8625\n",
            "Epoch [100/150], Loss: 8710.1292\n",
            "Epoch [110/150], Loss: 6928.7991\n",
            "Epoch [120/150], Loss: 3313.3884\n",
            "Epoch [130/150], Loss: 5009.4556\n",
            "Epoch [140/150], Loss: 3337.5940\n",
            "Epoch [150/150], Loss: 1761.6785\n",
            "Fold 4, RMSE: 42.42988586425781\n",
            "Epoch [10/150], Loss: 15189.4771\n",
            "Epoch [20/150], Loss: 11947.6575\n",
            "Epoch [30/150], Loss: 8632.8241\n",
            "Epoch [40/150], Loss: 6822.2682\n",
            "Epoch [50/150], Loss: 2840.5195\n",
            "Epoch [60/150], Loss: 6546.5975\n",
            "Epoch [70/150], Loss: 9035.9343\n",
            "Epoch [80/150], Loss: 6914.3937\n",
            "Epoch [90/150], Loss: 6102.1990\n",
            "Epoch [100/150], Loss: 6218.6973\n",
            "Epoch [110/150], Loss: 2058.8578\n",
            "Epoch [120/150], Loss: 3802.7104\n",
            "Epoch [130/150], Loss: 1627.6554\n",
            "Epoch [140/150], Loss: 2204.8512\n",
            "Epoch [150/150], Loss: 1732.6856\n",
            "Fold 5, RMSE: 44.68836975097656\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 63.558294677734374\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 12572.4773\n",
            "Epoch [20/100], Loss: 9691.2422\n",
            "Epoch [30/100], Loss: 6792.6051\n",
            "Epoch [40/100], Loss: 7442.5350\n",
            "Epoch [50/100], Loss: 3692.2266\n",
            "Epoch [60/100], Loss: 2097.2326\n",
            "Epoch [70/100], Loss: 3063.9786\n",
            "Epoch [80/100], Loss: 4251.0614\n",
            "Epoch [90/100], Loss: 2046.6409\n",
            "Epoch [100/100], Loss: 3481.4211\n",
            "Fold 1, RMSE: 58.21820831298828\n",
            "Epoch [10/100], Loss: 9784.0608\n",
            "Epoch [20/100], Loss: 10124.6636\n",
            "Epoch [30/100], Loss: 9267.9780\n",
            "Epoch [40/100], Loss: 5959.6768\n",
            "Epoch [50/100], Loss: 4754.5615\n",
            "Epoch [60/100], Loss: 4825.3613\n",
            "Epoch [70/100], Loss: 2287.9641\n",
            "Epoch [80/100], Loss: 3631.7327\n",
            "Epoch [90/100], Loss: 3856.5473\n",
            "Epoch [100/100], Loss: 12781.8750\n",
            "Fold 2, RMSE: 68.32513427734375\n",
            "Epoch [10/100], Loss: 6693.5269\n",
            "Epoch [20/100], Loss: 7543.8149\n",
            "Epoch [30/100], Loss: 3010.3848\n",
            "Epoch [40/100], Loss: 3021.8233\n",
            "Epoch [50/100], Loss: 4929.7225\n",
            "Epoch [60/100], Loss: 4459.2167\n",
            "Epoch [70/100], Loss: 2990.2346\n",
            "Epoch [80/100], Loss: 2065.7980\n",
            "Epoch [90/100], Loss: 2984.1504\n",
            "Epoch [100/100], Loss: 2894.6303\n",
            "Fold 3, RMSE: 89.01840209960938\n",
            "Epoch [10/100], Loss: 13021.6809\n",
            "Epoch [20/100], Loss: 12230.6581\n",
            "Epoch [30/100], Loss: 5681.1184\n",
            "Epoch [40/100], Loss: 5821.8920\n",
            "Epoch [50/100], Loss: 3836.7676\n",
            "Epoch [60/100], Loss: 1610.3468\n",
            "Epoch [70/100], Loss: 3553.3519\n",
            "Epoch [80/100], Loss: 2373.6312\n",
            "Epoch [90/100], Loss: 2378.7413\n",
            "Epoch [100/100], Loss: 4070.0140\n",
            "Fold 4, RMSE: 39.84585952758789\n",
            "Epoch [10/100], Loss: 17689.5483\n",
            "Epoch [20/100], Loss: 9692.7399\n",
            "Epoch [30/100], Loss: 10879.4121\n",
            "Epoch [40/100], Loss: 3009.4017\n",
            "Epoch [50/100], Loss: 4949.8652\n",
            "Epoch [60/100], Loss: 2523.3027\n",
            "Epoch [70/100], Loss: 2262.6186\n",
            "Epoch [80/100], Loss: 1343.3148\n",
            "Epoch [90/100], Loss: 3274.8882\n",
            "Epoch [100/100], Loss: 1507.3111\n",
            "Fold 5, RMSE: 44.216548919677734\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 59.92483062744141\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 9788.7013\n",
            "Epoch [20/150], Loss: 15457.8958\n",
            "Epoch [30/150], Loss: 6745.4729\n",
            "Epoch [40/150], Loss: 2751.8249\n",
            "Epoch [50/150], Loss: 3677.2071\n",
            "Epoch [60/150], Loss: 7827.0830\n",
            "Epoch [70/150], Loss: 6376.1414\n",
            "Epoch [80/150], Loss: 2257.7458\n",
            "Epoch [90/150], Loss: 1686.0726\n",
            "Epoch [100/150], Loss: 4433.1611\n",
            "Epoch [110/150], Loss: 1845.2237\n",
            "Epoch [120/150], Loss: 2178.3542\n",
            "Epoch [130/150], Loss: 1542.4428\n",
            "Epoch [140/150], Loss: 2943.3125\n",
            "Epoch [150/150], Loss: 1532.2498\n",
            "Fold 1, RMSE: 57.5772590637207\n",
            "Epoch [10/150], Loss: 16366.9236\n",
            "Epoch [20/150], Loss: 9298.5229\n",
            "Epoch [30/150], Loss: 8342.1622\n",
            "Epoch [40/150], Loss: 17482.7363\n",
            "Epoch [50/150], Loss: 7403.6769\n",
            "Epoch [60/150], Loss: 4954.0077\n",
            "Epoch [70/150], Loss: 3094.1259\n",
            "Epoch [80/150], Loss: 5523.1483\n",
            "Epoch [90/150], Loss: 1754.2843\n",
            "Epoch [100/150], Loss: 3698.9480\n",
            "Epoch [110/150], Loss: 3369.2064\n",
            "Epoch [120/150], Loss: 1973.7470\n",
            "Epoch [130/150], Loss: 5967.6522\n",
            "Epoch [140/150], Loss: 1474.4758\n",
            "Epoch [150/150], Loss: 1859.5077\n",
            "Fold 2, RMSE: 68.17980194091797\n",
            "Epoch [10/150], Loss: 11697.0554\n",
            "Epoch [20/150], Loss: 6221.7660\n",
            "Epoch [30/150], Loss: 4377.9290\n",
            "Epoch [40/150], Loss: 3037.9412\n",
            "Epoch [50/150], Loss: 5511.6777\n",
            "Epoch [60/150], Loss: 1900.2293\n",
            "Epoch [70/150], Loss: 1573.8254\n",
            "Epoch [80/150], Loss: 2932.1246\n",
            "Epoch [90/150], Loss: 6609.2493\n",
            "Epoch [100/150], Loss: 3462.0098\n",
            "Epoch [110/150], Loss: 2112.7632\n",
            "Epoch [120/150], Loss: 3435.1305\n",
            "Epoch [130/150], Loss: 3079.8923\n",
            "Epoch [140/150], Loss: 3406.4095\n",
            "Epoch [150/150], Loss: 2066.8206\n",
            "Fold 3, RMSE: 97.11808776855469\n",
            "Epoch [10/150], Loss: 15572.6340\n",
            "Epoch [20/150], Loss: 10042.6658\n",
            "Epoch [30/150], Loss: 10444.3601\n",
            "Epoch [40/150], Loss: 4667.5558\n",
            "Epoch [50/150], Loss: 2917.2056\n",
            "Epoch [60/150], Loss: 2765.9013\n",
            "Epoch [70/150], Loss: 2675.2012\n",
            "Epoch [80/150], Loss: 2337.9003\n",
            "Epoch [90/150], Loss: 4366.9852\n",
            "Epoch [100/150], Loss: 2334.2084\n",
            "Epoch [110/150], Loss: 1716.9783\n",
            "Epoch [120/150], Loss: 4658.4282\n",
            "Epoch [130/150], Loss: 3358.2781\n",
            "Epoch [140/150], Loss: 4986.8975\n",
            "Epoch [150/150], Loss: 2654.5612\n",
            "Fold 4, RMSE: 44.442710876464844\n",
            "Epoch [10/150], Loss: 15305.6443\n",
            "Epoch [20/150], Loss: 11328.7145\n",
            "Epoch [30/150], Loss: 6276.2450\n",
            "Epoch [40/150], Loss: 5271.3213\n",
            "Epoch [50/150], Loss: 4219.2602\n",
            "Epoch [60/150], Loss: 5444.2950\n",
            "Epoch [70/150], Loss: 5459.2950\n",
            "Epoch [80/150], Loss: 2672.2482\n",
            "Epoch [90/150], Loss: 3049.8719\n",
            "Epoch [100/150], Loss: 4094.3822\n",
            "Epoch [110/150], Loss: 2532.9541\n",
            "Epoch [120/150], Loss: 2692.5139\n",
            "Epoch [130/150], Loss: 2102.4121\n",
            "Epoch [140/150], Loss: 3446.7977\n",
            "Epoch [150/150], Loss: 2324.4497\n",
            "Fold 5, RMSE: 45.04385757446289\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 62.47234344482422\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 27205.3730\n",
            "Epoch [20/100], Loss: 16062.3835\n",
            "Epoch [30/100], Loss: 26420.6660\n",
            "Epoch [40/100], Loss: 17824.8918\n",
            "Epoch [50/100], Loss: 19644.6021\n",
            "Epoch [60/100], Loss: 16688.5635\n",
            "Epoch [70/100], Loss: 19535.5381\n",
            "Epoch [80/100], Loss: 17681.8708\n",
            "Epoch [90/100], Loss: 17400.7764\n",
            "Epoch [100/100], Loss: 22108.3477\n",
            "Fold 1, RMSE: 67.57839965820312\n",
            "Epoch [10/100], Loss: 15316.6335\n",
            "Epoch [20/100], Loss: 18339.8882\n",
            "Epoch [30/100], Loss: 16682.7339\n",
            "Epoch [40/100], Loss: 15545.0947\n",
            "Epoch [50/100], Loss: 14304.5302\n",
            "Epoch [60/100], Loss: 15449.2637\n",
            "Epoch [70/100], Loss: 17139.5427\n",
            "Epoch [80/100], Loss: 20176.6287\n",
            "Epoch [90/100], Loss: 15014.3096\n",
            "Epoch [100/100], Loss: 14586.0591\n",
            "Fold 2, RMSE: 86.95838928222656\n",
            "Epoch [10/100], Loss: 13711.7902\n",
            "Epoch [20/100], Loss: 15663.5603\n",
            "Epoch [30/100], Loss: 9283.9941\n",
            "Epoch [40/100], Loss: 12566.1606\n",
            "Epoch [50/100], Loss: 8525.4343\n",
            "Epoch [60/100], Loss: 6754.7843\n",
            "Epoch [70/100], Loss: 5088.3197\n",
            "Epoch [80/100], Loss: 5463.0082\n",
            "Epoch [90/100], Loss: 5516.7897\n",
            "Epoch [100/100], Loss: 3201.6187\n",
            "Fold 3, RMSE: 92.70346069335938\n",
            "Epoch [10/100], Loss: 17442.1777\n",
            "Epoch [20/100], Loss: 20832.7573\n",
            "Epoch [30/100], Loss: 14251.7144\n",
            "Epoch [40/100], Loss: 16455.6296\n",
            "Epoch [50/100], Loss: 13222.6633\n",
            "Epoch [60/100], Loss: 7636.4188\n",
            "Epoch [70/100], Loss: 12423.6517\n",
            "Epoch [80/100], Loss: 7984.2125\n",
            "Epoch [90/100], Loss: 5277.9724\n",
            "Epoch [100/100], Loss: 11394.9158\n",
            "Fold 4, RMSE: 41.87211990356445\n",
            "Epoch [10/100], Loss: 18198.2061\n",
            "Epoch [20/100], Loss: 13121.9497\n",
            "Epoch [30/100], Loss: 8349.1596\n",
            "Epoch [40/100], Loss: 13096.3752\n",
            "Epoch [50/100], Loss: 9781.3094\n",
            "Epoch [60/100], Loss: 7083.5109\n",
            "Epoch [70/100], Loss: 4420.0952\n",
            "Epoch [80/100], Loss: 6213.0410\n",
            "Epoch [90/100], Loss: 6383.2216\n",
            "Epoch [100/100], Loss: 2671.4436\n",
            "Fold 5, RMSE: 45.25747299194336\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 66.87396850585938\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19967.9521\n",
            "Epoch [20/150], Loss: 16053.5745\n",
            "Epoch [30/150], Loss: 19805.8408\n",
            "Epoch [40/150], Loss: 23250.4238\n",
            "Epoch [50/150], Loss: 17280.6934\n",
            "Epoch [60/150], Loss: 16136.7799\n",
            "Epoch [70/150], Loss: 19679.9756\n",
            "Epoch [80/150], Loss: 16261.1199\n",
            "Epoch [90/150], Loss: 16165.3914\n",
            "Epoch [100/150], Loss: 27664.7656\n",
            "Epoch [110/150], Loss: 19724.3689\n",
            "Epoch [120/150], Loss: 19463.1279\n",
            "Epoch [130/150], Loss: 17484.5649\n",
            "Epoch [140/150], Loss: 15722.4008\n",
            "Epoch [150/150], Loss: 16747.5088\n",
            "Fold 1, RMSE: 66.97927856445312\n",
            "Epoch [10/150], Loss: 23934.2686\n",
            "Epoch [20/150], Loss: 11661.4255\n",
            "Epoch [30/150], Loss: 10962.5612\n",
            "Epoch [40/150], Loss: 8125.2953\n",
            "Epoch [50/150], Loss: 8296.1144\n",
            "Epoch [60/150], Loss: 8412.3010\n",
            "Epoch [70/150], Loss: 5720.9592\n",
            "Epoch [80/150], Loss: 7850.7502\n",
            "Epoch [90/150], Loss: 7228.0856\n",
            "Epoch [100/150], Loss: 6770.6563\n",
            "Epoch [110/150], Loss: 2735.4753\n",
            "Epoch [120/150], Loss: 2187.3991\n",
            "Epoch [130/150], Loss: 3866.6675\n",
            "Epoch [140/150], Loss: 3040.4915\n",
            "Epoch [150/150], Loss: 2862.5169\n",
            "Fold 2, RMSE: 67.71271514892578\n",
            "Epoch [10/150], Loss: 14249.9707\n",
            "Epoch [20/150], Loss: 11049.7316\n",
            "Epoch [30/150], Loss: 18319.5986\n",
            "Epoch [40/150], Loss: 13580.7029\n",
            "Epoch [50/150], Loss: 11918.9055\n",
            "Epoch [60/150], Loss: 10984.7155\n",
            "Epoch [70/150], Loss: 13187.3796\n",
            "Epoch [80/150], Loss: 14453.8083\n",
            "Epoch [90/150], Loss: 11931.3586\n",
            "Epoch [100/150], Loss: 12192.0942\n",
            "Epoch [110/150], Loss: 12011.1805\n",
            "Epoch [120/150], Loss: 11273.7963\n",
            "Epoch [130/150], Loss: 13219.0005\n",
            "Epoch [140/150], Loss: 12513.2236\n",
            "Epoch [150/150], Loss: 14201.2993\n",
            "Fold 3, RMSE: 109.3743667602539\n",
            "Epoch [10/150], Loss: 22859.2056\n",
            "Epoch [20/150], Loss: 25573.9370\n",
            "Epoch [30/150], Loss: 19521.0356\n",
            "Epoch [40/150], Loss: 16795.7698\n",
            "Epoch [50/150], Loss: 23009.0776\n",
            "Epoch [60/150], Loss: 18686.0042\n",
            "Epoch [70/150], Loss: 28689.4185\n",
            "Epoch [80/150], Loss: 25423.9851\n",
            "Epoch [90/150], Loss: 17639.0457\n",
            "Epoch [100/150], Loss: 19568.3315\n",
            "Epoch [110/150], Loss: 19282.3066\n",
            "Epoch [120/150], Loss: 17779.1790\n",
            "Epoch [130/150], Loss: 22346.7932\n",
            "Epoch [140/150], Loss: 9011.3370\n",
            "Epoch [150/150], Loss: 10838.9639\n",
            "Fold 4, RMSE: 40.54561996459961\n",
            "Epoch [10/150], Loss: 17512.1399\n",
            "Epoch [20/150], Loss: 20159.3110\n",
            "Epoch [30/150], Loss: 12621.8691\n",
            "Epoch [40/150], Loss: 13071.2246\n",
            "Epoch [50/150], Loss: 13890.4470\n",
            "Epoch [60/150], Loss: 9653.8875\n",
            "Epoch [70/150], Loss: 7724.5698\n",
            "Epoch [80/150], Loss: 4993.8207\n",
            "Epoch [90/150], Loss: 7149.4213\n",
            "Epoch [100/150], Loss: 8028.9923\n",
            "Epoch [110/150], Loss: 4786.8417\n",
            "Epoch [120/150], Loss: 13254.9417\n",
            "Epoch [130/150], Loss: 8034.7291\n",
            "Epoch [140/150], Loss: 9119.1593\n",
            "Epoch [150/150], Loss: 9416.2463\n",
            "Fold 5, RMSE: 44.060054779052734\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 65.73440704345703\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14666.9141\n",
            "Epoch [20/100], Loss: 11325.0425\n",
            "Epoch [30/100], Loss: 10406.6479\n",
            "Epoch [40/100], Loss: 7550.2700\n",
            "Epoch [50/100], Loss: 8978.8313\n",
            "Epoch [60/100], Loss: 6308.9745\n",
            "Epoch [70/100], Loss: 8241.9175\n",
            "Epoch [80/100], Loss: 3140.1467\n",
            "Epoch [90/100], Loss: 2957.7203\n",
            "Epoch [100/100], Loss: 3455.3782\n",
            "Fold 1, RMSE: 60.31471633911133\n",
            "Epoch [10/100], Loss: 20196.5781\n",
            "Epoch [20/100], Loss: 17693.6741\n",
            "Epoch [30/100], Loss: 8732.1969\n",
            "Epoch [40/100], Loss: 9087.8157\n",
            "Epoch [50/100], Loss: 8077.0271\n",
            "Epoch [60/100], Loss: 5342.2436\n",
            "Epoch [70/100], Loss: 5439.3416\n",
            "Epoch [80/100], Loss: 6400.0842\n",
            "Epoch [90/100], Loss: 5164.9689\n",
            "Epoch [100/100], Loss: 5388.8298\n",
            "Fold 2, RMSE: 61.37579345703125\n",
            "Epoch [10/100], Loss: 12858.1516\n",
            "Epoch [20/100], Loss: 9678.6414\n",
            "Epoch [30/100], Loss: 6530.6326\n",
            "Epoch [40/100], Loss: 4641.5784\n",
            "Epoch [50/100], Loss: 2667.2085\n",
            "Epoch [60/100], Loss: 5013.6833\n",
            "Epoch [70/100], Loss: 7477.2626\n",
            "Epoch [80/100], Loss: 4010.4830\n",
            "Epoch [90/100], Loss: 3605.6872\n",
            "Epoch [100/100], Loss: 7836.7219\n",
            "Fold 3, RMSE: 105.2492446899414\n",
            "Epoch [10/100], Loss: 12054.5149\n",
            "Epoch [20/100], Loss: 10588.8309\n",
            "Epoch [30/100], Loss: 8255.4015\n",
            "Epoch [40/100], Loss: 13525.1157\n",
            "Epoch [50/100], Loss: 6038.2008\n",
            "Epoch [60/100], Loss: 4163.4111\n",
            "Epoch [70/100], Loss: 5752.7402\n",
            "Epoch [80/100], Loss: 8447.9687\n",
            "Epoch [90/100], Loss: 2657.6835\n",
            "Epoch [100/100], Loss: 6984.7024\n",
            "Fold 4, RMSE: 50.90460205078125\n",
            "Epoch [10/100], Loss: 11916.5747\n",
            "Epoch [20/100], Loss: 17326.5552\n",
            "Epoch [30/100], Loss: 6133.0514\n",
            "Epoch [40/100], Loss: 5175.5300\n",
            "Epoch [50/100], Loss: 6140.6423\n",
            "Epoch [60/100], Loss: 4042.5756\n",
            "Epoch [70/100], Loss: 2887.9202\n",
            "Epoch [80/100], Loss: 5353.8408\n",
            "Epoch [90/100], Loss: 5675.1300\n",
            "Epoch [100/100], Loss: 2361.6395\n",
            "Fold 5, RMSE: 43.681373596191406\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 64.30514602661133\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 18227.3159\n",
            "Epoch [20/150], Loss: 10875.7019\n",
            "Epoch [30/150], Loss: 5782.9951\n",
            "Epoch [40/150], Loss: 11378.1704\n",
            "Epoch [50/150], Loss: 9707.8503\n",
            "Epoch [60/150], Loss: 8163.6649\n",
            "Epoch [70/150], Loss: 7153.7493\n",
            "Epoch [80/150], Loss: 6248.4133\n",
            "Epoch [90/150], Loss: 3522.1089\n",
            "Epoch [100/150], Loss: 1776.3939\n",
            "Epoch [110/150], Loss: 5232.0648\n",
            "Epoch [120/150], Loss: 2793.6143\n",
            "Epoch [130/150], Loss: 3885.2091\n",
            "Epoch [140/150], Loss: 4536.1205\n",
            "Epoch [150/150], Loss: 3388.3600\n",
            "Fold 1, RMSE: 53.75932693481445\n",
            "Epoch [10/150], Loss: 16945.5305\n",
            "Epoch [20/150], Loss: 17327.4561\n",
            "Epoch [30/150], Loss: 13615.0405\n",
            "Epoch [40/150], Loss: 19618.2124\n",
            "Epoch [50/150], Loss: 4093.8027\n",
            "Epoch [60/150], Loss: 5113.3256\n",
            "Epoch [70/150], Loss: 10710.0489\n",
            "Epoch [80/150], Loss: 10736.9938\n",
            "Epoch [90/150], Loss: 6246.8972\n",
            "Epoch [100/150], Loss: 10346.5510\n",
            "Epoch [110/150], Loss: 4224.5243\n",
            "Epoch [120/150], Loss: 4480.3901\n",
            "Epoch [130/150], Loss: 10075.9016\n",
            "Epoch [140/150], Loss: 3068.0651\n",
            "Epoch [150/150], Loss: 3985.9655\n",
            "Fold 2, RMSE: 68.03247833251953\n",
            "Epoch [10/150], Loss: 10923.1616\n",
            "Epoch [20/150], Loss: 10610.8359\n",
            "Epoch [30/150], Loss: 5625.5638\n",
            "Epoch [40/150], Loss: 10326.2561\n",
            "Epoch [50/150], Loss: 6236.7742\n",
            "Epoch [60/150], Loss: 5277.3607\n",
            "Epoch [70/150], Loss: 2895.5078\n",
            "Epoch [80/150], Loss: 3641.2594\n",
            "Epoch [90/150], Loss: 4412.4528\n",
            "Epoch [100/150], Loss: 5841.7133\n",
            "Epoch [110/150], Loss: 4758.0096\n",
            "Epoch [120/150], Loss: 8926.0225\n",
            "Epoch [130/150], Loss: 6970.3395\n",
            "Epoch [140/150], Loss: 9510.7915\n",
            "Epoch [150/150], Loss: 4608.4339\n",
            "Fold 3, RMSE: 97.09821319580078\n",
            "Epoch [10/150], Loss: 17549.2183\n",
            "Epoch [20/150], Loss: 12528.4102\n",
            "Epoch [30/150], Loss: 7099.1255\n",
            "Epoch [40/150], Loss: 11764.6403\n",
            "Epoch [50/150], Loss: 2881.7834\n",
            "Epoch [60/150], Loss: 4254.3641\n",
            "Epoch [70/150], Loss: 3477.1596\n",
            "Epoch [80/150], Loss: 2745.0688\n",
            "Epoch [90/150], Loss: 4484.6276\n",
            "Epoch [100/150], Loss: 1926.1536\n",
            "Epoch [110/150], Loss: 3034.7009\n",
            "Epoch [120/150], Loss: 3839.2769\n",
            "Epoch [130/150], Loss: 6191.0234\n",
            "Epoch [140/150], Loss: 2667.7479\n",
            "Epoch [150/150], Loss: 2082.4602\n",
            "Fold 4, RMSE: 40.581504821777344\n",
            "Epoch [10/150], Loss: 15823.1851\n",
            "Epoch [20/150], Loss: 9615.6400\n",
            "Epoch [30/150], Loss: 10851.9314\n",
            "Epoch [40/150], Loss: 16530.5244\n",
            "Epoch [50/150], Loss: 6565.0930\n",
            "Epoch [60/150], Loss: 4110.1202\n",
            "Epoch [70/150], Loss: 5364.7992\n",
            "Epoch [80/150], Loss: 3435.0420\n",
            "Epoch [90/150], Loss: 2390.6248\n",
            "Epoch [100/150], Loss: 949.9984\n",
            "Epoch [110/150], Loss: 2867.1225\n",
            "Epoch [120/150], Loss: 4218.3152\n",
            "Epoch [130/150], Loss: 2294.9038\n",
            "Epoch [140/150], Loss: 1335.9468\n",
            "Epoch [150/150], Loss: 1802.1714\n",
            "Fold 5, RMSE: 44.8346061706543\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 60.86122589111328\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 36415.9600\n",
            "Epoch [20/100], Loss: 15142.5654\n",
            "Epoch [30/100], Loss: 9686.9451\n",
            "Epoch [40/100], Loss: 7792.5686\n",
            "Epoch [50/100], Loss: 9731.6595\n",
            "Epoch [60/100], Loss: 3294.8568\n",
            "Epoch [70/100], Loss: 5915.0654\n",
            "Epoch [80/100], Loss: 7448.5713\n",
            "Epoch [90/100], Loss: 5133.2439\n",
            "Epoch [100/100], Loss: 19149.1577\n",
            "Fold 1, RMSE: 62.031822204589844\n",
            "Epoch [10/100], Loss: 17951.1133\n",
            "Epoch [20/100], Loss: 8444.1008\n",
            "Epoch [30/100], Loss: 10429.8901\n",
            "Epoch [40/100], Loss: 17959.3671\n",
            "Epoch [50/100], Loss: 7034.7087\n",
            "Epoch [60/100], Loss: 4287.8274\n",
            "Epoch [70/100], Loss: 11604.1023\n",
            "Epoch [80/100], Loss: 5367.6666\n",
            "Epoch [90/100], Loss: 4742.7677\n",
            "Epoch [100/100], Loss: 2411.5541\n",
            "Fold 2, RMSE: 72.54685974121094\n",
            "Epoch [10/100], Loss: 11371.5508\n",
            "Epoch [20/100], Loss: 12479.9016\n",
            "Epoch [30/100], Loss: 8608.2761\n",
            "Epoch [40/100], Loss: 9824.3047\n",
            "Epoch [50/100], Loss: 7324.8888\n",
            "Epoch [60/100], Loss: 3929.3887\n",
            "Epoch [70/100], Loss: 5571.5214\n",
            "Epoch [80/100], Loss: 5615.2468\n",
            "Epoch [90/100], Loss: 4664.2041\n",
            "Epoch [100/100], Loss: 8430.1905\n",
            "Fold 3, RMSE: 91.87384033203125\n",
            "Epoch [10/100], Loss: 24507.7910\n",
            "Epoch [20/100], Loss: 15710.4016\n",
            "Epoch [30/100], Loss: 11075.7361\n",
            "Epoch [40/100], Loss: 6332.7198\n",
            "Epoch [50/100], Loss: 9368.4410\n",
            "Epoch [60/100], Loss: 4258.0782\n",
            "Epoch [70/100], Loss: 4081.7066\n",
            "Epoch [80/100], Loss: 3909.0989\n",
            "Epoch [90/100], Loss: 6269.1497\n",
            "Epoch [100/100], Loss: 5051.3820\n",
            "Fold 4, RMSE: 42.16548538208008\n",
            "Epoch [10/100], Loss: 19572.0547\n",
            "Epoch [20/100], Loss: 10268.2601\n",
            "Epoch [30/100], Loss: 11091.3325\n",
            "Epoch [40/100], Loss: 5258.9022\n",
            "Epoch [50/100], Loss: 9268.8779\n",
            "Epoch [60/100], Loss: 7020.6071\n",
            "Epoch [70/100], Loss: 3085.0825\n",
            "Epoch [80/100], Loss: 3709.0741\n",
            "Epoch [90/100], Loss: 5844.4751\n",
            "Epoch [100/100], Loss: 8951.2015\n",
            "Fold 5, RMSE: 48.715911865234375\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 63.466783905029295\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 26527.1318\n",
            "Epoch [20/150], Loss: 21505.0747\n",
            "Epoch [30/150], Loss: 16459.0034\n",
            "Epoch [40/150], Loss: 16587.9026\n",
            "Epoch [50/150], Loss: 17218.3918\n",
            "Epoch [60/150], Loss: 19793.7148\n",
            "Epoch [70/150], Loss: 26401.7344\n",
            "Epoch [80/150], Loss: 18386.2168\n",
            "Epoch [90/150], Loss: 20218.9165\n",
            "Epoch [100/150], Loss: 16167.2750\n",
            "Epoch [110/150], Loss: 24951.7930\n",
            "Epoch [120/150], Loss: 17366.0369\n",
            "Epoch [130/150], Loss: 17218.3286\n",
            "Epoch [140/150], Loss: 15464.7843\n",
            "Epoch [150/150], Loss: 15752.6934\n",
            "Fold 1, RMSE: 67.14002227783203\n",
            "Epoch [10/150], Loss: 17495.8315\n",
            "Epoch [20/150], Loss: 17953.1438\n",
            "Epoch [30/150], Loss: 12755.4314\n",
            "Epoch [40/150], Loss: 8214.7046\n",
            "Epoch [50/150], Loss: 8946.5989\n",
            "Epoch [60/150], Loss: 6579.1655\n",
            "Epoch [70/150], Loss: 14852.8951\n",
            "Epoch [80/150], Loss: 7200.8702\n",
            "Epoch [90/150], Loss: 14709.1152\n",
            "Epoch [100/150], Loss: 7963.1655\n",
            "Epoch [110/150], Loss: 6786.3055\n",
            "Epoch [120/150], Loss: 10908.0103\n",
            "Epoch [130/150], Loss: 4352.4789\n",
            "Epoch [140/150], Loss: 6012.0789\n",
            "Epoch [150/150], Loss: 17566.5588\n",
            "Fold 2, RMSE: 70.39553833007812\n",
            "Epoch [10/150], Loss: 14398.5908\n",
            "Epoch [20/150], Loss: 14123.7383\n",
            "Epoch [30/150], Loss: 14623.8130\n",
            "Epoch [40/150], Loss: 15759.3218\n",
            "Epoch [50/150], Loss: 10681.8657\n",
            "Epoch [60/150], Loss: 7363.7058\n",
            "Epoch [70/150], Loss: 6937.2312\n",
            "Epoch [80/150], Loss: 3029.1996\n",
            "Epoch [90/150], Loss: 9543.9160\n",
            "Epoch [100/150], Loss: 3818.4983\n",
            "Epoch [110/150], Loss: 4477.6889\n",
            "Epoch [120/150], Loss: 4933.5374\n",
            "Epoch [130/150], Loss: 3889.7752\n",
            "Epoch [140/150], Loss: 7649.5297\n",
            "Epoch [150/150], Loss: 3784.7076\n",
            "Fold 3, RMSE: 92.31147766113281\n",
            "Epoch [10/150], Loss: 17316.1328\n",
            "Epoch [20/150], Loss: 19738.2449\n",
            "Epoch [30/150], Loss: 11809.9250\n",
            "Epoch [40/150], Loss: 9506.5352\n",
            "Epoch [50/150], Loss: 7963.5776\n",
            "Epoch [60/150], Loss: 5272.5604\n",
            "Epoch [70/150], Loss: 8243.6772\n",
            "Epoch [80/150], Loss: 8859.3824\n",
            "Epoch [90/150], Loss: 5971.9640\n",
            "Epoch [100/150], Loss: 3873.6299\n",
            "Epoch [110/150], Loss: 3180.3456\n",
            "Epoch [120/150], Loss: 3854.0292\n",
            "Epoch [130/150], Loss: 3143.1290\n",
            "Epoch [140/150], Loss: 6864.1564\n",
            "Epoch [150/150], Loss: 3680.1716\n",
            "Fold 4, RMSE: 38.62995529174805\n",
            "Epoch [10/150], Loss: 17457.5278\n",
            "Epoch [20/150], Loss: 9761.1390\n",
            "Epoch [30/150], Loss: 9222.5867\n",
            "Epoch [40/150], Loss: 9979.7156\n",
            "Epoch [50/150], Loss: 4709.1001\n",
            "Epoch [60/150], Loss: 5003.9250\n",
            "Epoch [70/150], Loss: 3484.3224\n",
            "Epoch [80/150], Loss: 3844.7547\n",
            "Epoch [90/150], Loss: 4213.6227\n",
            "Epoch [100/150], Loss: 2624.4129\n",
            "Epoch [110/150], Loss: 2356.1684\n",
            "Epoch [120/150], Loss: 3557.7879\n",
            "Epoch [130/150], Loss: 7877.5051\n",
            "Epoch [140/150], Loss: 5668.9873\n",
            "Epoch [150/150], Loss: 5886.3947\n",
            "Fold 5, RMSE: 47.028072357177734\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 63.10101318359375\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18216.8230\n",
            "Epoch [20/100], Loss: 8045.7451\n",
            "Epoch [30/100], Loss: 7756.7058\n",
            "Epoch [40/100], Loss: 7638.7417\n",
            "Epoch [50/100], Loss: 5876.2012\n",
            "Epoch [60/100], Loss: 7376.0873\n",
            "Epoch [70/100], Loss: 3960.4855\n",
            "Epoch [80/100], Loss: 3050.6044\n",
            "Epoch [90/100], Loss: 7667.8390\n",
            "Epoch [100/100], Loss: 3557.1228\n",
            "Fold 1, RMSE: 60.94047546386719\n",
            "Epoch [10/100], Loss: 13394.9241\n",
            "Epoch [20/100], Loss: 10509.9829\n",
            "Epoch [30/100], Loss: 8066.0757\n",
            "Epoch [40/100], Loss: 10632.6884\n",
            "Epoch [50/100], Loss: 4539.9247\n",
            "Epoch [60/100], Loss: 3392.0028\n",
            "Epoch [70/100], Loss: 3300.8198\n",
            "Epoch [80/100], Loss: 7433.4869\n",
            "Epoch [90/100], Loss: 6911.1625\n",
            "Epoch [100/100], Loss: 2021.8858\n",
            "Fold 2, RMSE: 63.90781784057617\n",
            "Epoch [10/100], Loss: 5730.2532\n",
            "Epoch [20/100], Loss: 13205.4761\n",
            "Epoch [30/100], Loss: 14357.4980\n",
            "Epoch [40/100], Loss: 13392.9893\n",
            "Epoch [50/100], Loss: 16723.3950\n",
            "Epoch [60/100], Loss: 12426.8662\n",
            "Epoch [70/100], Loss: 15348.2949\n",
            "Epoch [80/100], Loss: 11044.0253\n",
            "Epoch [90/100], Loss: 12136.6545\n",
            "Epoch [100/100], Loss: 13251.1865\n",
            "Fold 3, RMSE: 109.00575256347656\n",
            "Epoch [10/100], Loss: 12838.4368\n",
            "Epoch [20/100], Loss: 7343.8358\n",
            "Epoch [30/100], Loss: 4849.4721\n",
            "Epoch [40/100], Loss: 7113.8934\n",
            "Epoch [50/100], Loss: 7044.0192\n",
            "Epoch [60/100], Loss: 6147.7873\n",
            "Epoch [70/100], Loss: 6355.7214\n",
            "Epoch [80/100], Loss: 4958.9975\n",
            "Epoch [90/100], Loss: 3898.0204\n",
            "Epoch [100/100], Loss: 3027.7178\n",
            "Fold 4, RMSE: 39.110809326171875\n",
            "Epoch [10/100], Loss: 17646.9067\n",
            "Epoch [20/100], Loss: 12475.2786\n",
            "Epoch [30/100], Loss: 8244.5997\n",
            "Epoch [40/100], Loss: 8481.1990\n",
            "Epoch [50/100], Loss: 15662.7567\n",
            "Epoch [60/100], Loss: 5347.0563\n",
            "Epoch [70/100], Loss: 6751.8123\n",
            "Epoch [80/100], Loss: 2239.1256\n",
            "Epoch [90/100], Loss: 2944.2180\n",
            "Epoch [100/100], Loss: 9053.1317\n",
            "Fold 5, RMSE: 47.43465042114258\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 64.07990112304688\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 10056.9668\n",
            "Epoch [20/150], Loss: 7497.9910\n",
            "Epoch [30/150], Loss: 4661.8793\n",
            "Epoch [40/150], Loss: 10420.7300\n",
            "Epoch [50/150], Loss: 4818.9276\n",
            "Epoch [60/150], Loss: 4759.8752\n",
            "Epoch [70/150], Loss: 3482.3434\n",
            "Epoch [80/150], Loss: 5258.5312\n",
            "Epoch [90/150], Loss: 2799.2661\n",
            "Epoch [100/150], Loss: 7104.8777\n",
            "Epoch [110/150], Loss: 6789.2898\n",
            "Epoch [120/150], Loss: 3152.3953\n",
            "Epoch [130/150], Loss: 2030.5303\n",
            "Epoch [140/150], Loss: 1845.0184\n",
            "Epoch [150/150], Loss: 3221.7515\n",
            "Fold 1, RMSE: 57.043087005615234\n",
            "Epoch [10/150], Loss: 21410.4272\n",
            "Epoch [20/150], Loss: 12683.6343\n",
            "Epoch [30/150], Loss: 9369.6456\n",
            "Epoch [40/150], Loss: 6072.5963\n",
            "Epoch [50/150], Loss: 4514.8647\n",
            "Epoch [60/150], Loss: 7439.0587\n",
            "Epoch [70/150], Loss: 2339.9162\n",
            "Epoch [80/150], Loss: 4324.5967\n",
            "Epoch [90/150], Loss: 6785.3101\n",
            "Epoch [100/150], Loss: 5652.6742\n",
            "Epoch [110/150], Loss: 5781.7747\n",
            "Epoch [120/150], Loss: 7482.7073\n",
            "Epoch [130/150], Loss: 2754.6488\n",
            "Epoch [140/150], Loss: 4332.2444\n",
            "Epoch [150/150], Loss: 1950.3440\n",
            "Fold 2, RMSE: 63.833770751953125\n",
            "Epoch [10/150], Loss: 15500.4060\n",
            "Epoch [20/150], Loss: 5919.2413\n",
            "Epoch [30/150], Loss: 3353.7639\n",
            "Epoch [40/150], Loss: 4354.0010\n",
            "Epoch [50/150], Loss: 1809.4957\n",
            "Epoch [60/150], Loss: 1830.1576\n",
            "Epoch [70/150], Loss: 5193.1688\n",
            "Epoch [80/150], Loss: 3778.3710\n",
            "Epoch [90/150], Loss: 1346.8606\n",
            "Epoch [100/150], Loss: 1725.5093\n",
            "Epoch [110/150], Loss: 3262.4075\n",
            "Epoch [120/150], Loss: 2261.8032\n",
            "Epoch [130/150], Loss: 4799.2981\n",
            "Epoch [140/150], Loss: 769.2769\n",
            "Epoch [150/150], Loss: 3044.7582\n",
            "Fold 3, RMSE: 93.46806335449219\n",
            "Epoch [10/150], Loss: 13072.7390\n",
            "Epoch [20/150], Loss: 10446.4556\n",
            "Epoch [30/150], Loss: 9498.0359\n",
            "Epoch [40/150], Loss: 3590.8161\n",
            "Epoch [50/150], Loss: 5584.0447\n",
            "Epoch [60/150], Loss: 9067.4479\n",
            "Epoch [70/150], Loss: 4835.0087\n",
            "Epoch [80/150], Loss: 4053.7493\n",
            "Epoch [90/150], Loss: 5337.8448\n",
            "Epoch [100/150], Loss: 2740.5492\n",
            "Epoch [110/150], Loss: 4869.4403\n",
            "Epoch [120/150], Loss: 2027.8547\n",
            "Epoch [130/150], Loss: 3251.4264\n",
            "Epoch [140/150], Loss: 1909.7432\n",
            "Epoch [150/150], Loss: 4386.4617\n",
            "Fold 4, RMSE: 43.95911407470703\n",
            "Epoch [10/150], Loss: 14484.2446\n",
            "Epoch [20/150], Loss: 12014.5388\n",
            "Epoch [30/150], Loss: 6703.4542\n",
            "Epoch [40/150], Loss: 3928.4954\n",
            "Epoch [50/150], Loss: 3224.9683\n",
            "Epoch [60/150], Loss: 4873.2901\n",
            "Epoch [70/150], Loss: 2364.5497\n",
            "Epoch [80/150], Loss: 4298.4130\n",
            "Epoch [90/150], Loss: 4885.4134\n",
            "Epoch [100/150], Loss: 3770.2963\n",
            "Epoch [110/150], Loss: 5274.8613\n",
            "Epoch [120/150], Loss: 3718.4260\n",
            "Epoch [130/150], Loss: 1938.5201\n",
            "Epoch [140/150], Loss: 2856.4156\n",
            "Epoch [150/150], Loss: 3041.2443\n",
            "Fold 5, RMSE: 43.85991668701172\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 60.43279037475586\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 10737.9526\n",
            "Epoch [20/100], Loss: 12578.8940\n",
            "Epoch [30/100], Loss: 10161.7832\n",
            "Epoch [40/100], Loss: 5029.5966\n",
            "Epoch [50/100], Loss: 7354.0711\n",
            "Epoch [60/100], Loss: 8559.1230\n",
            "Epoch [70/100], Loss: 14776.7949\n",
            "Epoch [80/100], Loss: 5554.2511\n",
            "Epoch [90/100], Loss: 8641.9810\n",
            "Epoch [100/100], Loss: 5711.7953\n",
            "Fold 1, RMSE: 60.401878356933594\n",
            "Epoch [10/100], Loss: 21667.6362\n",
            "Epoch [20/100], Loss: 12985.1721\n",
            "Epoch [30/100], Loss: 10139.6787\n",
            "Epoch [40/100], Loss: 10594.9758\n",
            "Epoch [50/100], Loss: 8472.7225\n",
            "Epoch [60/100], Loss: 8820.9224\n",
            "Epoch [70/100], Loss: 5180.3762\n",
            "Epoch [80/100], Loss: 4300.1560\n",
            "Epoch [90/100], Loss: 3818.2568\n",
            "Epoch [100/100], Loss: 3170.5316\n",
            "Fold 2, RMSE: 62.136165618896484\n",
            "Epoch [10/100], Loss: 10312.1570\n",
            "Epoch [20/100], Loss: 7253.5654\n",
            "Epoch [30/100], Loss: 8110.3179\n",
            "Epoch [40/100], Loss: 7216.9861\n",
            "Epoch [50/100], Loss: 3929.5291\n",
            "Epoch [60/100], Loss: 3420.8740\n",
            "Epoch [70/100], Loss: 2728.3488\n",
            "Epoch [80/100], Loss: 3022.0842\n",
            "Epoch [90/100], Loss: 5178.6456\n",
            "Epoch [100/100], Loss: 3732.9672\n",
            "Fold 3, RMSE: 100.54228210449219\n",
            "Epoch [10/100], Loss: 22545.8315\n",
            "Epoch [20/100], Loss: 13041.3240\n",
            "Epoch [30/100], Loss: 9751.3206\n",
            "Epoch [40/100], Loss: 11113.5057\n",
            "Epoch [50/100], Loss: 10594.9082\n",
            "Epoch [60/100], Loss: 4343.7144\n",
            "Epoch [70/100], Loss: 8953.7227\n",
            "Epoch [80/100], Loss: 7743.3929\n",
            "Epoch [90/100], Loss: 2885.7218\n",
            "Epoch [100/100], Loss: 4961.9811\n",
            "Fold 4, RMSE: 45.228370666503906\n",
            "Epoch [10/100], Loss: 11503.1025\n",
            "Epoch [20/100], Loss: 11795.4540\n",
            "Epoch [30/100], Loss: 15598.1885\n",
            "Epoch [40/100], Loss: 12016.4830\n",
            "Epoch [50/100], Loss: 6145.5677\n",
            "Epoch [60/100], Loss: 6795.2841\n",
            "Epoch [70/100], Loss: 4217.0526\n",
            "Epoch [80/100], Loss: 4060.9460\n",
            "Epoch [90/100], Loss: 6591.7198\n",
            "Epoch [100/100], Loss: 9057.2712\n",
            "Fold 5, RMSE: 44.30677795410156\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 62.523094940185544\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19974.0229\n",
            "Epoch [20/150], Loss: 20603.6833\n",
            "Epoch [30/150], Loss: 17063.7349\n",
            "Epoch [40/150], Loss: 15502.8102\n",
            "Epoch [50/150], Loss: 26569.1196\n",
            "Epoch [60/150], Loss: 22884.5171\n",
            "Epoch [70/150], Loss: 20125.4829\n",
            "Epoch [80/150], Loss: 21606.1152\n",
            "Epoch [90/150], Loss: 19604.1963\n",
            "Epoch [100/150], Loss: 22219.7817\n",
            "Epoch [110/150], Loss: 23384.9346\n",
            "Epoch [120/150], Loss: 24023.5186\n",
            "Epoch [130/150], Loss: 24155.1653\n",
            "Epoch [140/150], Loss: 18995.3457\n",
            "Epoch [150/150], Loss: 16752.0088\n",
            "Fold 1, RMSE: 67.2691650390625\n",
            "Epoch [10/150], Loss: 14173.3530\n",
            "Epoch [20/150], Loss: 14576.9707\n",
            "Epoch [30/150], Loss: 15095.6047\n",
            "Epoch [40/150], Loss: 17632.5828\n",
            "Epoch [50/150], Loss: 25024.9176\n",
            "Epoch [60/150], Loss: 17277.3613\n",
            "Epoch [70/150], Loss: 17160.6414\n",
            "Epoch [80/150], Loss: 15063.6116\n",
            "Epoch [90/150], Loss: 26607.2275\n",
            "Epoch [100/150], Loss: 23123.7759\n",
            "Epoch [110/150], Loss: 16383.0083\n",
            "Epoch [120/150], Loss: 14443.7136\n",
            "Epoch [130/150], Loss: 18396.5112\n",
            "Epoch [140/150], Loss: 15439.0703\n",
            "Epoch [150/150], Loss: 14355.4331\n",
            "Fold 2, RMSE: 87.1116714477539\n",
            "Epoch [10/150], Loss: 17506.6724\n",
            "Epoch [20/150], Loss: 10695.5142\n",
            "Epoch [30/150], Loss: 9237.0171\n",
            "Epoch [40/150], Loss: 11382.4614\n",
            "Epoch [50/150], Loss: 7066.3030\n",
            "Epoch [60/150], Loss: 8802.1987\n",
            "Epoch [70/150], Loss: 4974.3862\n",
            "Epoch [80/150], Loss: 5610.0298\n",
            "Epoch [90/150], Loss: 4546.9396\n",
            "Epoch [100/150], Loss: 4026.0954\n",
            "Epoch [110/150], Loss: 8003.4697\n",
            "Epoch [120/150], Loss: 2426.8984\n",
            "Epoch [130/150], Loss: 4776.7656\n",
            "Epoch [140/150], Loss: 7002.4517\n",
            "Epoch [150/150], Loss: 4789.7769\n",
            "Fold 3, RMSE: 107.1279296875\n",
            "Epoch [10/150], Loss: 13959.4868\n",
            "Epoch [20/150], Loss: 11582.5027\n",
            "Epoch [30/150], Loss: 6067.1194\n",
            "Epoch [40/150], Loss: 9955.9018\n",
            "Epoch [50/150], Loss: 10556.4465\n",
            "Epoch [60/150], Loss: 6235.8157\n",
            "Epoch [70/150], Loss: 5659.2941\n",
            "Epoch [80/150], Loss: 7606.9036\n",
            "Epoch [90/150], Loss: 6748.4072\n",
            "Epoch [100/150], Loss: 6097.4435\n",
            "Epoch [110/150], Loss: 13090.4609\n",
            "Epoch [120/150], Loss: 12366.1046\n",
            "Epoch [130/150], Loss: 6319.8467\n",
            "Epoch [140/150], Loss: 11770.2732\n",
            "Epoch [150/150], Loss: 4479.4642\n",
            "Fold 4, RMSE: 42.638916015625\n",
            "Epoch [10/150], Loss: 16902.0796\n",
            "Epoch [20/150], Loss: 13371.6235\n",
            "Epoch [30/150], Loss: 8816.3977\n",
            "Epoch [40/150], Loss: 8494.5062\n",
            "Epoch [50/150], Loss: 8284.0311\n",
            "Epoch [60/150], Loss: 4531.8038\n",
            "Epoch [70/150], Loss: 8605.0144\n",
            "Epoch [80/150], Loss: 7928.7089\n",
            "Epoch [90/150], Loss: 6698.2681\n",
            "Epoch [100/150], Loss: 6650.3346\n",
            "Epoch [110/150], Loss: 9045.2023\n",
            "Epoch [120/150], Loss: 4212.2717\n",
            "Epoch [130/150], Loss: 6541.9125\n",
            "Epoch [140/150], Loss: 4486.0397\n",
            "Epoch [150/150], Loss: 7384.5575\n",
            "Fold 5, RMSE: 45.53406524658203\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 69.93634948730468\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 15916.3560\n",
            "Epoch [20/100], Loss: 8981.2847\n",
            "Epoch [30/100], Loss: 8897.4617\n",
            "Epoch [40/100], Loss: 11054.7195\n",
            "Epoch [50/100], Loss: 10424.7551\n",
            "Epoch [60/100], Loss: 3922.6356\n",
            "Epoch [70/100], Loss: 4787.3312\n",
            "Epoch [80/100], Loss: 3617.0479\n",
            "Epoch [90/100], Loss: 1590.3382\n",
            "Epoch [100/100], Loss: 3469.3442\n",
            "Fold 1, RMSE: 60.38620376586914\n",
            "Epoch [10/100], Loss: 11861.7864\n",
            "Epoch [20/100], Loss: 7096.7484\n",
            "Epoch [30/100], Loss: 6747.0121\n",
            "Epoch [40/100], Loss: 6329.9011\n",
            "Epoch [50/100], Loss: 2275.2040\n",
            "Epoch [60/100], Loss: 5916.9821\n",
            "Epoch [70/100], Loss: 2410.2830\n",
            "Epoch [80/100], Loss: 2963.7374\n",
            "Epoch [90/100], Loss: 3697.0250\n",
            "Epoch [100/100], Loss: 5387.9319\n",
            "Fold 2, RMSE: 69.64453125\n",
            "Epoch [10/100], Loss: 8535.4407\n",
            "Epoch [20/100], Loss: 7482.0802\n",
            "Epoch [30/100], Loss: 5422.2354\n",
            "Epoch [40/100], Loss: 3619.2386\n",
            "Epoch [50/100], Loss: 5961.8533\n",
            "Epoch [60/100], Loss: 3372.6580\n",
            "Epoch [70/100], Loss: 6772.6487\n",
            "Epoch [80/100], Loss: 2735.2319\n",
            "Epoch [90/100], Loss: 2682.0352\n",
            "Epoch [100/100], Loss: 4229.0334\n",
            "Fold 3, RMSE: 91.8179702758789\n",
            "Epoch [10/100], Loss: 13680.7129\n",
            "Epoch [20/100], Loss: 12828.6697\n",
            "Epoch [30/100], Loss: 9221.3274\n",
            "Epoch [40/100], Loss: 5848.2155\n",
            "Epoch [50/100], Loss: 9136.1629\n",
            "Epoch [60/100], Loss: 5368.9033\n",
            "Epoch [70/100], Loss: 5858.6628\n",
            "Epoch [80/100], Loss: 4504.7505\n",
            "Epoch [90/100], Loss: 10942.5830\n",
            "Epoch [100/100], Loss: 11822.7449\n",
            "Fold 4, RMSE: 46.49128341674805\n",
            "Epoch [10/100], Loss: 14204.2281\n",
            "Epoch [20/100], Loss: 18299.7715\n",
            "Epoch [30/100], Loss: 10944.1716\n",
            "Epoch [40/100], Loss: 6666.3247\n",
            "Epoch [50/100], Loss: 9291.3000\n",
            "Epoch [60/100], Loss: 2800.4948\n",
            "Epoch [70/100], Loss: 7907.0410\n",
            "Epoch [80/100], Loss: 11033.0356\n",
            "Epoch [90/100], Loss: 5073.6967\n",
            "Epoch [100/100], Loss: 4475.0873\n",
            "Fold 5, RMSE: 50.67428970336914\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 63.80285568237305\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13988.1663\n",
            "Epoch [20/150], Loss: 9293.9025\n",
            "Epoch [30/150], Loss: 7622.9152\n",
            "Epoch [40/150], Loss: 4773.0449\n",
            "Epoch [50/150], Loss: 6515.5156\n",
            "Epoch [60/150], Loss: 6809.7749\n",
            "Epoch [70/150], Loss: 5397.5250\n",
            "Epoch [80/150], Loss: 5906.6641\n",
            "Epoch [90/150], Loss: 4329.0243\n",
            "Epoch [100/150], Loss: 2405.5912\n",
            "Epoch [110/150], Loss: 1548.9100\n",
            "Epoch [120/150], Loss: 3878.2867\n",
            "Epoch [130/150], Loss: 2555.6450\n",
            "Epoch [140/150], Loss: 2688.2337\n",
            "Epoch [150/150], Loss: 3521.2119\n",
            "Fold 1, RMSE: 54.5289192199707\n",
            "Epoch [10/150], Loss: 24262.5759\n",
            "Epoch [20/150], Loss: 10993.5378\n",
            "Epoch [30/150], Loss: 10730.7397\n",
            "Epoch [40/150], Loss: 14017.4128\n",
            "Epoch [50/150], Loss: 6125.2906\n",
            "Epoch [60/150], Loss: 5881.8408\n",
            "Epoch [70/150], Loss: 7336.9677\n",
            "Epoch [80/150], Loss: 3986.5335\n",
            "Epoch [90/150], Loss: 1947.3815\n",
            "Epoch [100/150], Loss: 4745.1000\n",
            "Epoch [110/150], Loss: 4988.4794\n",
            "Epoch [120/150], Loss: 12932.5667\n",
            "Epoch [130/150], Loss: 3486.7856\n",
            "Epoch [140/150], Loss: 14731.6276\n",
            "Epoch [150/150], Loss: 3231.9996\n",
            "Fold 2, RMSE: 68.90119934082031\n",
            "Epoch [10/150], Loss: 11938.5825\n",
            "Epoch [20/150], Loss: 6293.7588\n",
            "Epoch [30/150], Loss: 6166.1049\n",
            "Epoch [40/150], Loss: 5689.8337\n",
            "Epoch [50/150], Loss: 6448.0236\n",
            "Epoch [60/150], Loss: 6346.6674\n",
            "Epoch [70/150], Loss: 3081.3435\n",
            "Epoch [80/150], Loss: 2830.5984\n",
            "Epoch [90/150], Loss: 5687.8104\n",
            "Epoch [100/150], Loss: 2154.2600\n",
            "Epoch [110/150], Loss: 2032.0229\n",
            "Epoch [120/150], Loss: 4088.3058\n",
            "Epoch [130/150], Loss: 2426.1525\n",
            "Epoch [140/150], Loss: 2947.9852\n",
            "Epoch [150/150], Loss: 1065.5441\n",
            "Fold 3, RMSE: 95.24693298339844\n",
            "Epoch [10/150], Loss: 12663.3002\n",
            "Epoch [20/150], Loss: 9085.6333\n",
            "Epoch [30/150], Loss: 7884.1285\n",
            "Epoch [40/150], Loss: 4788.3896\n",
            "Epoch [50/150], Loss: 3968.5350\n",
            "Epoch [60/150], Loss: 6548.3939\n",
            "Epoch [70/150], Loss: 5103.9860\n",
            "Epoch [80/150], Loss: 5144.3155\n",
            "Epoch [90/150], Loss: 3949.3236\n",
            "Epoch [100/150], Loss: 2741.8499\n",
            "Epoch [110/150], Loss: 1937.6666\n",
            "Epoch [120/150], Loss: 1972.4194\n",
            "Epoch [130/150], Loss: 1356.9919\n",
            "Epoch [140/150], Loss: 14642.6833\n",
            "Epoch [150/150], Loss: 1061.1013\n",
            "Fold 4, RMSE: 42.809776306152344\n",
            "Epoch [10/150], Loss: 21052.0723\n",
            "Epoch [20/150], Loss: 13857.2727\n",
            "Epoch [30/150], Loss: 11957.5198\n",
            "Epoch [40/150], Loss: 6295.9794\n",
            "Epoch [50/150], Loss: 9546.1338\n",
            "Epoch [60/150], Loss: 9394.3446\n",
            "Epoch [70/150], Loss: 5219.5265\n",
            "Epoch [80/150], Loss: 3594.4647\n",
            "Epoch [90/150], Loss: 2639.7917\n",
            "Epoch [100/150], Loss: 4356.7783\n",
            "Epoch [110/150], Loss: 8518.8691\n",
            "Epoch [120/150], Loss: 6468.8066\n",
            "Epoch [130/150], Loss: 1111.5771\n",
            "Epoch [140/150], Loss: 3185.1317\n",
            "Epoch [150/150], Loss: 6424.6406\n",
            "Fold 5, RMSE: 45.71461868286133\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 61.44028930664062\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 20544.0776\n",
            "Epoch [20/100], Loss: 16657.6609\n",
            "Epoch [30/100], Loss: 20100.8938\n",
            "Epoch [40/100], Loss: 19982.7603\n",
            "Epoch [50/100], Loss: 16589.3860\n",
            "Epoch [60/100], Loss: 20339.0142\n",
            "Epoch [70/100], Loss: 19488.9492\n",
            "Epoch [80/100], Loss: 19518.5815\n",
            "Epoch [90/100], Loss: 25325.7156\n",
            "Epoch [100/100], Loss: 16914.2163\n",
            "Fold 1, RMSE: 67.4876480102539\n",
            "Epoch [10/100], Loss: 121564.9453\n",
            "Epoch [20/100], Loss: 29815.3369\n",
            "Epoch [30/100], Loss: 25285.7354\n",
            "Epoch [40/100], Loss: 13322.9355\n",
            "Epoch [50/100], Loss: 13588.7146\n",
            "Epoch [60/100], Loss: 14847.7178\n",
            "Epoch [70/100], Loss: 15933.0377\n",
            "Epoch [80/100], Loss: 11572.5688\n",
            "Epoch [90/100], Loss: 12482.8390\n",
            "Epoch [100/100], Loss: 9693.9447\n",
            "Fold 2, RMSE: 69.29475402832031\n",
            "Epoch [10/100], Loss: 50086.2578\n",
            "Epoch [20/100], Loss: 11946.9746\n",
            "Epoch [30/100], Loss: 12276.5747\n",
            "Epoch [40/100], Loss: 11089.1287\n",
            "Epoch [50/100], Loss: 13245.6394\n",
            "Epoch [60/100], Loss: 10929.5232\n",
            "Epoch [70/100], Loss: 8480.8687\n",
            "Epoch [80/100], Loss: 8661.1909\n",
            "Epoch [90/100], Loss: 8647.8789\n",
            "Epoch [100/100], Loss: 7057.1902\n",
            "Fold 3, RMSE: 92.01190185546875\n",
            "Epoch [10/100], Loss: 82013.2109\n",
            "Epoch [20/100], Loss: 15908.1726\n",
            "Epoch [30/100], Loss: 15718.6135\n",
            "Epoch [40/100], Loss: 13447.0496\n",
            "Epoch [50/100], Loss: 15642.0684\n",
            "Epoch [60/100], Loss: 14237.7686\n",
            "Epoch [70/100], Loss: 11131.7700\n",
            "Epoch [80/100], Loss: 10951.2507\n",
            "Epoch [90/100], Loss: 11010.1353\n",
            "Epoch [100/100], Loss: 8126.5798\n",
            "Fold 4, RMSE: 36.77366638183594\n",
            "Epoch [10/100], Loss: 598683.1719\n",
            "Epoch [20/100], Loss: 61915.4922\n",
            "Epoch [30/100], Loss: 26272.1479\n",
            "Epoch [40/100], Loss: 19587.2048\n",
            "Epoch [50/100], Loss: 14445.5581\n",
            "Epoch [60/100], Loss: 14167.7405\n",
            "Epoch [70/100], Loss: 17264.7363\n",
            "Epoch [80/100], Loss: 13248.8547\n",
            "Epoch [90/100], Loss: 9990.1431\n",
            "Epoch [100/100], Loss: 11400.4097\n",
            "Fold 5, RMSE: 47.608455657958984\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 62.635285186767575\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 404635.4297\n",
            "Epoch [20/150], Loss: 65935.4844\n",
            "Epoch [30/150], Loss: 29570.4668\n",
            "Epoch [40/150], Loss: 17639.0229\n",
            "Epoch [50/150], Loss: 17481.0400\n",
            "Epoch [60/150], Loss: 12093.2593\n",
            "Epoch [70/150], Loss: 18001.6282\n",
            "Epoch [80/150], Loss: 10568.4783\n",
            "Epoch [90/150], Loss: 17380.7229\n",
            "Epoch [100/150], Loss: 15949.9475\n",
            "Epoch [110/150], Loss: 10879.4263\n",
            "Epoch [120/150], Loss: 13355.6584\n",
            "Epoch [130/150], Loss: 11021.1162\n",
            "Epoch [140/150], Loss: 9501.8496\n",
            "Epoch [150/150], Loss: 11637.6812\n",
            "Fold 1, RMSE: 49.741783142089844\n",
            "Epoch [10/150], Loss: 17525.4453\n",
            "Epoch [20/150], Loss: 17050.9712\n",
            "Epoch [30/150], Loss: 16819.9971\n",
            "Epoch [40/150], Loss: 15082.2087\n",
            "Epoch [50/150], Loss: 14832.6753\n",
            "Epoch [60/150], Loss: 14078.3102\n",
            "Epoch [70/150], Loss: 23393.1465\n",
            "Epoch [80/150], Loss: 16775.8428\n",
            "Epoch [90/150], Loss: 17083.5986\n",
            "Epoch [100/150], Loss: 14982.7861\n",
            "Epoch [110/150], Loss: 15282.8555\n",
            "Epoch [120/150], Loss: 18233.7148\n",
            "Epoch [130/150], Loss: 22847.2014\n",
            "Epoch [140/150], Loss: 23522.9634\n",
            "Epoch [150/150], Loss: 16474.8408\n",
            "Fold 2, RMSE: 86.73304748535156\n",
            "Epoch [10/150], Loss: 197113.3594\n",
            "Epoch [20/150], Loss: 31711.4883\n",
            "Epoch [30/150], Loss: 21555.1655\n",
            "Epoch [40/150], Loss: 16604.5933\n",
            "Epoch [50/150], Loss: 12065.3079\n",
            "Epoch [60/150], Loss: 10450.2698\n",
            "Epoch [70/150], Loss: 9311.3866\n",
            "Epoch [80/150], Loss: 11250.7029\n",
            "Epoch [90/150], Loss: 9699.4810\n",
            "Epoch [100/150], Loss: 6611.4161\n",
            "Epoch [110/150], Loss: 9072.6396\n",
            "Epoch [120/150], Loss: 13095.7537\n",
            "Epoch [130/150], Loss: 8216.9961\n",
            "Epoch [140/150], Loss: 10387.7341\n",
            "Epoch [150/150], Loss: 6665.3853\n",
            "Fold 3, RMSE: 93.81770324707031\n",
            "Epoch [10/150], Loss: 27671.1401\n",
            "Epoch [20/150], Loss: 19330.0981\n",
            "Epoch [30/150], Loss: 16829.6635\n",
            "Epoch [40/150], Loss: 18633.3503\n",
            "Epoch [50/150], Loss: 23777.0801\n",
            "Epoch [60/150], Loss: 26463.3655\n",
            "Epoch [70/150], Loss: 23532.2524\n",
            "Epoch [80/150], Loss: 16840.6954\n",
            "Epoch [90/150], Loss: 20969.5542\n",
            "Epoch [100/150], Loss: 27554.7888\n",
            "Epoch [110/150], Loss: 18710.2910\n",
            "Epoch [120/150], Loss: 17430.3915\n",
            "Epoch [130/150], Loss: 21979.9243\n",
            "Epoch [140/150], Loss: 19938.5942\n",
            "Epoch [150/150], Loss: 22116.0200\n",
            "Fold 4, RMSE: 54.034339904785156\n",
            "Epoch [10/150], Loss: 21583.4604\n",
            "Epoch [20/150], Loss: 14267.5208\n",
            "Epoch [30/150], Loss: 14640.8579\n",
            "Epoch [40/150], Loss: 14697.7571\n",
            "Epoch [50/150], Loss: 15433.5732\n",
            "Epoch [60/150], Loss: 12706.1511\n",
            "Epoch [70/150], Loss: 12492.8467\n",
            "Epoch [80/150], Loss: 15614.0044\n",
            "Epoch [90/150], Loss: 16127.5400\n",
            "Epoch [100/150], Loss: 7596.7874\n",
            "Epoch [110/150], Loss: 7681.4530\n",
            "Epoch [120/150], Loss: 10681.9685\n",
            "Epoch [130/150], Loss: 14167.9346\n",
            "Epoch [140/150], Loss: 8197.0631\n",
            "Epoch [150/150], Loss: 14640.4004\n",
            "Fold 5, RMSE: 45.537071228027344\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 65.97278900146485\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 19664.6401\n",
            "Epoch [20/100], Loss: 19120.0796\n",
            "Epoch [30/100], Loss: 24627.6377\n",
            "Epoch [40/100], Loss: 19413.5142\n",
            "Epoch [50/100], Loss: 15721.9727\n",
            "Epoch [60/100], Loss: 18849.3442\n",
            "Epoch [70/100], Loss: 16272.4570\n",
            "Epoch [80/100], Loss: 16254.4399\n",
            "Epoch [90/100], Loss: 11925.5752\n",
            "Epoch [100/100], Loss: 9707.8340\n",
            "Fold 1, RMSE: 49.264923095703125\n",
            "Epoch [10/100], Loss: 23282.7898\n",
            "Epoch [20/100], Loss: 10184.9834\n",
            "Epoch [30/100], Loss: 12512.2371\n",
            "Epoch [40/100], Loss: 17761.3845\n",
            "Epoch [50/100], Loss: 9760.2167\n",
            "Epoch [60/100], Loss: 16614.8550\n",
            "Epoch [70/100], Loss: 9143.1583\n",
            "Epoch [80/100], Loss: 10408.9415\n",
            "Epoch [90/100], Loss: 8424.2090\n",
            "Epoch [100/100], Loss: 9114.0808\n",
            "Fold 2, RMSE: 72.36113739013672\n",
            "Epoch [10/100], Loss: 16246.4536\n",
            "Epoch [20/100], Loss: 11666.3530\n",
            "Epoch [30/100], Loss: 9845.5706\n",
            "Epoch [40/100], Loss: 9101.6523\n",
            "Epoch [50/100], Loss: 8177.6344\n",
            "Epoch [60/100], Loss: 9006.4805\n",
            "Epoch [70/100], Loss: 6915.8959\n",
            "Epoch [80/100], Loss: 6435.5292\n",
            "Epoch [90/100], Loss: 8328.9789\n",
            "Epoch [100/100], Loss: 5875.6597\n",
            "Fold 3, RMSE: 92.0650405883789\n",
            "Epoch [10/100], Loss: 56256.6270\n",
            "Epoch [20/100], Loss: 29578.5430\n",
            "Epoch [30/100], Loss: 14053.9102\n",
            "Epoch [40/100], Loss: 22143.5938\n",
            "Epoch [50/100], Loss: 22572.4546\n",
            "Epoch [60/100], Loss: 15226.5425\n",
            "Epoch [70/100], Loss: 13972.5132\n",
            "Epoch [80/100], Loss: 12759.3618\n",
            "Epoch [90/100], Loss: 16601.3757\n",
            "Epoch [100/100], Loss: 13880.7842\n",
            "Fold 4, RMSE: 39.81196212768555\n",
            "Epoch [10/100], Loss: 21979.1270\n",
            "Epoch [20/100], Loss: 20694.6172\n",
            "Epoch [30/100], Loss: 16317.2886\n",
            "Epoch [40/100], Loss: 16398.5664\n",
            "Epoch [50/100], Loss: 7624.5548\n",
            "Epoch [60/100], Loss: 6136.5586\n",
            "Epoch [70/100], Loss: 9709.8353\n",
            "Epoch [80/100], Loss: 9223.1621\n",
            "Epoch [90/100], Loss: 8388.5675\n",
            "Epoch [100/100], Loss: 9702.7091\n",
            "Fold 5, RMSE: 45.226810455322266\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 59.745974731445315\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 23324.9299\n",
            "Epoch [20/150], Loss: 15438.9111\n",
            "Epoch [30/150], Loss: 21661.9141\n",
            "Epoch [40/150], Loss: 12907.7220\n",
            "Epoch [50/150], Loss: 13926.8538\n",
            "Epoch [60/150], Loss: 25871.7769\n",
            "Epoch [70/150], Loss: 10337.7617\n",
            "Epoch [80/150], Loss: 9451.1230\n",
            "Epoch [90/150], Loss: 11156.6409\n",
            "Epoch [100/150], Loss: 7053.1250\n",
            "Epoch [110/150], Loss: 7635.8845\n",
            "Epoch [120/150], Loss: 14983.6439\n",
            "Epoch [130/150], Loss: 7957.5769\n",
            "Epoch [140/150], Loss: 4334.5751\n",
            "Epoch [150/150], Loss: 8175.0066\n",
            "Fold 1, RMSE: 61.735984802246094\n",
            "Epoch [10/150], Loss: 23900.5723\n",
            "Epoch [20/150], Loss: 24747.7546\n",
            "Epoch [30/150], Loss: 12354.0396\n",
            "Epoch [40/150], Loss: 13065.9387\n",
            "Epoch [50/150], Loss: 11689.7372\n",
            "Epoch [60/150], Loss: 9614.8429\n",
            "Epoch [70/150], Loss: 9635.0831\n",
            "Epoch [80/150], Loss: 11761.2157\n",
            "Epoch [90/150], Loss: 13385.8066\n",
            "Epoch [100/150], Loss: 9289.2700\n",
            "Epoch [110/150], Loss: 9461.7275\n",
            "Epoch [120/150], Loss: 6421.6863\n",
            "Epoch [130/150], Loss: 8502.2312\n",
            "Epoch [140/150], Loss: 4319.0743\n",
            "Epoch [150/150], Loss: 2214.6999\n",
            "Fold 2, RMSE: 72.17351531982422\n",
            "Epoch [10/150], Loss: 13447.0852\n",
            "Epoch [20/150], Loss: 11790.7312\n",
            "Epoch [30/150], Loss: 10201.4045\n",
            "Epoch [40/150], Loss: 9425.5107\n",
            "Epoch [50/150], Loss: 7268.4954\n",
            "Epoch [60/150], Loss: 6468.7492\n",
            "Epoch [70/150], Loss: 5904.3717\n",
            "Epoch [80/150], Loss: 6289.0823\n",
            "Epoch [90/150], Loss: 5463.3502\n",
            "Epoch [100/150], Loss: 6892.9830\n",
            "Epoch [110/150], Loss: 4606.6144\n",
            "Epoch [120/150], Loss: 3466.0197\n",
            "Epoch [130/150], Loss: 5859.8019\n",
            "Epoch [140/150], Loss: 2763.1050\n",
            "Epoch [150/150], Loss: 1519.1451\n",
            "Fold 3, RMSE: 94.57408905029297\n",
            "Epoch [10/150], Loss: 71080.6758\n",
            "Epoch [20/150], Loss: 23538.6421\n",
            "Epoch [30/150], Loss: 20361.1890\n",
            "Epoch [40/150], Loss: 24088.1960\n",
            "Epoch [50/150], Loss: 11909.1223\n",
            "Epoch [60/150], Loss: 17659.5967\n",
            "Epoch [70/150], Loss: 15839.5242\n",
            "Epoch [80/150], Loss: 15029.2756\n",
            "Epoch [90/150], Loss: 15458.1045\n",
            "Epoch [100/150], Loss: 10768.3904\n",
            "Epoch [110/150], Loss: 11614.8965\n",
            "Epoch [120/150], Loss: 12689.2703\n",
            "Epoch [130/150], Loss: 10646.8687\n",
            "Epoch [140/150], Loss: 14893.3418\n",
            "Epoch [150/150], Loss: 11538.3206\n",
            "Fold 4, RMSE: 40.249900817871094\n",
            "Epoch [10/150], Loss: 20947.3721\n",
            "Epoch [20/150], Loss: 21028.9653\n",
            "Epoch [30/150], Loss: 18651.4944\n",
            "Epoch [40/150], Loss: 18359.1086\n",
            "Epoch [50/150], Loss: 18976.6079\n",
            "Epoch [60/150], Loss: 19495.8086\n",
            "Epoch [70/150], Loss: 16622.8136\n",
            "Epoch [80/150], Loss: 25516.9055\n",
            "Epoch [90/150], Loss: 15489.7897\n",
            "Epoch [100/150], Loss: 18827.3770\n",
            "Epoch [110/150], Loss: 23935.8577\n",
            "Epoch [120/150], Loss: 13443.1052\n",
            "Epoch [130/150], Loss: 23051.4851\n",
            "Epoch [140/150], Loss: 11789.6481\n",
            "Epoch [150/150], Loss: 11298.8557\n",
            "Fold 5, RMSE: 47.41012954711914\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 63.2287239074707\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 175199.1367\n",
            "Epoch [20/100], Loss: 35532.9668\n",
            "Epoch [30/100], Loss: 21741.1846\n",
            "Epoch [40/100], Loss: 18136.3005\n",
            "Epoch [50/100], Loss: 18886.1880\n",
            "Epoch [60/100], Loss: 12479.4464\n",
            "Epoch [70/100], Loss: 15220.6565\n",
            "Epoch [80/100], Loss: 12022.7668\n",
            "Epoch [90/100], Loss: 11285.7334\n",
            "Epoch [100/100], Loss: 11650.5845\n",
            "Fold 1, RMSE: 49.88504409790039\n",
            "Epoch [10/100], Loss: 237466.0498\n",
            "Epoch [20/100], Loss: 36902.9058\n",
            "Epoch [30/100], Loss: 26895.2661\n",
            "Epoch [40/100], Loss: 20838.9297\n",
            "Epoch [50/100], Loss: 16584.3784\n",
            "Epoch [60/100], Loss: 19784.9927\n",
            "Epoch [70/100], Loss: 13808.5044\n",
            "Epoch [80/100], Loss: 17304.1326\n",
            "Epoch [90/100], Loss: 16004.7197\n",
            "Epoch [100/100], Loss: 12613.0063\n",
            "Fold 2, RMSE: 75.02272033691406\n",
            "Epoch [10/100], Loss: 49615.6875\n",
            "Epoch [20/100], Loss: 13943.9761\n",
            "Epoch [30/100], Loss: 16632.4512\n",
            "Epoch [40/100], Loss: 12333.3645\n",
            "Epoch [50/100], Loss: 20175.1567\n",
            "Epoch [60/100], Loss: 10902.0178\n",
            "Epoch [70/100], Loss: 16979.6353\n",
            "Epoch [80/100], Loss: 11694.7252\n",
            "Epoch [90/100], Loss: 13177.5632\n",
            "Epoch [100/100], Loss: 13643.3252\n",
            "Fold 3, RMSE: 107.90220642089844\n",
            "Epoch [10/100], Loss: 34774.8047\n",
            "Epoch [20/100], Loss: 18763.3967\n",
            "Epoch [30/100], Loss: 25475.9392\n",
            "Epoch [40/100], Loss: 13346.8708\n",
            "Epoch [50/100], Loss: 20449.3066\n",
            "Epoch [60/100], Loss: 15299.7175\n",
            "Epoch [70/100], Loss: 13507.5970\n",
            "Epoch [80/100], Loss: 11491.1394\n",
            "Epoch [90/100], Loss: 12243.0054\n",
            "Epoch [100/100], Loss: 18418.3850\n",
            "Fold 4, RMSE: 37.57241439819336\n",
            "Epoch [10/100], Loss: 88052.3789\n",
            "Epoch [20/100], Loss: 20121.5254\n",
            "Epoch [30/100], Loss: 13180.6934\n",
            "Epoch [40/100], Loss: 13960.4756\n",
            "Epoch [50/100], Loss: 12630.0364\n",
            "Epoch [60/100], Loss: 9722.8445\n",
            "Epoch [70/100], Loss: 12136.3268\n",
            "Epoch [80/100], Loss: 9262.3940\n",
            "Epoch [90/100], Loss: 12992.5135\n",
            "Epoch [100/100], Loss: 8735.1233\n",
            "Fold 5, RMSE: 45.88787078857422\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 63.254051208496094\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 454458.7031\n",
            "Epoch [20/150], Loss: 51958.0205\n",
            "Epoch [30/150], Loss: 46764.5332\n",
            "Epoch [40/150], Loss: 36896.2295\n",
            "Epoch [50/150], Loss: 20235.9531\n",
            "Epoch [60/150], Loss: 15641.8975\n",
            "Epoch [70/150], Loss: 24095.6323\n",
            "Epoch [80/150], Loss: 13067.0712\n",
            "Epoch [90/150], Loss: 20744.1743\n",
            "Epoch [100/150], Loss: 12664.0190\n",
            "Epoch [110/150], Loss: 13680.5417\n",
            "Epoch [120/150], Loss: 14567.7754\n",
            "Epoch [130/150], Loss: 11054.3784\n",
            "Epoch [140/150], Loss: 11950.8832\n",
            "Epoch [150/150], Loss: 16080.7422\n",
            "Fold 1, RMSE: 46.56196975708008\n",
            "Epoch [10/150], Loss: 192281.2969\n",
            "Epoch [20/150], Loss: 21650.8115\n",
            "Epoch [30/150], Loss: 22728.6460\n",
            "Epoch [40/150], Loss: 19627.2402\n",
            "Epoch [50/150], Loss: 12001.9236\n",
            "Epoch [60/150], Loss: 12439.9502\n",
            "Epoch [70/150], Loss: 10855.7909\n",
            "Epoch [80/150], Loss: 12713.3472\n",
            "Epoch [90/150], Loss: 10712.3108\n",
            "Epoch [100/150], Loss: 9343.4763\n",
            "Epoch [110/150], Loss: 9232.6854\n",
            "Epoch [120/150], Loss: 10687.5927\n",
            "Epoch [130/150], Loss: 9215.2302\n",
            "Epoch [140/150], Loss: 7073.0771\n",
            "Epoch [150/150], Loss: 8553.9790\n",
            "Fold 2, RMSE: 65.7233657836914\n",
            "Epoch [10/150], Loss: 42127.1367\n",
            "Epoch [20/150], Loss: 16538.4688\n",
            "Epoch [30/150], Loss: 11219.1570\n",
            "Epoch [40/150], Loss: 16162.8074\n",
            "Epoch [50/150], Loss: 12238.7893\n",
            "Epoch [60/150], Loss: 11059.9424\n",
            "Epoch [70/150], Loss: 12383.8159\n",
            "Epoch [80/150], Loss: 9267.2771\n",
            "Epoch [90/150], Loss: 9270.6749\n",
            "Epoch [100/150], Loss: 5643.0529\n",
            "Epoch [110/150], Loss: 7470.7798\n",
            "Epoch [120/150], Loss: 5480.3445\n",
            "Epoch [130/150], Loss: 6185.9963\n",
            "Epoch [140/150], Loss: 7933.8807\n",
            "Epoch [150/150], Loss: 4224.7524\n",
            "Fold 3, RMSE: 93.72935485839844\n",
            "Epoch [10/150], Loss: 382962.5703\n",
            "Epoch [20/150], Loss: 98500.7129\n",
            "Epoch [30/150], Loss: 58125.6621\n",
            "Epoch [40/150], Loss: 63416.4609\n",
            "Epoch [50/150], Loss: 25999.8848\n",
            "Epoch [60/150], Loss: 39331.1587\n",
            "Epoch [70/150], Loss: 29088.6416\n",
            "Epoch [80/150], Loss: 25368.3809\n",
            "Epoch [90/150], Loss: 24748.8801\n",
            "Epoch [100/150], Loss: 18575.4075\n",
            "Epoch [110/150], Loss: 18816.7012\n",
            "Epoch [120/150], Loss: 20586.4316\n",
            "Epoch [130/150], Loss: 19630.9807\n",
            "Epoch [140/150], Loss: 13360.7891\n",
            "Epoch [150/150], Loss: 15876.0198\n",
            "Fold 4, RMSE: 41.689239501953125\n",
            "Epoch [10/150], Loss: 192782.5039\n",
            "Epoch [20/150], Loss: 17833.3755\n",
            "Epoch [30/150], Loss: 17942.6743\n",
            "Epoch [40/150], Loss: 16040.1145\n",
            "Epoch [50/150], Loss: 14671.8770\n",
            "Epoch [60/150], Loss: 13997.1382\n",
            "Epoch [70/150], Loss: 13218.7925\n",
            "Epoch [80/150], Loss: 16999.6125\n",
            "Epoch [90/150], Loss: 12095.0918\n",
            "Epoch [100/150], Loss: 9831.5853\n",
            "Epoch [110/150], Loss: 10983.4014\n",
            "Epoch [120/150], Loss: 8698.8386\n",
            "Epoch [130/150], Loss: 7704.6193\n",
            "Epoch [140/150], Loss: 8735.1796\n",
            "Epoch [150/150], Loss: 5308.4309\n",
            "Fold 5, RMSE: 43.77168655395508\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 58.29512329101563\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 12976.0500\n",
            "Epoch [20/100], Loss: 10038.1890\n",
            "Epoch [30/100], Loss: 9008.2734\n",
            "Epoch [40/100], Loss: 5590.5833\n",
            "Epoch [50/100], Loss: 8567.7511\n",
            "Epoch [60/100], Loss: 6466.8904\n",
            "Epoch [70/100], Loss: 9282.8517\n",
            "Epoch [80/100], Loss: 5151.4930\n",
            "Epoch [90/100], Loss: 17310.2980\n",
            "Epoch [100/100], Loss: 5214.2786\n",
            "Fold 1, RMSE: 52.136810302734375\n",
            "Epoch [10/100], Loss: 44658.5352\n",
            "Epoch [20/100], Loss: 12496.1855\n",
            "Epoch [30/100], Loss: 12032.5562\n",
            "Epoch [40/100], Loss: 12794.0940\n",
            "Epoch [50/100], Loss: 9877.0519\n",
            "Epoch [60/100], Loss: 10659.5428\n",
            "Epoch [70/100], Loss: 10635.3333\n",
            "Epoch [80/100], Loss: 12768.4432\n",
            "Epoch [90/100], Loss: 9288.8605\n",
            "Epoch [100/100], Loss: 9910.2908\n",
            "Fold 2, RMSE: 74.43656921386719\n",
            "Epoch [10/100], Loss: 63214.4121\n",
            "Epoch [20/100], Loss: 18710.9275\n",
            "Epoch [30/100], Loss: 16768.3823\n",
            "Epoch [40/100], Loss: 9234.3328\n",
            "Epoch [50/100], Loss: 9623.5400\n",
            "Epoch [60/100], Loss: 9985.9729\n",
            "Epoch [70/100], Loss: 10416.0940\n",
            "Epoch [80/100], Loss: 7217.1743\n",
            "Epoch [90/100], Loss: 7763.7731\n",
            "Epoch [100/100], Loss: 10778.8528\n",
            "Fold 3, RMSE: 99.10992431640625\n",
            "Epoch [10/100], Loss: 55211.8057\n",
            "Epoch [20/100], Loss: 12370.8398\n",
            "Epoch [30/100], Loss: 16969.1023\n",
            "Epoch [40/100], Loss: 13902.9434\n",
            "Epoch [50/100], Loss: 10861.9172\n",
            "Epoch [60/100], Loss: 11939.3928\n",
            "Epoch [70/100], Loss: 12313.0842\n",
            "Epoch [80/100], Loss: 12244.3324\n",
            "Epoch [90/100], Loss: 11907.1281\n",
            "Epoch [100/100], Loss: 20500.2988\n",
            "Fold 4, RMSE: 35.554752349853516\n",
            "Epoch [10/100], Loss: 43355.1448\n",
            "Epoch [20/100], Loss: 31252.7065\n",
            "Epoch [30/100], Loss: 12906.0691\n",
            "Epoch [40/100], Loss: 14904.1797\n",
            "Epoch [50/100], Loss: 30588.7268\n",
            "Epoch [60/100], Loss: 15523.5681\n",
            "Epoch [70/100], Loss: 14585.2485\n",
            "Epoch [80/100], Loss: 11369.4980\n",
            "Epoch [90/100], Loss: 13261.7109\n",
            "Epoch [100/100], Loss: 11162.5542\n",
            "Fold 5, RMSE: 47.127044677734375\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 61.67302017211914\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 20232.3823\n",
            "Epoch [20/150], Loss: 11051.2809\n",
            "Epoch [30/150], Loss: 11590.3706\n",
            "Epoch [40/150], Loss: 12043.2543\n",
            "Epoch [50/150], Loss: 15091.1802\n",
            "Epoch [60/150], Loss: 8662.1482\n",
            "Epoch [70/150], Loss: 6035.5320\n",
            "Epoch [80/150], Loss: 7232.8481\n",
            "Epoch [90/150], Loss: 6310.2162\n",
            "Epoch [100/150], Loss: 6267.3512\n",
            "Epoch [110/150], Loss: 5430.1449\n",
            "Epoch [120/150], Loss: 4925.4171\n",
            "Epoch [130/150], Loss: 5276.2144\n",
            "Epoch [140/150], Loss: 3106.7897\n",
            "Epoch [150/150], Loss: 1584.0429\n",
            "Fold 1, RMSE: 53.94615936279297\n",
            "Epoch [10/150], Loss: 45507.6738\n",
            "Epoch [20/150], Loss: 23520.9478\n",
            "Epoch [30/150], Loss: 21105.8413\n",
            "Epoch [40/150], Loss: 11651.0808\n",
            "Epoch [50/150], Loss: 10490.2689\n",
            "Epoch [60/150], Loss: 10198.7651\n",
            "Epoch [70/150], Loss: 10667.3547\n",
            "Epoch [80/150], Loss: 13330.0796\n",
            "Epoch [90/150], Loss: 19044.7631\n",
            "Epoch [100/150], Loss: 11485.1606\n",
            "Epoch [110/150], Loss: 11302.8745\n",
            "Epoch [120/150], Loss: 18524.2627\n",
            "Epoch [130/150], Loss: 9976.8602\n",
            "Epoch [140/150], Loss: 19475.5125\n",
            "Epoch [150/150], Loss: 10363.7916\n",
            "Fold 2, RMSE: 72.68978881835938\n",
            "Epoch [10/150], Loss: 27215.6519\n",
            "Epoch [20/150], Loss: 12785.8994\n",
            "Epoch [30/150], Loss: 11816.7913\n",
            "Epoch [40/150], Loss: 8402.6912\n",
            "Epoch [50/150], Loss: 7625.2312\n",
            "Epoch [60/150], Loss: 9037.2102\n",
            "Epoch [70/150], Loss: 6564.5035\n",
            "Epoch [80/150], Loss: 7454.8268\n",
            "Epoch [90/150], Loss: 9266.1995\n",
            "Epoch [100/150], Loss: 9810.7084\n",
            "Epoch [110/150], Loss: 5981.9323\n",
            "Epoch [120/150], Loss: 11131.4963\n",
            "Epoch [130/150], Loss: 7807.1920\n",
            "Epoch [140/150], Loss: 7215.4967\n",
            "Epoch [150/150], Loss: 7431.0325\n",
            "Fold 3, RMSE: 95.1651611328125\n",
            "Epoch [10/150], Loss: 84007.2754\n",
            "Epoch [20/150], Loss: 21762.5869\n",
            "Epoch [30/150], Loss: 17899.1069\n",
            "Epoch [40/150], Loss: 15201.8486\n",
            "Epoch [50/150], Loss: 11565.8563\n",
            "Epoch [60/150], Loss: 13265.1150\n",
            "Epoch [70/150], Loss: 14107.2712\n",
            "Epoch [80/150], Loss: 16448.5510\n",
            "Epoch [90/150], Loss: 11303.8167\n",
            "Epoch [100/150], Loss: 12767.1460\n",
            "Epoch [110/150], Loss: 13620.8101\n",
            "Epoch [120/150], Loss: 11943.0098\n",
            "Epoch [130/150], Loss: 16468.8362\n",
            "Epoch [140/150], Loss: 14299.3938\n",
            "Epoch [150/150], Loss: 12630.4187\n",
            "Fold 4, RMSE: 38.54639434814453\n",
            "Epoch [10/150], Loss: 80117.1523\n",
            "Epoch [20/150], Loss: 28110.6636\n",
            "Epoch [30/150], Loss: 18804.9688\n",
            "Epoch [40/150], Loss: 14271.0176\n",
            "Epoch [50/150], Loss: 14616.2717\n",
            "Epoch [60/150], Loss: 10702.8726\n",
            "Epoch [70/150], Loss: 15168.4980\n",
            "Epoch [80/150], Loss: 14380.5056\n",
            "Epoch [90/150], Loss: 11647.0322\n",
            "Epoch [100/150], Loss: 12807.0560\n",
            "Epoch [110/150], Loss: 12034.0090\n",
            "Epoch [120/150], Loss: 21565.9060\n",
            "Epoch [130/150], Loss: 15779.2013\n",
            "Epoch [140/150], Loss: 11494.6934\n",
            "Epoch [150/150], Loss: 15629.7728\n",
            "Fold 5, RMSE: 47.8935546875\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 61.64821166992188\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 156168.4414\n",
            "Epoch [20/100], Loss: 17449.4185\n",
            "Epoch [30/100], Loss: 16825.1978\n",
            "Epoch [40/100], Loss: 15937.1240\n",
            "Epoch [50/100], Loss: 11953.8564\n",
            "Epoch [60/100], Loss: 16011.0291\n",
            "Epoch [70/100], Loss: 20215.1846\n",
            "Epoch [80/100], Loss: 11316.0349\n",
            "Epoch [90/100], Loss: 13954.8569\n",
            "Epoch [100/100], Loss: 10906.1500\n",
            "Fold 1, RMSE: 47.94417190551758\n",
            "Epoch [10/100], Loss: 273680.4844\n",
            "Epoch [20/100], Loss: 53166.6562\n",
            "Epoch [30/100], Loss: 19286.0188\n",
            "Epoch [40/100], Loss: 18419.1050\n",
            "Epoch [50/100], Loss: 18028.5542\n",
            "Epoch [60/100], Loss: 11946.4020\n",
            "Epoch [70/100], Loss: 11974.6812\n",
            "Epoch [80/100], Loss: 11470.9324\n",
            "Epoch [90/100], Loss: 11844.0637\n",
            "Epoch [100/100], Loss: 10078.9418\n",
            "Fold 2, RMSE: 74.32726287841797\n",
            "Epoch [10/100], Loss: 237533.0859\n",
            "Epoch [20/100], Loss: 25646.5918\n",
            "Epoch [30/100], Loss: 14468.0762\n",
            "Epoch [40/100], Loss: 14733.6255\n",
            "Epoch [50/100], Loss: 12593.3438\n",
            "Epoch [60/100], Loss: 11763.2405\n",
            "Epoch [70/100], Loss: 7133.8407\n",
            "Epoch [80/100], Loss: 8125.6094\n",
            "Epoch [90/100], Loss: 7438.1038\n",
            "Epoch [100/100], Loss: 6224.9427\n",
            "Fold 3, RMSE: 91.32511138916016\n",
            "Epoch [10/100], Loss: 144494.2656\n",
            "Epoch [20/100], Loss: 47765.1943\n",
            "Epoch [30/100], Loss: 19145.6487\n",
            "Epoch [40/100], Loss: 26513.8472\n",
            "Epoch [50/100], Loss: 22963.7930\n",
            "Epoch [60/100], Loss: 17069.7482\n",
            "Epoch [70/100], Loss: 21121.1113\n",
            "Epoch [80/100], Loss: 18365.1414\n",
            "Epoch [90/100], Loss: 13028.3708\n",
            "Epoch [100/100], Loss: 13078.5872\n",
            "Fold 4, RMSE: 36.644622802734375\n",
            "Epoch [10/100], Loss: 145840.1875\n",
            "Epoch [20/100], Loss: 35240.4844\n",
            "Epoch [30/100], Loss: 20775.3118\n",
            "Epoch [40/100], Loss: 13193.1165\n",
            "Epoch [50/100], Loss: 12997.9487\n",
            "Epoch [60/100], Loss: 10455.1149\n",
            "Epoch [70/100], Loss: 6386.1750\n",
            "Epoch [80/100], Loss: 13917.6121\n",
            "Epoch [90/100], Loss: 11672.1075\n",
            "Epoch [100/100], Loss: 10793.7891\n",
            "Fold 5, RMSE: 43.547637939453125\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 58.75776138305664\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 177905.6211\n",
            "Epoch [20/150], Loss: 28394.3813\n",
            "Epoch [30/150], Loss: 26115.7158\n",
            "Epoch [40/150], Loss: 13438.7295\n",
            "Epoch [50/150], Loss: 16811.4202\n",
            "Epoch [60/150], Loss: 13702.6514\n",
            "Epoch [70/150], Loss: 11982.8462\n",
            "Epoch [80/150], Loss: 13570.3384\n",
            "Epoch [90/150], Loss: 18787.0166\n",
            "Epoch [100/150], Loss: 15453.9729\n",
            "Epoch [110/150], Loss: 18142.7209\n",
            "Epoch [120/150], Loss: 11830.8157\n",
            "Epoch [130/150], Loss: 15395.1089\n",
            "Epoch [140/150], Loss: 18216.9559\n",
            "Epoch [150/150], Loss: 14495.5024\n",
            "Fold 1, RMSE: 44.97887420654297\n",
            "Epoch [10/150], Loss: 1321523.4062\n",
            "Epoch [20/150], Loss: 103629.0820\n",
            "Epoch [30/150], Loss: 55765.6357\n",
            "Epoch [40/150], Loss: 27181.1475\n",
            "Epoch [50/150], Loss: 21879.8320\n",
            "Epoch [60/150], Loss: 26311.8462\n",
            "Epoch [70/150], Loss: 14090.6187\n",
            "Epoch [80/150], Loss: 22076.1802\n",
            "Epoch [90/150], Loss: 12160.1782\n",
            "Epoch [100/150], Loss: 12341.1003\n",
            "Epoch [110/150], Loss: 14885.0232\n",
            "Epoch [120/150], Loss: 26197.2373\n",
            "Epoch [130/150], Loss: 30870.8916\n",
            "Epoch [140/150], Loss: 9404.8810\n",
            "Epoch [150/150], Loss: 19717.1125\n",
            "Fold 2, RMSE: 70.89021301269531\n",
            "Epoch [10/150], Loss: 47309.1709\n",
            "Epoch [20/150], Loss: 14615.8748\n",
            "Epoch [30/150], Loss: 10317.5564\n",
            "Epoch [40/150], Loss: 11238.8723\n",
            "Epoch [50/150], Loss: 8872.2297\n",
            "Epoch [60/150], Loss: 8261.4050\n",
            "Epoch [70/150], Loss: 5874.7749\n",
            "Epoch [80/150], Loss: 7248.7185\n",
            "Epoch [90/150], Loss: 3934.3047\n",
            "Epoch [100/150], Loss: 8599.2133\n",
            "Epoch [110/150], Loss: 6465.3021\n",
            "Epoch [120/150], Loss: 3479.7315\n",
            "Epoch [130/150], Loss: 4895.2870\n",
            "Epoch [140/150], Loss: 3516.9343\n",
            "Epoch [150/150], Loss: 3148.1785\n",
            "Fold 3, RMSE: 91.72960662841797\n",
            "Epoch [10/150], Loss: 98781.1660\n",
            "Epoch [20/150], Loss: 18608.4709\n",
            "Epoch [30/150], Loss: 24277.8721\n",
            "Epoch [40/150], Loss: 16568.0249\n",
            "Epoch [50/150], Loss: 19820.2734\n",
            "Epoch [60/150], Loss: 25421.4619\n",
            "Epoch [70/150], Loss: 16089.4907\n",
            "Epoch [80/150], Loss: 13875.3335\n",
            "Epoch [90/150], Loss: 13854.0398\n",
            "Epoch [100/150], Loss: 13505.4238\n",
            "Epoch [110/150], Loss: 12107.0466\n",
            "Epoch [120/150], Loss: 15148.4136\n",
            "Epoch [130/150], Loss: 10241.7982\n",
            "Epoch [140/150], Loss: 10086.6649\n",
            "Epoch [150/150], Loss: 11168.5710\n",
            "Fold 4, RMSE: 37.628273010253906\n",
            "Epoch [10/150], Loss: 23063.9253\n",
            "Epoch [20/150], Loss: 15963.1060\n",
            "Epoch [30/150], Loss: 15688.1099\n",
            "Epoch [40/150], Loss: 16882.1497\n",
            "Epoch [50/150], Loss: 16341.2581\n",
            "Epoch [60/150], Loss: 12263.6807\n",
            "Epoch [70/150], Loss: 14901.1663\n",
            "Epoch [80/150], Loss: 11330.1729\n",
            "Epoch [90/150], Loss: 9486.7212\n",
            "Epoch [100/150], Loss: 6691.4538\n",
            "Epoch [110/150], Loss: 7085.2354\n",
            "Epoch [120/150], Loss: 6293.7775\n",
            "Epoch [130/150], Loss: 5640.4233\n",
            "Epoch [140/150], Loss: 5917.1442\n",
            "Epoch [150/150], Loss: 6283.6177\n",
            "Fold 5, RMSE: 43.67619323730469\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 57.78063201904297\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 27826.8252\n",
            "Epoch [20/100], Loss: 20356.0957\n",
            "Epoch [30/100], Loss: 12008.8191\n",
            "Epoch [40/100], Loss: 14673.6162\n",
            "Epoch [50/100], Loss: 10126.3521\n",
            "Epoch [60/100], Loss: 9835.9664\n",
            "Epoch [70/100], Loss: 10192.3115\n",
            "Epoch [80/100], Loss: 6859.4873\n",
            "Epoch [90/100], Loss: 11645.5408\n",
            "Epoch [100/100], Loss: 7996.5212\n",
            "Fold 1, RMSE: 54.10847473144531\n",
            "Epoch [10/100], Loss: 19328.8657\n",
            "Epoch [20/100], Loss: 14511.5706\n",
            "Epoch [30/100], Loss: 18028.2622\n",
            "Epoch [40/100], Loss: 17294.6577\n",
            "Epoch [50/100], Loss: 17620.7378\n",
            "Epoch [60/100], Loss: 14494.4788\n",
            "Epoch [70/100], Loss: 17565.1484\n",
            "Epoch [80/100], Loss: 18477.2791\n",
            "Epoch [90/100], Loss: 14324.1836\n",
            "Epoch [100/100], Loss: 16315.5010\n",
            "Fold 2, RMSE: 81.63372802734375\n",
            "Epoch [10/100], Loss: 26171.7422\n",
            "Epoch [20/100], Loss: 10392.9011\n",
            "Epoch [30/100], Loss: 10184.7937\n",
            "Epoch [40/100], Loss: 8501.1112\n",
            "Epoch [50/100], Loss: 7698.4252\n",
            "Epoch [60/100], Loss: 11246.4722\n",
            "Epoch [70/100], Loss: 7083.7230\n",
            "Epoch [80/100], Loss: 7745.5735\n",
            "Epoch [90/100], Loss: 12062.0397\n",
            "Epoch [100/100], Loss: 7670.1179\n",
            "Fold 3, RMSE: 90.47344970703125\n",
            "Epoch [10/100], Loss: 80942.8770\n",
            "Epoch [20/100], Loss: 38449.1660\n",
            "Epoch [30/100], Loss: 27014.2979\n",
            "Epoch [40/100], Loss: 17688.2786\n",
            "Epoch [50/100], Loss: 13868.3761\n",
            "Epoch [60/100], Loss: 18959.9312\n",
            "Epoch [70/100], Loss: 14245.4037\n",
            "Epoch [80/100], Loss: 12022.2288\n",
            "Epoch [90/100], Loss: 12160.6722\n",
            "Epoch [100/100], Loss: 11413.6246\n",
            "Fold 4, RMSE: 36.22713088989258\n",
            "Epoch [10/100], Loss: 16658.4258\n",
            "Epoch [20/100], Loss: 20737.8301\n",
            "Epoch [30/100], Loss: 12210.1155\n",
            "Epoch [40/100], Loss: 9522.0289\n",
            "Epoch [50/100], Loss: 8457.4570\n",
            "Epoch [60/100], Loss: 7748.2668\n",
            "Epoch [70/100], Loss: 11816.9778\n",
            "Epoch [80/100], Loss: 5437.1843\n",
            "Epoch [90/100], Loss: 5552.9172\n",
            "Epoch [100/100], Loss: 4788.4258\n",
            "Fold 5, RMSE: 44.567115783691406\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 61.40197982788086\n",
            "Training with neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 71308.5977\n",
            "Epoch [20/150], Loss: 27102.4043\n",
            "Epoch [30/150], Loss: 17532.8882\n",
            "Epoch [40/150], Loss: 15981.3806\n",
            "Epoch [50/150], Loss: 10680.7331\n",
            "Epoch [60/150], Loss: 18684.9463\n",
            "Epoch [70/150], Loss: 12375.3970\n",
            "Epoch [80/150], Loss: 18841.2505\n",
            "Epoch [90/150], Loss: 13832.7375\n",
            "Epoch [100/150], Loss: 12348.4080\n",
            "Epoch [110/150], Loss: 17962.8606\n",
            "Epoch [120/150], Loss: 17620.5754\n",
            "Epoch [130/150], Loss: 17640.0603\n",
            "Epoch [140/150], Loss: 12165.6274\n",
            "Epoch [150/150], Loss: 17918.3340\n",
            "Fold 1, RMSE: 51.59242248535156\n",
            "Epoch [10/150], Loss: 172469.4180\n",
            "Epoch [20/150], Loss: 21304.8154\n",
            "Epoch [30/150], Loss: 20150.5267\n",
            "Epoch [40/150], Loss: 14271.8397\n",
            "Epoch [50/150], Loss: 18808.1826\n",
            "Epoch [60/150], Loss: 11759.4255\n",
            "Epoch [70/150], Loss: 14226.1848\n",
            "Epoch [80/150], Loss: 10102.6582\n",
            "Epoch [90/150], Loss: 9846.1270\n",
            "Epoch [100/150], Loss: 12587.4148\n",
            "Epoch [110/150], Loss: 10931.6932\n",
            "Epoch [120/150], Loss: 10637.5786\n",
            "Epoch [130/150], Loss: 11676.9309\n",
            "Epoch [140/150], Loss: 9649.2857\n",
            "Epoch [150/150], Loss: 9800.6078\n",
            "Fold 2, RMSE: 74.99222564697266\n",
            "Epoch [10/150], Loss: 24440.6272\n",
            "Epoch [20/150], Loss: 13852.8328\n",
            "Epoch [30/150], Loss: 8428.2830\n",
            "Epoch [40/150], Loss: 18373.3278\n",
            "Epoch [50/150], Loss: 12199.3318\n",
            "Epoch [60/150], Loss: 9458.2234\n",
            "Epoch [70/150], Loss: 7791.2397\n",
            "Epoch [80/150], Loss: 9836.4343\n",
            "Epoch [90/150], Loss: 7349.9948\n",
            "Epoch [100/150], Loss: 6278.6908\n",
            "Epoch [110/150], Loss: 5299.9357\n",
            "Epoch [120/150], Loss: 7050.0847\n",
            "Epoch [130/150], Loss: 4988.6306\n",
            "Epoch [140/150], Loss: 4678.4152\n",
            "Epoch [150/150], Loss: 3435.3048\n",
            "Fold 3, RMSE: 96.09120178222656\n",
            "Epoch [10/150], Loss: 78417.7051\n",
            "Epoch [20/150], Loss: 18712.7153\n",
            "Epoch [30/150], Loss: 15205.4006\n",
            "Epoch [40/150], Loss: 21721.9336\n",
            "Epoch [50/150], Loss: 17270.5278\n",
            "Epoch [60/150], Loss: 13238.5281\n",
            "Epoch [70/150], Loss: 17280.5989\n",
            "Epoch [80/150], Loss: 13558.9775\n",
            "Epoch [90/150], Loss: 12329.3196\n",
            "Epoch [100/150], Loss: 13424.7188\n",
            "Epoch [110/150], Loss: 12187.3530\n",
            "Epoch [120/150], Loss: 12123.5680\n",
            "Epoch [130/150], Loss: 17323.4548\n",
            "Epoch [140/150], Loss: 14659.8495\n",
            "Epoch [150/150], Loss: 9450.2758\n",
            "Fold 4, RMSE: 38.87017059326172\n",
            "Epoch [10/150], Loss: 21891.4004\n",
            "Epoch [20/150], Loss: 20724.3569\n",
            "Epoch [30/150], Loss: 22486.7954\n",
            "Epoch [40/150], Loss: 27492.8096\n",
            "Epoch [50/150], Loss: 18284.1948\n",
            "Epoch [60/150], Loss: 16939.3108\n",
            "Epoch [70/150], Loss: 21170.5229\n",
            "Epoch [80/150], Loss: 17925.1890\n",
            "Epoch [90/150], Loss: 13648.4658\n",
            "Epoch [100/150], Loss: 12710.6591\n",
            "Epoch [110/150], Loss: 14350.0513\n",
            "Epoch [120/150], Loss: 12553.3833\n",
            "Epoch [130/150], Loss: 12879.9956\n",
            "Epoch [140/150], Loss: 12302.5164\n",
            "Epoch [150/150], Loss: 12915.5637\n",
            "Fold 5, RMSE: 47.8913459777832\n",
            "Avg RMSE for neurons=64, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 61.88747329711914\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 18067.9517\n",
            "Epoch [20/100], Loss: 17245.1084\n",
            "Epoch [30/100], Loss: 17399.7754\n",
            "Epoch [40/100], Loss: 10202.3602\n",
            "Epoch [50/100], Loss: 9045.5520\n",
            "Epoch [60/100], Loss: 17423.1936\n",
            "Epoch [70/100], Loss: 8627.8472\n",
            "Epoch [80/100], Loss: 10620.6921\n",
            "Epoch [90/100], Loss: 11379.2588\n",
            "Epoch [100/100], Loss: 11082.6077\n",
            "Fold 1, RMSE: 58.41067123413086\n",
            "Epoch [10/100], Loss: 16707.5671\n",
            "Epoch [20/100], Loss: 8514.9718\n",
            "Epoch [30/100], Loss: 11058.3827\n",
            "Epoch [40/100], Loss: 10833.0389\n",
            "Epoch [50/100], Loss: 10452.2992\n",
            "Epoch [60/100], Loss: 10544.3494\n",
            "Epoch [70/100], Loss: 15227.2964\n",
            "Epoch [80/100], Loss: 9358.7919\n",
            "Epoch [90/100], Loss: 12457.0066\n",
            "Epoch [100/100], Loss: 13909.4082\n",
            "Fold 2, RMSE: 74.21347045898438\n",
            "Epoch [10/100], Loss: 12690.6145\n",
            "Epoch [20/100], Loss: 10722.1562\n",
            "Epoch [30/100], Loss: 9341.4847\n",
            "Epoch [40/100], Loss: 12839.4807\n",
            "Epoch [50/100], Loss: 8811.5255\n",
            "Epoch [60/100], Loss: 6868.1917\n",
            "Epoch [70/100], Loss: 9444.7002\n",
            "Epoch [80/100], Loss: 10713.8542\n",
            "Epoch [90/100], Loss: 9627.6521\n",
            "Epoch [100/100], Loss: 10326.8369\n",
            "Fold 3, RMSE: 104.14280700683594\n",
            "Epoch [10/100], Loss: 22114.1108\n",
            "Epoch [20/100], Loss: 16209.0200\n",
            "Epoch [30/100], Loss: 15122.5862\n",
            "Epoch [40/100], Loss: 13632.5930\n",
            "Epoch [50/100], Loss: 10286.2386\n",
            "Epoch [60/100], Loss: 20796.4897\n",
            "Epoch [70/100], Loss: 16261.9353\n",
            "Epoch [80/100], Loss: 8369.8297\n",
            "Epoch [90/100], Loss: 13356.1180\n",
            "Epoch [100/100], Loss: 11575.6208\n",
            "Fold 4, RMSE: 44.977027893066406\n",
            "Epoch [10/100], Loss: 10652.7021\n",
            "Epoch [20/100], Loss: 14650.3462\n",
            "Epoch [30/100], Loss: 10838.3608\n",
            "Epoch [40/100], Loss: 10249.8591\n",
            "Epoch [50/100], Loss: 9303.3579\n",
            "Epoch [60/100], Loss: 7902.2471\n",
            "Epoch [70/100], Loss: 9133.7780\n",
            "Epoch [80/100], Loss: 13698.4026\n",
            "Epoch [90/100], Loss: 21478.4193\n",
            "Epoch [100/100], Loss: 11378.7148\n",
            "Fold 5, RMSE: 45.13751220703125\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 65.37629776000976\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 26227.3872\n",
            "Epoch [20/150], Loss: 22577.2915\n",
            "Epoch [30/150], Loss: 16507.8796\n",
            "Epoch [40/150], Loss: 17487.6997\n",
            "Epoch [50/150], Loss: 18460.0085\n",
            "Epoch [60/150], Loss: 17696.5178\n",
            "Epoch [70/150], Loss: 16713.0100\n",
            "Epoch [80/150], Loss: 18159.6245\n",
            "Epoch [90/150], Loss: 15985.5087\n",
            "Epoch [100/150], Loss: 18413.1318\n",
            "Epoch [110/150], Loss: 18063.3101\n",
            "Epoch [120/150], Loss: 22713.8706\n",
            "Epoch [130/150], Loss: 19199.1929\n",
            "Epoch [140/150], Loss: 17065.1267\n",
            "Epoch [150/150], Loss: 17794.9106\n",
            "Fold 1, RMSE: 67.40699768066406\n",
            "Epoch [10/150], Loss: 15439.6235\n",
            "Epoch [20/150], Loss: 17576.6301\n",
            "Epoch [30/150], Loss: 16839.7539\n",
            "Epoch [40/150], Loss: 14575.4076\n",
            "Epoch [50/150], Loss: 15709.4624\n",
            "Epoch [60/150], Loss: 15556.3489\n",
            "Epoch [70/150], Loss: 14863.3434\n",
            "Epoch [80/150], Loss: 14486.4700\n",
            "Epoch [90/150], Loss: 14575.4117\n",
            "Epoch [100/150], Loss: 23025.3774\n",
            "Epoch [110/150], Loss: 13937.4408\n",
            "Epoch [120/150], Loss: 15999.0774\n",
            "Epoch [130/150], Loss: 23174.8774\n",
            "Epoch [140/150], Loss: 18595.5684\n",
            "Epoch [150/150], Loss: 16726.8025\n",
            "Fold 2, RMSE: 87.34090423583984\n",
            "Epoch [10/150], Loss: 9259.0309\n",
            "Epoch [20/150], Loss: 7416.8036\n",
            "Epoch [30/150], Loss: 9477.5316\n",
            "Epoch [40/150], Loss: 9188.4199\n",
            "Epoch [50/150], Loss: 5064.5834\n",
            "Epoch [60/150], Loss: 7661.8025\n",
            "Epoch [70/150], Loss: 6359.8660\n",
            "Epoch [80/150], Loss: 4542.4458\n",
            "Epoch [90/150], Loss: 5712.7587\n",
            "Epoch [100/150], Loss: 7793.7151\n",
            "Epoch [110/150], Loss: 5009.6874\n",
            "Epoch [120/150], Loss: 6010.6431\n",
            "Epoch [130/150], Loss: 4786.7904\n",
            "Epoch [140/150], Loss: 4145.6395\n",
            "Epoch [150/150], Loss: 3561.8377\n",
            "Fold 3, RMSE: 94.95181274414062\n",
            "Epoch [10/150], Loss: 22601.8000\n",
            "Epoch [20/150], Loss: 15765.9365\n",
            "Epoch [30/150], Loss: 17611.2280\n",
            "Epoch [40/150], Loss: 11182.7285\n",
            "Epoch [50/150], Loss: 11127.2273\n",
            "Epoch [60/150], Loss: 11886.8274\n",
            "Epoch [70/150], Loss: 13992.6836\n",
            "Epoch [80/150], Loss: 14482.6306\n",
            "Epoch [90/150], Loss: 12593.1283\n",
            "Epoch [100/150], Loss: 15268.2017\n",
            "Epoch [110/150], Loss: 18787.1221\n",
            "Epoch [120/150], Loss: 19268.8052\n",
            "Epoch [130/150], Loss: 21666.4736\n",
            "Epoch [140/150], Loss: 17584.8445\n",
            "Epoch [150/150], Loss: 15340.0947\n",
            "Fold 4, RMSE: 51.259822845458984\n",
            "Epoch [10/150], Loss: 20255.7617\n",
            "Epoch [20/150], Loss: 18263.7771\n",
            "Epoch [30/150], Loss: 18651.0264\n",
            "Epoch [40/150], Loss: 20127.0190\n",
            "Epoch [50/150], Loss: 18178.7737\n",
            "Epoch [60/150], Loss: 16909.1350\n",
            "Epoch [70/150], Loss: 20421.5828\n",
            "Epoch [80/150], Loss: 17571.3997\n",
            "Epoch [90/150], Loss: 20352.4326\n",
            "Epoch [100/150], Loss: 18335.3926\n",
            "Epoch [110/150], Loss: 17314.6498\n",
            "Epoch [120/150], Loss: 19684.8174\n",
            "Epoch [130/150], Loss: 19409.4004\n",
            "Epoch [140/150], Loss: 23791.8960\n",
            "Epoch [150/150], Loss: 18979.9082\n",
            "Fold 5, RMSE: 57.76237106323242\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 71.74438171386718\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16890.6187\n",
            "Epoch [20/100], Loss: 12838.1211\n",
            "Epoch [30/100], Loss: 13601.9766\n",
            "Epoch [40/100], Loss: 9981.9653\n",
            "Epoch [50/100], Loss: 7672.8123\n",
            "Epoch [60/100], Loss: 10185.9064\n",
            "Epoch [70/100], Loss: 8881.8457\n",
            "Epoch [80/100], Loss: 11867.6165\n",
            "Epoch [90/100], Loss: 6013.9600\n",
            "Epoch [100/100], Loss: 14085.2124\n",
            "Fold 1, RMSE: 66.8750228881836\n",
            "Epoch [10/100], Loss: 15087.1841\n",
            "Epoch [20/100], Loss: 15352.3657\n",
            "Epoch [30/100], Loss: 9325.6646\n",
            "Epoch [40/100], Loss: 11791.1360\n",
            "Epoch [50/100], Loss: 15407.7894\n",
            "Epoch [60/100], Loss: 8102.0447\n",
            "Epoch [70/100], Loss: 9200.3488\n",
            "Epoch [80/100], Loss: 10504.2686\n",
            "Epoch [90/100], Loss: 22214.5479\n",
            "Epoch [100/100], Loss: 9426.3673\n",
            "Fold 2, RMSE: 71.3992919921875\n",
            "Epoch [10/100], Loss: 15168.4546\n",
            "Epoch [20/100], Loss: 11652.3850\n",
            "Epoch [30/100], Loss: 9701.1715\n",
            "Epoch [40/100], Loss: 12008.4963\n",
            "Epoch [50/100], Loss: 7399.2066\n",
            "Epoch [60/100], Loss: 11106.5295\n",
            "Epoch [70/100], Loss: 4652.6110\n",
            "Epoch [80/100], Loss: 10400.2678\n",
            "Epoch [90/100], Loss: 9991.9698\n",
            "Epoch [100/100], Loss: 5680.8176\n",
            "Fold 3, RMSE: 107.16481018066406\n",
            "Epoch [10/100], Loss: 16270.6431\n",
            "Epoch [20/100], Loss: 13046.9663\n",
            "Epoch [30/100], Loss: 21093.7209\n",
            "Epoch [40/100], Loss: 17434.4419\n",
            "Epoch [50/100], Loss: 11962.1865\n",
            "Epoch [60/100], Loss: 10631.8521\n",
            "Epoch [70/100], Loss: 15503.7446\n",
            "Epoch [80/100], Loss: 8727.6283\n",
            "Epoch [90/100], Loss: 12884.2863\n",
            "Epoch [100/100], Loss: 10967.3112\n",
            "Fold 4, RMSE: 48.952659606933594\n",
            "Epoch [10/100], Loss: 18997.8552\n",
            "Epoch [20/100], Loss: 12739.9990\n",
            "Epoch [30/100], Loss: 15720.2765\n",
            "Epoch [40/100], Loss: 15257.6699\n",
            "Epoch [50/100], Loss: 14538.6283\n",
            "Epoch [60/100], Loss: 16725.7256\n",
            "Epoch [70/100], Loss: 24863.3308\n",
            "Epoch [80/100], Loss: 13180.6375\n",
            "Epoch [90/100], Loss: 12789.4315\n",
            "Epoch [100/100], Loss: 14619.7061\n",
            "Fold 5, RMSE: 47.10022735595703\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 68.29840240478515\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 17027.1650\n",
            "Epoch [20/150], Loss: 16472.7605\n",
            "Epoch [30/150], Loss: 15941.3159\n",
            "Epoch [40/150], Loss: 21741.7620\n",
            "Epoch [50/150], Loss: 13979.3822\n",
            "Epoch [60/150], Loss: 10726.3765\n",
            "Epoch [70/150], Loss: 13233.8169\n",
            "Epoch [80/150], Loss: 14192.3625\n",
            "Epoch [90/150], Loss: 5775.4492\n",
            "Epoch [100/150], Loss: 11178.3876\n",
            "Epoch [110/150], Loss: 8282.7739\n",
            "Epoch [120/150], Loss: 7542.5521\n",
            "Epoch [130/150], Loss: 8942.4539\n",
            "Epoch [140/150], Loss: 16943.8330\n",
            "Epoch [150/150], Loss: 5456.7241\n",
            "Fold 1, RMSE: 52.2963981628418\n",
            "Epoch [10/150], Loss: 21152.4736\n",
            "Epoch [20/150], Loss: 15510.1582\n",
            "Epoch [30/150], Loss: 15242.1008\n",
            "Epoch [40/150], Loss: 17161.9124\n",
            "Epoch [50/150], Loss: 17437.3254\n",
            "Epoch [60/150], Loss: 8816.0359\n",
            "Epoch [70/150], Loss: 18406.1560\n",
            "Epoch [80/150], Loss: 7487.7515\n",
            "Epoch [90/150], Loss: 13260.5464\n",
            "Epoch [100/150], Loss: 8504.0042\n",
            "Epoch [110/150], Loss: 15090.8384\n",
            "Epoch [120/150], Loss: 5593.9666\n",
            "Epoch [130/150], Loss: 18702.2107\n",
            "Epoch [140/150], Loss: 8984.6565\n",
            "Epoch [150/150], Loss: 9834.7053\n",
            "Fold 2, RMSE: 77.2641830444336\n",
            "Epoch [10/150], Loss: 14564.1660\n",
            "Epoch [20/150], Loss: 12035.9592\n",
            "Epoch [30/150], Loss: 14919.9478\n",
            "Epoch [40/150], Loss: 13257.0093\n",
            "Epoch [50/150], Loss: 6618.5729\n",
            "Epoch [60/150], Loss: 5031.1710\n",
            "Epoch [70/150], Loss: 7978.4490\n",
            "Epoch [80/150], Loss: 9418.0129\n",
            "Epoch [90/150], Loss: 7892.0574\n",
            "Epoch [100/150], Loss: 9254.8806\n",
            "Epoch [110/150], Loss: 5878.4375\n",
            "Epoch [120/150], Loss: 10716.2231\n",
            "Epoch [130/150], Loss: 8427.7736\n",
            "Epoch [140/150], Loss: 9250.4145\n",
            "Epoch [150/150], Loss: 7955.6945\n",
            "Fold 3, RMSE: 103.2486343383789\n",
            "Epoch [10/150], Loss: 23515.7246\n",
            "Epoch [20/150], Loss: 18883.4614\n",
            "Epoch [30/150], Loss: 16600.0879\n",
            "Epoch [40/150], Loss: 12698.0808\n",
            "Epoch [50/150], Loss: 10448.6311\n",
            "Epoch [60/150], Loss: 20139.7275\n",
            "Epoch [70/150], Loss: 8166.6785\n",
            "Epoch [80/150], Loss: 16136.1277\n",
            "Epoch [90/150], Loss: 17354.3582\n",
            "Epoch [100/150], Loss: 13453.9585\n",
            "Epoch [110/150], Loss: 8773.8767\n",
            "Epoch [120/150], Loss: 14236.9939\n",
            "Epoch [130/150], Loss: 15036.4702\n",
            "Epoch [140/150], Loss: 10547.1610\n",
            "Epoch [150/150], Loss: 17580.7583\n",
            "Fold 4, RMSE: 39.8471565246582\n",
            "Epoch [10/150], Loss: 15368.8535\n",
            "Epoch [20/150], Loss: 15959.7603\n",
            "Epoch [30/150], Loss: 16723.4121\n",
            "Epoch [40/150], Loss: 12663.4705\n",
            "Epoch [50/150], Loss: 17923.4106\n",
            "Epoch [60/150], Loss: 15505.5046\n",
            "Epoch [70/150], Loss: 13226.6845\n",
            "Epoch [80/150], Loss: 12081.2947\n",
            "Epoch [90/150], Loss: 10906.2716\n",
            "Epoch [100/150], Loss: 14973.6984\n",
            "Epoch [110/150], Loss: 11250.0826\n",
            "Epoch [120/150], Loss: 7872.2278\n",
            "Epoch [130/150], Loss: 14590.1409\n",
            "Epoch [140/150], Loss: 12138.6201\n",
            "Epoch [150/150], Loss: 18181.4092\n",
            "Fold 5, RMSE: 48.792633056640625\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 64.28980102539063\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 18862.5210\n",
            "Epoch [20/100], Loss: 18364.5815\n",
            "Epoch [30/100], Loss: 13817.3928\n",
            "Epoch [40/100], Loss: 15014.3147\n",
            "Epoch [50/100], Loss: 15137.1951\n",
            "Epoch [60/100], Loss: 6384.4884\n",
            "Epoch [70/100], Loss: 12086.9241\n",
            "Epoch [80/100], Loss: 12233.3370\n",
            "Epoch [90/100], Loss: 7663.3340\n",
            "Epoch [100/100], Loss: 13380.3989\n",
            "Fold 1, RMSE: 53.87378692626953\n",
            "Epoch [10/100], Loss: 17489.3389\n",
            "Epoch [20/100], Loss: 14491.4338\n",
            "Epoch [30/100], Loss: 16320.7466\n",
            "Epoch [40/100], Loss: 15661.9763\n",
            "Epoch [50/100], Loss: 13681.6745\n",
            "Epoch [60/100], Loss: 14546.8319\n",
            "Epoch [70/100], Loss: 15255.7705\n",
            "Epoch [80/100], Loss: 16523.1660\n",
            "Epoch [90/100], Loss: 13841.9435\n",
            "Epoch [100/100], Loss: 18991.4474\n",
            "Fold 2, RMSE: 87.34278869628906\n",
            "Epoch [10/100], Loss: 12441.3816\n",
            "Epoch [20/100], Loss: 12563.2051\n",
            "Epoch [30/100], Loss: 13632.9463\n",
            "Epoch [40/100], Loss: 10883.2136\n",
            "Epoch [50/100], Loss: 11745.5344\n",
            "Epoch [60/100], Loss: 10442.7966\n",
            "Epoch [70/100], Loss: 6860.6582\n",
            "Epoch [80/100], Loss: 7684.0093\n",
            "Epoch [90/100], Loss: 9850.9049\n",
            "Epoch [100/100], Loss: 11687.6062\n",
            "Fold 3, RMSE: 100.88723754882812\n",
            "Epoch [10/100], Loss: 25616.0977\n",
            "Epoch [20/100], Loss: 20433.6245\n",
            "Epoch [30/100], Loss: 15027.9426\n",
            "Epoch [40/100], Loss: 26034.2969\n",
            "Epoch [50/100], Loss: 13428.4443\n",
            "Epoch [60/100], Loss: 12019.9539\n",
            "Epoch [70/100], Loss: 17697.1903\n",
            "Epoch [80/100], Loss: 14851.7371\n",
            "Epoch [90/100], Loss: 10150.3259\n",
            "Epoch [100/100], Loss: 10268.1982\n",
            "Fold 4, RMSE: 48.45621109008789\n",
            "Epoch [10/100], Loss: 17911.3499\n",
            "Epoch [20/100], Loss: 15096.6196\n",
            "Epoch [30/100], Loss: 15273.3508\n",
            "Epoch [40/100], Loss: 17549.3218\n",
            "Epoch [50/100], Loss: 12701.1057\n",
            "Epoch [60/100], Loss: 15724.0554\n",
            "Epoch [70/100], Loss: 11467.8948\n",
            "Epoch [80/100], Loss: 8934.8340\n",
            "Epoch [90/100], Loss: 16532.8667\n",
            "Epoch [100/100], Loss: 20874.2329\n",
            "Fold 5, RMSE: 44.35268020629883\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 66.98254089355468\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 21731.8860\n",
            "Epoch [20/150], Loss: 14219.3411\n",
            "Epoch [30/150], Loss: 15794.8003\n",
            "Epoch [40/150], Loss: 9569.5737\n",
            "Epoch [50/150], Loss: 12671.1257\n",
            "Epoch [60/150], Loss: 12969.1050\n",
            "Epoch [70/150], Loss: 6556.7141\n",
            "Epoch [80/150], Loss: 11203.6852\n",
            "Epoch [90/150], Loss: 16694.4333\n",
            "Epoch [100/150], Loss: 8905.3584\n",
            "Epoch [110/150], Loss: 7928.8466\n",
            "Epoch [120/150], Loss: 8140.9937\n",
            "Epoch [130/150], Loss: 10507.7378\n",
            "Epoch [140/150], Loss: 5066.6857\n",
            "Epoch [150/150], Loss: 12024.7571\n",
            "Fold 1, RMSE: 61.56684112548828\n",
            "Epoch [10/150], Loss: 15757.0691\n",
            "Epoch [20/150], Loss: 20470.1633\n",
            "Epoch [30/150], Loss: 17333.2107\n",
            "Epoch [40/150], Loss: 17928.2522\n",
            "Epoch [50/150], Loss: 14174.3876\n",
            "Epoch [60/150], Loss: 14212.4041\n",
            "Epoch [70/150], Loss: 18075.0586\n",
            "Epoch [80/150], Loss: 13872.9572\n",
            "Epoch [90/150], Loss: 25771.7043\n",
            "Epoch [100/150], Loss: 22342.3433\n",
            "Epoch [110/150], Loss: 16959.2671\n",
            "Epoch [120/150], Loss: 14966.4438\n",
            "Epoch [130/150], Loss: 22878.9541\n",
            "Epoch [140/150], Loss: 24317.0776\n",
            "Epoch [150/150], Loss: 17368.7271\n",
            "Fold 2, RMSE: 87.19844818115234\n",
            "Epoch [10/150], Loss: 10918.8633\n",
            "Epoch [20/150], Loss: 14353.5684\n",
            "Epoch [30/150], Loss: 11966.5901\n",
            "Epoch [40/150], Loss: 7901.7366\n",
            "Epoch [50/150], Loss: 12423.2986\n",
            "Epoch [60/150], Loss: 14194.0544\n",
            "Epoch [70/150], Loss: 7610.3250\n",
            "Epoch [80/150], Loss: 8516.9973\n",
            "Epoch [90/150], Loss: 8341.6846\n",
            "Epoch [100/150], Loss: 8288.2230\n",
            "Epoch [110/150], Loss: 11737.8450\n",
            "Epoch [120/150], Loss: 6442.4246\n",
            "Epoch [130/150], Loss: 8570.1631\n",
            "Epoch [140/150], Loss: 11129.1626\n",
            "Epoch [150/150], Loss: 11910.5864\n",
            "Fold 3, RMSE: 105.73210906982422\n",
            "Epoch [10/150], Loss: 18087.1604\n",
            "Epoch [20/150], Loss: 18972.8342\n",
            "Epoch [30/150], Loss: 17774.3066\n",
            "Epoch [40/150], Loss: 19807.2554\n",
            "Epoch [50/150], Loss: 23764.8521\n",
            "Epoch [60/150], Loss: 18637.8064\n",
            "Epoch [70/150], Loss: 17715.1714\n",
            "Epoch [80/150], Loss: 19769.9324\n",
            "Epoch [90/150], Loss: 16500.4341\n",
            "Epoch [100/150], Loss: 14584.1428\n",
            "Epoch [110/150], Loss: 14130.1279\n",
            "Epoch [120/150], Loss: 15989.6682\n",
            "Epoch [130/150], Loss: 9278.3477\n",
            "Epoch [140/150], Loss: 11857.0518\n",
            "Epoch [150/150], Loss: 13595.8893\n",
            "Fold 4, RMSE: 54.414859771728516\n",
            "Epoch [10/150], Loss: 15876.2673\n",
            "Epoch [20/150], Loss: 14389.5024\n",
            "Epoch [30/150], Loss: 11155.9944\n",
            "Epoch [40/150], Loss: 16807.8145\n",
            "Epoch [50/150], Loss: 16894.1682\n",
            "Epoch [60/150], Loss: 13146.8618\n",
            "Epoch [70/150], Loss: 11355.8121\n",
            "Epoch [80/150], Loss: 12124.5132\n",
            "Epoch [90/150], Loss: 18989.0652\n",
            "Epoch [100/150], Loss: 7932.3280\n",
            "Epoch [110/150], Loss: 11634.1919\n",
            "Epoch [120/150], Loss: 8815.4860\n",
            "Epoch [130/150], Loss: 9394.7612\n",
            "Epoch [140/150], Loss: 7652.7811\n",
            "Epoch [150/150], Loss: 9080.6523\n",
            "Fold 5, RMSE: 45.66895294189453\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 70.91624221801757\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18600.9497\n",
            "Epoch [20/100], Loss: 17138.1309\n",
            "Epoch [30/100], Loss: 14562.4495\n",
            "Epoch [40/100], Loss: 10542.1438\n",
            "Epoch [50/100], Loss: 12607.3871\n",
            "Epoch [60/100], Loss: 17389.4189\n",
            "Epoch [70/100], Loss: 13041.2156\n",
            "Epoch [80/100], Loss: 11100.9490\n",
            "Epoch [90/100], Loss: 7833.5164\n",
            "Epoch [100/100], Loss: 7495.6724\n",
            "Fold 1, RMSE: 58.689510345458984\n",
            "Epoch [10/100], Loss: 13972.1138\n",
            "Epoch [20/100], Loss: 15243.5215\n",
            "Epoch [30/100], Loss: 15832.5713\n",
            "Epoch [40/100], Loss: 19145.5581\n",
            "Epoch [50/100], Loss: 17690.7012\n",
            "Epoch [60/100], Loss: 16535.7524\n",
            "Epoch [70/100], Loss: 25673.0835\n",
            "Epoch [80/100], Loss: 15396.2373\n",
            "Epoch [90/100], Loss: 14453.3577\n",
            "Epoch [100/100], Loss: 14313.1283\n",
            "Fold 2, RMSE: 86.73162078857422\n",
            "Epoch [10/100], Loss: 14174.7478\n",
            "Epoch [20/100], Loss: 16123.9441\n",
            "Epoch [30/100], Loss: 11501.3413\n",
            "Epoch [40/100], Loss: 12127.9495\n",
            "Epoch [50/100], Loss: 8444.4550\n",
            "Epoch [60/100], Loss: 6881.8455\n",
            "Epoch [70/100], Loss: 12518.4617\n",
            "Epoch [80/100], Loss: 12219.6515\n",
            "Epoch [90/100], Loss: 19789.0386\n",
            "Epoch [100/100], Loss: 11766.2939\n",
            "Fold 3, RMSE: 109.62029266357422\n",
            "Epoch [10/100], Loss: 20775.5615\n",
            "Epoch [20/100], Loss: 15648.6934\n",
            "Epoch [30/100], Loss: 18585.0205\n",
            "Epoch [40/100], Loss: 12655.9976\n",
            "Epoch [50/100], Loss: 14004.9482\n",
            "Epoch [60/100], Loss: 13591.9376\n",
            "Epoch [70/100], Loss: 14115.2332\n",
            "Epoch [80/100], Loss: 16990.8489\n",
            "Epoch [90/100], Loss: 5114.2879\n",
            "Epoch [100/100], Loss: 6277.9208\n",
            "Fold 4, RMSE: 41.58246612548828\n",
            "Epoch [10/100], Loss: 22551.3354\n",
            "Epoch [20/100], Loss: 18170.3235\n",
            "Epoch [30/100], Loss: 9687.4104\n",
            "Epoch [40/100], Loss: 8848.3171\n",
            "Epoch [50/100], Loss: 8401.8911\n",
            "Epoch [60/100], Loss: 9897.1990\n",
            "Epoch [70/100], Loss: 8492.7849\n",
            "Epoch [80/100], Loss: 7095.9805\n",
            "Epoch [90/100], Loss: 8686.4743\n",
            "Epoch [100/100], Loss: 6966.7979\n",
            "Fold 5, RMSE: 44.80884552001953\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 68.28654708862305\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15036.1841\n",
            "Epoch [20/150], Loss: 8136.3151\n",
            "Epoch [30/150], Loss: 12790.4341\n",
            "Epoch [40/150], Loss: 12573.8638\n",
            "Epoch [50/150], Loss: 10616.7866\n",
            "Epoch [60/150], Loss: 6795.0271\n",
            "Epoch [70/150], Loss: 7249.3068\n",
            "Epoch [80/150], Loss: 13301.9175\n",
            "Epoch [90/150], Loss: 12292.6180\n",
            "Epoch [100/150], Loss: 6485.4446\n",
            "Epoch [110/150], Loss: 9009.6433\n",
            "Epoch [120/150], Loss: 6332.1003\n",
            "Epoch [130/150], Loss: 9344.6017\n",
            "Epoch [140/150], Loss: 5615.7587\n",
            "Epoch [150/150], Loss: 9188.9983\n",
            "Fold 1, RMSE: 60.79129409790039\n",
            "Epoch [10/150], Loss: 17569.5779\n",
            "Epoch [20/150], Loss: 13619.3120\n",
            "Epoch [30/150], Loss: 11196.2737\n",
            "Epoch [40/150], Loss: 13082.8472\n",
            "Epoch [50/150], Loss: 11330.4951\n",
            "Epoch [60/150], Loss: 12506.6841\n",
            "Epoch [70/150], Loss: 9748.9425\n",
            "Epoch [80/150], Loss: 13611.4270\n",
            "Epoch [90/150], Loss: 14736.8110\n",
            "Epoch [100/150], Loss: 7164.3340\n",
            "Epoch [110/150], Loss: 8240.6463\n",
            "Epoch [120/150], Loss: 9362.8254\n",
            "Epoch [130/150], Loss: 10672.8315\n",
            "Epoch [140/150], Loss: 16172.3923\n",
            "Epoch [150/150], Loss: 5838.1700\n",
            "Fold 2, RMSE: 74.36261749267578\n",
            "Epoch [10/150], Loss: 11864.7383\n",
            "Epoch [20/150], Loss: 14048.4104\n",
            "Epoch [30/150], Loss: 11689.6836\n",
            "Epoch [40/150], Loss: 11838.0227\n",
            "Epoch [50/150], Loss: 13592.5833\n",
            "Epoch [60/150], Loss: 15959.9819\n",
            "Epoch [70/150], Loss: 12314.0752\n",
            "Epoch [80/150], Loss: 17178.9656\n",
            "Epoch [90/150], Loss: 14945.4443\n",
            "Epoch [100/150], Loss: 14243.2085\n",
            "Epoch [110/150], Loss: 11530.6687\n",
            "Epoch [120/150], Loss: 12241.4966\n",
            "Epoch [130/150], Loss: 15821.1777\n",
            "Epoch [140/150], Loss: 12498.9263\n",
            "Epoch [150/150], Loss: 14174.6340\n",
            "Fold 3, RMSE: 109.37580108642578\n",
            "Epoch [10/150], Loss: 16287.2559\n",
            "Epoch [20/150], Loss: 18489.1958\n",
            "Epoch [30/150], Loss: 10939.8247\n",
            "Epoch [40/150], Loss: 16365.9736\n",
            "Epoch [50/150], Loss: 9848.6765\n",
            "Epoch [60/150], Loss: 14124.9641\n",
            "Epoch [70/150], Loss: 7708.1513\n",
            "Epoch [80/150], Loss: 19641.6030\n",
            "Epoch [90/150], Loss: 16467.0781\n",
            "Epoch [100/150], Loss: 9617.9404\n",
            "Epoch [110/150], Loss: 19469.1650\n",
            "Epoch [120/150], Loss: 8764.3695\n",
            "Epoch [130/150], Loss: 13009.4229\n",
            "Epoch [140/150], Loss: 12782.8309\n",
            "Epoch [150/150], Loss: 12800.1997\n",
            "Fold 4, RMSE: 48.67323684692383\n",
            "Epoch [10/150], Loss: 13660.8785\n",
            "Epoch [20/150], Loss: 14330.5408\n",
            "Epoch [30/150], Loss: 15058.9761\n",
            "Epoch [40/150], Loss: 18900.2087\n",
            "Epoch [50/150], Loss: 13458.3748\n",
            "Epoch [60/150], Loss: 6635.6337\n",
            "Epoch [70/150], Loss: 18682.7737\n",
            "Epoch [80/150], Loss: 12502.6538\n",
            "Epoch [90/150], Loss: 9878.6917\n",
            "Epoch [100/150], Loss: 11560.8494\n",
            "Epoch [110/150], Loss: 7949.6771\n",
            "Epoch [120/150], Loss: 9115.6252\n",
            "Epoch [130/150], Loss: 9608.7823\n",
            "Epoch [140/150], Loss: 9481.4121\n",
            "Epoch [150/150], Loss: 12844.9934\n",
            "Fold 5, RMSE: 48.10549545288086\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 68.26168899536133\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17553.7490\n",
            "Epoch [20/100], Loss: 17898.5364\n",
            "Epoch [30/100], Loss: 20964.3340\n",
            "Epoch [40/100], Loss: 18731.4297\n",
            "Epoch [50/100], Loss: 21919.9944\n",
            "Epoch [60/100], Loss: 16668.2410\n",
            "Epoch [70/100], Loss: 26668.6538\n",
            "Epoch [80/100], Loss: 23004.8813\n",
            "Epoch [90/100], Loss: 16595.9714\n",
            "Epoch [100/100], Loss: 20104.1147\n",
            "Fold 1, RMSE: 67.62149810791016\n",
            "Epoch [10/100], Loss: 11398.4707\n",
            "Epoch [20/100], Loss: 18479.5935\n",
            "Epoch [30/100], Loss: 17225.4250\n",
            "Epoch [40/100], Loss: 12290.0537\n",
            "Epoch [50/100], Loss: 10358.3478\n",
            "Epoch [60/100], Loss: 9858.3274\n",
            "Epoch [70/100], Loss: 6480.8093\n",
            "Epoch [80/100], Loss: 7000.8346\n",
            "Epoch [90/100], Loss: 8327.8440\n",
            "Epoch [100/100], Loss: 8966.3450\n",
            "Fold 2, RMSE: 72.88294982910156\n",
            "Epoch [10/100], Loss: 19245.7351\n",
            "Epoch [20/100], Loss: 11682.6807\n",
            "Epoch [30/100], Loss: 17783.8081\n",
            "Epoch [40/100], Loss: 12047.4458\n",
            "Epoch [50/100], Loss: 15726.1411\n",
            "Epoch [60/100], Loss: 12754.7588\n",
            "Epoch [70/100], Loss: 11710.1265\n",
            "Epoch [80/100], Loss: 11104.5370\n",
            "Epoch [90/100], Loss: 19348.2761\n",
            "Epoch [100/100], Loss: 15207.9927\n",
            "Fold 3, RMSE: 109.75321197509766\n",
            "Epoch [10/100], Loss: 18877.8462\n",
            "Epoch [20/100], Loss: 13838.3181\n",
            "Epoch [30/100], Loss: 24743.5435\n",
            "Epoch [40/100], Loss: 24737.1433\n",
            "Epoch [50/100], Loss: 18452.9336\n",
            "Epoch [60/100], Loss: 10937.6360\n",
            "Epoch [70/100], Loss: 11777.7832\n",
            "Epoch [80/100], Loss: 12381.5073\n",
            "Epoch [90/100], Loss: 18311.4231\n",
            "Epoch [100/100], Loss: 11725.1410\n",
            "Fold 4, RMSE: 45.57380294799805\n",
            "Epoch [10/100], Loss: 26125.9458\n",
            "Epoch [20/100], Loss: 19607.1362\n",
            "Epoch [30/100], Loss: 19227.5229\n",
            "Epoch [40/100], Loss: 11942.4417\n",
            "Epoch [50/100], Loss: 20731.8318\n",
            "Epoch [60/100], Loss: 7510.7623\n",
            "Epoch [70/100], Loss: 7504.5254\n",
            "Epoch [80/100], Loss: 11392.1396\n",
            "Epoch [90/100], Loss: 6906.0339\n",
            "Epoch [100/100], Loss: 13853.8530\n",
            "Fold 5, RMSE: 44.99262619018555\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 68.1648178100586\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18739.8901\n",
            "Epoch [20/150], Loss: 16438.2612\n",
            "Epoch [30/150], Loss: 14072.9995\n",
            "Epoch [40/150], Loss: 20985.9492\n",
            "Epoch [50/150], Loss: 19674.4302\n",
            "Epoch [60/150], Loss: 9806.9895\n",
            "Epoch [70/150], Loss: 13565.5413\n",
            "Epoch [80/150], Loss: 18036.4491\n",
            "Epoch [90/150], Loss: 7364.2499\n",
            "Epoch [100/150], Loss: 16151.2769\n",
            "Epoch [110/150], Loss: 18770.7319\n",
            "Epoch [120/150], Loss: 13366.4014\n",
            "Epoch [130/150], Loss: 11037.6930\n",
            "Epoch [140/150], Loss: 16409.0400\n",
            "Epoch [150/150], Loss: 10680.6180\n",
            "Fold 1, RMSE: 58.69010543823242\n",
            "Epoch [10/150], Loss: 13818.7256\n",
            "Epoch [20/150], Loss: 10348.5330\n",
            "Epoch [30/150], Loss: 14381.9604\n",
            "Epoch [40/150], Loss: 15793.6008\n",
            "Epoch [50/150], Loss: 8284.3531\n",
            "Epoch [60/150], Loss: 8445.0389\n",
            "Epoch [70/150], Loss: 7525.5656\n",
            "Epoch [80/150], Loss: 10424.4667\n",
            "Epoch [90/150], Loss: 12875.9670\n",
            "Epoch [100/150], Loss: 10656.7598\n",
            "Epoch [110/150], Loss: 10397.8792\n",
            "Epoch [120/150], Loss: 12108.3708\n",
            "Epoch [130/150], Loss: 6012.5469\n",
            "Epoch [140/150], Loss: 10976.8831\n",
            "Epoch [150/150], Loss: 9761.9563\n",
            "Fold 2, RMSE: 74.95134735107422\n",
            "Epoch [10/150], Loss: 14140.1282\n",
            "Epoch [20/150], Loss: 14222.7012\n",
            "Epoch [30/150], Loss: 11261.0424\n",
            "Epoch [40/150], Loss: 12742.2979\n",
            "Epoch [50/150], Loss: 15503.3442\n",
            "Epoch [60/150], Loss: 14247.5132\n",
            "Epoch [70/150], Loss: 13962.3611\n",
            "Epoch [80/150], Loss: 11419.2720\n",
            "Epoch [90/150], Loss: 14008.8906\n",
            "Epoch [100/150], Loss: 15480.6604\n",
            "Epoch [110/150], Loss: 15473.1255\n",
            "Epoch [120/150], Loss: 13015.5486\n",
            "Epoch [130/150], Loss: 12944.7285\n",
            "Epoch [140/150], Loss: 13597.1204\n",
            "Epoch [150/150], Loss: 12511.4124\n",
            "Fold 3, RMSE: 109.66214752197266\n",
            "Epoch [10/150], Loss: 27211.4590\n",
            "Epoch [20/150], Loss: 22126.7373\n",
            "Epoch [30/150], Loss: 20076.7090\n",
            "Epoch [40/150], Loss: 24256.9365\n",
            "Epoch [50/150], Loss: 22927.7686\n",
            "Epoch [60/150], Loss: 22127.8687\n",
            "Epoch [70/150], Loss: 17321.3672\n",
            "Epoch [80/150], Loss: 26446.8755\n",
            "Epoch [90/150], Loss: 18328.0581\n",
            "Epoch [100/150], Loss: 20072.0078\n",
            "Epoch [110/150], Loss: 19234.9102\n",
            "Epoch [120/150], Loss: 20856.0220\n",
            "Epoch [130/150], Loss: 19480.9678\n",
            "Epoch [140/150], Loss: 18594.7590\n",
            "Epoch [150/150], Loss: 19287.1255\n",
            "Fold 4, RMSE: 54.347755432128906\n",
            "Epoch [10/150], Loss: 25904.3154\n",
            "Epoch [20/150], Loss: 15174.2175\n",
            "Epoch [30/150], Loss: 21527.7588\n",
            "Epoch [40/150], Loss: 16898.0688\n",
            "Epoch [50/150], Loss: 9999.5527\n",
            "Epoch [60/150], Loss: 13647.9164\n",
            "Epoch [70/150], Loss: 11511.7545\n",
            "Epoch [80/150], Loss: 9860.5232\n",
            "Epoch [90/150], Loss: 12756.4871\n",
            "Epoch [100/150], Loss: 9838.5306\n",
            "Epoch [110/150], Loss: 14153.9856\n",
            "Epoch [120/150], Loss: 12318.7604\n",
            "Epoch [130/150], Loss: 10283.3455\n",
            "Epoch [140/150], Loss: 6960.1415\n",
            "Epoch [150/150], Loss: 12716.6963\n",
            "Fold 5, RMSE: 44.76513671875\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 68.48329849243164\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16914.4482\n",
            "Epoch [20/100], Loss: 14510.9746\n",
            "Epoch [30/100], Loss: 14369.4774\n",
            "Epoch [40/100], Loss: 18337.1826\n",
            "Epoch [50/100], Loss: 18319.9385\n",
            "Epoch [60/100], Loss: 19924.7898\n",
            "Epoch [70/100], Loss: 15587.7985\n",
            "Epoch [80/100], Loss: 23970.2178\n",
            "Epoch [90/100], Loss: 19530.6592\n",
            "Epoch [100/100], Loss: 20835.7644\n",
            "Fold 1, RMSE: 67.526123046875\n",
            "Epoch [10/100], Loss: 14507.4851\n",
            "Epoch [20/100], Loss: 13108.3737\n",
            "Epoch [30/100], Loss: 11158.1591\n",
            "Epoch [40/100], Loss: 12572.0691\n",
            "Epoch [50/100], Loss: 9244.9775\n",
            "Epoch [60/100], Loss: 10951.6865\n",
            "Epoch [70/100], Loss: 17821.9398\n",
            "Epoch [80/100], Loss: 5658.6769\n",
            "Epoch [90/100], Loss: 10515.0894\n",
            "Epoch [100/100], Loss: 3613.7451\n",
            "Fold 2, RMSE: 73.8154067993164\n",
            "Epoch [10/100], Loss: 11572.3103\n",
            "Epoch [20/100], Loss: 9883.5513\n",
            "Epoch [30/100], Loss: 12403.6589\n",
            "Epoch [40/100], Loss: 13400.8113\n",
            "Epoch [50/100], Loss: 6814.6465\n",
            "Epoch [60/100], Loss: 4868.0376\n",
            "Epoch [70/100], Loss: 7544.0623\n",
            "Epoch [80/100], Loss: 6234.9072\n",
            "Epoch [90/100], Loss: 6875.3298\n",
            "Epoch [100/100], Loss: 7098.6340\n",
            "Fold 3, RMSE: 99.34833526611328\n",
            "Epoch [10/100], Loss: 17408.5303\n",
            "Epoch [20/100], Loss: 19944.2063\n",
            "Epoch [30/100], Loss: 15605.1239\n",
            "Epoch [40/100], Loss: 11932.8708\n",
            "Epoch [50/100], Loss: 7725.8369\n",
            "Epoch [60/100], Loss: 14517.0898\n",
            "Epoch [70/100], Loss: 9091.8983\n",
            "Epoch [80/100], Loss: 11096.1630\n",
            "Epoch [90/100], Loss: 17106.1089\n",
            "Epoch [100/100], Loss: 15047.8589\n",
            "Fold 4, RMSE: 52.69989776611328\n",
            "Epoch [10/100], Loss: 19593.3013\n",
            "Epoch [20/100], Loss: 13464.0518\n",
            "Epoch [30/100], Loss: 15287.6262\n",
            "Epoch [40/100], Loss: 12701.5437\n",
            "Epoch [50/100], Loss: 16157.4761\n",
            "Epoch [60/100], Loss: 11219.9746\n",
            "Epoch [70/100], Loss: 9617.6837\n",
            "Epoch [80/100], Loss: 20322.6313\n",
            "Epoch [90/100], Loss: 16466.2731\n",
            "Epoch [100/100], Loss: 19390.5825\n",
            "Fold 5, RMSE: 45.582210540771484\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 67.7943946838379\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 25617.9277\n",
            "Epoch [20/150], Loss: 14270.2887\n",
            "Epoch [30/150], Loss: 15082.3856\n",
            "Epoch [40/150], Loss: 9971.9465\n",
            "Epoch [50/150], Loss: 15812.6138\n",
            "Epoch [60/150], Loss: 8499.1208\n",
            "Epoch [70/150], Loss: 14163.7659\n",
            "Epoch [80/150], Loss: 11709.7438\n",
            "Epoch [90/150], Loss: 7362.6410\n",
            "Epoch [100/150], Loss: 8706.7830\n",
            "Epoch [110/150], Loss: 5862.5592\n",
            "Epoch [120/150], Loss: 10130.7409\n",
            "Epoch [130/150], Loss: 5456.6573\n",
            "Epoch [140/150], Loss: 9447.9834\n",
            "Epoch [150/150], Loss: 5948.3585\n",
            "Fold 1, RMSE: 59.20015335083008\n",
            "Epoch [10/150], Loss: 14300.5073\n",
            "Epoch [20/150], Loss: 13052.5906\n",
            "Epoch [30/150], Loss: 11637.8967\n",
            "Epoch [40/150], Loss: 12096.2385\n",
            "Epoch [50/150], Loss: 17983.7046\n",
            "Epoch [60/150], Loss: 17052.1128\n",
            "Epoch [70/150], Loss: 21783.1631\n",
            "Epoch [80/150], Loss: 14297.1405\n",
            "Epoch [90/150], Loss: 16587.9971\n",
            "Epoch [100/150], Loss: 15412.5601\n",
            "Epoch [110/150], Loss: 14760.8582\n",
            "Epoch [120/150], Loss: 14845.0911\n",
            "Epoch [130/150], Loss: 15272.8345\n",
            "Epoch [140/150], Loss: 22516.0684\n",
            "Epoch [150/150], Loss: 15259.7480\n",
            "Fold 2, RMSE: 87.0242919921875\n",
            "Epoch [10/150], Loss: 16165.4099\n",
            "Epoch [20/150], Loss: 13562.2168\n",
            "Epoch [30/150], Loss: 12540.8552\n",
            "Epoch [40/150], Loss: 14772.2007\n",
            "Epoch [50/150], Loss: 16582.0647\n",
            "Epoch [60/150], Loss: 6210.0014\n",
            "Epoch [70/150], Loss: 8243.7620\n",
            "Epoch [80/150], Loss: 5160.1017\n",
            "Epoch [90/150], Loss: 11054.9631\n",
            "Epoch [100/150], Loss: 8625.9132\n",
            "Epoch [110/150], Loss: 12736.0032\n",
            "Epoch [120/150], Loss: 11547.1637\n",
            "Epoch [130/150], Loss: 11810.2935\n",
            "Epoch [140/150], Loss: 12205.6638\n",
            "Epoch [150/150], Loss: 12964.5645\n",
            "Fold 3, RMSE: 109.51145935058594\n",
            "Epoch [10/150], Loss: 18506.4175\n",
            "Epoch [20/150], Loss: 11991.3177\n",
            "Epoch [30/150], Loss: 12944.6113\n",
            "Epoch [40/150], Loss: 12378.4617\n",
            "Epoch [50/150], Loss: 13089.1636\n",
            "Epoch [60/150], Loss: 16494.5063\n",
            "Epoch [70/150], Loss: 11022.5208\n",
            "Epoch [80/150], Loss: 9027.8626\n",
            "Epoch [90/150], Loss: 11917.8621\n",
            "Epoch [100/150], Loss: 15697.6284\n",
            "Epoch [110/150], Loss: 10742.6357\n",
            "Epoch [120/150], Loss: 14915.5217\n",
            "Epoch [130/150], Loss: 13718.3696\n",
            "Epoch [140/150], Loss: 14417.7963\n",
            "Epoch [150/150], Loss: 8073.0392\n",
            "Fold 4, RMSE: 44.31545639038086\n",
            "Epoch [10/150], Loss: 16532.2759\n",
            "Epoch [20/150], Loss: 13692.6609\n",
            "Epoch [30/150], Loss: 14122.9890\n",
            "Epoch [40/150], Loss: 13561.0383\n",
            "Epoch [50/150], Loss: 14091.6238\n",
            "Epoch [60/150], Loss: 12055.1270\n",
            "Epoch [70/150], Loss: 17207.5989\n",
            "Epoch [80/150], Loss: 14160.8521\n",
            "Epoch [90/150], Loss: 12009.8066\n",
            "Epoch [100/150], Loss: 13145.2024\n",
            "Epoch [110/150], Loss: 14701.6672\n",
            "Epoch [120/150], Loss: 13768.3367\n",
            "Epoch [130/150], Loss: 10596.4244\n",
            "Epoch [140/150], Loss: 7411.8779\n",
            "Epoch [150/150], Loss: 12250.5779\n",
            "Fold 5, RMSE: 44.90182876586914\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 68.9906379699707\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 18659.6533\n",
            "Epoch [20/100], Loss: 16354.8658\n",
            "Epoch [30/100], Loss: 16012.9037\n",
            "Epoch [40/100], Loss: 17739.8540\n",
            "Epoch [50/100], Loss: 18077.6392\n",
            "Epoch [60/100], Loss: 18986.2754\n",
            "Epoch [70/100], Loss: 20926.7930\n",
            "Epoch [80/100], Loss: 16572.4958\n",
            "Epoch [90/100], Loss: 26188.2920\n",
            "Epoch [100/100], Loss: 16991.9082\n",
            "Fold 1, RMSE: 67.57994079589844\n",
            "Epoch [10/100], Loss: 17229.4827\n",
            "Epoch [20/100], Loss: 16142.0715\n",
            "Epoch [30/100], Loss: 17281.1233\n",
            "Epoch [40/100], Loss: 15975.9495\n",
            "Epoch [50/100], Loss: 15038.4961\n",
            "Epoch [60/100], Loss: 16095.1567\n",
            "Epoch [70/100], Loss: 15417.1003\n",
            "Epoch [80/100], Loss: 14784.0073\n",
            "Epoch [90/100], Loss: 14603.5050\n",
            "Epoch [100/100], Loss: 17211.2578\n",
            "Fold 2, RMSE: 86.43893432617188\n",
            "Epoch [10/100], Loss: 13272.7883\n",
            "Epoch [20/100], Loss: 12375.2158\n",
            "Epoch [30/100], Loss: 16167.9852\n",
            "Epoch [40/100], Loss: 12421.8242\n",
            "Epoch [50/100], Loss: 13393.6829\n",
            "Epoch [60/100], Loss: 14687.5063\n",
            "Epoch [70/100], Loss: 12759.4888\n",
            "Epoch [80/100], Loss: 13391.0996\n",
            "Epoch [90/100], Loss: 16706.4290\n",
            "Epoch [100/100], Loss: 13514.5686\n",
            "Fold 3, RMSE: 109.75758361816406\n",
            "Epoch [10/100], Loss: 22108.1362\n",
            "Epoch [20/100], Loss: 18726.7102\n",
            "Epoch [30/100], Loss: 19195.9062\n",
            "Epoch [40/100], Loss: 29389.9883\n",
            "Epoch [50/100], Loss: 19764.0166\n",
            "Epoch [60/100], Loss: 17910.8406\n",
            "Epoch [70/100], Loss: 29102.4531\n",
            "Epoch [80/100], Loss: 19131.4521\n",
            "Epoch [90/100], Loss: 18807.8557\n",
            "Epoch [100/100], Loss: 24042.9819\n",
            "Fold 4, RMSE: 54.43269348144531\n",
            "Epoch [10/100], Loss: 20329.8052\n",
            "Epoch [20/100], Loss: 18571.3342\n",
            "Epoch [30/100], Loss: 19749.8560\n",
            "Epoch [40/100], Loss: 19621.5498\n",
            "Epoch [50/100], Loss: 27467.2583\n",
            "Epoch [60/100], Loss: 19143.0337\n",
            "Epoch [70/100], Loss: 17323.9940\n",
            "Epoch [80/100], Loss: 17973.0605\n",
            "Epoch [90/100], Loss: 20155.0181\n",
            "Epoch [100/100], Loss: 22961.1533\n",
            "Fold 5, RMSE: 57.91722106933594\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 75.22527465820312\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18295.9150\n",
            "Epoch [20/150], Loss: 22169.5220\n",
            "Epoch [30/150], Loss: 18909.9639\n",
            "Epoch [40/150], Loss: 19980.9482\n",
            "Epoch [50/150], Loss: 18369.2478\n",
            "Epoch [60/150], Loss: 20575.2524\n",
            "Epoch [70/150], Loss: 19393.7417\n",
            "Epoch [80/150], Loss: 29251.0500\n",
            "Epoch [90/150], Loss: 17833.4727\n",
            "Epoch [100/150], Loss: 15713.9543\n",
            "Epoch [110/150], Loss: 19871.4902\n",
            "Epoch [120/150], Loss: 26359.7090\n",
            "Epoch [130/150], Loss: 18846.8345\n",
            "Epoch [140/150], Loss: 27708.9331\n",
            "Epoch [150/150], Loss: 15657.9271\n",
            "Fold 1, RMSE: 67.38238525390625\n",
            "Epoch [10/150], Loss: 16248.7341\n",
            "Epoch [20/150], Loss: 15371.4419\n",
            "Epoch [30/150], Loss: 15518.3923\n",
            "Epoch [40/150], Loss: 15290.7080\n",
            "Epoch [50/150], Loss: 14679.9255\n",
            "Epoch [60/150], Loss: 15109.9775\n",
            "Epoch [70/150], Loss: 16524.1841\n",
            "Epoch [80/150], Loss: 15693.6458\n",
            "Epoch [90/150], Loss: 17591.2468\n",
            "Epoch [100/150], Loss: 15765.2795\n",
            "Epoch [110/150], Loss: 16174.5544\n",
            "Epoch [120/150], Loss: 14552.9744\n",
            "Epoch [130/150], Loss: 15691.7346\n",
            "Epoch [140/150], Loss: 14317.7380\n",
            "Epoch [150/150], Loss: 16823.5000\n",
            "Fold 2, RMSE: 86.99263000488281\n",
            "Epoch [10/150], Loss: 12083.3499\n",
            "Epoch [20/150], Loss: 14235.4924\n",
            "Epoch [30/150], Loss: 17678.2942\n",
            "Epoch [40/150], Loss: 12823.9402\n",
            "Epoch [50/150], Loss: 12584.3196\n",
            "Epoch [60/150], Loss: 12133.3496\n",
            "Epoch [70/150], Loss: 12739.7021\n",
            "Epoch [80/150], Loss: 11257.1066\n",
            "Epoch [90/150], Loss: 11364.0012\n",
            "Epoch [100/150], Loss: 15151.3650\n",
            "Epoch [110/150], Loss: 14484.3706\n",
            "Epoch [120/150], Loss: 15451.3827\n",
            "Epoch [130/150], Loss: 15776.9502\n",
            "Epoch [140/150], Loss: 13763.1880\n",
            "Epoch [150/150], Loss: 18771.8430\n",
            "Fold 3, RMSE: 109.45012664794922\n",
            "Epoch [10/150], Loss: 25052.6719\n",
            "Epoch [20/150], Loss: 18323.2349\n",
            "Epoch [30/150], Loss: 17612.0204\n",
            "Epoch [40/150], Loss: 17124.7051\n",
            "Epoch [50/150], Loss: 16610.0768\n",
            "Epoch [60/150], Loss: 20299.3901\n",
            "Epoch [70/150], Loss: 26519.2180\n",
            "Epoch [80/150], Loss: 19202.6479\n",
            "Epoch [90/150], Loss: 24181.9131\n",
            "Epoch [100/150], Loss: 20227.4897\n",
            "Epoch [110/150], Loss: 19627.3206\n",
            "Epoch [120/150], Loss: 18055.8921\n",
            "Epoch [130/150], Loss: 17992.1001\n",
            "Epoch [140/150], Loss: 21746.2617\n",
            "Epoch [150/150], Loss: 18195.8328\n",
            "Fold 4, RMSE: 54.36608123779297\n",
            "Epoch [10/150], Loss: 19723.0444\n",
            "Epoch [20/150], Loss: 19373.6782\n",
            "Epoch [30/150], Loss: 13978.7324\n",
            "Epoch [40/150], Loss: 11616.8093\n",
            "Epoch [50/150], Loss: 15545.7869\n",
            "Epoch [60/150], Loss: 14845.3853\n",
            "Epoch [70/150], Loss: 12805.4723\n",
            "Epoch [80/150], Loss: 11458.1406\n",
            "Epoch [90/150], Loss: 12002.7429\n",
            "Epoch [100/150], Loss: 11002.9719\n",
            "Epoch [110/150], Loss: 21675.3531\n",
            "Epoch [120/150], Loss: 8943.2946\n",
            "Epoch [130/150], Loss: 12670.8589\n",
            "Epoch [140/150], Loss: 11446.3218\n",
            "Epoch [150/150], Loss: 9010.7605\n",
            "Fold 5, RMSE: 46.81252670288086\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 73.00074996948243\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 21086.8853\n",
            "Epoch [20/100], Loss: 18224.7695\n",
            "Epoch [30/100], Loss: 15603.1758\n",
            "Epoch [40/100], Loss: 18792.6575\n",
            "Epoch [50/100], Loss: 19046.5879\n",
            "Epoch [60/100], Loss: 22388.1052\n",
            "Epoch [70/100], Loss: 17110.1570\n",
            "Epoch [80/100], Loss: 16568.8821\n",
            "Epoch [90/100], Loss: 16306.1459\n",
            "Epoch [100/100], Loss: 20723.2285\n",
            "Fold 1, RMSE: 67.02178955078125\n",
            "Epoch [10/100], Loss: 15973.7754\n",
            "Epoch [20/100], Loss: 15119.7620\n",
            "Epoch [30/100], Loss: 17445.5649\n",
            "Epoch [40/100], Loss: 21807.6140\n",
            "Epoch [50/100], Loss: 17306.5693\n",
            "Epoch [60/100], Loss: 17457.9326\n",
            "Epoch [70/100], Loss: 17895.2368\n",
            "Epoch [80/100], Loss: 16436.8999\n",
            "Epoch [90/100], Loss: 18940.3010\n",
            "Epoch [100/100], Loss: 14168.6461\n",
            "Fold 2, RMSE: 86.8062515258789\n",
            "Epoch [10/100], Loss: 11122.6079\n",
            "Epoch [20/100], Loss: 13943.5161\n",
            "Epoch [30/100], Loss: 13840.5000\n",
            "Epoch [40/100], Loss: 14761.3960\n",
            "Epoch [50/100], Loss: 12085.0696\n",
            "Epoch [60/100], Loss: 7980.8327\n",
            "Epoch [70/100], Loss: 12069.0752\n",
            "Epoch [80/100], Loss: 10879.2048\n",
            "Epoch [90/100], Loss: 13357.9597\n",
            "Epoch [100/100], Loss: 8681.3313\n",
            "Fold 3, RMSE: 101.31990051269531\n",
            "Epoch [10/100], Loss: 19442.5757\n",
            "Epoch [20/100], Loss: 20787.5029\n",
            "Epoch [30/100], Loss: 19300.5327\n",
            "Epoch [40/100], Loss: 24217.8308\n",
            "Epoch [50/100], Loss: 20333.2593\n",
            "Epoch [60/100], Loss: 22108.1870\n",
            "Epoch [70/100], Loss: 17754.9258\n",
            "Epoch [80/100], Loss: 25894.2153\n",
            "Epoch [90/100], Loss: 25497.5923\n",
            "Epoch [100/100], Loss: 19121.4849\n",
            "Fold 4, RMSE: 54.05195999145508\n",
            "Epoch [10/100], Loss: 17509.4180\n",
            "Epoch [20/100], Loss: 12743.3066\n",
            "Epoch [30/100], Loss: 19409.2776\n",
            "Epoch [40/100], Loss: 12968.9561\n",
            "Epoch [50/100], Loss: 11997.3350\n",
            "Epoch [60/100], Loss: 9468.6699\n",
            "Epoch [70/100], Loss: 8542.7228\n",
            "Epoch [80/100], Loss: 12017.7571\n",
            "Epoch [90/100], Loss: 11357.8920\n",
            "Epoch [100/100], Loss: 14727.2312\n",
            "Fold 5, RMSE: 45.09658432006836\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 70.85929718017579\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 16328.2158\n",
            "Epoch [20/150], Loss: 21667.1614\n",
            "Epoch [30/150], Loss: 18763.6890\n",
            "Epoch [40/150], Loss: 11084.3379\n",
            "Epoch [50/150], Loss: 11702.3650\n",
            "Epoch [60/150], Loss: 9774.2715\n",
            "Epoch [70/150], Loss: 10315.0417\n",
            "Epoch [80/150], Loss: 14999.4180\n",
            "Epoch [90/150], Loss: 11936.7693\n",
            "Epoch [100/150], Loss: 12314.7158\n",
            "Epoch [110/150], Loss: 10280.8700\n",
            "Epoch [120/150], Loss: 22569.5444\n",
            "Epoch [130/150], Loss: 15516.0585\n",
            "Epoch [140/150], Loss: 18936.2686\n",
            "Epoch [150/150], Loss: 16063.2194\n",
            "Fold 1, RMSE: 66.78628540039062\n",
            "Epoch [10/150], Loss: 22458.8279\n",
            "Epoch [20/150], Loss: 15047.6638\n",
            "Epoch [30/150], Loss: 13294.8221\n",
            "Epoch [40/150], Loss: 10580.7559\n",
            "Epoch [50/150], Loss: 12253.5610\n",
            "Epoch [60/150], Loss: 17330.6016\n",
            "Epoch [70/150], Loss: 16209.6672\n",
            "Epoch [80/150], Loss: 16444.3862\n",
            "Epoch [90/150], Loss: 24905.1348\n",
            "Epoch [100/150], Loss: 13866.8156\n",
            "Epoch [110/150], Loss: 22601.5356\n",
            "Epoch [120/150], Loss: 14698.2329\n",
            "Epoch [130/150], Loss: 15244.8428\n",
            "Epoch [140/150], Loss: 14042.6346\n",
            "Epoch [150/150], Loss: 14506.7899\n",
            "Fold 2, RMSE: 86.66041564941406\n",
            "Epoch [10/150], Loss: 14621.1748\n",
            "Epoch [20/150], Loss: 12694.6641\n",
            "Epoch [30/150], Loss: 12581.9231\n",
            "Epoch [40/150], Loss: 11507.0275\n",
            "Epoch [50/150], Loss: 16189.9502\n",
            "Epoch [60/150], Loss: 10945.0980\n",
            "Epoch [70/150], Loss: 13225.6926\n",
            "Epoch [80/150], Loss: 13819.7886\n",
            "Epoch [90/150], Loss: 13165.6064\n",
            "Epoch [100/150], Loss: 11486.1980\n",
            "Epoch [110/150], Loss: 14141.8789\n",
            "Epoch [120/150], Loss: 12586.9094\n",
            "Epoch [130/150], Loss: 12989.4136\n",
            "Epoch [140/150], Loss: 12162.5276\n",
            "Epoch [150/150], Loss: 12197.4983\n",
            "Fold 3, RMSE: 109.14601135253906\n",
            "Epoch [10/150], Loss: 17581.6567\n",
            "Epoch [20/150], Loss: 20299.3081\n",
            "Epoch [30/150], Loss: 18772.2026\n",
            "Epoch [40/150], Loss: 22110.1484\n",
            "Epoch [50/150], Loss: 17798.0103\n",
            "Epoch [60/150], Loss: 18125.3279\n",
            "Epoch [70/150], Loss: 23105.8311\n",
            "Epoch [80/150], Loss: 29163.5146\n",
            "Epoch [90/150], Loss: 25435.5623\n",
            "Epoch [100/150], Loss: 18947.3555\n",
            "Epoch [110/150], Loss: 19983.5112\n",
            "Epoch [120/150], Loss: 23029.8706\n",
            "Epoch [130/150], Loss: 17343.2936\n",
            "Epoch [140/150], Loss: 20193.2354\n",
            "Epoch [150/150], Loss: 20405.2393\n",
            "Fold 4, RMSE: 53.66704177856445\n",
            "Epoch [10/150], Loss: 25704.6064\n",
            "Epoch [20/150], Loss: 22510.7908\n",
            "Epoch [30/150], Loss: 18250.2715\n",
            "Epoch [40/150], Loss: 16772.9663\n",
            "Epoch [50/150], Loss: 19033.6475\n",
            "Epoch [60/150], Loss: 9769.0246\n",
            "Epoch [70/150], Loss: 9161.2844\n",
            "Epoch [80/150], Loss: 8915.9305\n",
            "Epoch [90/150], Loss: 11919.6702\n",
            "Epoch [100/150], Loss: 13586.9004\n",
            "Epoch [110/150], Loss: 16750.0962\n",
            "Epoch [120/150], Loss: 9367.5476\n",
            "Epoch [130/150], Loss: 10263.4471\n",
            "Epoch [140/150], Loss: 12616.6222\n",
            "Epoch [150/150], Loss: 14247.3658\n",
            "Fold 5, RMSE: 48.56554412841797\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 72.96505966186524\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17094.6445\n",
            "Epoch [20/100], Loss: 16802.6309\n",
            "Epoch [30/100], Loss: 17641.3726\n",
            "Epoch [40/100], Loss: 19359.2529\n",
            "Epoch [50/100], Loss: 23558.6357\n",
            "Epoch [60/100], Loss: 17719.5681\n",
            "Epoch [70/100], Loss: 16470.9572\n",
            "Epoch [80/100], Loss: 18435.0967\n",
            "Epoch [90/100], Loss: 25966.6743\n",
            "Epoch [100/100], Loss: 17600.0239\n",
            "Fold 1, RMSE: 67.49296569824219\n",
            "Epoch [10/100], Loss: 17405.4907\n",
            "Epoch [20/100], Loss: 14160.2992\n",
            "Epoch [30/100], Loss: 14976.7988\n",
            "Epoch [40/100], Loss: 15830.7202\n",
            "Epoch [50/100], Loss: 16065.8857\n",
            "Epoch [60/100], Loss: 17905.5354\n",
            "Epoch [70/100], Loss: 18701.4736\n",
            "Epoch [80/100], Loss: 22239.3638\n",
            "Epoch [90/100], Loss: 14654.0347\n",
            "Epoch [100/100], Loss: 20768.6653\n",
            "Fold 2, RMSE: 87.36703491210938\n",
            "Epoch [10/100], Loss: 11640.2830\n",
            "Epoch [20/100], Loss: 12040.8384\n",
            "Epoch [30/100], Loss: 12360.3164\n",
            "Epoch [40/100], Loss: 15762.4612\n",
            "Epoch [50/100], Loss: 12300.1824\n",
            "Epoch [60/100], Loss: 9873.9688\n",
            "Epoch [70/100], Loss: 11462.5872\n",
            "Epoch [80/100], Loss: 8690.0480\n",
            "Epoch [90/100], Loss: 11329.2941\n",
            "Epoch [100/100], Loss: 7755.2372\n",
            "Fold 3, RMSE: 97.25392150878906\n",
            "Epoch [10/100], Loss: 22794.8848\n",
            "Epoch [20/100], Loss: 30056.5142\n",
            "Epoch [30/100], Loss: 22001.7495\n",
            "Epoch [40/100], Loss: 18773.6584\n",
            "Epoch [50/100], Loss: 19535.0483\n",
            "Epoch [60/100], Loss: 17175.0522\n",
            "Epoch [70/100], Loss: 23939.3658\n",
            "Epoch [80/100], Loss: 24089.1504\n",
            "Epoch [90/100], Loss: 18784.3904\n",
            "Epoch [100/100], Loss: 18302.9692\n",
            "Fold 4, RMSE: 54.43150329589844\n",
            "Epoch [10/100], Loss: 19280.7134\n",
            "Epoch [20/100], Loss: 24198.4678\n",
            "Epoch [30/100], Loss: 26057.1675\n",
            "Epoch [40/100], Loss: 17944.4924\n",
            "Epoch [50/100], Loss: 16864.7487\n",
            "Epoch [60/100], Loss: 18683.1443\n",
            "Epoch [70/100], Loss: 18610.8926\n",
            "Epoch [80/100], Loss: 26107.4023\n",
            "Epoch [90/100], Loss: 29743.7271\n",
            "Epoch [100/100], Loss: 17875.7837\n",
            "Fold 5, RMSE: 57.85238265991211\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 72.87956161499024\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16417.5886\n",
            "Epoch [20/150], Loss: 22538.6147\n",
            "Epoch [30/150], Loss: 20026.3569\n",
            "Epoch [40/150], Loss: 21239.5161\n",
            "Epoch [50/150], Loss: 16303.5837\n",
            "Epoch [60/150], Loss: 21064.6978\n",
            "Epoch [70/150], Loss: 17232.9565\n",
            "Epoch [80/150], Loss: 18906.3120\n",
            "Epoch [90/150], Loss: 18687.4766\n",
            "Epoch [100/150], Loss: 18976.2712\n",
            "Epoch [110/150], Loss: 20767.4419\n",
            "Epoch [120/150], Loss: 16584.0308\n",
            "Epoch [130/150], Loss: 18393.9766\n",
            "Epoch [140/150], Loss: 18655.0469\n",
            "Epoch [150/150], Loss: 17206.2439\n",
            "Fold 1, RMSE: 67.25934600830078\n",
            "Epoch [10/150], Loss: 14774.7708\n",
            "Epoch [20/150], Loss: 27511.9954\n",
            "Epoch [30/150], Loss: 17527.7095\n",
            "Epoch [40/150], Loss: 26190.5466\n",
            "Epoch [50/150], Loss: 26004.0203\n",
            "Epoch [60/150], Loss: 14821.0408\n",
            "Epoch [70/150], Loss: 15618.3242\n",
            "Epoch [80/150], Loss: 29491.0952\n",
            "Epoch [90/150], Loss: 16867.7954\n",
            "Epoch [100/150], Loss: 16553.6509\n",
            "Epoch [110/150], Loss: 22477.2959\n",
            "Epoch [120/150], Loss: 14475.2748\n",
            "Epoch [130/150], Loss: 14221.3574\n",
            "Epoch [140/150], Loss: 17061.7871\n",
            "Epoch [150/150], Loss: 13717.8613\n",
            "Fold 2, RMSE: 87.14303588867188\n",
            "Epoch [10/150], Loss: 14163.6533\n",
            "Epoch [20/150], Loss: 14810.3368\n",
            "Epoch [30/150], Loss: 11388.9235\n",
            "Epoch [40/150], Loss: 18546.1721\n",
            "Epoch [50/150], Loss: 10699.2056\n",
            "Epoch [60/150], Loss: 11384.7672\n",
            "Epoch [70/150], Loss: 18614.6953\n",
            "Epoch [80/150], Loss: 12961.1753\n",
            "Epoch [90/150], Loss: 15159.7588\n",
            "Epoch [100/150], Loss: 17959.9265\n",
            "Epoch [110/150], Loss: 14924.8943\n",
            "Epoch [120/150], Loss: 11318.4875\n",
            "Epoch [130/150], Loss: 11712.4343\n",
            "Epoch [140/150], Loss: 12620.5891\n",
            "Epoch [150/150], Loss: 16880.9087\n",
            "Fold 3, RMSE: 109.33195495605469\n",
            "Epoch [10/150], Loss: 20746.5835\n",
            "Epoch [20/150], Loss: 19231.6567\n",
            "Epoch [30/150], Loss: 19333.5195\n",
            "Epoch [40/150], Loss: 27465.4521\n",
            "Epoch [50/150], Loss: 18463.1899\n",
            "Epoch [60/150], Loss: 17778.2961\n",
            "Epoch [70/150], Loss: 23548.7012\n",
            "Epoch [80/150], Loss: 17627.8333\n",
            "Epoch [90/150], Loss: 21704.2705\n",
            "Epoch [100/150], Loss: 22526.8340\n",
            "Epoch [110/150], Loss: 20380.6709\n",
            "Epoch [120/150], Loss: 17482.0765\n",
            "Epoch [130/150], Loss: 17126.5857\n",
            "Epoch [140/150], Loss: 18673.2007\n",
            "Epoch [150/150], Loss: 19133.0542\n",
            "Fold 4, RMSE: 54.0814208984375\n",
            "Epoch [10/150], Loss: 17300.5410\n",
            "Epoch [20/150], Loss: 17386.7773\n",
            "Epoch [30/150], Loss: 28071.1311\n",
            "Epoch [40/150], Loss: 24533.3896\n",
            "Epoch [50/150], Loss: 16838.0544\n",
            "Epoch [60/150], Loss: 15820.0374\n",
            "Epoch [70/150], Loss: 18823.8281\n",
            "Epoch [80/150], Loss: 11778.4473\n",
            "Epoch [90/150], Loss: 16488.5869\n",
            "Epoch [100/150], Loss: 7824.7656\n",
            "Epoch [110/150], Loss: 13828.9087\n",
            "Epoch [120/150], Loss: 23455.3643\n",
            "Epoch [130/150], Loss: 14704.2744\n",
            "Epoch [140/150], Loss: 16584.5784\n",
            "Epoch [150/150], Loss: 13041.3334\n",
            "Fold 5, RMSE: 44.04358673095703\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 72.37186889648437\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 22397.4141\n",
            "Epoch [20/100], Loss: 19136.6543\n",
            "Epoch [30/100], Loss: 18871.6416\n",
            "Epoch [40/100], Loss: 21223.0803\n",
            "Epoch [50/100], Loss: 31330.8948\n",
            "Epoch [60/100], Loss: 26482.4878\n",
            "Epoch [70/100], Loss: 19066.3164\n",
            "Epoch [80/100], Loss: 19624.9585\n",
            "Epoch [90/100], Loss: 19518.4143\n",
            "Epoch [100/100], Loss: 19570.1333\n",
            "Fold 1, RMSE: 67.1735610961914\n",
            "Epoch [10/100], Loss: 24437.9468\n",
            "Epoch [20/100], Loss: 14118.1028\n",
            "Epoch [30/100], Loss: 13145.9212\n",
            "Epoch [40/100], Loss: 15697.8384\n",
            "Epoch [50/100], Loss: 16863.5950\n",
            "Epoch [60/100], Loss: 15839.5750\n",
            "Epoch [70/100], Loss: 9871.5568\n",
            "Epoch [80/100], Loss: 11430.9176\n",
            "Epoch [90/100], Loss: 10686.0327\n",
            "Epoch [100/100], Loss: 7466.4094\n",
            "Fold 2, RMSE: 84.50022888183594\n",
            "Epoch [10/100], Loss: 14588.3750\n",
            "Epoch [20/100], Loss: 10928.6803\n",
            "Epoch [30/100], Loss: 14710.1572\n",
            "Epoch [40/100], Loss: 18549.5383\n",
            "Epoch [50/100], Loss: 16483.6382\n",
            "Epoch [60/100], Loss: 13443.1582\n",
            "Epoch [70/100], Loss: 11341.5880\n",
            "Epoch [80/100], Loss: 12378.8784\n",
            "Epoch [90/100], Loss: 17587.2932\n",
            "Epoch [100/100], Loss: 12191.2051\n",
            "Fold 3, RMSE: 109.40935516357422\n",
            "Epoch [10/100], Loss: 16800.6045\n",
            "Epoch [20/100], Loss: 16640.2211\n",
            "Epoch [30/100], Loss: 20298.7368\n",
            "Epoch [40/100], Loss: 19011.3926\n",
            "Epoch [50/100], Loss: 17549.8752\n",
            "Epoch [60/100], Loss: 21343.5474\n",
            "Epoch [70/100], Loss: 23716.0947\n",
            "Epoch [80/100], Loss: 21171.8428\n",
            "Epoch [90/100], Loss: 21264.7466\n",
            "Epoch [100/100], Loss: 19201.1484\n",
            "Fold 4, RMSE: 53.94549560546875\n",
            "Epoch [10/100], Loss: 17668.3843\n",
            "Epoch [20/100], Loss: 17959.8848\n",
            "Epoch [30/100], Loss: 19341.3481\n",
            "Epoch [40/100], Loss: 18528.1179\n",
            "Epoch [50/100], Loss: 26380.2793\n",
            "Epoch [60/100], Loss: 19945.9229\n",
            "Epoch [70/100], Loss: 20481.4575\n",
            "Epoch [80/100], Loss: 24784.3394\n",
            "Epoch [90/100], Loss: 24795.9155\n",
            "Epoch [100/100], Loss: 17071.2461\n",
            "Fold 5, RMSE: 57.37238311767578\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 74.48020477294922\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 11954.4233\n",
            "Epoch [20/150], Loss: 19590.3350\n",
            "Epoch [30/150], Loss: 19492.7041\n",
            "Epoch [40/150], Loss: 19645.5200\n",
            "Epoch [50/150], Loss: 20076.8184\n",
            "Epoch [60/150], Loss: 19494.7349\n",
            "Epoch [70/150], Loss: 24997.6865\n",
            "Epoch [80/150], Loss: 20829.3862\n",
            "Epoch [90/150], Loss: 23677.3000\n",
            "Epoch [100/150], Loss: 17291.4824\n",
            "Epoch [110/150], Loss: 16581.0396\n",
            "Epoch [120/150], Loss: 21732.0530\n",
            "Epoch [130/150], Loss: 17831.5176\n",
            "Epoch [140/150], Loss: 15764.8810\n",
            "Epoch [150/150], Loss: 26575.2661\n",
            "Fold 1, RMSE: 66.78458404541016\n",
            "Epoch [10/150], Loss: 14328.3362\n",
            "Epoch [20/150], Loss: 15126.4373\n",
            "Epoch [30/150], Loss: 14212.2175\n",
            "Epoch [40/150], Loss: 16889.9214\n",
            "Epoch [50/150], Loss: 17622.2153\n",
            "Epoch [60/150], Loss: 15713.9131\n",
            "Epoch [70/150], Loss: 17494.2229\n",
            "Epoch [80/150], Loss: 14775.1787\n",
            "Epoch [90/150], Loss: 16547.8298\n",
            "Epoch [100/150], Loss: 15827.2915\n",
            "Epoch [110/150], Loss: 17816.1348\n",
            "Epoch [120/150], Loss: 14559.4838\n",
            "Epoch [130/150], Loss: 15275.9092\n",
            "Epoch [140/150], Loss: 14959.1707\n",
            "Epoch [150/150], Loss: 14413.2120\n",
            "Fold 2, RMSE: 86.44034576416016\n",
            "Epoch [10/150], Loss: 14859.5178\n",
            "Epoch [20/150], Loss: 10625.8330\n",
            "Epoch [30/150], Loss: 14137.4912\n",
            "Epoch [40/150], Loss: 13216.9517\n",
            "Epoch [50/150], Loss: 14275.8711\n",
            "Epoch [60/150], Loss: 14142.7346\n",
            "Epoch [70/150], Loss: 12052.2037\n",
            "Epoch [80/150], Loss: 11882.1353\n",
            "Epoch [90/150], Loss: 11155.8190\n",
            "Epoch [100/150], Loss: 16219.2002\n",
            "Epoch [110/150], Loss: 11989.5682\n",
            "Epoch [120/150], Loss: 12938.6619\n",
            "Epoch [130/150], Loss: 13024.0957\n",
            "Epoch [140/150], Loss: 17343.8655\n",
            "Epoch [150/150], Loss: 13555.9233\n",
            "Fold 3, RMSE: 108.7580795288086\n",
            "Epoch [10/150], Loss: 26319.7778\n",
            "Epoch [20/150], Loss: 16525.7540\n",
            "Epoch [30/150], Loss: 18797.8748\n",
            "Epoch [40/150], Loss: 19967.2368\n",
            "Epoch [50/150], Loss: 22229.7900\n",
            "Epoch [60/150], Loss: 17834.5471\n",
            "Epoch [70/150], Loss: 24276.9875\n",
            "Epoch [80/150], Loss: 20130.3455\n",
            "Epoch [90/150], Loss: 28467.3223\n",
            "Epoch [100/150], Loss: 17455.0620\n",
            "Epoch [110/150], Loss: 21899.0537\n",
            "Epoch [120/150], Loss: 18580.2021\n",
            "Epoch [130/150], Loss: 19352.9380\n",
            "Epoch [140/150], Loss: 23465.4526\n",
            "Epoch [150/150], Loss: 21537.4302\n",
            "Fold 4, RMSE: 53.84758758544922\n",
            "Epoch [10/150], Loss: 21895.8657\n",
            "Epoch [20/150], Loss: 28006.4575\n",
            "Epoch [30/150], Loss: 17378.9734\n",
            "Epoch [40/150], Loss: 23041.2065\n",
            "Epoch [50/150], Loss: 20330.6978\n",
            "Epoch [60/150], Loss: 22256.2466\n",
            "Epoch [70/150], Loss: 19024.7612\n",
            "Epoch [80/150], Loss: 19440.7798\n",
            "Epoch [90/150], Loss: 17712.6111\n",
            "Epoch [100/150], Loss: 16994.2612\n",
            "Epoch [110/150], Loss: 16726.2672\n",
            "Epoch [120/150], Loss: 18553.0540\n",
            "Epoch [130/150], Loss: 16383.1428\n",
            "Epoch [140/150], Loss: 17951.3137\n",
            "Epoch [150/150], Loss: 19319.2915\n",
            "Fold 5, RMSE: 57.25867462158203\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 74.61785430908203\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 16875.9124\n",
            "Epoch [20/100], Loss: 21673.0542\n",
            "Epoch [30/100], Loss: 16562.9290\n",
            "Epoch [40/100], Loss: 17274.0610\n",
            "Epoch [50/100], Loss: 17332.4624\n",
            "Epoch [60/100], Loss: 19287.9937\n",
            "Epoch [70/100], Loss: 18961.0605\n",
            "Epoch [80/100], Loss: 16321.2742\n",
            "Epoch [90/100], Loss: 21481.4160\n",
            "Epoch [100/100], Loss: 16359.0059\n",
            "Fold 1, RMSE: 67.43904113769531\n",
            "Epoch [10/100], Loss: 17198.2778\n",
            "Epoch [20/100], Loss: 13298.6724\n",
            "Epoch [30/100], Loss: 17860.6064\n",
            "Epoch [40/100], Loss: 9766.8870\n",
            "Epoch [50/100], Loss: 9043.7186\n",
            "Epoch [60/100], Loss: 13573.0540\n",
            "Epoch [70/100], Loss: 9313.3035\n",
            "Epoch [80/100], Loss: 11522.9725\n",
            "Epoch [90/100], Loss: 9561.7744\n",
            "Epoch [100/100], Loss: 11898.4939\n",
            "Fold 2, RMSE: 84.94288635253906\n",
            "Epoch [10/100], Loss: 11492.2032\n",
            "Epoch [20/100], Loss: 17702.3433\n",
            "Epoch [30/100], Loss: 12277.0530\n",
            "Epoch [40/100], Loss: 15609.0513\n",
            "Epoch [50/100], Loss: 12651.3518\n",
            "Epoch [60/100], Loss: 12771.3702\n",
            "Epoch [70/100], Loss: 12179.4709\n",
            "Epoch [80/100], Loss: 12606.6548\n",
            "Epoch [90/100], Loss: 13468.9873\n",
            "Epoch [100/100], Loss: 18880.5288\n",
            "Fold 3, RMSE: 109.62867736816406\n",
            "Epoch [10/100], Loss: 20975.4253\n",
            "Epoch [20/100], Loss: 25135.3374\n",
            "Epoch [30/100], Loss: 21270.7271\n",
            "Epoch [40/100], Loss: 22229.1248\n",
            "Epoch [50/100], Loss: 17495.8522\n",
            "Epoch [60/100], Loss: 24258.5310\n",
            "Epoch [70/100], Loss: 18372.8777\n",
            "Epoch [80/100], Loss: 18654.0732\n",
            "Epoch [90/100], Loss: 24449.0603\n",
            "Epoch [100/100], Loss: 27179.2749\n",
            "Fold 4, RMSE: 54.47486114501953\n",
            "Epoch [10/100], Loss: 26226.3716\n",
            "Epoch [20/100], Loss: 20895.8989\n",
            "Epoch [30/100], Loss: 17117.3330\n",
            "Epoch [40/100], Loss: 22552.7676\n",
            "Epoch [50/100], Loss: 18032.5027\n",
            "Epoch [60/100], Loss: 20172.6699\n",
            "Epoch [70/100], Loss: 24197.2690\n",
            "Epoch [80/100], Loss: 23767.8589\n",
            "Epoch [90/100], Loss: 24400.9136\n",
            "Epoch [100/100], Loss: 19998.5000\n",
            "Fold 5, RMSE: 57.86572265625\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 74.8702377319336\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 25478.1831\n",
            "Epoch [20/150], Loss: 19363.2500\n",
            "Epoch [30/150], Loss: 17060.4448\n",
            "Epoch [40/150], Loss: 17442.4700\n",
            "Epoch [50/150], Loss: 18034.5518\n",
            "Epoch [60/150], Loss: 17198.5774\n",
            "Epoch [70/150], Loss: 17476.3083\n",
            "Epoch [80/150], Loss: 16559.2716\n",
            "Epoch [90/150], Loss: 18171.7573\n",
            "Epoch [100/150], Loss: 17274.3875\n",
            "Epoch [110/150], Loss: 20276.7109\n",
            "Epoch [120/150], Loss: 18953.0557\n",
            "Epoch [130/150], Loss: 22014.5713\n",
            "Epoch [140/150], Loss: 23345.5361\n",
            "Epoch [150/150], Loss: 19754.2070\n",
            "Fold 1, RMSE: 67.19296264648438\n",
            "Epoch [10/150], Loss: 16101.3792\n",
            "Epoch [20/150], Loss: 21208.6189\n",
            "Epoch [30/150], Loss: 16475.1089\n",
            "Epoch [40/150], Loss: 15298.9658\n",
            "Epoch [50/150], Loss: 17957.4316\n",
            "Epoch [60/150], Loss: 16616.0293\n",
            "Epoch [70/150], Loss: 17183.1968\n",
            "Epoch [80/150], Loss: 18246.3164\n",
            "Epoch [90/150], Loss: 16615.3555\n",
            "Epoch [100/150], Loss: 17537.6055\n",
            "Epoch [110/150], Loss: 18543.1426\n",
            "Epoch [120/150], Loss: 16037.7534\n",
            "Epoch [130/150], Loss: 14701.1040\n",
            "Epoch [140/150], Loss: 14725.5303\n",
            "Epoch [150/150], Loss: 15000.4766\n",
            "Fold 2, RMSE: 86.97044372558594\n",
            "Epoch [10/150], Loss: 11382.2015\n",
            "Epoch [20/150], Loss: 13647.7505\n",
            "Epoch [30/150], Loss: 11552.7628\n",
            "Epoch [40/150], Loss: 14727.2212\n",
            "Epoch [50/150], Loss: 16631.4944\n",
            "Epoch [60/150], Loss: 11351.3656\n",
            "Epoch [70/150], Loss: 15212.5569\n",
            "Epoch [80/150], Loss: 13190.9146\n",
            "Epoch [90/150], Loss: 11793.5714\n",
            "Epoch [100/150], Loss: 12690.9553\n",
            "Epoch [110/150], Loss: 14824.0601\n",
            "Epoch [120/150], Loss: 12682.5986\n",
            "Epoch [130/150], Loss: 11724.5474\n",
            "Epoch [140/150], Loss: 14859.1772\n",
            "Epoch [150/150], Loss: 12181.9050\n",
            "Fold 3, RMSE: 109.42320251464844\n",
            "Epoch [10/150], Loss: 18654.7773\n",
            "Epoch [20/150], Loss: 21364.6782\n",
            "Epoch [30/150], Loss: 21385.5195\n",
            "Epoch [40/150], Loss: 18747.9456\n",
            "Epoch [50/150], Loss: 21889.0161\n",
            "Epoch [60/150], Loss: 19746.1055\n",
            "Epoch [70/150], Loss: 25318.6001\n",
            "Epoch [80/150], Loss: 19306.8813\n",
            "Epoch [90/150], Loss: 24527.5688\n",
            "Epoch [100/150], Loss: 18759.6028\n",
            "Epoch [110/150], Loss: 18003.9927\n",
            "Epoch [120/150], Loss: 17830.0647\n",
            "Epoch [130/150], Loss: 29134.1855\n",
            "Epoch [140/150], Loss: 19402.6401\n",
            "Epoch [150/150], Loss: 22761.0962\n",
            "Fold 4, RMSE: 54.24089813232422\n",
            "Epoch [10/150], Loss: 19096.4473\n",
            "Epoch [20/150], Loss: 17796.1936\n",
            "Epoch [30/150], Loss: 22568.4302\n",
            "Epoch [40/150], Loss: 17234.4856\n",
            "Epoch [50/150], Loss: 18949.0425\n",
            "Epoch [60/150], Loss: 19529.3696\n",
            "Epoch [70/150], Loss: 19658.0503\n",
            "Epoch [80/150], Loss: 19711.0249\n",
            "Epoch [90/150], Loss: 17317.5923\n",
            "Epoch [100/150], Loss: 21172.8970\n",
            "Epoch [110/150], Loss: 19633.0693\n",
            "Epoch [120/150], Loss: 23177.6733\n",
            "Epoch [130/150], Loss: 18957.7520\n",
            "Epoch [140/150], Loss: 19448.1494\n",
            "Epoch [150/150], Loss: 22937.3740\n",
            "Fold 5, RMSE: 57.797428131103516\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 75.1249870300293\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18570.3635\n",
            "Epoch [20/100], Loss: 18726.4246\n",
            "Epoch [30/100], Loss: 13276.8170\n",
            "Epoch [40/100], Loss: 13910.3281\n",
            "Epoch [50/100], Loss: 10521.8265\n",
            "Epoch [60/100], Loss: 10233.6294\n",
            "Epoch [70/100], Loss: 10942.0278\n",
            "Epoch [80/100], Loss: 11512.1323\n",
            "Epoch [90/100], Loss: 15085.6738\n",
            "Epoch [100/100], Loss: 7182.2790\n",
            "Fold 1, RMSE: 61.085636138916016\n",
            "Epoch [10/100], Loss: 21958.4429\n",
            "Epoch [20/100], Loss: 11808.8032\n",
            "Epoch [30/100], Loss: 15069.7874\n",
            "Epoch [40/100], Loss: 17331.6384\n",
            "Epoch [50/100], Loss: 15475.0508\n",
            "Epoch [60/100], Loss: 18402.4214\n",
            "Epoch [70/100], Loss: 15485.2427\n",
            "Epoch [80/100], Loss: 13811.9758\n",
            "Epoch [90/100], Loss: 14093.1124\n",
            "Epoch [100/100], Loss: 12795.2212\n",
            "Fold 2, RMSE: 85.54473114013672\n",
            "Epoch [10/100], Loss: 12920.6868\n",
            "Epoch [20/100], Loss: 11417.3423\n",
            "Epoch [30/100], Loss: 12643.1970\n",
            "Epoch [40/100], Loss: 11160.7393\n",
            "Epoch [50/100], Loss: 8882.6272\n",
            "Epoch [60/100], Loss: 11076.9141\n",
            "Epoch [70/100], Loss: 22052.7659\n",
            "Epoch [80/100], Loss: 15529.1475\n",
            "Epoch [90/100], Loss: 11363.6671\n",
            "Epoch [100/100], Loss: 15810.7295\n",
            "Fold 3, RMSE: 109.1419677734375\n",
            "Epoch [10/100], Loss: 26588.5603\n",
            "Epoch [20/100], Loss: 20094.1465\n",
            "Epoch [30/100], Loss: 20358.0596\n",
            "Epoch [40/100], Loss: 13398.6406\n",
            "Epoch [50/100], Loss: 8425.4663\n",
            "Epoch [60/100], Loss: 13543.7429\n",
            "Epoch [70/100], Loss: 14890.5532\n",
            "Epoch [80/100], Loss: 18437.6245\n",
            "Epoch [90/100], Loss: 12291.9250\n",
            "Epoch [100/100], Loss: 10344.7925\n",
            "Fold 4, RMSE: 46.153846740722656\n",
            "Epoch [10/100], Loss: 18629.9097\n",
            "Epoch [20/100], Loss: 17983.1240\n",
            "Epoch [30/100], Loss: 21808.7095\n",
            "Epoch [40/100], Loss: 19135.5610\n",
            "Epoch [50/100], Loss: 21057.0415\n",
            "Epoch [60/100], Loss: 18623.1313\n",
            "Epoch [70/100], Loss: 20347.1353\n",
            "Epoch [80/100], Loss: 18710.8779\n",
            "Epoch [90/100], Loss: 16314.2551\n",
            "Epoch [100/100], Loss: 17913.7290\n",
            "Fold 5, RMSE: 57.582035064697266\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 71.90164337158203\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 25417.4326\n",
            "Epoch [20/150], Loss: 16270.1631\n",
            "Epoch [30/150], Loss: 23825.2288\n",
            "Epoch [40/150], Loss: 26703.1824\n",
            "Epoch [50/150], Loss: 17423.8611\n",
            "Epoch [60/150], Loss: 18722.3760\n",
            "Epoch [70/150], Loss: 16515.4087\n",
            "Epoch [80/150], Loss: 17250.1064\n",
            "Epoch [90/150], Loss: 15162.3318\n",
            "Epoch [100/150], Loss: 16467.4498\n",
            "Epoch [110/150], Loss: 21719.4771\n",
            "Epoch [120/150], Loss: 20431.8569\n",
            "Epoch [130/150], Loss: 24389.4448\n",
            "Epoch [140/150], Loss: 17730.5708\n",
            "Epoch [150/150], Loss: 16427.3901\n",
            "Fold 1, RMSE: 66.6861801147461\n",
            "Epoch [10/150], Loss: 17976.5972\n",
            "Epoch [20/150], Loss: 13219.7228\n",
            "Epoch [30/150], Loss: 16626.2549\n",
            "Epoch [40/150], Loss: 15369.7546\n",
            "Epoch [50/150], Loss: 16302.0278\n",
            "Epoch [60/150], Loss: 17536.5708\n",
            "Epoch [70/150], Loss: 24418.5840\n",
            "Epoch [80/150], Loss: 26477.7217\n",
            "Epoch [90/150], Loss: 15574.7158\n",
            "Epoch [100/150], Loss: 24200.8120\n",
            "Epoch [110/150], Loss: 17469.4688\n",
            "Epoch [120/150], Loss: 16769.0823\n",
            "Epoch [130/150], Loss: 14701.3364\n",
            "Epoch [140/150], Loss: 17492.1768\n",
            "Epoch [150/150], Loss: 18716.6577\n",
            "Fold 2, RMSE: 86.14546203613281\n",
            "Epoch [10/150], Loss: 16430.9678\n",
            "Epoch [20/150], Loss: 12383.5613\n",
            "Epoch [30/150], Loss: 13521.7493\n",
            "Epoch [40/150], Loss: 14028.6497\n",
            "Epoch [50/150], Loss: 11599.1876\n",
            "Epoch [60/150], Loss: 12124.5303\n",
            "Epoch [70/150], Loss: 18174.4553\n",
            "Epoch [80/150], Loss: 11849.2471\n",
            "Epoch [90/150], Loss: 11269.9204\n",
            "Epoch [100/150], Loss: 12769.3032\n",
            "Epoch [110/150], Loss: 15055.1882\n",
            "Epoch [120/150], Loss: 14235.4326\n",
            "Epoch [130/150], Loss: 12194.1523\n",
            "Epoch [140/150], Loss: 12280.2582\n",
            "Epoch [150/150], Loss: 14643.4875\n",
            "Fold 3, RMSE: 108.4188232421875\n",
            "Epoch [10/150], Loss: 23013.1621\n",
            "Epoch [20/150], Loss: 20487.3774\n",
            "Epoch [30/150], Loss: 17790.8030\n",
            "Epoch [40/150], Loss: 20298.5146\n",
            "Epoch [50/150], Loss: 19360.9966\n",
            "Epoch [60/150], Loss: 18899.8652\n",
            "Epoch [70/150], Loss: 16569.3550\n",
            "Epoch [80/150], Loss: 17893.4404\n",
            "Epoch [90/150], Loss: 19187.8745\n",
            "Epoch [100/150], Loss: 17136.5355\n",
            "Epoch [110/150], Loss: 22956.8853\n",
            "Epoch [120/150], Loss: 25484.1436\n",
            "Epoch [130/150], Loss: 17480.6128\n",
            "Epoch [140/150], Loss: 21262.5015\n",
            "Epoch [150/150], Loss: 18529.0117\n",
            "Fold 4, RMSE: 52.94029998779297\n",
            "Epoch [10/150], Loss: 20456.7827\n",
            "Epoch [20/150], Loss: 22079.0796\n",
            "Epoch [30/150], Loss: 19622.8423\n",
            "Epoch [40/150], Loss: 19450.9214\n",
            "Epoch [50/150], Loss: 25741.5596\n",
            "Epoch [60/150], Loss: 26577.0146\n",
            "Epoch [70/150], Loss: 25814.9419\n",
            "Epoch [80/150], Loss: 24842.9177\n",
            "Epoch [90/150], Loss: 18805.2068\n",
            "Epoch [100/150], Loss: 16997.5819\n",
            "Epoch [110/150], Loss: 19125.4946\n",
            "Epoch [120/150], Loss: 24773.5430\n",
            "Epoch [130/150], Loss: 20026.4370\n",
            "Epoch [140/150], Loss: 18457.6606\n",
            "Epoch [150/150], Loss: 17100.6926\n",
            "Fold 5, RMSE: 54.95960998535156\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 73.83007507324218\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 42519.4985\n",
            "Epoch [20/100], Loss: 15635.5656\n",
            "Epoch [30/100], Loss: 18110.2747\n",
            "Epoch [40/100], Loss: 18210.3882\n",
            "Epoch [50/100], Loss: 20877.2803\n",
            "Epoch [60/100], Loss: 18937.1030\n",
            "Epoch [70/100], Loss: 26705.6357\n",
            "Epoch [80/100], Loss: 17187.9534\n",
            "Epoch [90/100], Loss: 17549.2109\n",
            "Epoch [100/100], Loss: 19139.2778\n",
            "Fold 1, RMSE: 67.32669067382812\n",
            "Epoch [10/100], Loss: 22381.7852\n",
            "Epoch [20/100], Loss: 16014.0188\n",
            "Epoch [30/100], Loss: 16337.9131\n",
            "Epoch [40/100], Loss: 15830.5747\n",
            "Epoch [50/100], Loss: 16567.0986\n",
            "Epoch [60/100], Loss: 15182.1021\n",
            "Epoch [70/100], Loss: 15858.1409\n",
            "Epoch [80/100], Loss: 19542.2109\n",
            "Epoch [90/100], Loss: 14866.0801\n",
            "Epoch [100/100], Loss: 17177.8213\n",
            "Fold 2, RMSE: 87.05387878417969\n",
            "Epoch [10/100], Loss: 21475.3872\n",
            "Epoch [20/100], Loss: 13103.2363\n",
            "Epoch [30/100], Loss: 14464.8250\n",
            "Epoch [40/100], Loss: 11964.5991\n",
            "Epoch [50/100], Loss: 13784.7349\n",
            "Epoch [60/100], Loss: 19034.2986\n",
            "Epoch [70/100], Loss: 13945.5132\n",
            "Epoch [80/100], Loss: 13356.8594\n",
            "Epoch [90/100], Loss: 13478.4478\n",
            "Epoch [100/100], Loss: 11258.0802\n",
            "Fold 3, RMSE: 109.49830627441406\n",
            "Epoch [10/100], Loss: 40635.4097\n",
            "Epoch [20/100], Loss: 20487.4189\n",
            "Epoch [30/100], Loss: 17660.4548\n",
            "Epoch [40/100], Loss: 24902.6211\n",
            "Epoch [50/100], Loss: 25899.9570\n",
            "Epoch [60/100], Loss: 18631.4641\n",
            "Epoch [70/100], Loss: 20651.6123\n",
            "Epoch [80/100], Loss: 21822.5679\n",
            "Epoch [90/100], Loss: 18179.6682\n",
            "Epoch [100/100], Loss: 18607.2056\n",
            "Fold 4, RMSE: 55.395057678222656\n",
            "Epoch [10/100], Loss: 19820.2983\n",
            "Epoch [20/100], Loss: 19055.3940\n",
            "Epoch [30/100], Loss: 21156.5698\n",
            "Epoch [40/100], Loss: 20487.3940\n",
            "Epoch [50/100], Loss: 17530.6223\n",
            "Epoch [60/100], Loss: 20343.2334\n",
            "Epoch [70/100], Loss: 24850.7302\n",
            "Epoch [80/100], Loss: 22168.1733\n",
            "Epoch [90/100], Loss: 18469.3008\n",
            "Epoch [100/100], Loss: 24484.7852\n",
            "Fold 5, RMSE: 57.89651107788086\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 75.43408889770508\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 17981.0779\n",
            "Epoch [20/150], Loss: 18568.2134\n",
            "Epoch [30/150], Loss: 17892.9136\n",
            "Epoch [40/150], Loss: 18736.2830\n",
            "Epoch [50/150], Loss: 17140.8945\n",
            "Epoch [60/150], Loss: 18117.2207\n",
            "Epoch [70/150], Loss: 19688.8713\n",
            "Epoch [80/150], Loss: 22292.3857\n",
            "Epoch [90/150], Loss: 16630.3125\n",
            "Epoch [100/150], Loss: 19239.0356\n",
            "Epoch [110/150], Loss: 28591.5779\n",
            "Epoch [120/150], Loss: 16002.8000\n",
            "Epoch [130/150], Loss: 17347.7134\n",
            "Epoch [140/150], Loss: 18787.7634\n",
            "Epoch [150/150], Loss: 16494.0652\n",
            "Fold 1, RMSE: 61.18599319458008\n",
            "Epoch [10/150], Loss: 15525.6951\n",
            "Epoch [20/150], Loss: 18833.0652\n",
            "Epoch [30/150], Loss: 15093.5393\n",
            "Epoch [40/150], Loss: 16677.4209\n",
            "Epoch [50/150], Loss: 17665.6558\n",
            "Epoch [60/150], Loss: 15236.5696\n",
            "Epoch [70/150], Loss: 13887.8101\n",
            "Epoch [80/150], Loss: 17058.3003\n",
            "Epoch [90/150], Loss: 15124.3315\n",
            "Epoch [100/150], Loss: 19576.3521\n",
            "Epoch [110/150], Loss: 14420.0366\n",
            "Epoch [120/150], Loss: 16767.6094\n",
            "Epoch [130/150], Loss: 23390.6140\n",
            "Epoch [140/150], Loss: 22225.1786\n",
            "Epoch [150/150], Loss: 14691.0085\n",
            "Fold 2, RMSE: 87.00392150878906\n",
            "Epoch [10/150], Loss: 343801.8828\n",
            "Epoch [20/150], Loss: 57561.6895\n",
            "Epoch [30/150], Loss: 30627.6108\n",
            "Epoch [40/150], Loss: 20336.0420\n",
            "Epoch [50/150], Loss: 14706.9270\n",
            "Epoch [60/150], Loss: 13920.5288\n",
            "Epoch [70/150], Loss: 11894.8572\n",
            "Epoch [80/150], Loss: 11722.3625\n",
            "Epoch [90/150], Loss: 17234.7749\n",
            "Epoch [100/150], Loss: 10291.6077\n",
            "Epoch [110/150], Loss: 10521.2840\n",
            "Epoch [120/150], Loss: 10503.5173\n",
            "Epoch [130/150], Loss: 11043.9919\n",
            "Epoch [140/150], Loss: 8924.6581\n",
            "Epoch [150/150], Loss: 11274.6663\n",
            "Fold 3, RMSE: 97.97765350341797\n",
            "Epoch [10/150], Loss: 18445.2642\n",
            "Epoch [20/150], Loss: 16952.4119\n",
            "Epoch [30/150], Loss: 19418.6792\n",
            "Epoch [40/150], Loss: 19359.6265\n",
            "Epoch [50/150], Loss: 20789.9229\n",
            "Epoch [60/150], Loss: 18394.4199\n",
            "Epoch [70/150], Loss: 24806.5518\n",
            "Epoch [80/150], Loss: 25409.2812\n",
            "Epoch [90/150], Loss: 21380.3862\n",
            "Epoch [100/150], Loss: 19622.7964\n",
            "Epoch [110/150], Loss: 17384.0157\n",
            "Epoch [120/150], Loss: 20926.9502\n",
            "Epoch [130/150], Loss: 19029.1838\n",
            "Epoch [140/150], Loss: 17816.1047\n",
            "Epoch [150/150], Loss: 30507.4729\n",
            "Fold 4, RMSE: 54.01312255859375\n",
            "Epoch [10/150], Loss: 19744.1318\n",
            "Epoch [20/150], Loss: 20257.0654\n",
            "Epoch [30/150], Loss: 17166.1298\n",
            "Epoch [40/150], Loss: 19208.3589\n",
            "Epoch [50/150], Loss: 18524.2839\n",
            "Epoch [60/150], Loss: 26607.2930\n",
            "Epoch [70/150], Loss: 20723.3647\n",
            "Epoch [80/150], Loss: 21639.3889\n",
            "Epoch [90/150], Loss: 19095.8232\n",
            "Epoch [100/150], Loss: 19802.3022\n",
            "Epoch [110/150], Loss: 18668.9275\n",
            "Epoch [120/150], Loss: 17628.0820\n",
            "Epoch [130/150], Loss: 18576.6819\n",
            "Epoch [140/150], Loss: 25674.3066\n",
            "Epoch [150/150], Loss: 25762.3740\n",
            "Fold 5, RMSE: 57.87499237060547\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 71.61113662719727\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18949.7812\n",
            "Epoch [20/100], Loss: 18180.4336\n",
            "Epoch [30/100], Loss: 17361.6509\n",
            "Epoch [40/100], Loss: 19357.5493\n",
            "Epoch [50/100], Loss: 19069.1357\n",
            "Epoch [60/100], Loss: 19349.1052\n",
            "Epoch [70/100], Loss: 21247.1562\n",
            "Epoch [80/100], Loss: 17302.9209\n",
            "Epoch [90/100], Loss: 23835.2910\n",
            "Epoch [100/100], Loss: 22075.7559\n",
            "Fold 1, RMSE: 67.25798797607422\n",
            "Epoch [10/100], Loss: 205997.8242\n",
            "Epoch [20/100], Loss: 35532.7642\n",
            "Epoch [30/100], Loss: 24838.8716\n",
            "Epoch [40/100], Loss: 17798.4736\n",
            "Epoch [50/100], Loss: 19017.1726\n",
            "Epoch [60/100], Loss: 14767.6541\n",
            "Epoch [70/100], Loss: 12533.9926\n",
            "Epoch [80/100], Loss: 17018.4966\n",
            "Epoch [90/100], Loss: 14386.1138\n",
            "Epoch [100/100], Loss: 14763.2273\n",
            "Fold 2, RMSE: 82.24658203125\n",
            "Epoch [10/100], Loss: 12579.7126\n",
            "Epoch [20/100], Loss: 16713.8652\n",
            "Epoch [30/100], Loss: 14165.4402\n",
            "Epoch [40/100], Loss: 16420.8394\n",
            "Epoch [50/100], Loss: 11733.0564\n",
            "Epoch [60/100], Loss: 12185.5552\n",
            "Epoch [70/100], Loss: 11696.1923\n",
            "Epoch [80/100], Loss: 15652.1025\n",
            "Epoch [90/100], Loss: 11668.5017\n",
            "Epoch [100/100], Loss: 13274.1143\n",
            "Fold 3, RMSE: 107.25065612792969\n",
            "Epoch [10/100], Loss: 23385.0806\n",
            "Epoch [20/100], Loss: 18384.4224\n",
            "Epoch [30/100], Loss: 19492.3286\n",
            "Epoch [40/100], Loss: 20235.4575\n",
            "Epoch [50/100], Loss: 17433.5928\n",
            "Epoch [60/100], Loss: 17640.6484\n",
            "Epoch [70/100], Loss: 17681.0498\n",
            "Epoch [80/100], Loss: 16989.1343\n",
            "Epoch [90/100], Loss: 17838.8621\n",
            "Epoch [100/100], Loss: 16985.1873\n",
            "Fold 4, RMSE: 50.48248291015625\n",
            "Epoch [10/100], Loss: 18469.9783\n",
            "Epoch [20/100], Loss: 25766.4814\n",
            "Epoch [30/100], Loss: 22835.2153\n",
            "Epoch [40/100], Loss: 17276.5492\n",
            "Epoch [50/100], Loss: 20047.9429\n",
            "Epoch [60/100], Loss: 20100.8042\n",
            "Epoch [70/100], Loss: 18764.7085\n",
            "Epoch [80/100], Loss: 21702.6143\n",
            "Epoch [90/100], Loss: 25519.3408\n",
            "Epoch [100/100], Loss: 18508.4915\n",
            "Fold 5, RMSE: 57.57377624511719\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 72.96229705810546\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 29038.5859\n",
            "Epoch [20/150], Loss: 24583.6978\n",
            "Epoch [30/150], Loss: 17226.7803\n",
            "Epoch [40/150], Loss: 17670.8442\n",
            "Epoch [50/150], Loss: 15998.3286\n",
            "Epoch [60/150], Loss: 17248.5168\n",
            "Epoch [70/150], Loss: 19677.8306\n",
            "Epoch [80/150], Loss: 20644.3936\n",
            "Epoch [90/150], Loss: 15807.6599\n",
            "Epoch [100/150], Loss: 18817.5898\n",
            "Epoch [110/150], Loss: 18644.6191\n",
            "Epoch [120/150], Loss: 23860.2026\n",
            "Epoch [130/150], Loss: 16899.6584\n",
            "Epoch [140/150], Loss: 15323.8785\n",
            "Epoch [150/150], Loss: 24519.8123\n",
            "Fold 1, RMSE: 64.52069854736328\n",
            "Epoch [10/150], Loss: 14011.3752\n",
            "Epoch [20/150], Loss: 15175.2124\n",
            "Epoch [30/150], Loss: 19846.2427\n",
            "Epoch [40/150], Loss: 15602.3652\n",
            "Epoch [50/150], Loss: 15483.2129\n",
            "Epoch [60/150], Loss: 15009.4919\n",
            "Epoch [70/150], Loss: 24722.9072\n",
            "Epoch [80/150], Loss: 17217.6733\n",
            "Epoch [90/150], Loss: 15932.1768\n",
            "Epoch [100/150], Loss: 14219.0425\n",
            "Epoch [110/150], Loss: 15133.1001\n",
            "Epoch [120/150], Loss: 17037.1339\n",
            "Epoch [130/150], Loss: 20199.3193\n",
            "Epoch [140/150], Loss: 14150.0366\n",
            "Epoch [150/150], Loss: 11466.2819\n",
            "Fold 2, RMSE: 76.58419799804688\n",
            "Epoch [10/150], Loss: 35326.6511\n",
            "Epoch [20/150], Loss: 12851.6597\n",
            "Epoch [30/150], Loss: 12933.2122\n",
            "Epoch [40/150], Loss: 15789.2666\n",
            "Epoch [50/150], Loss: 11894.7810\n",
            "Epoch [60/150], Loss: 11898.3551\n",
            "Epoch [70/150], Loss: 18486.7026\n",
            "Epoch [80/150], Loss: 11793.2446\n",
            "Epoch [90/150], Loss: 13733.2863\n",
            "Epoch [100/150], Loss: 12987.2939\n",
            "Epoch [110/150], Loss: 11530.3770\n",
            "Epoch [120/150], Loss: 12296.2910\n",
            "Epoch [130/150], Loss: 11829.1565\n",
            "Epoch [140/150], Loss: 21820.2017\n",
            "Epoch [150/150], Loss: 10914.4186\n",
            "Fold 3, RMSE: 108.32048034667969\n",
            "Epoch [10/150], Loss: 17377.5280\n",
            "Epoch [20/150], Loss: 19780.5679\n",
            "Epoch [30/150], Loss: 22326.9956\n",
            "Epoch [40/150], Loss: 17000.9840\n",
            "Epoch [50/150], Loss: 29991.5601\n",
            "Epoch [60/150], Loss: 20040.9277\n",
            "Epoch [70/150], Loss: 20520.2744\n",
            "Epoch [80/150], Loss: 18947.7241\n",
            "Epoch [90/150], Loss: 21885.9138\n",
            "Epoch [100/150], Loss: 18586.5271\n",
            "Epoch [110/150], Loss: 19092.1030\n",
            "Epoch [120/150], Loss: 14950.9835\n",
            "Epoch [130/150], Loss: 23933.5513\n",
            "Epoch [140/150], Loss: 17439.5901\n",
            "Epoch [150/150], Loss: 20898.2542\n",
            "Fold 4, RMSE: 42.89863204956055\n",
            "Epoch [10/150], Loss: 20485.9170\n",
            "Epoch [20/150], Loss: 21410.1138\n",
            "Epoch [30/150], Loss: 17696.4541\n",
            "Epoch [40/150], Loss: 25386.0781\n",
            "Epoch [50/150], Loss: 25195.2424\n",
            "Epoch [60/150], Loss: 26862.4185\n",
            "Epoch [70/150], Loss: 18623.7939\n",
            "Epoch [80/150], Loss: 15571.2769\n",
            "Epoch [90/150], Loss: 17001.1909\n",
            "Epoch [100/150], Loss: 18541.5454\n",
            "Epoch [110/150], Loss: 20460.2236\n",
            "Epoch [120/150], Loss: 19429.0288\n",
            "Epoch [130/150], Loss: 19513.8521\n",
            "Epoch [140/150], Loss: 23562.3584\n",
            "Epoch [150/150], Loss: 15360.8077\n",
            "Fold 5, RMSE: 53.848968505859375\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 69.23459548950196\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 122628.6777\n",
            "Epoch [20/100], Loss: 35958.7100\n",
            "Epoch [30/100], Loss: 18967.7378\n",
            "Epoch [40/100], Loss: 17937.8359\n",
            "Epoch [50/100], Loss: 16810.7749\n",
            "Epoch [60/100], Loss: 14481.2544\n",
            "Epoch [70/100], Loss: 17902.1675\n",
            "Epoch [80/100], Loss: 20138.6299\n",
            "Epoch [90/100], Loss: 16570.8015\n",
            "Epoch [100/100], Loss: 16897.5857\n",
            "Fold 1, RMSE: 54.95830154418945\n",
            "Epoch [10/100], Loss: 16558.2559\n",
            "Epoch [20/100], Loss: 17744.1924\n",
            "Epoch [30/100], Loss: 15858.1853\n",
            "Epoch [40/100], Loss: 13984.0950\n",
            "Epoch [50/100], Loss: 17491.6497\n",
            "Epoch [60/100], Loss: 22551.3652\n",
            "Epoch [70/100], Loss: 23375.6313\n",
            "Epoch [80/100], Loss: 22142.4761\n",
            "Epoch [90/100], Loss: 22718.6074\n",
            "Epoch [100/100], Loss: 14417.1951\n",
            "Fold 2, RMSE: 86.38932037353516\n",
            "Epoch [10/100], Loss: 14145.4182\n",
            "Epoch [20/100], Loss: 20254.5774\n",
            "Epoch [30/100], Loss: 11273.4911\n",
            "Epoch [40/100], Loss: 16755.9954\n",
            "Epoch [50/100], Loss: 17741.6477\n",
            "Epoch [60/100], Loss: 15400.4297\n",
            "Epoch [70/100], Loss: 17270.1523\n",
            "Epoch [80/100], Loss: 12606.6917\n",
            "Epoch [90/100], Loss: 16788.3511\n",
            "Epoch [100/100], Loss: 18197.1250\n",
            "Fold 3, RMSE: 109.72967529296875\n",
            "Epoch [10/100], Loss: 20205.7925\n",
            "Epoch [20/100], Loss: 19128.5791\n",
            "Epoch [30/100], Loss: 21314.7427\n",
            "Epoch [40/100], Loss: 18983.6191\n",
            "Epoch [50/100], Loss: 23265.6313\n",
            "Epoch [60/100], Loss: 23645.7974\n",
            "Epoch [70/100], Loss: 22841.6875\n",
            "Epoch [80/100], Loss: 17954.4170\n",
            "Epoch [90/100], Loss: 17891.0784\n",
            "Epoch [100/100], Loss: 19159.1958\n",
            "Fold 4, RMSE: 54.54894256591797\n",
            "Epoch [10/100], Loss: 24014.2495\n",
            "Epoch [20/100], Loss: 29181.7937\n",
            "Epoch [30/100], Loss: 20200.7427\n",
            "Epoch [40/100], Loss: 20720.4272\n",
            "Epoch [50/100], Loss: 20835.0229\n",
            "Epoch [60/100], Loss: 26254.6436\n",
            "Epoch [70/100], Loss: 17540.7397\n",
            "Epoch [80/100], Loss: 20137.9839\n",
            "Epoch [90/100], Loss: 20868.1807\n",
            "Epoch [100/100], Loss: 25347.0381\n",
            "Fold 5, RMSE: 57.817054748535156\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 72.6886589050293\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 23780.3228\n",
            "Epoch [20/150], Loss: 20803.4941\n",
            "Epoch [30/150], Loss: 17271.8521\n",
            "Epoch [40/150], Loss: 24599.8071\n",
            "Epoch [50/150], Loss: 15287.9420\n",
            "Epoch [60/150], Loss: 12066.4606\n",
            "Epoch [70/150], Loss: 15476.1165\n",
            "Epoch [80/150], Loss: 18780.9304\n",
            "Epoch [90/150], Loss: 15147.6501\n",
            "Epoch [100/150], Loss: 15191.2866\n",
            "Epoch [110/150], Loss: 13841.2749\n",
            "Epoch [120/150], Loss: 10959.9478\n",
            "Epoch [130/150], Loss: 14908.5078\n",
            "Epoch [140/150], Loss: 15383.5627\n",
            "Epoch [150/150], Loss: 17358.3369\n",
            "Fold 1, RMSE: 65.02240753173828\n",
            "Epoch [10/150], Loss: 14709.3832\n",
            "Epoch [20/150], Loss: 15179.7327\n",
            "Epoch [30/150], Loss: 16621.8772\n",
            "Epoch [40/150], Loss: 16491.1946\n",
            "Epoch [50/150], Loss: 16680.6025\n",
            "Epoch [60/150], Loss: 16212.3743\n",
            "Epoch [70/150], Loss: 14262.7485\n",
            "Epoch [80/150], Loss: 16825.5381\n",
            "Epoch [90/150], Loss: 16526.4321\n",
            "Epoch [100/150], Loss: 15710.2100\n",
            "Epoch [110/150], Loss: 28588.8308\n",
            "Epoch [120/150], Loss: 16280.1670\n",
            "Epoch [130/150], Loss: 17850.5352\n",
            "Epoch [140/150], Loss: 20119.9697\n",
            "Epoch [150/150], Loss: 15774.2417\n",
            "Fold 2, RMSE: 87.103515625\n",
            "Epoch [10/150], Loss: 17026.0337\n",
            "Epoch [20/150], Loss: 16412.6997\n",
            "Epoch [30/150], Loss: 11659.7598\n",
            "Epoch [40/150], Loss: 16777.2729\n",
            "Epoch [50/150], Loss: 16162.3523\n",
            "Epoch [60/150], Loss: 12055.7507\n",
            "Epoch [70/150], Loss: 12577.8384\n",
            "Epoch [80/150], Loss: 12537.2861\n",
            "Epoch [90/150], Loss: 11816.3556\n",
            "Epoch [100/150], Loss: 15805.9873\n",
            "Epoch [110/150], Loss: 12453.8447\n",
            "Epoch [120/150], Loss: 13266.9016\n",
            "Epoch [130/150], Loss: 14233.4819\n",
            "Epoch [140/150], Loss: 15052.5669\n",
            "Epoch [150/150], Loss: 11776.0247\n",
            "Fold 3, RMSE: 109.35921478271484\n",
            "Epoch [10/150], Loss: 104941.8320\n",
            "Epoch [20/150], Loss: 26107.6313\n",
            "Epoch [30/150], Loss: 21311.2180\n",
            "Epoch [40/150], Loss: 27656.9746\n",
            "Epoch [50/150], Loss: 25999.3730\n",
            "Epoch [60/150], Loss: 17747.0068\n",
            "Epoch [70/150], Loss: 18860.8228\n",
            "Epoch [80/150], Loss: 26129.8828\n",
            "Epoch [90/150], Loss: 19465.4550\n",
            "Epoch [100/150], Loss: 18470.4600\n",
            "Epoch [110/150], Loss: 25115.7207\n",
            "Epoch [120/150], Loss: 17262.8860\n",
            "Epoch [130/150], Loss: 14536.7260\n",
            "Epoch [140/150], Loss: 26950.2007\n",
            "Epoch [150/150], Loss: 16045.1904\n",
            "Fold 4, RMSE: 39.94722366333008\n",
            "Epoch [10/150], Loss: 24936.3926\n",
            "Epoch [20/150], Loss: 19665.4395\n",
            "Epoch [30/150], Loss: 24900.2773\n",
            "Epoch [40/150], Loss: 18638.8206\n",
            "Epoch [50/150], Loss: 26581.3242\n",
            "Epoch [60/150], Loss: 19060.9521\n",
            "Epoch [70/150], Loss: 22122.3975\n",
            "Epoch [80/150], Loss: 27314.5718\n",
            "Epoch [90/150], Loss: 18527.8950\n",
            "Epoch [100/150], Loss: 21596.5796\n",
            "Epoch [110/150], Loss: 27886.5850\n",
            "Epoch [120/150], Loss: 18672.0996\n",
            "Epoch [130/150], Loss: 19457.4663\n",
            "Epoch [140/150], Loss: 19241.6523\n",
            "Epoch [150/150], Loss: 20664.9307\n",
            "Fold 5, RMSE: 57.77629089355469\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 71.84173049926758\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 20468.7830\n",
            "Epoch [20/100], Loss: 17032.2114\n",
            "Epoch [30/100], Loss: 17679.4204\n",
            "Epoch [40/100], Loss: 17336.8296\n",
            "Epoch [50/100], Loss: 20291.2314\n",
            "Epoch [60/100], Loss: 25504.5537\n",
            "Epoch [70/100], Loss: 17772.0786\n",
            "Epoch [80/100], Loss: 18594.1333\n",
            "Epoch [90/100], Loss: 18694.5850\n",
            "Epoch [100/100], Loss: 17339.6187\n",
            "Fold 1, RMSE: 64.08854675292969\n",
            "Epoch [10/100], Loss: 15408.6296\n",
            "Epoch [20/100], Loss: 24749.2144\n",
            "Epoch [30/100], Loss: 14751.7958\n",
            "Epoch [40/100], Loss: 15385.3472\n",
            "Epoch [50/100], Loss: 24185.6543\n",
            "Epoch [60/100], Loss: 16590.9814\n",
            "Epoch [70/100], Loss: 14340.7136\n",
            "Epoch [80/100], Loss: 14407.3899\n",
            "Epoch [90/100], Loss: 19314.0708\n",
            "Epoch [100/100], Loss: 20399.8542\n",
            "Fold 2, RMSE: 86.66556549072266\n",
            "Epoch [10/100], Loss: 131555.0620\n",
            "Epoch [20/100], Loss: 18470.9941\n",
            "Epoch [30/100], Loss: 13555.3774\n",
            "Epoch [40/100], Loss: 24482.2981\n",
            "Epoch [50/100], Loss: 13391.8474\n",
            "Epoch [60/100], Loss: 10692.9990\n",
            "Epoch [70/100], Loss: 11082.9646\n",
            "Epoch [80/100], Loss: 12040.1729\n",
            "Epoch [90/100], Loss: 8082.6238\n",
            "Epoch [100/100], Loss: 12052.8413\n",
            "Fold 3, RMSE: 102.27532958984375\n",
            "Epoch [10/100], Loss: 101813.8047\n",
            "Epoch [20/100], Loss: 19779.3013\n",
            "Epoch [30/100], Loss: 18215.2803\n",
            "Epoch [40/100], Loss: 14985.9019\n",
            "Epoch [50/100], Loss: 15176.9758\n",
            "Epoch [60/100], Loss: 19256.5176\n",
            "Epoch [70/100], Loss: 18313.0312\n",
            "Epoch [80/100], Loss: 19578.2354\n",
            "Epoch [90/100], Loss: 16789.4053\n",
            "Epoch [100/100], Loss: 15560.5566\n",
            "Fold 4, RMSE: 48.01006317138672\n",
            "Epoch [10/100], Loss: 48088.9141\n",
            "Epoch [20/100], Loss: 19738.3088\n",
            "Epoch [30/100], Loss: 15354.9873\n",
            "Epoch [40/100], Loss: 15880.8943\n",
            "Epoch [50/100], Loss: 19635.2305\n",
            "Epoch [60/100], Loss: 13426.8970\n",
            "Epoch [70/100], Loss: 16982.1523\n",
            "Epoch [80/100], Loss: 17134.2065\n",
            "Epoch [90/100], Loss: 11144.9524\n",
            "Epoch [100/100], Loss: 19822.1675\n",
            "Fold 5, RMSE: 51.34757614135742\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 70.47741622924805\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 17505.8501\n",
            "Epoch [20/150], Loss: 25652.6064\n",
            "Epoch [30/150], Loss: 26417.7749\n",
            "Epoch [40/150], Loss: 19485.1824\n",
            "Epoch [50/150], Loss: 21204.2959\n",
            "Epoch [60/150], Loss: 19007.2363\n",
            "Epoch [70/150], Loss: 19487.7124\n",
            "Epoch [80/150], Loss: 18436.9917\n",
            "Epoch [90/150], Loss: 15192.2698\n",
            "Epoch [100/150], Loss: 20737.3784\n",
            "Epoch [110/150], Loss: 20896.8247\n",
            "Epoch [120/150], Loss: 19382.9165\n",
            "Epoch [130/150], Loss: 18222.0586\n",
            "Epoch [140/150], Loss: 17544.6931\n",
            "Epoch [150/150], Loss: 25494.7129\n",
            "Fold 1, RMSE: 64.68492889404297\n",
            "Epoch [10/150], Loss: 27522.3958\n",
            "Epoch [20/150], Loss: 24565.0718\n",
            "Epoch [30/150], Loss: 17903.6719\n",
            "Epoch [40/150], Loss: 20182.2593\n",
            "Epoch [50/150], Loss: 17993.0967\n",
            "Epoch [60/150], Loss: 14255.1293\n",
            "Epoch [70/150], Loss: 13926.1115\n",
            "Epoch [80/150], Loss: 22963.0576\n",
            "Epoch [90/150], Loss: 21931.4187\n",
            "Epoch [100/150], Loss: 13986.4380\n",
            "Epoch [110/150], Loss: 19109.8259\n",
            "Epoch [120/150], Loss: 15656.9602\n",
            "Epoch [130/150], Loss: 14671.8787\n",
            "Epoch [140/150], Loss: 17645.7031\n",
            "Epoch [150/150], Loss: 16505.8522\n",
            "Fold 2, RMSE: 84.1479721069336\n",
            "Epoch [10/150], Loss: 39337.8281\n",
            "Epoch [20/150], Loss: 13973.9446\n",
            "Epoch [30/150], Loss: 12757.3120\n",
            "Epoch [40/150], Loss: 16852.4907\n",
            "Epoch [50/150], Loss: 10564.4983\n",
            "Epoch [60/150], Loss: 9603.3754\n",
            "Epoch [70/150], Loss: 9824.5532\n",
            "Epoch [80/150], Loss: 8898.5340\n",
            "Epoch [90/150], Loss: 8832.7642\n",
            "Epoch [100/150], Loss: 9385.1930\n",
            "Epoch [110/150], Loss: 8575.6658\n",
            "Epoch [120/150], Loss: 7406.7058\n",
            "Epoch [130/150], Loss: 9593.8827\n",
            "Epoch [140/150], Loss: 8736.7965\n",
            "Epoch [150/150], Loss: 10012.8717\n",
            "Fold 3, RMSE: 98.62335968017578\n",
            "Epoch [10/150], Loss: 21282.5420\n",
            "Epoch [20/150], Loss: 19003.9136\n",
            "Epoch [30/150], Loss: 18757.6443\n",
            "Epoch [40/150], Loss: 18164.3662\n",
            "Epoch [50/150], Loss: 17478.4038\n",
            "Epoch [60/150], Loss: 21432.4182\n",
            "Epoch [70/150], Loss: 26247.3379\n",
            "Epoch [80/150], Loss: 17573.8308\n",
            "Epoch [90/150], Loss: 19341.8569\n",
            "Epoch [100/150], Loss: 18006.2461\n",
            "Epoch [110/150], Loss: 17725.6365\n",
            "Epoch [120/150], Loss: 19152.8408\n",
            "Epoch [130/150], Loss: 16597.9302\n",
            "Epoch [140/150], Loss: 19462.1201\n",
            "Epoch [150/150], Loss: 16150.9680\n",
            "Fold 4, RMSE: 45.42451477050781\n",
            "Epoch [10/150], Loss: 171583.2070\n",
            "Epoch [20/150], Loss: 25811.8086\n",
            "Epoch [30/150], Loss: 23107.2026\n",
            "Epoch [40/150], Loss: 17973.3081\n",
            "Epoch [50/150], Loss: 15154.0347\n",
            "Epoch [60/150], Loss: 17753.7546\n",
            "Epoch [70/150], Loss: 22929.2549\n",
            "Epoch [80/150], Loss: 13686.7910\n",
            "Epoch [90/150], Loss: 14318.3490\n",
            "Epoch [100/150], Loss: 13652.8435\n",
            "Epoch [110/150], Loss: 13935.5085\n",
            "Epoch [120/150], Loss: 13331.6853\n",
            "Epoch [130/150], Loss: 17030.4197\n",
            "Epoch [140/150], Loss: 18785.5188\n",
            "Epoch [150/150], Loss: 14170.4775\n",
            "Fold 5, RMSE: 55.491703033447266\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 69.67449569702148\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 146235.1797\n",
            "Epoch [20/100], Loss: 26974.4146\n",
            "Epoch [30/100], Loss: 18464.5039\n",
            "Epoch [40/100], Loss: 15948.8840\n",
            "Epoch [50/100], Loss: 12534.4951\n",
            "Epoch [60/100], Loss: 12404.2690\n",
            "Epoch [70/100], Loss: 14773.6270\n",
            "Epoch [80/100], Loss: 11431.7751\n",
            "Epoch [90/100], Loss: 21170.6885\n",
            "Epoch [100/100], Loss: 14273.6824\n",
            "Fold 1, RMSE: 56.939029693603516\n",
            "Epoch [10/100], Loss: 31875.4004\n",
            "Epoch [20/100], Loss: 13494.7554\n",
            "Epoch [30/100], Loss: 15222.9155\n",
            "Epoch [40/100], Loss: 16109.3181\n",
            "Epoch [50/100], Loss: 13889.2704\n",
            "Epoch [60/100], Loss: 14774.7351\n",
            "Epoch [70/100], Loss: 16365.1753\n",
            "Epoch [80/100], Loss: 18965.4802\n",
            "Epoch [90/100], Loss: 14815.6985\n",
            "Epoch [100/100], Loss: 14212.7458\n",
            "Fold 2, RMSE: 83.91885375976562\n",
            "Epoch [10/100], Loss: 40422.4380\n",
            "Epoch [20/100], Loss: 11728.0505\n",
            "Epoch [30/100], Loss: 16818.8064\n",
            "Epoch [40/100], Loss: 13600.7898\n",
            "Epoch [50/100], Loss: 13180.3089\n",
            "Epoch [60/100], Loss: 15855.4570\n",
            "Epoch [70/100], Loss: 18904.1226\n",
            "Epoch [80/100], Loss: 15308.0444\n",
            "Epoch [90/100], Loss: 11761.6317\n",
            "Epoch [100/100], Loss: 16642.3535\n",
            "Fold 3, RMSE: 109.88002014160156\n",
            "Epoch [10/100], Loss: 23494.0063\n",
            "Epoch [20/100], Loss: 17807.1875\n",
            "Epoch [30/100], Loss: 26404.7666\n",
            "Epoch [40/100], Loss: 21248.7676\n",
            "Epoch [50/100], Loss: 26536.6816\n",
            "Epoch [60/100], Loss: 21607.3496\n",
            "Epoch [70/100], Loss: 21352.7065\n",
            "Epoch [80/100], Loss: 24125.4390\n",
            "Epoch [90/100], Loss: 17935.9183\n",
            "Epoch [100/100], Loss: 20554.0923\n",
            "Fold 4, RMSE: 54.47927474975586\n",
            "Epoch [10/100], Loss: 64105.5732\n",
            "Epoch [20/100], Loss: 25428.6953\n",
            "Epoch [30/100], Loss: 23883.3867\n",
            "Epoch [40/100], Loss: 17581.0215\n",
            "Epoch [50/100], Loss: 19650.5493\n",
            "Epoch [60/100], Loss: 16936.6128\n",
            "Epoch [70/100], Loss: 19408.8765\n",
            "Epoch [80/100], Loss: 16165.6543\n",
            "Epoch [90/100], Loss: 16289.0898\n",
            "Epoch [100/100], Loss: 15510.2441\n",
            "Fold 5, RMSE: 48.64921569824219\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 70.77327880859374\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 175630.2695\n",
            "Epoch [20/150], Loss: 45928.7363\n",
            "Epoch [30/150], Loss: 29898.1401\n",
            "Epoch [40/150], Loss: 27347.5078\n",
            "Epoch [50/150], Loss: 30641.7778\n",
            "Epoch [60/150], Loss: 16403.4751\n",
            "Epoch [70/150], Loss: 17947.9043\n",
            "Epoch [80/150], Loss: 13406.3860\n",
            "Epoch [90/150], Loss: 17561.5234\n",
            "Epoch [100/150], Loss: 12571.5994\n",
            "Epoch [110/150], Loss: 13793.5022\n",
            "Epoch [120/150], Loss: 11949.6157\n",
            "Epoch [130/150], Loss: 10504.3590\n",
            "Epoch [140/150], Loss: 12814.6091\n",
            "Epoch [150/150], Loss: 15867.5410\n",
            "Fold 1, RMSE: 52.824954986572266\n",
            "Epoch [10/150], Loss: 14011.2385\n",
            "Epoch [20/150], Loss: 16447.8904\n",
            "Epoch [30/150], Loss: 14925.0317\n",
            "Epoch [40/150], Loss: 18840.5635\n",
            "Epoch [50/150], Loss: 14940.3760\n",
            "Epoch [60/150], Loss: 17162.6685\n",
            "Epoch [70/150], Loss: 16615.3928\n",
            "Epoch [80/150], Loss: 18828.6196\n",
            "Epoch [90/150], Loss: 16067.3477\n",
            "Epoch [100/150], Loss: 13702.5599\n",
            "Epoch [110/150], Loss: 14536.5315\n",
            "Epoch [120/150], Loss: 22046.0107\n",
            "Epoch [130/150], Loss: 16737.2734\n",
            "Epoch [140/150], Loss: 14526.3567\n",
            "Epoch [150/150], Loss: 18458.1357\n",
            "Fold 2, RMSE: 87.18309783935547\n",
            "Epoch [10/150], Loss: 11686.7817\n",
            "Epoch [20/150], Loss: 16240.1069\n",
            "Epoch [30/150], Loss: 12679.5723\n",
            "Epoch [40/150], Loss: 13697.9275\n",
            "Epoch [50/150], Loss: 15935.5203\n",
            "Epoch [60/150], Loss: 18562.6201\n",
            "Epoch [70/150], Loss: 18290.8950\n",
            "Epoch [80/150], Loss: 13027.4424\n",
            "Epoch [90/150], Loss: 12880.4250\n",
            "Epoch [100/150], Loss: 12281.9114\n",
            "Epoch [110/150], Loss: 13236.6528\n",
            "Epoch [120/150], Loss: 12026.0450\n",
            "Epoch [130/150], Loss: 12646.0593\n",
            "Epoch [140/150], Loss: 16012.7119\n",
            "Epoch [150/150], Loss: 11598.2069\n",
            "Fold 3, RMSE: 109.40969848632812\n",
            "Epoch [10/150], Loss: 36300.8110\n",
            "Epoch [20/150], Loss: 24650.1519\n",
            "Epoch [30/150], Loss: 20668.9346\n",
            "Epoch [40/150], Loss: 17701.1260\n",
            "Epoch [50/150], Loss: 19967.1753\n",
            "Epoch [60/150], Loss: 22891.4221\n",
            "Epoch [70/150], Loss: 19372.2642\n",
            "Epoch [80/150], Loss: 19098.4795\n",
            "Epoch [90/150], Loss: 23615.3594\n",
            "Epoch [100/150], Loss: 18863.2500\n",
            "Epoch [110/150], Loss: 22108.8311\n",
            "Epoch [120/150], Loss: 21956.7000\n",
            "Epoch [130/150], Loss: 19521.3979\n",
            "Epoch [140/150], Loss: 24798.2656\n",
            "Epoch [150/150], Loss: 26126.0156\n",
            "Fold 4, RMSE: 53.79600524902344\n",
            "Epoch [10/150], Loss: 66638.0615\n",
            "Epoch [20/150], Loss: 28161.5981\n",
            "Epoch [30/150], Loss: 17612.6847\n",
            "Epoch [40/150], Loss: 19290.0869\n",
            "Epoch [50/150], Loss: 17948.2549\n",
            "Epoch [60/150], Loss: 20581.2402\n",
            "Epoch [70/150], Loss: 23193.9058\n",
            "Epoch [80/150], Loss: 17631.3284\n",
            "Epoch [90/150], Loss: 16273.7114\n",
            "Epoch [100/150], Loss: 17909.4734\n",
            "Epoch [110/150], Loss: 15603.1240\n",
            "Epoch [120/150], Loss: 14129.3340\n",
            "Epoch [130/150], Loss: 12955.8136\n",
            "Epoch [140/150], Loss: 15470.4717\n",
            "Epoch [150/150], Loss: 17824.4358\n",
            "Fold 5, RMSE: 45.77109909057617\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 69.79697113037109\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 21159.1582\n",
            "Epoch [20/100], Loss: 21859.1665\n",
            "Epoch [30/100], Loss: 21167.6895\n",
            "Epoch [40/100], Loss: 18640.0688\n",
            "Epoch [50/100], Loss: 18712.1318\n",
            "Epoch [60/100], Loss: 18556.0322\n",
            "Epoch [70/100], Loss: 16247.4575\n",
            "Epoch [80/100], Loss: 15760.4846\n",
            "Epoch [90/100], Loss: 17939.8740\n",
            "Epoch [100/100], Loss: 18533.2080\n",
            "Fold 1, RMSE: 66.15855407714844\n",
            "Epoch [10/100], Loss: 19060.6882\n",
            "Epoch [20/100], Loss: 16933.7876\n",
            "Epoch [30/100], Loss: 16237.9941\n",
            "Epoch [40/100], Loss: 15303.2554\n",
            "Epoch [50/100], Loss: 18181.0913\n",
            "Epoch [60/100], Loss: 17864.8413\n",
            "Epoch [70/100], Loss: 13530.0031\n",
            "Epoch [80/100], Loss: 14593.1577\n",
            "Epoch [90/100], Loss: 26879.0188\n",
            "Epoch [100/100], Loss: 16965.6584\n",
            "Fold 2, RMSE: 83.17298126220703\n",
            "Epoch [10/100], Loss: 245253.2109\n",
            "Epoch [20/100], Loss: 25952.3652\n",
            "Epoch [30/100], Loss: 20356.5874\n",
            "Epoch [40/100], Loss: 20076.6680\n",
            "Epoch [50/100], Loss: 10857.3184\n",
            "Epoch [60/100], Loss: 22817.4546\n",
            "Epoch [70/100], Loss: 10399.5889\n",
            "Epoch [80/100], Loss: 14471.0470\n",
            "Epoch [90/100], Loss: 16832.3135\n",
            "Epoch [100/100], Loss: 9653.3477\n",
            "Fold 3, RMSE: 101.57595825195312\n",
            "Epoch [10/100], Loss: 58302.7881\n",
            "Epoch [20/100], Loss: 21378.8640\n",
            "Epoch [30/100], Loss: 24712.5259\n",
            "Epoch [40/100], Loss: 14038.5476\n",
            "Epoch [50/100], Loss: 19322.2344\n",
            "Epoch [60/100], Loss: 15621.9146\n",
            "Epoch [70/100], Loss: 21166.9121\n",
            "Epoch [80/100], Loss: 14416.7747\n",
            "Epoch [90/100], Loss: 13503.7961\n",
            "Epoch [100/100], Loss: 17074.1140\n",
            "Fold 4, RMSE: 43.673667907714844\n",
            "Epoch [10/100], Loss: 60733.5059\n",
            "Epoch [20/100], Loss: 19270.4404\n",
            "Epoch [30/100], Loss: 20674.9268\n",
            "Epoch [40/100], Loss: 19237.7749\n",
            "Epoch [50/100], Loss: 16162.4390\n",
            "Epoch [60/100], Loss: 16808.8933\n",
            "Epoch [70/100], Loss: 16521.9207\n",
            "Epoch [80/100], Loss: 14798.3464\n",
            "Epoch [90/100], Loss: 20661.4209\n",
            "Epoch [100/100], Loss: 22349.8789\n",
            "Fold 5, RMSE: 51.182167053222656\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 69.15266571044921\n",
            "Training with neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 302791.7031\n",
            "Epoch [20/150], Loss: 134118.2031\n",
            "Epoch [30/150], Loss: 41802.7510\n",
            "Epoch [40/150], Loss: 37256.5200\n",
            "Epoch [50/150], Loss: 38262.6162\n",
            "Epoch [60/150], Loss: 21653.5034\n",
            "Epoch [70/150], Loss: 28666.6211\n",
            "Epoch [80/150], Loss: 32485.0498\n",
            "Epoch [90/150], Loss: 23646.5200\n",
            "Epoch [100/150], Loss: 40773.6323\n",
            "Epoch [110/150], Loss: 18086.7827\n",
            "Epoch [120/150], Loss: 17482.0308\n",
            "Epoch [130/150], Loss: 23290.3975\n",
            "Epoch [140/150], Loss: 14997.7852\n",
            "Epoch [150/150], Loss: 20012.9436\n",
            "Fold 1, RMSE: 63.43376922607422\n",
            "Epoch [10/150], Loss: 15711.3748\n",
            "Epoch [20/150], Loss: 15919.9607\n",
            "Epoch [30/150], Loss: 20411.4570\n",
            "Epoch [40/150], Loss: 14062.2551\n",
            "Epoch [50/150], Loss: 16243.9604\n",
            "Epoch [60/150], Loss: 17737.3916\n",
            "Epoch [70/150], Loss: 14647.3064\n",
            "Epoch [80/150], Loss: 23544.8655\n",
            "Epoch [90/150], Loss: 15314.7322\n",
            "Epoch [100/150], Loss: 17643.8921\n",
            "Epoch [110/150], Loss: 13544.1616\n",
            "Epoch [120/150], Loss: 14000.7305\n",
            "Epoch [130/150], Loss: 12798.4438\n",
            "Epoch [140/150], Loss: 14202.8333\n",
            "Epoch [150/150], Loss: 19108.3972\n",
            "Fold 2, RMSE: 78.71736145019531\n",
            "Epoch [10/150], Loss: 91185.6719\n",
            "Epoch [20/150], Loss: 22249.5107\n",
            "Epoch [30/150], Loss: 9268.7125\n",
            "Epoch [40/150], Loss: 17364.0186\n",
            "Epoch [50/150], Loss: 10420.8875\n",
            "Epoch [60/150], Loss: 15502.9326\n",
            "Epoch [70/150], Loss: 10333.5718\n",
            "Epoch [80/150], Loss: 12803.9800\n",
            "Epoch [90/150], Loss: 12147.3071\n",
            "Epoch [100/150], Loss: 13135.2800\n",
            "Epoch [110/150], Loss: 8158.8660\n",
            "Epoch [120/150], Loss: 8907.1158\n",
            "Epoch [130/150], Loss: 10251.2126\n",
            "Epoch [140/150], Loss: 12057.7271\n",
            "Epoch [150/150], Loss: 8224.5745\n",
            "Fold 3, RMSE: 105.15303039550781\n",
            "Epoch [10/150], Loss: 20024.8037\n",
            "Epoch [20/150], Loss: 16655.5336\n",
            "Epoch [30/150], Loss: 18471.5146\n",
            "Epoch [40/150], Loss: 26511.3735\n",
            "Epoch [50/150], Loss: 20721.2119\n",
            "Epoch [60/150], Loss: 16816.8392\n",
            "Epoch [70/150], Loss: 26181.0962\n",
            "Epoch [80/150], Loss: 19940.7949\n",
            "Epoch [90/150], Loss: 16530.6724\n",
            "Epoch [100/150], Loss: 15662.9838\n",
            "Epoch [110/150], Loss: 20499.2212\n",
            "Epoch [120/150], Loss: 15423.2827\n",
            "Epoch [130/150], Loss: 24731.4067\n",
            "Epoch [140/150], Loss: 16476.1155\n",
            "Epoch [150/150], Loss: 13162.8069\n",
            "Fold 4, RMSE: 40.91949462890625\n",
            "Epoch [10/150], Loss: 26272.3589\n",
            "Epoch [20/150], Loss: 20711.7930\n",
            "Epoch [30/150], Loss: 19174.3843\n",
            "Epoch [40/150], Loss: 21808.2568\n",
            "Epoch [50/150], Loss: 23722.2915\n",
            "Epoch [60/150], Loss: 22812.3701\n",
            "Epoch [70/150], Loss: 17359.0662\n",
            "Epoch [80/150], Loss: 19901.3945\n",
            "Epoch [90/150], Loss: 16046.5828\n",
            "Epoch [100/150], Loss: 14863.0979\n",
            "Epoch [110/150], Loss: 15587.3037\n",
            "Epoch [120/150], Loss: 11479.8282\n",
            "Epoch [130/150], Loss: 14366.3311\n",
            "Epoch [140/150], Loss: 13222.3284\n",
            "Epoch [150/150], Loss: 13670.0013\n",
            "Fold 5, RMSE: 48.80012893676758\n",
            "Avg RMSE for neurons=64, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 67.40475692749024\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 22327.0229\n",
            "Epoch [20/100], Loss: 13034.8699\n",
            "Epoch [30/100], Loss: 5566.0116\n",
            "Epoch [40/100], Loss: 4601.5237\n",
            "Epoch [50/100], Loss: 2616.5509\n",
            "Epoch [60/100], Loss: 2843.9045\n",
            "Epoch [70/100], Loss: 2693.2759\n",
            "Epoch [80/100], Loss: 3532.1060\n",
            "Epoch [90/100], Loss: 1566.3473\n",
            "Epoch [100/100], Loss: 1687.3235\n",
            "Fold 1, RMSE: 59.91993713378906\n",
            "Epoch [10/100], Loss: 19626.5132\n",
            "Epoch [20/100], Loss: 7858.8389\n",
            "Epoch [30/100], Loss: 8621.8116\n",
            "Epoch [40/100], Loss: 3395.8453\n",
            "Epoch [50/100], Loss: 6028.8453\n",
            "Epoch [60/100], Loss: 2548.8577\n",
            "Epoch [70/100], Loss: 1784.0610\n",
            "Epoch [80/100], Loss: 1899.6861\n",
            "Epoch [90/100], Loss: 2698.7996\n",
            "Epoch [100/100], Loss: 2517.0468\n",
            "Fold 2, RMSE: 69.97650909423828\n",
            "Epoch [10/100], Loss: 15095.1895\n",
            "Epoch [20/100], Loss: 9272.0667\n",
            "Epoch [30/100], Loss: 5766.8445\n",
            "Epoch [40/100], Loss: 5378.1542\n",
            "Epoch [50/100], Loss: 1765.8235\n",
            "Epoch [60/100], Loss: 1669.9134\n",
            "Epoch [70/100], Loss: 2152.9787\n",
            "Epoch [80/100], Loss: 2022.3352\n",
            "Epoch [90/100], Loss: 877.2225\n",
            "Epoch [100/100], Loss: 1576.3075\n",
            "Fold 3, RMSE: 94.02761840820312\n",
            "Epoch [10/100], Loss: 15510.8662\n",
            "Epoch [20/100], Loss: 11611.6714\n",
            "Epoch [30/100], Loss: 6424.0311\n",
            "Epoch [40/100], Loss: 7598.1956\n",
            "Epoch [50/100], Loss: 2681.6301\n",
            "Epoch [60/100], Loss: 2186.4691\n",
            "Epoch [70/100], Loss: 5303.5632\n",
            "Epoch [80/100], Loss: 5409.7675\n",
            "Epoch [90/100], Loss: 2345.2741\n",
            "Epoch [100/100], Loss: 3492.4437\n",
            "Fold 4, RMSE: 36.414207458496094\n",
            "Epoch [10/100], Loss: 17974.8479\n",
            "Epoch [20/100], Loss: 7972.6658\n",
            "Epoch [30/100], Loss: 8394.0511\n",
            "Epoch [40/100], Loss: 5198.7018\n",
            "Epoch [50/100], Loss: 4633.2665\n",
            "Epoch [60/100], Loss: 2674.8774\n",
            "Epoch [70/100], Loss: 1983.0015\n",
            "Epoch [80/100], Loss: 2540.5884\n",
            "Epoch [90/100], Loss: 3466.8108\n",
            "Epoch [100/100], Loss: 2365.2591\n",
            "Fold 5, RMSE: 45.748130798339844\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 61.217280578613284\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 12169.8347\n",
            "Epoch [20/150], Loss: 9089.6162\n",
            "Epoch [30/150], Loss: 7958.8747\n",
            "Epoch [40/150], Loss: 4171.1545\n",
            "Epoch [50/150], Loss: 2666.5022\n",
            "Epoch [60/150], Loss: 2166.8882\n",
            "Epoch [70/150], Loss: 2439.9944\n",
            "Epoch [80/150], Loss: 3041.8560\n",
            "Epoch [90/150], Loss: 3294.5232\n",
            "Epoch [100/150], Loss: 3592.2285\n",
            "Epoch [110/150], Loss: 1514.2227\n",
            "Epoch [120/150], Loss: 1883.9631\n",
            "Epoch [130/150], Loss: 1624.5520\n",
            "Epoch [140/150], Loss: 3721.3550\n",
            "Epoch [150/150], Loss: 1401.0203\n",
            "Fold 1, RMSE: 61.05329513549805\n",
            "Epoch [10/150], Loss: 16072.0745\n",
            "Epoch [20/150], Loss: 9352.0482\n",
            "Epoch [30/150], Loss: 7507.2412\n",
            "Epoch [40/150], Loss: 6816.9199\n",
            "Epoch [50/150], Loss: 5104.2598\n",
            "Epoch [60/150], Loss: 1986.6484\n",
            "Epoch [70/150], Loss: 2367.3670\n",
            "Epoch [80/150], Loss: 1329.6130\n",
            "Epoch [90/150], Loss: 1545.4417\n",
            "Epoch [100/150], Loss: 3017.6529\n",
            "Epoch [110/150], Loss: 2884.2603\n",
            "Epoch [120/150], Loss: 3563.6021\n",
            "Epoch [130/150], Loss: 6254.2115\n",
            "Epoch [140/150], Loss: 3377.8099\n",
            "Epoch [150/150], Loss: 2282.1307\n",
            "Fold 2, RMSE: 62.17595291137695\n",
            "Epoch [10/150], Loss: 21879.5806\n",
            "Epoch [20/150], Loss: 6492.2655\n",
            "Epoch [30/150], Loss: 4733.8430\n",
            "Epoch [40/150], Loss: 4004.9342\n",
            "Epoch [50/150], Loss: 3113.8998\n",
            "Epoch [60/150], Loss: 1231.2866\n",
            "Epoch [70/150], Loss: 2978.1366\n",
            "Epoch [80/150], Loss: 1679.9402\n",
            "Epoch [90/150], Loss: 803.6976\n",
            "Epoch [100/150], Loss: 1418.4436\n",
            "Epoch [110/150], Loss: 1569.2705\n",
            "Epoch [120/150], Loss: 2145.1887\n",
            "Epoch [130/150], Loss: 1189.8274\n",
            "Epoch [140/150], Loss: 1350.9994\n",
            "Epoch [150/150], Loss: 707.9079\n",
            "Fold 3, RMSE: 93.7914047241211\n",
            "Epoch [10/150], Loss: 14602.9949\n",
            "Epoch [20/150], Loss: 12397.8765\n",
            "Epoch [30/150], Loss: 10323.9907\n",
            "Epoch [40/150], Loss: 13905.1731\n",
            "Epoch [50/150], Loss: 5645.9953\n",
            "Epoch [60/150], Loss: 4970.8412\n",
            "Epoch [70/150], Loss: 3212.5308\n",
            "Epoch [80/150], Loss: 4031.9528\n",
            "Epoch [90/150], Loss: 3077.5938\n",
            "Epoch [100/150], Loss: 2263.1035\n",
            "Epoch [110/150], Loss: 5096.4473\n",
            "Epoch [120/150], Loss: 2103.5990\n",
            "Epoch [130/150], Loss: 2452.8736\n",
            "Epoch [140/150], Loss: 2897.7648\n",
            "Epoch [150/150], Loss: 4837.0183\n",
            "Fold 4, RMSE: 41.0460090637207\n",
            "Epoch [10/150], Loss: 15966.1746\n",
            "Epoch [20/150], Loss: 10451.8457\n",
            "Epoch [30/150], Loss: 10459.8296\n",
            "Epoch [40/150], Loss: 5617.3566\n",
            "Epoch [50/150], Loss: 2848.9543\n",
            "Epoch [60/150], Loss: 1724.6738\n",
            "Epoch [70/150], Loss: 1934.8950\n",
            "Epoch [80/150], Loss: 1701.1598\n",
            "Epoch [90/150], Loss: 1705.7851\n",
            "Epoch [100/150], Loss: 2223.1407\n",
            "Epoch [110/150], Loss: 2860.0610\n",
            "Epoch [120/150], Loss: 2612.0535\n",
            "Epoch [130/150], Loss: 1062.4862\n",
            "Epoch [140/150], Loss: 2658.0922\n",
            "Epoch [150/150], Loss: 1245.1376\n",
            "Fold 5, RMSE: 46.0275993347168\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 60.818852233886716\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 10251.0356\n",
            "Epoch [20/100], Loss: 8274.6678\n",
            "Epoch [30/100], Loss: 7626.3083\n",
            "Epoch [40/100], Loss: 4715.5956\n",
            "Epoch [50/100], Loss: 6880.2450\n",
            "Epoch [60/100], Loss: 4856.2917\n",
            "Epoch [70/100], Loss: 2585.7092\n",
            "Epoch [80/100], Loss: 1314.0032\n",
            "Epoch [90/100], Loss: 2364.4230\n",
            "Epoch [100/100], Loss: 2394.2388\n",
            "Fold 1, RMSE: 62.03556442260742\n",
            "Epoch [10/100], Loss: 8652.0315\n",
            "Epoch [20/100], Loss: 6268.7462\n",
            "Epoch [30/100], Loss: 3160.1416\n",
            "Epoch [40/100], Loss: 1845.8335\n",
            "Epoch [50/100], Loss: 2845.3416\n",
            "Epoch [60/100], Loss: 4557.4750\n",
            "Epoch [70/100], Loss: 2117.9596\n",
            "Epoch [80/100], Loss: 1651.3438\n",
            "Epoch [90/100], Loss: 2364.7967\n",
            "Epoch [100/100], Loss: 649.6163\n",
            "Fold 2, RMSE: 68.65743255615234\n",
            "Epoch [10/100], Loss: 8458.9185\n",
            "Epoch [20/100], Loss: 11757.8904\n",
            "Epoch [30/100], Loss: 3354.0742\n",
            "Epoch [40/100], Loss: 2332.2962\n",
            "Epoch [50/100], Loss: 2069.5949\n",
            "Epoch [60/100], Loss: 1546.1310\n",
            "Epoch [70/100], Loss: 923.0827\n",
            "Epoch [80/100], Loss: 2733.1411\n",
            "Epoch [90/100], Loss: 1614.7403\n",
            "Epoch [100/100], Loss: 978.5649\n",
            "Fold 3, RMSE: 90.4099349975586\n",
            "Epoch [10/100], Loss: 16531.4963\n",
            "Epoch [20/100], Loss: 14500.0264\n",
            "Epoch [30/100], Loss: 9008.1807\n",
            "Epoch [40/100], Loss: 4278.7410\n",
            "Epoch [50/100], Loss: 3766.0329\n",
            "Epoch [60/100], Loss: 2961.0881\n",
            "Epoch [70/100], Loss: 4670.7372\n",
            "Epoch [80/100], Loss: 3326.0422\n",
            "Epoch [90/100], Loss: 3117.6241\n",
            "Epoch [100/100], Loss: 2541.7227\n",
            "Fold 4, RMSE: 43.252838134765625\n",
            "Epoch [10/100], Loss: 9252.9502\n",
            "Epoch [20/100], Loss: 7219.9113\n",
            "Epoch [30/100], Loss: 6368.8900\n",
            "Epoch [40/100], Loss: 7310.7072\n",
            "Epoch [50/100], Loss: 3639.4532\n",
            "Epoch [60/100], Loss: 3294.7650\n",
            "Epoch [70/100], Loss: 3686.5930\n",
            "Epoch [80/100], Loss: 3195.7673\n",
            "Epoch [90/100], Loss: 1529.6343\n",
            "Epoch [100/100], Loss: 2369.0935\n",
            "Fold 5, RMSE: 49.25355529785156\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 62.72186508178711\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 10794.7821\n",
            "Epoch [20/150], Loss: 9250.4045\n",
            "Epoch [30/150], Loss: 4361.2638\n",
            "Epoch [40/150], Loss: 3291.9254\n",
            "Epoch [50/150], Loss: 2592.5528\n",
            "Epoch [60/150], Loss: 4614.9666\n",
            "Epoch [70/150], Loss: 1667.2084\n",
            "Epoch [80/150], Loss: 4385.4816\n",
            "Epoch [90/150], Loss: 1641.9008\n",
            "Epoch [100/150], Loss: 5202.9988\n",
            "Epoch [110/150], Loss: 948.1542\n",
            "Epoch [120/150], Loss: 1480.2086\n",
            "Epoch [130/150], Loss: 1174.0802\n",
            "Epoch [140/150], Loss: 1224.3301\n",
            "Epoch [150/150], Loss: 1702.5990\n",
            "Fold 1, RMSE: 60.76075744628906\n",
            "Epoch [10/150], Loss: 12938.2871\n",
            "Epoch [20/150], Loss: 7471.8588\n",
            "Epoch [30/150], Loss: 5044.6960\n",
            "Epoch [40/150], Loss: 3497.0784\n",
            "Epoch [50/150], Loss: 2435.4789\n",
            "Epoch [60/150], Loss: 3675.8791\n",
            "Epoch [70/150], Loss: 1247.4421\n",
            "Epoch [80/150], Loss: 4676.4764\n",
            "Epoch [90/150], Loss: 3098.0040\n",
            "Epoch [100/150], Loss: 963.7847\n",
            "Epoch [110/150], Loss: 1610.3938\n",
            "Epoch [120/150], Loss: 893.0548\n",
            "Epoch [130/150], Loss: 1389.0609\n",
            "Epoch [140/150], Loss: 1663.8695\n",
            "Epoch [150/150], Loss: 1253.0059\n",
            "Fold 2, RMSE: 63.62406539916992\n",
            "Epoch [10/150], Loss: 10459.9557\n",
            "Epoch [20/150], Loss: 5123.5653\n",
            "Epoch [30/150], Loss: 5719.2113\n",
            "Epoch [40/150], Loss: 3067.9747\n",
            "Epoch [50/150], Loss: 3597.5822\n",
            "Epoch [60/150], Loss: 2596.6910\n",
            "Epoch [70/150], Loss: 1381.3730\n",
            "Epoch [80/150], Loss: 1480.9652\n",
            "Epoch [90/150], Loss: 924.6967\n",
            "Epoch [100/150], Loss: 1406.9997\n",
            "Epoch [110/150], Loss: 1741.4130\n",
            "Epoch [120/150], Loss: 5603.3712\n",
            "Epoch [130/150], Loss: 1081.2339\n",
            "Epoch [140/150], Loss: 1708.9208\n",
            "Epoch [150/150], Loss: 1604.4290\n",
            "Fold 3, RMSE: 94.86121368408203\n",
            "Epoch [10/150], Loss: 13002.1028\n",
            "Epoch [20/150], Loss: 7864.3749\n",
            "Epoch [30/150], Loss: 4909.7443\n",
            "Epoch [40/150], Loss: 2060.4445\n",
            "Epoch [50/150], Loss: 2836.6974\n",
            "Epoch [60/150], Loss: 2323.7227\n",
            "Epoch [70/150], Loss: 2435.8215\n",
            "Epoch [80/150], Loss: 3028.0919\n",
            "Epoch [90/150], Loss: 1032.9678\n",
            "Epoch [100/150], Loss: 968.1707\n",
            "Epoch [110/150], Loss: 1026.0350\n",
            "Epoch [120/150], Loss: 1252.8959\n",
            "Epoch [130/150], Loss: 576.6910\n",
            "Epoch [140/150], Loss: 1132.9985\n",
            "Epoch [150/150], Loss: 1323.2954\n",
            "Fold 4, RMSE: 39.979820251464844\n",
            "Epoch [10/150], Loss: 13684.2981\n",
            "Epoch [20/150], Loss: 6406.9764\n",
            "Epoch [30/150], Loss: 3314.3607\n",
            "Epoch [40/150], Loss: 5439.7187\n",
            "Epoch [50/150], Loss: 3106.0943\n",
            "Epoch [60/150], Loss: 2860.0700\n",
            "Epoch [70/150], Loss: 2288.1926\n",
            "Epoch [80/150], Loss: 2294.7998\n",
            "Epoch [90/150], Loss: 2001.8314\n",
            "Epoch [100/150], Loss: 1792.1885\n",
            "Epoch [110/150], Loss: 1280.4104\n",
            "Epoch [120/150], Loss: 3557.1396\n",
            "Epoch [130/150], Loss: 1112.7758\n",
            "Epoch [140/150], Loss: 1404.0960\n",
            "Epoch [150/150], Loss: 1126.7169\n",
            "Fold 5, RMSE: 46.546836853027344\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 61.15453872680664\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 15109.8589\n",
            "Epoch [20/100], Loss: 7792.4232\n",
            "Epoch [30/100], Loss: 4038.8578\n",
            "Epoch [40/100], Loss: 3452.3527\n",
            "Epoch [50/100], Loss: 2137.3587\n",
            "Epoch [60/100], Loss: 2954.4620\n",
            "Epoch [70/100], Loss: 3679.2208\n",
            "Epoch [80/100], Loss: 3216.8143\n",
            "Epoch [90/100], Loss: 2096.8418\n",
            "Epoch [100/100], Loss: 1976.3640\n",
            "Fold 1, RMSE: 58.78600311279297\n",
            "Epoch [10/100], Loss: 12749.2877\n",
            "Epoch [20/100], Loss: 12891.7673\n",
            "Epoch [30/100], Loss: 11409.5125\n",
            "Epoch [40/100], Loss: 5370.6929\n",
            "Epoch [50/100], Loss: 4527.5988\n",
            "Epoch [60/100], Loss: 2294.1128\n",
            "Epoch [70/100], Loss: 1812.5567\n",
            "Epoch [80/100], Loss: 2237.0400\n",
            "Epoch [90/100], Loss: 1767.4052\n",
            "Epoch [100/100], Loss: 2065.3035\n",
            "Fold 2, RMSE: 63.4073600769043\n",
            "Epoch [10/100], Loss: 14246.1938\n",
            "Epoch [20/100], Loss: 5847.3679\n",
            "Epoch [30/100], Loss: 5131.6678\n",
            "Epoch [40/100], Loss: 2930.7549\n",
            "Epoch [50/100], Loss: 3082.8849\n",
            "Epoch [60/100], Loss: 1713.4162\n",
            "Epoch [70/100], Loss: 2898.7697\n",
            "Epoch [80/100], Loss: 3033.8757\n",
            "Epoch [90/100], Loss: 2106.2538\n",
            "Epoch [100/100], Loss: 4021.0634\n",
            "Fold 3, RMSE: 98.97649383544922\n",
            "Epoch [10/100], Loss: 11275.0732\n",
            "Epoch [20/100], Loss: 6886.2416\n",
            "Epoch [30/100], Loss: 5438.0137\n",
            "Epoch [40/100], Loss: 4360.8217\n",
            "Epoch [50/100], Loss: 4309.4019\n",
            "Epoch [60/100], Loss: 3942.6794\n",
            "Epoch [70/100], Loss: 2552.9113\n",
            "Epoch [80/100], Loss: 2364.2211\n",
            "Epoch [90/100], Loss: 2506.3126\n",
            "Epoch [100/100], Loss: 2763.0116\n",
            "Fold 4, RMSE: 36.472633361816406\n",
            "Epoch [10/100], Loss: 18944.6919\n",
            "Epoch [20/100], Loss: 10277.2712\n",
            "Epoch [30/100], Loss: 6357.3857\n",
            "Epoch [40/100], Loss: 5660.1171\n",
            "Epoch [50/100], Loss: 2947.0401\n",
            "Epoch [60/100], Loss: 1892.4283\n",
            "Epoch [70/100], Loss: 1844.7639\n",
            "Epoch [80/100], Loss: 2614.4442\n",
            "Epoch [90/100], Loss: 2900.6354\n",
            "Epoch [100/100], Loss: 1995.7752\n",
            "Fold 5, RMSE: 51.116214752197266\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 61.75174102783203\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18223.3611\n",
            "Epoch [20/150], Loss: 9498.8365\n",
            "Epoch [30/150], Loss: 6947.6372\n",
            "Epoch [40/150], Loss: 6152.4894\n",
            "Epoch [50/150], Loss: 5310.1433\n",
            "Epoch [60/150], Loss: 7600.1877\n",
            "Epoch [70/150], Loss: 6391.1782\n",
            "Epoch [80/150], Loss: 8809.1325\n",
            "Epoch [90/150], Loss: 9710.2223\n",
            "Epoch [100/150], Loss: 8855.0977\n",
            "Epoch [110/150], Loss: 5628.3365\n",
            "Epoch [120/150], Loss: 3562.6477\n",
            "Epoch [130/150], Loss: 4911.8934\n",
            "Epoch [140/150], Loss: 1492.7922\n",
            "Epoch [150/150], Loss: 1635.7372\n",
            "Fold 1, RMSE: 61.79469680786133\n",
            "Epoch [10/150], Loss: 14415.3813\n",
            "Epoch [20/150], Loss: 8548.8840\n",
            "Epoch [30/150], Loss: 5407.9997\n",
            "Epoch [40/150], Loss: 3338.4908\n",
            "Epoch [50/150], Loss: 9158.2042\n",
            "Epoch [60/150], Loss: 3790.5839\n",
            "Epoch [70/150], Loss: 2175.2601\n",
            "Epoch [80/150], Loss: 1862.1199\n",
            "Epoch [90/150], Loss: 2585.0977\n",
            "Epoch [100/150], Loss: 3017.7933\n",
            "Epoch [110/150], Loss: 3407.7421\n",
            "Epoch [120/150], Loss: 1660.6881\n",
            "Epoch [130/150], Loss: 2300.3114\n",
            "Epoch [140/150], Loss: 847.5260\n",
            "Epoch [150/150], Loss: 837.0358\n",
            "Fold 2, RMSE: 68.18761444091797\n",
            "Epoch [10/150], Loss: 9200.0100\n",
            "Epoch [20/150], Loss: 5321.6219\n",
            "Epoch [30/150], Loss: 3663.5090\n",
            "Epoch [40/150], Loss: 1999.5424\n",
            "Epoch [50/150], Loss: 3200.7172\n",
            "Epoch [60/150], Loss: 7386.3464\n",
            "Epoch [70/150], Loss: 8587.7100\n",
            "Epoch [80/150], Loss: 3052.2559\n",
            "Epoch [90/150], Loss: 3383.7027\n",
            "Epoch [100/150], Loss: 1212.0750\n",
            "Epoch [110/150], Loss: 1550.3225\n",
            "Epoch [120/150], Loss: 1886.7921\n",
            "Epoch [130/150], Loss: 2142.7277\n",
            "Epoch [140/150], Loss: 2795.8513\n",
            "Epoch [150/150], Loss: 1445.0710\n",
            "Fold 3, RMSE: 89.10440826416016\n",
            "Epoch [10/150], Loss: 11112.0851\n",
            "Epoch [20/150], Loss: 13302.8997\n",
            "Epoch [30/150], Loss: 6356.7087\n",
            "Epoch [40/150], Loss: 6955.4591\n",
            "Epoch [50/150], Loss: 7884.9157\n",
            "Epoch [60/150], Loss: 2130.1129\n",
            "Epoch [70/150], Loss: 2586.3842\n",
            "Epoch [80/150], Loss: 1636.0510\n",
            "Epoch [90/150], Loss: 1937.2972\n",
            "Epoch [100/150], Loss: 2979.3199\n",
            "Epoch [110/150], Loss: 3389.4298\n",
            "Epoch [120/150], Loss: 3098.7041\n",
            "Epoch [130/150], Loss: 4197.9625\n",
            "Epoch [140/150], Loss: 3772.4390\n",
            "Epoch [150/150], Loss: 2754.9923\n",
            "Fold 4, RMSE: 45.262142181396484\n",
            "Epoch [10/150], Loss: 17417.0300\n",
            "Epoch [20/150], Loss: 12838.7686\n",
            "Epoch [30/150], Loss: 3912.5181\n",
            "Epoch [40/150], Loss: 6472.0253\n",
            "Epoch [50/150], Loss: 4200.0426\n",
            "Epoch [60/150], Loss: 2865.9580\n",
            "Epoch [70/150], Loss: 1653.7080\n",
            "Epoch [80/150], Loss: 2020.7081\n",
            "Epoch [90/150], Loss: 4133.6888\n",
            "Epoch [100/150], Loss: 1198.2110\n",
            "Epoch [110/150], Loss: 1258.8902\n",
            "Epoch [120/150], Loss: 5089.5156\n",
            "Epoch [130/150], Loss: 1337.2573\n",
            "Epoch [140/150], Loss: 2835.3088\n",
            "Epoch [150/150], Loss: 1850.3557\n",
            "Fold 5, RMSE: 45.68364334106445\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 62.006501007080075\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14096.5774\n",
            "Epoch [20/100], Loss: 11641.7014\n",
            "Epoch [30/100], Loss: 4186.2676\n",
            "Epoch [40/100], Loss: 3813.6861\n",
            "Epoch [50/100], Loss: 3419.3579\n",
            "Epoch [60/100], Loss: 2302.0551\n",
            "Epoch [70/100], Loss: 2769.2791\n",
            "Epoch [80/100], Loss: 3405.0178\n",
            "Epoch [90/100], Loss: 1826.1671\n",
            "Epoch [100/100], Loss: 1951.6684\n",
            "Fold 1, RMSE: 61.95083999633789\n",
            "Epoch [10/100], Loss: 10183.7319\n",
            "Epoch [20/100], Loss: 6291.2495\n",
            "Epoch [30/100], Loss: 3528.6458\n",
            "Epoch [40/100], Loss: 2883.0874\n",
            "Epoch [50/100], Loss: 2876.2344\n",
            "Epoch [60/100], Loss: 3648.5621\n",
            "Epoch [70/100], Loss: 2149.5210\n",
            "Epoch [80/100], Loss: 3093.3587\n",
            "Epoch [90/100], Loss: 1802.7905\n",
            "Epoch [100/100], Loss: 2658.5275\n",
            "Fold 2, RMSE: 62.063079833984375\n",
            "Epoch [10/100], Loss: 7943.9102\n",
            "Epoch [20/100], Loss: 9907.1389\n",
            "Epoch [30/100], Loss: 4660.0898\n",
            "Epoch [40/100], Loss: 3630.8337\n",
            "Epoch [50/100], Loss: 1454.0319\n",
            "Epoch [60/100], Loss: 1799.2870\n",
            "Epoch [70/100], Loss: 1578.4482\n",
            "Epoch [80/100], Loss: 1609.0392\n",
            "Epoch [90/100], Loss: 1424.0847\n",
            "Epoch [100/100], Loss: 2974.9735\n",
            "Fold 3, RMSE: 96.28871154785156\n",
            "Epoch [10/100], Loss: 13128.1025\n",
            "Epoch [20/100], Loss: 11073.9871\n",
            "Epoch [30/100], Loss: 5002.7565\n",
            "Epoch [40/100], Loss: 3511.5010\n",
            "Epoch [50/100], Loss: 3445.4561\n",
            "Epoch [60/100], Loss: 3408.1294\n",
            "Epoch [70/100], Loss: 1868.4916\n",
            "Epoch [80/100], Loss: 2051.0373\n",
            "Epoch [90/100], Loss: 1730.3508\n",
            "Epoch [100/100], Loss: 2618.5114\n",
            "Fold 4, RMSE: 45.37215805053711\n",
            "Epoch [10/100], Loss: 14877.5649\n",
            "Epoch [20/100], Loss: 5502.2218\n",
            "Epoch [30/100], Loss: 2512.7497\n",
            "Epoch [40/100], Loss: 3326.4731\n",
            "Epoch [50/100], Loss: 3219.4135\n",
            "Epoch [60/100], Loss: 3228.1190\n",
            "Epoch [70/100], Loss: 1414.4863\n",
            "Epoch [80/100], Loss: 3928.3680\n",
            "Epoch [90/100], Loss: 1555.0322\n",
            "Epoch [100/100], Loss: 1732.9258\n",
            "Fold 5, RMSE: 47.39982604980469\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 62.614923095703126\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13795.1024\n",
            "Epoch [20/150], Loss: 6026.4104\n",
            "Epoch [30/150], Loss: 2474.4403\n",
            "Epoch [40/150], Loss: 3383.5867\n",
            "Epoch [50/150], Loss: 2336.4780\n",
            "Epoch [60/150], Loss: 1825.9096\n",
            "Epoch [70/150], Loss: 2042.1631\n",
            "Epoch [80/150], Loss: 902.4773\n",
            "Epoch [90/150], Loss: 3005.9246\n",
            "Epoch [100/150], Loss: 2667.2296\n",
            "Epoch [110/150], Loss: 3917.4886\n",
            "Epoch [120/150], Loss: 1856.5085\n",
            "Epoch [130/150], Loss: 1522.3039\n",
            "Epoch [140/150], Loss: 1753.3774\n",
            "Epoch [150/150], Loss: 954.6849\n",
            "Fold 1, RMSE: 56.09648132324219\n",
            "Epoch [10/150], Loss: 10895.7898\n",
            "Epoch [20/150], Loss: 6999.2930\n",
            "Epoch [30/150], Loss: 5730.1310\n",
            "Epoch [40/150], Loss: 4273.9484\n",
            "Epoch [50/150], Loss: 2013.7494\n",
            "Epoch [60/150], Loss: 2276.8437\n",
            "Epoch [70/150], Loss: 1475.0562\n",
            "Epoch [80/150], Loss: 1128.3695\n",
            "Epoch [90/150], Loss: 3362.1945\n",
            "Epoch [100/150], Loss: 1301.7780\n",
            "Epoch [110/150], Loss: 4099.2329\n",
            "Epoch [120/150], Loss: 1561.8876\n",
            "Epoch [130/150], Loss: 1466.2733\n",
            "Epoch [140/150], Loss: 1459.9123\n",
            "Epoch [150/150], Loss: 697.6386\n",
            "Fold 2, RMSE: 63.23677444458008\n",
            "Epoch [10/150], Loss: 8061.1138\n",
            "Epoch [20/150], Loss: 8258.5519\n",
            "Epoch [30/150], Loss: 5111.0055\n",
            "Epoch [40/150], Loss: 3946.9780\n",
            "Epoch [50/150], Loss: 2749.0757\n",
            "Epoch [60/150], Loss: 1265.5358\n",
            "Epoch [70/150], Loss: 1431.6415\n",
            "Epoch [80/150], Loss: 5714.8538\n",
            "Epoch [90/150], Loss: 1356.1378\n",
            "Epoch [100/150], Loss: 1095.1909\n",
            "Epoch [110/150], Loss: 3169.6035\n",
            "Epoch [120/150], Loss: 583.5368\n",
            "Epoch [130/150], Loss: 1853.2681\n",
            "Epoch [140/150], Loss: 959.5015\n",
            "Epoch [150/150], Loss: 1639.7065\n",
            "Fold 3, RMSE: 94.3088607788086\n",
            "Epoch [10/150], Loss: 12658.8254\n",
            "Epoch [20/150], Loss: 7205.0986\n",
            "Epoch [30/150], Loss: 6973.8452\n",
            "Epoch [40/150], Loss: 4331.4479\n",
            "Epoch [50/150], Loss: 8834.8857\n",
            "Epoch [60/150], Loss: 2973.7999\n",
            "Epoch [70/150], Loss: 2525.5387\n",
            "Epoch [80/150], Loss: 4818.4024\n",
            "Epoch [90/150], Loss: 2591.8267\n",
            "Epoch [100/150], Loss: 1065.2446\n",
            "Epoch [110/150], Loss: 829.3857\n",
            "Epoch [120/150], Loss: 2814.4724\n",
            "Epoch [130/150], Loss: 1883.0009\n",
            "Epoch [140/150], Loss: 1334.8554\n",
            "Epoch [150/150], Loss: 801.4052\n",
            "Fold 4, RMSE: 45.40107345581055\n",
            "Epoch [10/150], Loss: 12042.2925\n",
            "Epoch [20/150], Loss: 8161.6765\n",
            "Epoch [30/150], Loss: 2418.7780\n",
            "Epoch [40/150], Loss: 3621.2823\n",
            "Epoch [50/150], Loss: 3164.2472\n",
            "Epoch [60/150], Loss: 3128.7946\n",
            "Epoch [70/150], Loss: 2056.5190\n",
            "Epoch [80/150], Loss: 1887.9825\n",
            "Epoch [90/150], Loss: 832.5441\n",
            "Epoch [100/150], Loss: 1378.1487\n",
            "Epoch [110/150], Loss: 1298.0220\n",
            "Epoch [120/150], Loss: 1139.1836\n",
            "Epoch [130/150], Loss: 1591.7932\n",
            "Epoch [140/150], Loss: 1017.5963\n",
            "Epoch [150/150], Loss: 715.0524\n",
            "Fold 5, RMSE: 44.28525161743164\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 60.66568832397461\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 15005.5190\n",
            "Epoch [20/100], Loss: 16853.0923\n",
            "Epoch [30/100], Loss: 13374.7726\n",
            "Epoch [40/100], Loss: 7081.1082\n",
            "Epoch [50/100], Loss: 3809.8114\n",
            "Epoch [60/100], Loss: 4349.7914\n",
            "Epoch [70/100], Loss: 8996.6920\n",
            "Epoch [80/100], Loss: 4870.8369\n",
            "Epoch [90/100], Loss: 4536.9206\n",
            "Epoch [100/100], Loss: 3652.1151\n",
            "Fold 1, RMSE: 63.0147705078125\n",
            "Epoch [10/100], Loss: 21111.9312\n",
            "Epoch [20/100], Loss: 10710.4724\n",
            "Epoch [30/100], Loss: 5451.3531\n",
            "Epoch [40/100], Loss: 3280.4932\n",
            "Epoch [50/100], Loss: 1819.4250\n",
            "Epoch [60/100], Loss: 2197.8223\n",
            "Epoch [70/100], Loss: 3384.6730\n",
            "Epoch [80/100], Loss: 2084.4753\n",
            "Epoch [90/100], Loss: 1607.0085\n",
            "Epoch [100/100], Loss: 2386.5922\n",
            "Fold 2, RMSE: 58.60259246826172\n",
            "Epoch [10/100], Loss: 10909.4714\n",
            "Epoch [20/100], Loss: 9103.7714\n",
            "Epoch [30/100], Loss: 4258.4991\n",
            "Epoch [40/100], Loss: 4048.0955\n",
            "Epoch [50/100], Loss: 3759.0259\n",
            "Epoch [60/100], Loss: 1680.5873\n",
            "Epoch [70/100], Loss: 1293.6181\n",
            "Epoch [80/100], Loss: 949.1762\n",
            "Epoch [90/100], Loss: 1035.9629\n",
            "Epoch [100/100], Loss: 1115.5558\n",
            "Fold 3, RMSE: 92.83380126953125\n",
            "Epoch [10/100], Loss: 25504.5195\n",
            "Epoch [20/100], Loss: 17685.5212\n",
            "Epoch [30/100], Loss: 8691.3522\n",
            "Epoch [40/100], Loss: 5947.2632\n",
            "Epoch [50/100], Loss: 6166.0347\n",
            "Epoch [60/100], Loss: 2267.0012\n",
            "Epoch [70/100], Loss: 1713.8382\n",
            "Epoch [80/100], Loss: 1758.5801\n",
            "Epoch [90/100], Loss: 1655.3063\n",
            "Epoch [100/100], Loss: 1166.0239\n",
            "Fold 4, RMSE: 38.990699768066406\n",
            "Epoch [10/100], Loss: 10754.5197\n",
            "Epoch [20/100], Loss: 7151.2074\n",
            "Epoch [30/100], Loss: 4775.0780\n",
            "Epoch [40/100], Loss: 3299.3232\n",
            "Epoch [50/100], Loss: 2487.5607\n",
            "Epoch [60/100], Loss: 2550.2478\n",
            "Epoch [70/100], Loss: 2869.4114\n",
            "Epoch [80/100], Loss: 3262.8546\n",
            "Epoch [90/100], Loss: 3397.6006\n",
            "Epoch [100/100], Loss: 1853.7065\n",
            "Fold 5, RMSE: 47.32979202270508\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 60.15433120727539\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 13106.5784\n",
            "Epoch [20/150], Loss: 12437.5508\n",
            "Epoch [30/150], Loss: 11629.7700\n",
            "Epoch [40/150], Loss: 8890.5996\n",
            "Epoch [50/150], Loss: 5673.0226\n",
            "Epoch [60/150], Loss: 4069.3395\n",
            "Epoch [70/150], Loss: 7371.6641\n",
            "Epoch [80/150], Loss: 3352.8720\n",
            "Epoch [90/150], Loss: 1440.2537\n",
            "Epoch [100/150], Loss: 1500.6888\n",
            "Epoch [110/150], Loss: 2137.0987\n",
            "Epoch [120/150], Loss: 1495.9988\n",
            "Epoch [130/150], Loss: 7616.9879\n",
            "Epoch [140/150], Loss: 2360.3713\n",
            "Epoch [150/150], Loss: 1065.2221\n",
            "Fold 1, RMSE: 60.64929962158203\n",
            "Epoch [10/150], Loss: 21674.3711\n",
            "Epoch [20/150], Loss: 9276.5271\n",
            "Epoch [30/150], Loss: 5427.0485\n",
            "Epoch [40/150], Loss: 8122.0428\n",
            "Epoch [50/150], Loss: 4269.3868\n",
            "Epoch [60/150], Loss: 4880.5475\n",
            "Epoch [70/150], Loss: 2818.3954\n",
            "Epoch [80/150], Loss: 7346.6292\n",
            "Epoch [90/150], Loss: 2443.6270\n",
            "Epoch [100/150], Loss: 1273.4828\n",
            "Epoch [110/150], Loss: 4240.9254\n",
            "Epoch [120/150], Loss: 7940.4540\n",
            "Epoch [130/150], Loss: 6300.3912\n",
            "Epoch [140/150], Loss: 1933.3837\n",
            "Epoch [150/150], Loss: 2381.3248\n",
            "Fold 2, RMSE: 62.71109390258789\n",
            "Epoch [10/150], Loss: 7554.5748\n",
            "Epoch [20/150], Loss: 5137.0638\n",
            "Epoch [30/150], Loss: 7260.8269\n",
            "Epoch [40/150], Loss: 3894.3997\n",
            "Epoch [50/150], Loss: 3208.2787\n",
            "Epoch [60/150], Loss: 3601.5892\n",
            "Epoch [70/150], Loss: 2091.9746\n",
            "Epoch [80/150], Loss: 2974.2484\n",
            "Epoch [90/150], Loss: 3288.5206\n",
            "Epoch [100/150], Loss: 4417.5287\n",
            "Epoch [110/150], Loss: 2445.1304\n",
            "Epoch [120/150], Loss: 2671.4973\n",
            "Epoch [130/150], Loss: 2076.8037\n",
            "Epoch [140/150], Loss: 1770.3490\n",
            "Epoch [150/150], Loss: 1560.9270\n",
            "Fold 3, RMSE: 94.59490966796875\n",
            "Epoch [10/150], Loss: 10993.0846\n",
            "Epoch [20/150], Loss: 12211.0856\n",
            "Epoch [30/150], Loss: 4182.7554\n",
            "Epoch [40/150], Loss: 4143.1641\n",
            "Epoch [50/150], Loss: 3241.0007\n",
            "Epoch [60/150], Loss: 1446.5488\n",
            "Epoch [70/150], Loss: 2044.1516\n",
            "Epoch [80/150], Loss: 4273.4360\n",
            "Epoch [90/150], Loss: 2354.4164\n",
            "Epoch [100/150], Loss: 4102.7981\n",
            "Epoch [110/150], Loss: 7811.2003\n",
            "Epoch [120/150], Loss: 4062.0449\n",
            "Epoch [130/150], Loss: 6535.8625\n",
            "Epoch [140/150], Loss: 1070.9971\n",
            "Epoch [150/150], Loss: 2994.7053\n",
            "Fold 4, RMSE: 39.563262939453125\n",
            "Epoch [10/150], Loss: 14857.5723\n",
            "Epoch [20/150], Loss: 9859.2847\n",
            "Epoch [30/150], Loss: 9232.9729\n",
            "Epoch [40/150], Loss: 6091.0853\n",
            "Epoch [50/150], Loss: 3210.5632\n",
            "Epoch [60/150], Loss: 2496.0007\n",
            "Epoch [70/150], Loss: 1306.2119\n",
            "Epoch [80/150], Loss: 3356.6735\n",
            "Epoch [90/150], Loss: 972.2915\n",
            "Epoch [100/150], Loss: 1220.1890\n",
            "Epoch [110/150], Loss: 2938.4823\n",
            "Epoch [120/150], Loss: 1373.9418\n",
            "Epoch [130/150], Loss: 996.1980\n",
            "Epoch [140/150], Loss: 3048.7339\n",
            "Epoch [150/150], Loss: 4841.1286\n",
            "Fold 5, RMSE: 47.521156311035156\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 61.00794448852539\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 15206.7964\n",
            "Epoch [20/100], Loss: 7587.4921\n",
            "Epoch [30/100], Loss: 7598.0928\n",
            "Epoch [40/100], Loss: 4391.3433\n",
            "Epoch [50/100], Loss: 2775.7817\n",
            "Epoch [60/100], Loss: 2830.2441\n",
            "Epoch [70/100], Loss: 3537.2808\n",
            "Epoch [80/100], Loss: 1892.3565\n",
            "Epoch [90/100], Loss: 1883.6736\n",
            "Epoch [100/100], Loss: 990.7526\n",
            "Fold 1, RMSE: 60.43483352661133\n",
            "Epoch [10/100], Loss: 10323.3826\n",
            "Epoch [20/100], Loss: 11938.6354\n",
            "Epoch [30/100], Loss: 2721.7212\n",
            "Epoch [40/100], Loss: 3854.6207\n",
            "Epoch [50/100], Loss: 3447.2184\n",
            "Epoch [60/100], Loss: 1581.9574\n",
            "Epoch [70/100], Loss: 2057.3546\n",
            "Epoch [80/100], Loss: 1915.5561\n",
            "Epoch [90/100], Loss: 2334.9189\n",
            "Epoch [100/100], Loss: 2398.3720\n",
            "Fold 2, RMSE: 56.35065841674805\n",
            "Epoch [10/100], Loss: 9851.5994\n",
            "Epoch [20/100], Loss: 8513.8228\n",
            "Epoch [30/100], Loss: 5779.8295\n",
            "Epoch [40/100], Loss: 2697.0452\n",
            "Epoch [50/100], Loss: 2404.8843\n",
            "Epoch [60/100], Loss: 2139.8724\n",
            "Epoch [70/100], Loss: 1495.9584\n",
            "Epoch [80/100], Loss: 1068.9138\n",
            "Epoch [90/100], Loss: 1867.3766\n",
            "Epoch [100/100], Loss: 1264.8416\n",
            "Fold 3, RMSE: 95.30831146240234\n",
            "Epoch [10/100], Loss: 9403.8017\n",
            "Epoch [20/100], Loss: 9535.6865\n",
            "Epoch [30/100], Loss: 7005.7686\n",
            "Epoch [40/100], Loss: 5148.2274\n",
            "Epoch [50/100], Loss: 2664.3206\n",
            "Epoch [60/100], Loss: 3234.9572\n",
            "Epoch [70/100], Loss: 3646.5764\n",
            "Epoch [80/100], Loss: 955.5508\n",
            "Epoch [90/100], Loss: 3479.7172\n",
            "Epoch [100/100], Loss: 2316.6364\n",
            "Fold 4, RMSE: 44.997314453125\n",
            "Epoch [10/100], Loss: 16364.0142\n",
            "Epoch [20/100], Loss: 7885.7092\n",
            "Epoch [30/100], Loss: 7291.3185\n",
            "Epoch [40/100], Loss: 3795.1313\n",
            "Epoch [50/100], Loss: 4067.9676\n",
            "Epoch [60/100], Loss: 5234.2087\n",
            "Epoch [70/100], Loss: 2296.6143\n",
            "Epoch [80/100], Loss: 2946.0757\n",
            "Epoch [90/100], Loss: 4082.3058\n",
            "Epoch [100/100], Loss: 2304.0125\n",
            "Fold 5, RMSE: 46.75463104248047\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 60.76914978027344\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13914.7246\n",
            "Epoch [20/150], Loss: 9064.2493\n",
            "Epoch [30/150], Loss: 6685.9136\n",
            "Epoch [40/150], Loss: 4777.6102\n",
            "Epoch [50/150], Loss: 2113.4072\n",
            "Epoch [60/150], Loss: 4000.3550\n",
            "Epoch [70/150], Loss: 1590.1609\n",
            "Epoch [80/150], Loss: 1423.8553\n",
            "Epoch [90/150], Loss: 1416.8355\n",
            "Epoch [100/150], Loss: 1376.1014\n",
            "Epoch [110/150], Loss: 1851.5328\n",
            "Epoch [120/150], Loss: 1574.5290\n",
            "Epoch [130/150], Loss: 1051.0403\n",
            "Epoch [140/150], Loss: 1544.4551\n",
            "Epoch [150/150], Loss: 1528.1597\n",
            "Fold 1, RMSE: 57.362449645996094\n",
            "Epoch [10/150], Loss: 11723.8643\n",
            "Epoch [20/150], Loss: 6269.2787\n",
            "Epoch [30/150], Loss: 6137.5039\n",
            "Epoch [40/150], Loss: 3916.2883\n",
            "Epoch [50/150], Loss: 2711.9279\n",
            "Epoch [60/150], Loss: 2343.5036\n",
            "Epoch [70/150], Loss: 1709.7621\n",
            "Epoch [80/150], Loss: 4966.2029\n",
            "Epoch [90/150], Loss: 2601.8461\n",
            "Epoch [100/150], Loss: 2080.3488\n",
            "Epoch [110/150], Loss: 1860.2835\n",
            "Epoch [120/150], Loss: 1845.3132\n",
            "Epoch [130/150], Loss: 1902.0907\n",
            "Epoch [140/150], Loss: 863.7108\n",
            "Epoch [150/150], Loss: 1680.5894\n",
            "Fold 2, RMSE: 58.745540618896484\n",
            "Epoch [10/150], Loss: 13091.1074\n",
            "Epoch [20/150], Loss: 5793.0698\n",
            "Epoch [30/150], Loss: 2941.7023\n",
            "Epoch [40/150], Loss: 2198.3724\n",
            "Epoch [50/150], Loss: 2181.8817\n",
            "Epoch [60/150], Loss: 1372.4960\n",
            "Epoch [70/150], Loss: 1295.3089\n",
            "Epoch [80/150], Loss: 1469.7497\n",
            "Epoch [90/150], Loss: 597.2417\n",
            "Epoch [100/150], Loss: 1438.0627\n",
            "Epoch [110/150], Loss: 1450.7028\n",
            "Epoch [120/150], Loss: 776.1279\n",
            "Epoch [130/150], Loss: 877.9995\n",
            "Epoch [140/150], Loss: 1486.7613\n",
            "Epoch [150/150], Loss: 848.1074\n",
            "Fold 3, RMSE: 91.75190734863281\n",
            "Epoch [10/150], Loss: 13277.0486\n",
            "Epoch [20/150], Loss: 16544.5583\n",
            "Epoch [30/150], Loss: 3954.8874\n",
            "Epoch [40/150], Loss: 3208.8032\n",
            "Epoch [50/150], Loss: 2635.0103\n",
            "Epoch [60/150], Loss: 2142.3529\n",
            "Epoch [70/150], Loss: 2201.7875\n",
            "Epoch [80/150], Loss: 2660.0960\n",
            "Epoch [90/150], Loss: 2451.7740\n",
            "Epoch [100/150], Loss: 2129.0263\n",
            "Epoch [110/150], Loss: 1723.0532\n",
            "Epoch [120/150], Loss: 1216.4742\n",
            "Epoch [130/150], Loss: 2172.5452\n",
            "Epoch [140/150], Loss: 1676.0574\n",
            "Epoch [150/150], Loss: 1737.0214\n",
            "Fold 4, RMSE: 42.241275787353516\n",
            "Epoch [10/150], Loss: 12279.3181\n",
            "Epoch [20/150], Loss: 6752.2761\n",
            "Epoch [30/150], Loss: 3422.8010\n",
            "Epoch [40/150], Loss: 2798.2565\n",
            "Epoch [50/150], Loss: 2647.4799\n",
            "Epoch [60/150], Loss: 2610.9088\n",
            "Epoch [70/150], Loss: 1245.2308\n",
            "Epoch [80/150], Loss: 1056.0131\n",
            "Epoch [90/150], Loss: 943.0496\n",
            "Epoch [100/150], Loss: 2266.6360\n",
            "Epoch [110/150], Loss: 961.6648\n",
            "Epoch [120/150], Loss: 1230.6852\n",
            "Epoch [130/150], Loss: 1239.8548\n",
            "Epoch [140/150], Loss: 642.6684\n",
            "Epoch [150/150], Loss: 1165.9745\n",
            "Fold 5, RMSE: 47.434593200683594\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 59.5071533203125\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14383.7708\n",
            "Epoch [20/100], Loss: 12107.7117\n",
            "Epoch [30/100], Loss: 6095.5747\n",
            "Epoch [40/100], Loss: 4742.9644\n",
            "Epoch [50/100], Loss: 6668.2414\n",
            "Epoch [60/100], Loss: 6101.8229\n",
            "Epoch [70/100], Loss: 2269.9097\n",
            "Epoch [80/100], Loss: 2081.1024\n",
            "Epoch [90/100], Loss: 5884.9122\n",
            "Epoch [100/100], Loss: 3738.7275\n",
            "Fold 1, RMSE: 60.37729263305664\n",
            "Epoch [10/100], Loss: 19731.1279\n",
            "Epoch [20/100], Loss: 15720.7881\n",
            "Epoch [30/100], Loss: 8874.3771\n",
            "Epoch [40/100], Loss: 8525.4070\n",
            "Epoch [50/100], Loss: 7296.1829\n",
            "Epoch [60/100], Loss: 6194.2085\n",
            "Epoch [70/100], Loss: 2358.5792\n",
            "Epoch [80/100], Loss: 2060.2800\n",
            "Epoch [90/100], Loss: 1700.9217\n",
            "Epoch [100/100], Loss: 1874.4221\n",
            "Fold 2, RMSE: 70.40309143066406\n",
            "Epoch [10/100], Loss: 16093.4475\n",
            "Epoch [20/100], Loss: 9034.0969\n",
            "Epoch [30/100], Loss: 7836.9186\n",
            "Epoch [40/100], Loss: 6665.6670\n",
            "Epoch [50/100], Loss: 3605.5602\n",
            "Epoch [60/100], Loss: 4440.7788\n",
            "Epoch [70/100], Loss: 3218.2167\n",
            "Epoch [80/100], Loss: 2086.3939\n",
            "Epoch [90/100], Loss: 3118.2915\n",
            "Epoch [100/100], Loss: 2717.5015\n",
            "Fold 3, RMSE: 90.8694839477539\n",
            "Epoch [10/100], Loss: 22622.3965\n",
            "Epoch [20/100], Loss: 20835.4771\n",
            "Epoch [30/100], Loss: 16531.1946\n",
            "Epoch [40/100], Loss: 23148.1797\n",
            "Epoch [50/100], Loss: 17081.5822\n",
            "Epoch [60/100], Loss: 22564.8521\n",
            "Epoch [70/100], Loss: 27600.2129\n",
            "Epoch [80/100], Loss: 19266.3716\n",
            "Epoch [90/100], Loss: 23435.0615\n",
            "Epoch [100/100], Loss: 19449.0137\n",
            "Fold 4, RMSE: 54.35872268676758\n",
            "Epoch [10/100], Loss: 33277.2939\n",
            "Epoch [20/100], Loss: 12555.9219\n",
            "Epoch [30/100], Loss: 6423.8627\n",
            "Epoch [40/100], Loss: 5217.3478\n",
            "Epoch [50/100], Loss: 5559.8160\n",
            "Epoch [60/100], Loss: 4161.2715\n",
            "Epoch [70/100], Loss: 6887.5576\n",
            "Epoch [80/100], Loss: 4017.8820\n",
            "Epoch [90/100], Loss: 1516.6244\n",
            "Epoch [100/100], Loss: 2170.8093\n",
            "Fold 5, RMSE: 46.80790328979492\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 64.56329879760742\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19404.1577\n",
            "Epoch [20/150], Loss: 12689.8953\n",
            "Epoch [30/150], Loss: 13585.4797\n",
            "Epoch [40/150], Loss: 6816.6940\n",
            "Epoch [50/150], Loss: 8215.7126\n",
            "Epoch [60/150], Loss: 5036.5024\n",
            "Epoch [70/150], Loss: 1758.3952\n",
            "Epoch [80/150], Loss: 3135.0566\n",
            "Epoch [90/150], Loss: 4940.9517\n",
            "Epoch [100/150], Loss: 1725.1090\n",
            "Epoch [110/150], Loss: 2159.5064\n",
            "Epoch [120/150], Loss: 2109.5200\n",
            "Epoch [130/150], Loss: 1806.2880\n",
            "Epoch [140/150], Loss: 2319.4742\n",
            "Epoch [150/150], Loss: 5019.3454\n",
            "Fold 1, RMSE: 60.05522918701172\n",
            "Epoch [10/150], Loss: 23273.6421\n",
            "Epoch [20/150], Loss: 9566.2900\n",
            "Epoch [30/150], Loss: 9543.1687\n",
            "Epoch [40/150], Loss: 3576.8542\n",
            "Epoch [50/150], Loss: 3624.0936\n",
            "Epoch [60/150], Loss: 4063.1046\n",
            "Epoch [70/150], Loss: 3248.3858\n",
            "Epoch [80/150], Loss: 5079.1202\n",
            "Epoch [90/150], Loss: 3310.3486\n",
            "Epoch [100/150], Loss: 10151.3214\n",
            "Epoch [110/150], Loss: 6307.5187\n",
            "Epoch [120/150], Loss: 4390.8380\n",
            "Epoch [130/150], Loss: 3463.5766\n",
            "Epoch [140/150], Loss: 2596.9450\n",
            "Epoch [150/150], Loss: 4371.8681\n",
            "Fold 2, RMSE: 69.0055160522461\n",
            "Epoch [10/150], Loss: 33371.3159\n",
            "Epoch [20/150], Loss: 7016.0594\n",
            "Epoch [30/150], Loss: 6241.4476\n",
            "Epoch [40/150], Loss: 4059.0431\n",
            "Epoch [50/150], Loss: 2996.7798\n",
            "Epoch [60/150], Loss: 3076.1710\n",
            "Epoch [70/150], Loss: 2634.0862\n",
            "Epoch [80/150], Loss: 1895.0456\n",
            "Epoch [90/150], Loss: 2217.6284\n",
            "Epoch [100/150], Loss: 1572.9393\n",
            "Epoch [110/150], Loss: 1912.1725\n",
            "Epoch [120/150], Loss: 3670.6664\n",
            "Epoch [130/150], Loss: 2055.0411\n",
            "Epoch [140/150], Loss: 3159.5759\n",
            "Epoch [150/150], Loss: 2386.2720\n",
            "Fold 3, RMSE: 93.97676849365234\n",
            "Epoch [10/150], Loss: 21027.2622\n",
            "Epoch [20/150], Loss: 9133.6691\n",
            "Epoch [30/150], Loss: 13352.2710\n",
            "Epoch [40/150], Loss: 5322.7666\n",
            "Epoch [50/150], Loss: 5965.4203\n",
            "Epoch [60/150], Loss: 2880.5298\n",
            "Epoch [70/150], Loss: 2699.8932\n",
            "Epoch [80/150], Loss: 3321.5238\n",
            "Epoch [90/150], Loss: 1711.0909\n",
            "Epoch [100/150], Loss: 4769.8407\n",
            "Epoch [110/150], Loss: 5563.7983\n",
            "Epoch [120/150], Loss: 8835.4025\n",
            "Epoch [130/150], Loss: 3750.9130\n",
            "Epoch [140/150], Loss: 4059.9297\n",
            "Epoch [150/150], Loss: 1808.5452\n",
            "Fold 4, RMSE: 43.76515197753906\n",
            "Epoch [10/150], Loss: 16728.3394\n",
            "Epoch [20/150], Loss: 12258.9041\n",
            "Epoch [30/150], Loss: 11841.7406\n",
            "Epoch [40/150], Loss: 9661.6937\n",
            "Epoch [50/150], Loss: 5041.1476\n",
            "Epoch [60/150], Loss: 7185.4711\n",
            "Epoch [70/150], Loss: 3645.9773\n",
            "Epoch [80/150], Loss: 6492.2222\n",
            "Epoch [90/150], Loss: 8275.9379\n",
            "Epoch [100/150], Loss: 7317.3924\n",
            "Epoch [110/150], Loss: 4055.4112\n",
            "Epoch [120/150], Loss: 3041.9620\n",
            "Epoch [130/150], Loss: 6252.2793\n",
            "Epoch [140/150], Loss: 2615.3647\n",
            "Epoch [150/150], Loss: 12763.4937\n",
            "Fold 5, RMSE: 46.071773529052734\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 62.57488784790039\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 12553.9629\n",
            "Epoch [20/100], Loss: 10976.5874\n",
            "Epoch [30/100], Loss: 4478.2518\n",
            "Epoch [40/100], Loss: 3657.7176\n",
            "Epoch [50/100], Loss: 5024.3188\n",
            "Epoch [60/100], Loss: 3883.4036\n",
            "Epoch [70/100], Loss: 2256.2693\n",
            "Epoch [80/100], Loss: 1000.1808\n",
            "Epoch [90/100], Loss: 1490.3492\n",
            "Epoch [100/100], Loss: 787.2757\n",
            "Fold 1, RMSE: 57.55228042602539\n",
            "Epoch [10/100], Loss: 16825.7053\n",
            "Epoch [20/100], Loss: 9259.0522\n",
            "Epoch [30/100], Loss: 9813.8706\n",
            "Epoch [40/100], Loss: 6670.9874\n",
            "Epoch [50/100], Loss: 5008.6528\n",
            "Epoch [60/100], Loss: 2231.5958\n",
            "Epoch [70/100], Loss: 2157.3937\n",
            "Epoch [80/100], Loss: 3791.5303\n",
            "Epoch [90/100], Loss: 2151.1104\n",
            "Epoch [100/100], Loss: 1313.6301\n",
            "Fold 2, RMSE: 64.84333038330078\n",
            "Epoch [10/100], Loss: 9700.9661\n",
            "Epoch [20/100], Loss: 4170.0074\n",
            "Epoch [30/100], Loss: 4287.5521\n",
            "Epoch [40/100], Loss: 2352.8115\n",
            "Epoch [50/100], Loss: 3676.3608\n",
            "Epoch [60/100], Loss: 1783.1029\n",
            "Epoch [70/100], Loss: 946.4664\n",
            "Epoch [80/100], Loss: 902.6569\n",
            "Epoch [90/100], Loss: 903.7256\n",
            "Epoch [100/100], Loss: 1341.7345\n",
            "Fold 3, RMSE: 87.4507064819336\n",
            "Epoch [10/100], Loss: 11026.6410\n",
            "Epoch [20/100], Loss: 8815.4858\n",
            "Epoch [30/100], Loss: 5857.2430\n",
            "Epoch [40/100], Loss: 4406.5311\n",
            "Epoch [50/100], Loss: 6875.2279\n",
            "Epoch [60/100], Loss: 2971.1501\n",
            "Epoch [70/100], Loss: 2701.1300\n",
            "Epoch [80/100], Loss: 1150.0695\n",
            "Epoch [90/100], Loss: 2262.0088\n",
            "Epoch [100/100], Loss: 1109.3838\n",
            "Fold 4, RMSE: 41.230281829833984\n",
            "Epoch [10/100], Loss: 10210.7794\n",
            "Epoch [20/100], Loss: 7296.8131\n",
            "Epoch [30/100], Loss: 4179.3663\n",
            "Epoch [40/100], Loss: 5545.2427\n",
            "Epoch [50/100], Loss: 3117.7706\n",
            "Epoch [60/100], Loss: 1896.3666\n",
            "Epoch [70/100], Loss: 3235.1388\n",
            "Epoch [80/100], Loss: 2295.3851\n",
            "Epoch [90/100], Loss: 1229.0739\n",
            "Epoch [100/100], Loss: 878.6204\n",
            "Fold 5, RMSE: 44.59027099609375\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 59.1333740234375\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15628.4316\n",
            "Epoch [20/150], Loss: 11680.5692\n",
            "Epoch [30/150], Loss: 6022.6724\n",
            "Epoch [40/150], Loss: 6661.6592\n",
            "Epoch [50/150], Loss: 3837.7961\n",
            "Epoch [60/150], Loss: 3790.5231\n",
            "Epoch [70/150], Loss: 1219.4444\n",
            "Epoch [80/150], Loss: 2287.6971\n",
            "Epoch [90/150], Loss: 1762.3141\n",
            "Epoch [100/150], Loss: 1416.6909\n",
            "Epoch [110/150], Loss: 2815.3927\n",
            "Epoch [120/150], Loss: 1350.0367\n",
            "Epoch [130/150], Loss: 1151.9665\n",
            "Epoch [140/150], Loss: 1313.7828\n",
            "Epoch [150/150], Loss: 1024.9925\n",
            "Fold 1, RMSE: 55.93918991088867\n",
            "Epoch [10/150], Loss: 11905.0527\n",
            "Epoch [20/150], Loss: 7377.7076\n",
            "Epoch [30/150], Loss: 4878.1425\n",
            "Epoch [40/150], Loss: 9688.8203\n",
            "Epoch [50/150], Loss: 4723.8742\n",
            "Epoch [60/150], Loss: 3004.5005\n",
            "Epoch [70/150], Loss: 3198.4230\n",
            "Epoch [80/150], Loss: 7182.1517\n",
            "Epoch [90/150], Loss: 1540.9409\n",
            "Epoch [100/150], Loss: 3645.1706\n",
            "Epoch [110/150], Loss: 2705.3958\n",
            "Epoch [120/150], Loss: 5091.1332\n",
            "Epoch [130/150], Loss: 2060.1337\n",
            "Epoch [140/150], Loss: 2836.1268\n",
            "Epoch [150/150], Loss: 4972.1198\n",
            "Fold 2, RMSE: 64.34358215332031\n",
            "Epoch [10/150], Loss: 7875.6362\n",
            "Epoch [20/150], Loss: 5382.1329\n",
            "Epoch [30/150], Loss: 3758.6105\n",
            "Epoch [40/150], Loss: 3045.8123\n",
            "Epoch [50/150], Loss: 5391.2886\n",
            "Epoch [60/150], Loss: 3219.7736\n",
            "Epoch [70/150], Loss: 2914.1266\n",
            "Epoch [80/150], Loss: 2650.6175\n",
            "Epoch [90/150], Loss: 3512.8226\n",
            "Epoch [100/150], Loss: 948.2047\n",
            "Epoch [110/150], Loss: 3118.5487\n",
            "Epoch [120/150], Loss: 2793.9536\n",
            "Epoch [130/150], Loss: 2383.5333\n",
            "Epoch [140/150], Loss: 2772.9797\n",
            "Epoch [150/150], Loss: 1394.0264\n",
            "Fold 3, RMSE: 94.51730346679688\n",
            "Epoch [10/150], Loss: 14424.2612\n",
            "Epoch [20/150], Loss: 8318.0337\n",
            "Epoch [30/150], Loss: 5361.5400\n",
            "Epoch [40/150], Loss: 8173.6567\n",
            "Epoch [50/150], Loss: 4571.5433\n",
            "Epoch [60/150], Loss: 3060.1702\n",
            "Epoch [70/150], Loss: 2325.9407\n",
            "Epoch [80/150], Loss: 1891.7858\n",
            "Epoch [90/150], Loss: 2472.9155\n",
            "Epoch [100/150], Loss: 4155.0345\n",
            "Epoch [110/150], Loss: 4935.7449\n",
            "Epoch [120/150], Loss: 3672.4362\n",
            "Epoch [130/150], Loss: 2768.2209\n",
            "Epoch [140/150], Loss: 617.3138\n",
            "Epoch [150/150], Loss: 682.4951\n",
            "Fold 4, RMSE: 40.54863739013672\n",
            "Epoch [10/150], Loss: 9512.6737\n",
            "Epoch [20/150], Loss: 10361.5100\n",
            "Epoch [30/150], Loss: 5167.2198\n",
            "Epoch [40/150], Loss: 3980.9376\n",
            "Epoch [50/150], Loss: 5472.4214\n",
            "Epoch [60/150], Loss: 2308.8617\n",
            "Epoch [70/150], Loss: 2047.5545\n",
            "Epoch [80/150], Loss: 1750.7130\n",
            "Epoch [90/150], Loss: 2961.8841\n",
            "Epoch [100/150], Loss: 1506.7087\n",
            "Epoch [110/150], Loss: 1105.7914\n",
            "Epoch [120/150], Loss: 1306.9485\n",
            "Epoch [130/150], Loss: 1638.4788\n",
            "Epoch [140/150], Loss: 1201.0428\n",
            "Epoch [150/150], Loss: 2037.0550\n",
            "Fold 5, RMSE: 47.352882385253906\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 60.5403190612793\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17001.5674\n",
            "Epoch [20/100], Loss: 17304.9009\n",
            "Epoch [30/100], Loss: 6641.0645\n",
            "Epoch [40/100], Loss: 4021.9175\n",
            "Epoch [50/100], Loss: 3800.2446\n",
            "Epoch [60/100], Loss: 3364.1597\n",
            "Epoch [70/100], Loss: 5092.9225\n",
            "Epoch [80/100], Loss: 3493.4579\n",
            "Epoch [90/100], Loss: 3233.1249\n",
            "Epoch [100/100], Loss: 2050.9321\n",
            "Fold 1, RMSE: 66.16806030273438\n",
            "Epoch [10/100], Loss: 27846.9282\n",
            "Epoch [20/100], Loss: 9921.4199\n",
            "Epoch [30/100], Loss: 14495.5167\n",
            "Epoch [40/100], Loss: 7555.1707\n",
            "Epoch [50/100], Loss: 2419.6544\n",
            "Epoch [60/100], Loss: 2303.2509\n",
            "Epoch [70/100], Loss: 4463.0896\n",
            "Epoch [80/100], Loss: 2989.7853\n",
            "Epoch [90/100], Loss: 3244.7105\n",
            "Epoch [100/100], Loss: 3032.0306\n",
            "Fold 2, RMSE: 70.25374603271484\n",
            "Epoch [10/100], Loss: 35495.1992\n",
            "Epoch [20/100], Loss: 7589.5388\n",
            "Epoch [30/100], Loss: 8412.3878\n",
            "Epoch [40/100], Loss: 7382.7783\n",
            "Epoch [50/100], Loss: 3270.6455\n",
            "Epoch [60/100], Loss: 4571.4788\n",
            "Epoch [70/100], Loss: 2853.3828\n",
            "Epoch [80/100], Loss: 2396.8151\n",
            "Epoch [90/100], Loss: 2651.4930\n",
            "Epoch [100/100], Loss: 4627.9468\n",
            "Fold 3, RMSE: 91.44137573242188\n",
            "Epoch [10/100], Loss: 14504.7075\n",
            "Epoch [20/100], Loss: 12366.7844\n",
            "Epoch [30/100], Loss: 13307.0696\n",
            "Epoch [40/100], Loss: 6417.4774\n",
            "Epoch [50/100], Loss: 4895.1880\n",
            "Epoch [60/100], Loss: 2330.1409\n",
            "Epoch [70/100], Loss: 1921.9145\n",
            "Epoch [80/100], Loss: 3292.7932\n",
            "Epoch [90/100], Loss: 4733.1932\n",
            "Epoch [100/100], Loss: 2230.8617\n",
            "Fold 4, RMSE: 41.080291748046875\n",
            "Epoch [10/100], Loss: 37719.6104\n",
            "Epoch [20/100], Loss: 9044.5477\n",
            "Epoch [30/100], Loss: 9360.0886\n",
            "Epoch [40/100], Loss: 8117.2548\n",
            "Epoch [50/100], Loss: 6559.4375\n",
            "Epoch [60/100], Loss: 5010.1802\n",
            "Epoch [70/100], Loss: 2550.8266\n",
            "Epoch [80/100], Loss: 6474.1514\n",
            "Epoch [90/100], Loss: 5234.4181\n",
            "Epoch [100/100], Loss: 1849.8689\n",
            "Fold 5, RMSE: 45.36465072631836\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 62.86162490844727\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16148.9360\n",
            "Epoch [20/150], Loss: 15689.3335\n",
            "Epoch [30/150], Loss: 9134.6337\n",
            "Epoch [40/150], Loss: 5975.9868\n",
            "Epoch [50/150], Loss: 4548.5625\n",
            "Epoch [60/150], Loss: 3662.8575\n",
            "Epoch [70/150], Loss: 4633.1579\n",
            "Epoch [80/150], Loss: 5753.5787\n",
            "Epoch [90/150], Loss: 3761.0146\n",
            "Epoch [100/150], Loss: 4760.6107\n",
            "Epoch [110/150], Loss: 3844.3185\n",
            "Epoch [120/150], Loss: 4806.5614\n",
            "Epoch [130/150], Loss: 4015.8171\n",
            "Epoch [140/150], Loss: 1578.5090\n",
            "Epoch [150/150], Loss: 912.5626\n",
            "Fold 1, RMSE: 61.48984146118164\n",
            "Epoch [10/150], Loss: 29967.4844\n",
            "Epoch [20/150], Loss: 12380.8823\n",
            "Epoch [30/150], Loss: 12256.8451\n",
            "Epoch [40/150], Loss: 7544.5508\n",
            "Epoch [50/150], Loss: 7583.8176\n",
            "Epoch [60/150], Loss: 3693.0447\n",
            "Epoch [70/150], Loss: 3695.4786\n",
            "Epoch [80/150], Loss: 4004.9452\n",
            "Epoch [90/150], Loss: 7545.2153\n",
            "Epoch [100/150], Loss: 3529.5865\n",
            "Epoch [110/150], Loss: 2953.5877\n",
            "Epoch [120/150], Loss: 3866.9019\n",
            "Epoch [130/150], Loss: 3306.8736\n",
            "Epoch [140/150], Loss: 6327.9225\n",
            "Epoch [150/150], Loss: 1339.8208\n",
            "Fold 2, RMSE: 63.581459045410156\n",
            "Epoch [10/150], Loss: 15822.0186\n",
            "Epoch [20/150], Loss: 8380.3220\n",
            "Epoch [30/150], Loss: 3365.8209\n",
            "Epoch [40/150], Loss: 2398.7726\n",
            "Epoch [50/150], Loss: 4542.2951\n",
            "Epoch [60/150], Loss: 5764.1757\n",
            "Epoch [70/150], Loss: 3135.9608\n",
            "Epoch [80/150], Loss: 1907.0261\n",
            "Epoch [90/150], Loss: 5445.2285\n",
            "Epoch [100/150], Loss: 1446.4389\n",
            "Epoch [110/150], Loss: 4828.1915\n",
            "Epoch [120/150], Loss: 3375.8932\n",
            "Epoch [130/150], Loss: 3273.7549\n",
            "Epoch [140/150], Loss: 2726.9620\n",
            "Epoch [150/150], Loss: 5337.4510\n",
            "Fold 3, RMSE: 90.57601928710938\n",
            "Epoch [10/150], Loss: 15957.9203\n",
            "Epoch [20/150], Loss: 14487.7480\n",
            "Epoch [30/150], Loss: 13812.8528\n",
            "Epoch [40/150], Loss: 7181.5439\n",
            "Epoch [50/150], Loss: 13964.9343\n",
            "Epoch [60/150], Loss: 4436.1294\n",
            "Epoch [70/150], Loss: 3426.2207\n",
            "Epoch [80/150], Loss: 8860.5547\n",
            "Epoch [90/150], Loss: 5977.1936\n",
            "Epoch [100/150], Loss: 2167.3046\n",
            "Epoch [110/150], Loss: 5385.6910\n",
            "Epoch [120/150], Loss: 3177.8361\n",
            "Epoch [130/150], Loss: 4654.5167\n",
            "Epoch [140/150], Loss: 4227.1940\n",
            "Epoch [150/150], Loss: 7460.8904\n",
            "Fold 4, RMSE: 47.65766525268555\n",
            "Epoch [10/150], Loss: 20557.5503\n",
            "Epoch [20/150], Loss: 14338.1289\n",
            "Epoch [30/150], Loss: 10095.9363\n",
            "Epoch [40/150], Loss: 8677.4039\n",
            "Epoch [50/150], Loss: 7984.8177\n",
            "Epoch [60/150], Loss: 9115.2626\n",
            "Epoch [70/150], Loss: 5364.4521\n",
            "Epoch [80/150], Loss: 7819.7642\n",
            "Epoch [90/150], Loss: 4067.5428\n",
            "Epoch [100/150], Loss: 2764.7833\n",
            "Epoch [110/150], Loss: 4554.4826\n",
            "Epoch [120/150], Loss: 1791.6852\n",
            "Epoch [130/150], Loss: 2822.0988\n",
            "Epoch [140/150], Loss: 1458.7330\n",
            "Epoch [150/150], Loss: 3560.1105\n",
            "Fold 5, RMSE: 48.56693649291992\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 62.37438430786133\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 23886.0845\n",
            "Epoch [20/100], Loss: 20650.5459\n",
            "Epoch [30/100], Loss: 8083.5410\n",
            "Epoch [40/100], Loss: 5783.8840\n",
            "Epoch [50/100], Loss: 4623.3416\n",
            "Epoch [60/100], Loss: 1586.1606\n",
            "Epoch [70/100], Loss: 3777.8504\n",
            "Epoch [80/100], Loss: 2048.9310\n",
            "Epoch [90/100], Loss: 2405.9814\n",
            "Epoch [100/100], Loss: 929.9591\n",
            "Fold 1, RMSE: 56.67487716674805\n",
            "Epoch [10/100], Loss: 10138.3986\n",
            "Epoch [20/100], Loss: 23550.0850\n",
            "Epoch [30/100], Loss: 9266.1577\n",
            "Epoch [40/100], Loss: 3111.4155\n",
            "Epoch [50/100], Loss: 2110.4361\n",
            "Epoch [60/100], Loss: 4700.8784\n",
            "Epoch [70/100], Loss: 1345.1090\n",
            "Epoch [80/100], Loss: 2408.1896\n",
            "Epoch [90/100], Loss: 2821.4583\n",
            "Epoch [100/100], Loss: 3477.9122\n",
            "Fold 2, RMSE: 63.38740539550781\n",
            "Epoch [10/100], Loss: 8117.2371\n",
            "Epoch [20/100], Loss: 6697.7330\n",
            "Epoch [30/100], Loss: 5635.8477\n",
            "Epoch [40/100], Loss: 4758.0370\n",
            "Epoch [50/100], Loss: 3130.8671\n",
            "Epoch [60/100], Loss: 2450.8666\n",
            "Epoch [70/100], Loss: 2657.4882\n",
            "Epoch [80/100], Loss: 3256.1061\n",
            "Epoch [90/100], Loss: 2155.1839\n",
            "Epoch [100/100], Loss: 2156.1517\n",
            "Fold 3, RMSE: 86.83418273925781\n",
            "Epoch [10/100], Loss: 12313.6152\n",
            "Epoch [20/100], Loss: 12915.4181\n",
            "Epoch [30/100], Loss: 6102.8462\n",
            "Epoch [40/100], Loss: 4600.5725\n",
            "Epoch [50/100], Loss: 5211.2496\n",
            "Epoch [60/100], Loss: 4889.8287\n",
            "Epoch [70/100], Loss: 2465.2923\n",
            "Epoch [80/100], Loss: 2346.3250\n",
            "Epoch [90/100], Loss: 1137.4319\n",
            "Epoch [100/100], Loss: 998.3992\n",
            "Fold 4, RMSE: 40.47011947631836\n",
            "Epoch [10/100], Loss: 13287.7251\n",
            "Epoch [20/100], Loss: 8458.6923\n",
            "Epoch [30/100], Loss: 9362.4016\n",
            "Epoch [40/100], Loss: 7871.6088\n",
            "Epoch [50/100], Loss: 3442.8423\n",
            "Epoch [60/100], Loss: 1237.3004\n",
            "Epoch [70/100], Loss: 1163.9756\n",
            "Epoch [80/100], Loss: 1841.9923\n",
            "Epoch [90/100], Loss: 2931.4632\n",
            "Epoch [100/100], Loss: 3325.7514\n",
            "Fold 5, RMSE: 46.455177307128906\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 58.76435241699219\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 12763.4460\n",
            "Epoch [20/150], Loss: 8037.9343\n",
            "Epoch [30/150], Loss: 8047.3750\n",
            "Epoch [40/150], Loss: 2957.5073\n",
            "Epoch [50/150], Loss: 4265.5932\n",
            "Epoch [60/150], Loss: 3768.2936\n",
            "Epoch [70/150], Loss: 3413.3326\n",
            "Epoch [80/150], Loss: 954.7926\n",
            "Epoch [90/150], Loss: 1816.9778\n",
            "Epoch [100/150], Loss: 2287.6789\n",
            "Epoch [110/150], Loss: 1685.6344\n",
            "Epoch [120/150], Loss: 1747.3361\n",
            "Epoch [130/150], Loss: 1743.0292\n",
            "Epoch [140/150], Loss: 1520.4937\n",
            "Epoch [150/150], Loss: 2349.7314\n",
            "Fold 1, RMSE: 60.90732955932617\n",
            "Epoch [10/150], Loss: 12081.8647\n",
            "Epoch [20/150], Loss: 8583.9066\n",
            "Epoch [30/150], Loss: 6978.0021\n",
            "Epoch [40/150], Loss: 5363.9171\n",
            "Epoch [50/150], Loss: 2372.6061\n",
            "Epoch [60/150], Loss: 3198.8482\n",
            "Epoch [70/150], Loss: 1692.7412\n",
            "Epoch [80/150], Loss: 2450.4801\n",
            "Epoch [90/150], Loss: 2720.3562\n",
            "Epoch [100/150], Loss: 2303.6472\n",
            "Epoch [110/150], Loss: 1260.7009\n",
            "Epoch [120/150], Loss: 752.8624\n",
            "Epoch [130/150], Loss: 2033.8176\n",
            "Epoch [140/150], Loss: 2863.1083\n",
            "Epoch [150/150], Loss: 1247.9250\n",
            "Fold 2, RMSE: 69.3443603515625\n",
            "Epoch [10/150], Loss: 9915.3882\n",
            "Epoch [20/150], Loss: 6375.9076\n",
            "Epoch [30/150], Loss: 3410.4233\n",
            "Epoch [40/150], Loss: 3930.8669\n",
            "Epoch [50/150], Loss: 4115.4055\n",
            "Epoch [60/150], Loss: 1065.2672\n",
            "Epoch [70/150], Loss: 1606.0851\n",
            "Epoch [80/150], Loss: 1907.3217\n",
            "Epoch [90/150], Loss: 2555.3028\n",
            "Epoch [100/150], Loss: 661.7905\n",
            "Epoch [110/150], Loss: 976.6693\n",
            "Epoch [120/150], Loss: 1054.6952\n",
            "Epoch [130/150], Loss: 867.4957\n",
            "Epoch [140/150], Loss: 1317.9790\n",
            "Epoch [150/150], Loss: 1560.2455\n",
            "Fold 3, RMSE: 92.12814331054688\n",
            "Epoch [10/150], Loss: 12681.1704\n",
            "Epoch [20/150], Loss: 11533.1917\n",
            "Epoch [30/150], Loss: 5282.8885\n",
            "Epoch [40/150], Loss: 4367.3945\n",
            "Epoch [50/150], Loss: 3786.7919\n",
            "Epoch [60/150], Loss: 6242.0503\n",
            "Epoch [70/150], Loss: 2984.0189\n",
            "Epoch [80/150], Loss: 2615.7203\n",
            "Epoch [90/150], Loss: 2828.4117\n",
            "Epoch [100/150], Loss: 1426.6839\n",
            "Epoch [110/150], Loss: 1757.5622\n",
            "Epoch [120/150], Loss: 10792.2235\n",
            "Epoch [130/150], Loss: 4973.9146\n",
            "Epoch [140/150], Loss: 2467.5643\n",
            "Epoch [150/150], Loss: 542.5759\n",
            "Fold 4, RMSE: 41.45536422729492\n",
            "Epoch [10/150], Loss: 11011.5168\n",
            "Epoch [20/150], Loss: 6030.2325\n",
            "Epoch [30/150], Loss: 6739.3389\n",
            "Epoch [40/150], Loss: 2605.2397\n",
            "Epoch [50/150], Loss: 3670.3159\n",
            "Epoch [60/150], Loss: 4404.6902\n",
            "Epoch [70/150], Loss: 2399.5522\n",
            "Epoch [80/150], Loss: 2244.5329\n",
            "Epoch [90/150], Loss: 3757.4034\n",
            "Epoch [100/150], Loss: 2269.8826\n",
            "Epoch [110/150], Loss: 959.7027\n",
            "Epoch [120/150], Loss: 5298.9069\n",
            "Epoch [130/150], Loss: 4531.4166\n",
            "Epoch [140/150], Loss: 422.7721\n",
            "Epoch [150/150], Loss: 939.8046\n",
            "Fold 5, RMSE: 44.104331970214844\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 61.58790588378906\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 16306.9580\n",
            "Epoch [20/100], Loss: 12995.9136\n",
            "Epoch [30/100], Loss: 6802.6671\n",
            "Epoch [40/100], Loss: 6599.4890\n",
            "Epoch [50/100], Loss: 3633.1873\n",
            "Epoch [60/100], Loss: 7104.3309\n",
            "Epoch [70/100], Loss: 1689.1964\n",
            "Epoch [80/100], Loss: 3477.8815\n",
            "Epoch [90/100], Loss: 1504.7664\n",
            "Epoch [100/100], Loss: 3044.4796\n",
            "Fold 1, RMSE: 53.928924560546875\n",
            "Epoch [10/100], Loss: 29809.8936\n",
            "Epoch [20/100], Loss: 14502.2947\n",
            "Epoch [30/100], Loss: 9895.3085\n",
            "Epoch [40/100], Loss: 6140.9031\n",
            "Epoch [50/100], Loss: 4165.1617\n",
            "Epoch [60/100], Loss: 5242.6072\n",
            "Epoch [70/100], Loss: 3318.6214\n",
            "Epoch [80/100], Loss: 1791.9865\n",
            "Epoch [90/100], Loss: 1381.7782\n",
            "Epoch [100/100], Loss: 2054.4113\n",
            "Fold 2, RMSE: 61.901546478271484\n",
            "Epoch [10/100], Loss: 15579.8933\n",
            "Epoch [20/100], Loss: 8836.5990\n",
            "Epoch [30/100], Loss: 4514.2225\n",
            "Epoch [40/100], Loss: 3694.9102\n",
            "Epoch [50/100], Loss: 2295.4261\n",
            "Epoch [60/100], Loss: 2898.3649\n",
            "Epoch [70/100], Loss: 2106.3977\n",
            "Epoch [80/100], Loss: 2412.6100\n",
            "Epoch [90/100], Loss: 5948.6439\n",
            "Epoch [100/100], Loss: 4038.7070\n",
            "Fold 3, RMSE: 87.1522445678711\n",
            "Epoch [10/100], Loss: 18912.9478\n",
            "Epoch [20/100], Loss: 9546.5265\n",
            "Epoch [30/100], Loss: 8575.2351\n",
            "Epoch [40/100], Loss: 5625.1285\n",
            "Epoch [50/100], Loss: 5648.9296\n",
            "Epoch [60/100], Loss: 3076.9089\n",
            "Epoch [70/100], Loss: 3701.2574\n",
            "Epoch [80/100], Loss: 3143.4195\n",
            "Epoch [90/100], Loss: 2351.8949\n",
            "Epoch [100/100], Loss: 2821.8038\n",
            "Fold 4, RMSE: 36.88494873046875\n",
            "Epoch [10/100], Loss: 21016.2061\n",
            "Epoch [20/100], Loss: 11057.8203\n",
            "Epoch [30/100], Loss: 9454.0000\n",
            "Epoch [40/100], Loss: 8861.6544\n",
            "Epoch [50/100], Loss: 4605.0669\n",
            "Epoch [60/100], Loss: 5742.5886\n",
            "Epoch [70/100], Loss: 3787.8011\n",
            "Epoch [80/100], Loss: 2726.5969\n",
            "Epoch [90/100], Loss: 13797.3117\n",
            "Epoch [100/100], Loss: 4579.5740\n",
            "Fold 5, RMSE: 47.15542221069336\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 57.40461730957031\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 24329.7759\n",
            "Epoch [20/150], Loss: 10951.5125\n",
            "Epoch [30/150], Loss: 10219.6790\n",
            "Epoch [40/150], Loss: 8141.4562\n",
            "Epoch [50/150], Loss: 5316.0472\n",
            "Epoch [60/150], Loss: 7216.5264\n",
            "Epoch [70/150], Loss: 3911.5021\n",
            "Epoch [80/150], Loss: 5956.2886\n",
            "Epoch [90/150], Loss: 2173.2724\n",
            "Epoch [100/150], Loss: 4270.2314\n",
            "Epoch [110/150], Loss: 4698.2687\n",
            "Epoch [120/150], Loss: 1785.2817\n",
            "Epoch [130/150], Loss: 5979.6862\n",
            "Epoch [140/150], Loss: 983.8916\n",
            "Epoch [150/150], Loss: 1391.1062\n",
            "Fold 1, RMSE: 62.742347717285156\n",
            "Epoch [10/150], Loss: 58475.3340\n",
            "Epoch [20/150], Loss: 13408.4180\n",
            "Epoch [30/150], Loss: 9886.8467\n",
            "Epoch [40/150], Loss: 11612.0457\n",
            "Epoch [50/150], Loss: 14001.2510\n",
            "Epoch [60/150], Loss: 10380.0546\n",
            "Epoch [70/150], Loss: 6970.9987\n",
            "Epoch [80/150], Loss: 11333.2592\n",
            "Epoch [90/150], Loss: 4225.2274\n",
            "Epoch [100/150], Loss: 4176.1416\n",
            "Epoch [110/150], Loss: 2436.1891\n",
            "Epoch [120/150], Loss: 1292.3998\n",
            "Epoch [130/150], Loss: 7579.3286\n",
            "Epoch [140/150], Loss: 14040.3834\n",
            "Epoch [150/150], Loss: 5793.8000\n",
            "Fold 2, RMSE: 68.3650131225586\n",
            "Epoch [10/150], Loss: 45622.5942\n",
            "Epoch [20/150], Loss: 9167.1318\n",
            "Epoch [30/150], Loss: 6586.0046\n",
            "Epoch [40/150], Loss: 3827.9981\n",
            "Epoch [50/150], Loss: 8869.1796\n",
            "Epoch [60/150], Loss: 2498.6160\n",
            "Epoch [70/150], Loss: 3987.6193\n",
            "Epoch [80/150], Loss: 3257.9464\n",
            "Epoch [90/150], Loss: 1668.5000\n",
            "Epoch [100/150], Loss: 1256.4650\n",
            "Epoch [110/150], Loss: 1330.6774\n",
            "Epoch [120/150], Loss: 7536.9716\n",
            "Epoch [130/150], Loss: 2414.1045\n",
            "Epoch [140/150], Loss: 2218.8962\n",
            "Epoch [150/150], Loss: 2052.1895\n",
            "Fold 3, RMSE: 95.2469253540039\n",
            "Epoch [10/150], Loss: 16544.1870\n",
            "Epoch [20/150], Loss: 11158.5559\n",
            "Epoch [30/150], Loss: 11469.7289\n",
            "Epoch [40/150], Loss: 7615.3015\n",
            "Epoch [50/150], Loss: 7032.3492\n",
            "Epoch [60/150], Loss: 8309.2072\n",
            "Epoch [70/150], Loss: 7385.4398\n",
            "Epoch [80/150], Loss: 5602.0859\n",
            "Epoch [90/150], Loss: 2513.2218\n",
            "Epoch [100/150], Loss: 3801.9510\n",
            "Epoch [110/150], Loss: 4441.6301\n",
            "Epoch [120/150], Loss: 2217.9391\n",
            "Epoch [130/150], Loss: 4629.9480\n",
            "Epoch [140/150], Loss: 2099.4063\n",
            "Epoch [150/150], Loss: 3532.2816\n",
            "Fold 4, RMSE: 39.56100845336914\n",
            "Epoch [10/150], Loss: 29725.5405\n",
            "Epoch [20/150], Loss: 8887.2872\n",
            "Epoch [30/150], Loss: 8609.0993\n",
            "Epoch [40/150], Loss: 5333.3373\n",
            "Epoch [50/150], Loss: 5512.1776\n",
            "Epoch [60/150], Loss: 3599.3196\n",
            "Epoch [70/150], Loss: 1927.4964\n",
            "Epoch [80/150], Loss: 3577.7089\n",
            "Epoch [90/150], Loss: 2111.4513\n",
            "Epoch [100/150], Loss: 3276.2740\n",
            "Epoch [110/150], Loss: 1298.0974\n",
            "Epoch [120/150], Loss: 2378.3398\n",
            "Epoch [130/150], Loss: 2806.1197\n",
            "Epoch [140/150], Loss: 5691.9586\n",
            "Epoch [150/150], Loss: 2416.1613\n",
            "Fold 5, RMSE: 48.687232971191406\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 62.92050552368164\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 21350.6680\n",
            "Epoch [20/100], Loss: 8925.7695\n",
            "Epoch [30/100], Loss: 6898.7277\n",
            "Epoch [40/100], Loss: 4590.3042\n",
            "Epoch [50/100], Loss: 3566.1553\n",
            "Epoch [60/100], Loss: 2142.2090\n",
            "Epoch [70/100], Loss: 2283.8868\n",
            "Epoch [80/100], Loss: 2139.2495\n",
            "Epoch [90/100], Loss: 1289.8055\n",
            "Epoch [100/100], Loss: 1399.7751\n",
            "Fold 1, RMSE: 52.22433853149414\n",
            "Epoch [10/100], Loss: 16102.8660\n",
            "Epoch [20/100], Loss: 12434.4143\n",
            "Epoch [30/100], Loss: 8956.8955\n",
            "Epoch [40/100], Loss: 7730.3218\n",
            "Epoch [50/100], Loss: 6646.9973\n",
            "Epoch [60/100], Loss: 2705.7356\n",
            "Epoch [70/100], Loss: 2521.8002\n",
            "Epoch [80/100], Loss: 2633.1731\n",
            "Epoch [90/100], Loss: 2220.0736\n",
            "Epoch [100/100], Loss: 1201.5006\n",
            "Fold 2, RMSE: 61.22301483154297\n",
            "Epoch [10/100], Loss: 11263.6997\n",
            "Epoch [20/100], Loss: 9846.1506\n",
            "Epoch [30/100], Loss: 4854.4064\n",
            "Epoch [40/100], Loss: 4862.8439\n",
            "Epoch [50/100], Loss: 1678.0658\n",
            "Epoch [60/100], Loss: 1525.0196\n",
            "Epoch [70/100], Loss: 894.3635\n",
            "Epoch [80/100], Loss: 2671.3248\n",
            "Epoch [90/100], Loss: 2419.1015\n",
            "Epoch [100/100], Loss: 2872.1690\n",
            "Fold 3, RMSE: 92.83000946044922\n",
            "Epoch [10/100], Loss: 14412.9775\n",
            "Epoch [20/100], Loss: 8866.7552\n",
            "Epoch [30/100], Loss: 5990.3416\n",
            "Epoch [40/100], Loss: 3099.2231\n",
            "Epoch [50/100], Loss: 2690.2607\n",
            "Epoch [60/100], Loss: 2360.0480\n",
            "Epoch [70/100], Loss: 1642.4771\n",
            "Epoch [80/100], Loss: 1046.3067\n",
            "Epoch [90/100], Loss: 1869.1619\n",
            "Epoch [100/100], Loss: 3369.2594\n",
            "Fold 4, RMSE: 44.338279724121094\n",
            "Epoch [10/100], Loss: 15091.0679\n",
            "Epoch [20/100], Loss: 8695.8979\n",
            "Epoch [30/100], Loss: 7663.9694\n",
            "Epoch [40/100], Loss: 6746.1248\n",
            "Epoch [50/100], Loss: 2306.0459\n",
            "Epoch [60/100], Loss: 2285.2521\n",
            "Epoch [70/100], Loss: 2210.9408\n",
            "Epoch [80/100], Loss: 1534.7431\n",
            "Epoch [90/100], Loss: 3220.6327\n",
            "Epoch [100/100], Loss: 2627.5423\n",
            "Fold 5, RMSE: 46.52594757080078\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 59.42831802368164\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 16179.7866\n",
            "Epoch [20/150], Loss: 9776.3530\n",
            "Epoch [30/150], Loss: 5415.5243\n",
            "Epoch [40/150], Loss: 3678.2947\n",
            "Epoch [50/150], Loss: 3452.1190\n",
            "Epoch [60/150], Loss: 3750.8569\n",
            "Epoch [70/150], Loss: 1714.8980\n",
            "Epoch [80/150], Loss: 2828.6765\n",
            "Epoch [90/150], Loss: 3025.9553\n",
            "Epoch [100/150], Loss: 656.2232\n",
            "Epoch [110/150], Loss: 2283.9664\n",
            "Epoch [120/150], Loss: 1265.8397\n",
            "Epoch [130/150], Loss: 994.9914\n",
            "Epoch [140/150], Loss: 878.2304\n",
            "Epoch [150/150], Loss: 1185.2972\n",
            "Fold 1, RMSE: 58.21110916137695\n",
            "Epoch [10/150], Loss: 45732.2510\n",
            "Epoch [20/150], Loss: 10949.5074\n",
            "Epoch [30/150], Loss: 11702.4468\n",
            "Epoch [40/150], Loss: 5207.6598\n",
            "Epoch [50/150], Loss: 1866.6573\n",
            "Epoch [60/150], Loss: 4725.2252\n",
            "Epoch [70/150], Loss: 1817.1793\n",
            "Epoch [80/150], Loss: 2959.8257\n",
            "Epoch [90/150], Loss: 1917.1881\n",
            "Epoch [100/150], Loss: 3312.7728\n",
            "Epoch [110/150], Loss: 7646.0405\n",
            "Epoch [120/150], Loss: 3859.3783\n",
            "Epoch [130/150], Loss: 2607.2548\n",
            "Epoch [140/150], Loss: 1884.6690\n",
            "Epoch [150/150], Loss: 2827.2904\n",
            "Fold 2, RMSE: 69.63665008544922\n",
            "Epoch [10/150], Loss: 9978.4927\n",
            "Epoch [20/150], Loss: 5767.6058\n",
            "Epoch [30/150], Loss: 3775.9369\n",
            "Epoch [40/150], Loss: 3199.8239\n",
            "Epoch [50/150], Loss: 2910.9994\n",
            "Epoch [60/150], Loss: 1416.5999\n",
            "Epoch [70/150], Loss: 1185.0246\n",
            "Epoch [80/150], Loss: 1580.0470\n",
            "Epoch [90/150], Loss: 1186.6447\n",
            "Epoch [100/150], Loss: 3308.1581\n",
            "Epoch [110/150], Loss: 3673.1137\n",
            "Epoch [120/150], Loss: 528.3012\n",
            "Epoch [130/150], Loss: 808.2101\n",
            "Epoch [140/150], Loss: 870.6832\n",
            "Epoch [150/150], Loss: 879.4551\n",
            "Fold 3, RMSE: 93.161865234375\n",
            "Epoch [10/150], Loss: 10837.1951\n",
            "Epoch [20/150], Loss: 10632.8972\n",
            "Epoch [30/150], Loss: 4414.0696\n",
            "Epoch [40/150], Loss: 2382.0837\n",
            "Epoch [50/150], Loss: 2242.6513\n",
            "Epoch [60/150], Loss: 3099.8858\n",
            "Epoch [70/150], Loss: 1149.6057\n",
            "Epoch [80/150], Loss: 1378.1282\n",
            "Epoch [90/150], Loss: 1575.9664\n",
            "Epoch [100/150], Loss: 3237.1487\n",
            "Epoch [110/150], Loss: 1270.9184\n",
            "Epoch [120/150], Loss: 3287.0821\n",
            "Epoch [130/150], Loss: 2878.3026\n",
            "Epoch [140/150], Loss: 3069.5896\n",
            "Epoch [150/150], Loss: 1258.6537\n",
            "Fold 4, RMSE: 40.64358139038086\n",
            "Epoch [10/150], Loss: 8644.5320\n",
            "Epoch [20/150], Loss: 7805.2921\n",
            "Epoch [30/150], Loss: 5388.8141\n",
            "Epoch [40/150], Loss: 4122.7418\n",
            "Epoch [50/150], Loss: 2424.0516\n",
            "Epoch [60/150], Loss: 5371.6830\n",
            "Epoch [70/150], Loss: 2489.1004\n",
            "Epoch [80/150], Loss: 3403.1246\n",
            "Epoch [90/150], Loss: 6028.0846\n",
            "Epoch [100/150], Loss: 7296.2689\n",
            "Epoch [110/150], Loss: 2018.2125\n",
            "Epoch [120/150], Loss: 1626.8524\n",
            "Epoch [130/150], Loss: 900.6623\n",
            "Epoch [140/150], Loss: 1120.9146\n",
            "Epoch [150/150], Loss: 1466.3335\n",
            "Fold 5, RMSE: 47.764747619628906\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 61.88359069824219\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 82068.9004\n",
            "Epoch [20/100], Loss: 21188.5723\n",
            "Epoch [30/100], Loss: 21714.7095\n",
            "Epoch [40/100], Loss: 13447.9720\n",
            "Epoch [50/100], Loss: 8155.1315\n",
            "Epoch [60/100], Loss: 7207.8743\n",
            "Epoch [70/100], Loss: 4357.9620\n",
            "Epoch [80/100], Loss: 8515.4315\n",
            "Epoch [90/100], Loss: 3963.9374\n",
            "Epoch [100/100], Loss: 6821.7847\n",
            "Fold 1, RMSE: 59.59162139892578\n",
            "Epoch [10/100], Loss: 307507.8984\n",
            "Epoch [20/100], Loss: 19434.7751\n",
            "Epoch [30/100], Loss: 16735.6272\n",
            "Epoch [40/100], Loss: 12175.5901\n",
            "Epoch [50/100], Loss: 13700.6921\n",
            "Epoch [60/100], Loss: 9730.2463\n",
            "Epoch [70/100], Loss: 9212.7838\n",
            "Epoch [80/100], Loss: 11680.0925\n",
            "Epoch [90/100], Loss: 7202.5642\n",
            "Epoch [100/100], Loss: 9080.4288\n",
            "Fold 2, RMSE: 67.1286392211914\n",
            "Epoch [10/100], Loss: 149636.5703\n",
            "Epoch [20/100], Loss: 73207.6602\n",
            "Epoch [30/100], Loss: 15997.1865\n",
            "Epoch [40/100], Loss: 11371.8915\n",
            "Epoch [50/100], Loss: 11220.3835\n",
            "Epoch [60/100], Loss: 6659.7039\n",
            "Epoch [70/100], Loss: 6439.2395\n",
            "Epoch [80/100], Loss: 5905.9231\n",
            "Epoch [90/100], Loss: 8435.8141\n",
            "Epoch [100/100], Loss: 4907.7671\n",
            "Fold 3, RMSE: 89.7128677368164\n",
            "Epoch [10/100], Loss: 211717.6836\n",
            "Epoch [20/100], Loss: 47532.0215\n",
            "Epoch [30/100], Loss: 22825.6567\n",
            "Epoch [40/100], Loss: 29642.5435\n",
            "Epoch [50/100], Loss: 16590.6121\n",
            "Epoch [60/100], Loss: 14674.9736\n",
            "Epoch [70/100], Loss: 14699.3040\n",
            "Epoch [80/100], Loss: 14579.2268\n",
            "Epoch [90/100], Loss: 11533.1785\n",
            "Epoch [100/100], Loss: 21304.5146\n",
            "Fold 4, RMSE: 33.739322662353516\n",
            "Epoch [10/100], Loss: 290633.0547\n",
            "Epoch [20/100], Loss: 22427.2932\n",
            "Epoch [30/100], Loss: 17386.7490\n",
            "Epoch [40/100], Loss: 13690.0979\n",
            "Epoch [50/100], Loss: 13812.1455\n",
            "Epoch [60/100], Loss: 18451.6804\n",
            "Epoch [70/100], Loss: 16622.4019\n",
            "Epoch [80/100], Loss: 13620.3125\n",
            "Epoch [90/100], Loss: 12824.7578\n",
            "Epoch [100/100], Loss: 9781.1594\n",
            "Fold 5, RMSE: 45.8697395324707\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 59.208438110351565\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 75492.5898\n",
            "Epoch [20/150], Loss: 23897.2183\n",
            "Epoch [30/150], Loss: 17159.1035\n",
            "Epoch [40/150], Loss: 22825.1216\n",
            "Epoch [50/150], Loss: 16419.2576\n",
            "Epoch [60/150], Loss: 21331.2056\n",
            "Epoch [70/150], Loss: 14667.2417\n",
            "Epoch [80/150], Loss: 13276.3806\n",
            "Epoch [90/150], Loss: 9701.3547\n",
            "Epoch [100/150], Loss: 11311.7257\n",
            "Epoch [110/150], Loss: 7329.0645\n",
            "Epoch [120/150], Loss: 9428.2388\n",
            "Epoch [130/150], Loss: 6270.3628\n",
            "Epoch [140/150], Loss: 6082.4381\n",
            "Epoch [150/150], Loss: 3861.7890\n",
            "Fold 1, RMSE: 58.88434982299805\n",
            "Epoch [10/150], Loss: 390243.1328\n",
            "Epoch [20/150], Loss: 38616.1025\n",
            "Epoch [30/150], Loss: 30667.4912\n",
            "Epoch [40/150], Loss: 15828.1406\n",
            "Epoch [50/150], Loss: 13067.7760\n",
            "Epoch [60/150], Loss: 14589.7661\n",
            "Epoch [70/150], Loss: 18828.4885\n",
            "Epoch [80/150], Loss: 14358.2480\n",
            "Epoch [90/150], Loss: 12909.1458\n",
            "Epoch [100/150], Loss: 8971.4636\n",
            "Epoch [110/150], Loss: 9288.6799\n",
            "Epoch [120/150], Loss: 10013.5143\n",
            "Epoch [130/150], Loss: 15823.0112\n",
            "Epoch [140/150], Loss: 10412.1094\n",
            "Epoch [150/150], Loss: 9629.4237\n",
            "Fold 2, RMSE: 65.08294677734375\n",
            "Epoch [10/150], Loss: 221376.9805\n",
            "Epoch [20/150], Loss: 37097.7803\n",
            "Epoch [30/150], Loss: 13777.1726\n",
            "Epoch [40/150], Loss: 11949.6042\n",
            "Epoch [50/150], Loss: 10665.1504\n",
            "Epoch [60/150], Loss: 12856.6697\n",
            "Epoch [70/150], Loss: 9244.5283\n",
            "Epoch [80/150], Loss: 11634.3486\n",
            "Epoch [90/150], Loss: 9714.0490\n",
            "Epoch [100/150], Loss: 9982.9622\n",
            "Epoch [110/150], Loss: 14420.5483\n",
            "Epoch [120/150], Loss: 7988.3574\n",
            "Epoch [130/150], Loss: 6977.3577\n",
            "Epoch [140/150], Loss: 7972.3918\n",
            "Epoch [150/150], Loss: 6610.4281\n",
            "Fold 3, RMSE: 88.61679077148438\n",
            "Epoch [10/150], Loss: 64710.2686\n",
            "Epoch [20/150], Loss: 17890.2493\n",
            "Epoch [30/150], Loss: 25900.3547\n",
            "Epoch [40/150], Loss: 16376.9507\n",
            "Epoch [50/150], Loss: 14584.9546\n",
            "Epoch [60/150], Loss: 14413.4697\n",
            "Epoch [70/150], Loss: 12246.5044\n",
            "Epoch [80/150], Loss: 10402.8641\n",
            "Epoch [90/150], Loss: 9917.9937\n",
            "Epoch [100/150], Loss: 9011.4775\n",
            "Epoch [110/150], Loss: 12483.5280\n",
            "Epoch [120/150], Loss: 6010.8365\n",
            "Epoch [130/150], Loss: 3601.5770\n",
            "Epoch [140/150], Loss: 3363.3904\n",
            "Epoch [150/150], Loss: 3366.0815\n",
            "Fold 4, RMSE: 42.91949462890625\n",
            "Epoch [10/150], Loss: 48526.0039\n",
            "Epoch [20/150], Loss: 12464.4725\n",
            "Epoch [30/150], Loss: 13899.6460\n",
            "Epoch [40/150], Loss: 10707.3889\n",
            "Epoch [50/150], Loss: 11504.1904\n",
            "Epoch [60/150], Loss: 14156.3638\n",
            "Epoch [70/150], Loss: 6809.4419\n",
            "Epoch [80/150], Loss: 7311.7537\n",
            "Epoch [90/150], Loss: 6611.9886\n",
            "Epoch [100/150], Loss: 9065.0786\n",
            "Epoch [110/150], Loss: 3928.2908\n",
            "Epoch [120/150], Loss: 1629.8317\n",
            "Epoch [130/150], Loss: 8158.2709\n",
            "Epoch [140/150], Loss: 4366.4725\n",
            "Epoch [150/150], Loss: 8460.3054\n",
            "Fold 5, RMSE: 47.08152389526367\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 60.51702117919922\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 31111.1035\n",
            "Epoch [20/100], Loss: 24275.1426\n",
            "Epoch [30/100], Loss: 11209.5779\n",
            "Epoch [40/100], Loss: 12464.6279\n",
            "Epoch [50/100], Loss: 8863.5297\n",
            "Epoch [60/100], Loss: 9879.8330\n",
            "Epoch [70/100], Loss: 10244.6897\n",
            "Epoch [80/100], Loss: 6714.2002\n",
            "Epoch [90/100], Loss: 5003.6670\n",
            "Epoch [100/100], Loss: 2490.4767\n",
            "Fold 1, RMSE: 59.97323989868164\n",
            "Epoch [10/100], Loss: 39496.8877\n",
            "Epoch [20/100], Loss: 14336.5186\n",
            "Epoch [30/100], Loss: 10275.1471\n",
            "Epoch [40/100], Loss: 11242.5100\n",
            "Epoch [50/100], Loss: 14408.8755\n",
            "Epoch [60/100], Loss: 10471.1689\n",
            "Epoch [70/100], Loss: 12571.0831\n",
            "Epoch [80/100], Loss: 9354.0018\n",
            "Epoch [90/100], Loss: 7825.3935\n",
            "Epoch [100/100], Loss: 6983.4086\n",
            "Fold 2, RMSE: 70.4019546508789\n",
            "Epoch [10/100], Loss: 56899.3037\n",
            "Epoch [20/100], Loss: 10471.1626\n",
            "Epoch [30/100], Loss: 13335.0854\n",
            "Epoch [40/100], Loss: 8399.4749\n",
            "Epoch [50/100], Loss: 8444.1836\n",
            "Epoch [60/100], Loss: 9086.8118\n",
            "Epoch [70/100], Loss: 6618.6122\n",
            "Epoch [80/100], Loss: 8544.5432\n",
            "Epoch [90/100], Loss: 6571.6442\n",
            "Epoch [100/100], Loss: 8151.2783\n",
            "Fold 3, RMSE: 91.79053497314453\n",
            "Epoch [10/100], Loss: 30979.3164\n",
            "Epoch [20/100], Loss: 23121.5874\n",
            "Epoch [30/100], Loss: 12955.8462\n",
            "Epoch [40/100], Loss: 11987.5996\n",
            "Epoch [50/100], Loss: 11032.5204\n",
            "Epoch [60/100], Loss: 11696.8337\n",
            "Epoch [70/100], Loss: 11015.7207\n",
            "Epoch [80/100], Loss: 13157.1687\n",
            "Epoch [90/100], Loss: 12985.9655\n",
            "Epoch [100/100], Loss: 10918.6163\n",
            "Fold 4, RMSE: 36.461727142333984\n",
            "Epoch [10/100], Loss: 55445.5801\n",
            "Epoch [20/100], Loss: 15594.5955\n",
            "Epoch [30/100], Loss: 11750.0613\n",
            "Epoch [40/100], Loss: 17240.5242\n",
            "Epoch [50/100], Loss: 11588.9473\n",
            "Epoch [60/100], Loss: 11129.4614\n",
            "Epoch [70/100], Loss: 11448.1215\n",
            "Epoch [80/100], Loss: 9087.0997\n",
            "Epoch [90/100], Loss: 11747.8918\n",
            "Epoch [100/100], Loss: 11369.5125\n",
            "Fold 5, RMSE: 48.62338638305664\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 61.45016860961914\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 28969.2798\n",
            "Epoch [20/150], Loss: 10625.2762\n",
            "Epoch [30/150], Loss: 11347.6846\n",
            "Epoch [40/150], Loss: 10712.1641\n",
            "Epoch [50/150], Loss: 12090.0074\n",
            "Epoch [60/150], Loss: 8782.6462\n",
            "Epoch [70/150], Loss: 10290.4548\n",
            "Epoch [80/150], Loss: 12025.4258\n",
            "Epoch [90/150], Loss: 6657.2909\n",
            "Epoch [100/150], Loss: 7081.5616\n",
            "Epoch [110/150], Loss: 6901.9078\n",
            "Epoch [120/150], Loss: 6341.7477\n",
            "Epoch [130/150], Loss: 3959.9940\n",
            "Epoch [140/150], Loss: 5979.4844\n",
            "Epoch [150/150], Loss: 4750.5764\n",
            "Fold 1, RMSE: 58.023860931396484\n",
            "Epoch [10/150], Loss: 15259.6780\n",
            "Epoch [20/150], Loss: 11843.5642\n",
            "Epoch [30/150], Loss: 11750.8763\n",
            "Epoch [40/150], Loss: 8667.6782\n",
            "Epoch [50/150], Loss: 8789.9828\n",
            "Epoch [60/150], Loss: 6079.8464\n",
            "Epoch [70/150], Loss: 4601.5193\n",
            "Epoch [80/150], Loss: 6330.5324\n",
            "Epoch [90/150], Loss: 7576.5788\n",
            "Epoch [100/150], Loss: 2333.0600\n",
            "Epoch [110/150], Loss: 3169.4731\n",
            "Epoch [120/150], Loss: 6797.0717\n",
            "Epoch [130/150], Loss: 15148.9614\n",
            "Epoch [140/150], Loss: 10491.7073\n",
            "Epoch [150/150], Loss: 15699.7841\n",
            "Fold 2, RMSE: 65.81725311279297\n",
            "Epoch [10/150], Loss: 32699.0225\n",
            "Epoch [20/150], Loss: 11014.5840\n",
            "Epoch [30/150], Loss: 6707.9208\n",
            "Epoch [40/150], Loss: 8189.3977\n",
            "Epoch [50/150], Loss: 10819.3806\n",
            "Epoch [60/150], Loss: 9476.4465\n",
            "Epoch [70/150], Loss: 6757.7661\n",
            "Epoch [80/150], Loss: 6486.3037\n",
            "Epoch [90/150], Loss: 6534.2098\n",
            "Epoch [100/150], Loss: 9569.7675\n",
            "Epoch [110/150], Loss: 5852.8616\n",
            "Epoch [120/150], Loss: 4985.6366\n",
            "Epoch [130/150], Loss: 3017.2388\n",
            "Epoch [140/150], Loss: 6565.2397\n",
            "Epoch [150/150], Loss: 3014.4561\n",
            "Fold 3, RMSE: 93.14825439453125\n",
            "Epoch [10/150], Loss: 39029.9282\n",
            "Epoch [20/150], Loss: 14621.2793\n",
            "Epoch [30/150], Loss: 20824.1565\n",
            "Epoch [40/150], Loss: 11774.6821\n",
            "Epoch [50/150], Loss: 11392.1875\n",
            "Epoch [60/150], Loss: 11463.3027\n",
            "Epoch [70/150], Loss: 11169.9971\n",
            "Epoch [80/150], Loss: 12475.5308\n",
            "Epoch [90/150], Loss: 11279.3264\n",
            "Epoch [100/150], Loss: 10977.2161\n",
            "Epoch [110/150], Loss: 12655.3170\n",
            "Epoch [120/150], Loss: 17208.7083\n",
            "Epoch [130/150], Loss: 13449.1392\n",
            "Epoch [140/150], Loss: 16689.3064\n",
            "Epoch [150/150], Loss: 9580.3411\n",
            "Fold 4, RMSE: 34.04600524902344\n",
            "Epoch [10/150], Loss: 46839.7305\n",
            "Epoch [20/150], Loss: 15773.5447\n",
            "Epoch [30/150], Loss: 21044.0471\n",
            "Epoch [40/150], Loss: 14902.3945\n",
            "Epoch [50/150], Loss: 11131.4830\n",
            "Epoch [60/150], Loss: 13758.0493\n",
            "Epoch [70/150], Loss: 11933.2102\n",
            "Epoch [80/150], Loss: 19752.9143\n",
            "Epoch [90/150], Loss: 9959.1306\n",
            "Epoch [100/150], Loss: 10979.3457\n",
            "Epoch [110/150], Loss: 10760.0979\n",
            "Epoch [120/150], Loss: 14824.0140\n",
            "Epoch [130/150], Loss: 11862.2266\n",
            "Epoch [140/150], Loss: 8589.9121\n",
            "Epoch [150/150], Loss: 11216.8687\n",
            "Fold 5, RMSE: 47.11549377441406\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 59.63017349243164\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 314302.1250\n",
            "Epoch [20/100], Loss: 29171.0586\n",
            "Epoch [30/100], Loss: 27922.4854\n",
            "Epoch [40/100], Loss: 18321.2959\n",
            "Epoch [50/100], Loss: 13423.4338\n",
            "Epoch [60/100], Loss: 12613.7969\n",
            "Epoch [70/100], Loss: 11408.8872\n",
            "Epoch [80/100], Loss: 8369.1218\n",
            "Epoch [90/100], Loss: 9233.1415\n",
            "Epoch [100/100], Loss: 8780.3763\n",
            "Fold 1, RMSE: 46.710166931152344\n",
            "Epoch [10/100], Loss: 308130.0547\n",
            "Epoch [20/100], Loss: 69788.2637\n",
            "Epoch [30/100], Loss: 25450.5813\n",
            "Epoch [40/100], Loss: 24666.2837\n",
            "Epoch [50/100], Loss: 23450.6660\n",
            "Epoch [60/100], Loss: 17181.3064\n",
            "Epoch [70/100], Loss: 14760.2109\n",
            "Epoch [80/100], Loss: 16909.4229\n",
            "Epoch [90/100], Loss: 11026.0302\n",
            "Epoch [100/100], Loss: 12163.5505\n",
            "Fold 2, RMSE: 67.19693756103516\n",
            "Epoch [10/100], Loss: 285334.3672\n",
            "Epoch [20/100], Loss: 29208.6450\n",
            "Epoch [30/100], Loss: 19938.9697\n",
            "Epoch [40/100], Loss: 12525.5063\n",
            "Epoch [50/100], Loss: 19215.8689\n",
            "Epoch [60/100], Loss: 14791.2957\n",
            "Epoch [70/100], Loss: 13450.1333\n",
            "Epoch [80/100], Loss: 8568.6873\n",
            "Epoch [90/100], Loss: 11558.2493\n",
            "Epoch [100/100], Loss: 15558.4272\n",
            "Fold 3, RMSE: 90.71646881103516\n",
            "Epoch [10/100], Loss: 205965.7441\n",
            "Epoch [20/100], Loss: 59529.0337\n",
            "Epoch [30/100], Loss: 38164.1045\n",
            "Epoch [40/100], Loss: 39654.7842\n",
            "Epoch [50/100], Loss: 27335.2917\n",
            "Epoch [60/100], Loss: 20797.9082\n",
            "Epoch [70/100], Loss: 14300.7585\n",
            "Epoch [80/100], Loss: 16765.6394\n",
            "Epoch [90/100], Loss: 14906.5017\n",
            "Epoch [100/100], Loss: 19963.6587\n",
            "Fold 4, RMSE: 34.57301712036133\n",
            "Epoch [10/100], Loss: 349673.9766\n",
            "Epoch [20/100], Loss: 28970.1338\n",
            "Epoch [30/100], Loss: 28840.4683\n",
            "Epoch [40/100], Loss: 24839.3853\n",
            "Epoch [50/100], Loss: 24612.6367\n",
            "Epoch [60/100], Loss: 10575.6692\n",
            "Epoch [70/100], Loss: 24926.0508\n",
            "Epoch [80/100], Loss: 13517.7395\n",
            "Epoch [90/100], Loss: 16252.6084\n",
            "Epoch [100/100], Loss: 12597.4612\n",
            "Fold 5, RMSE: 48.95561981201172\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 57.63044204711914\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 273648.5703\n",
            "Epoch [20/150], Loss: 42798.0859\n",
            "Epoch [30/150], Loss: 21906.0596\n",
            "Epoch [40/150], Loss: 11570.1350\n",
            "Epoch [50/150], Loss: 11531.6531\n",
            "Epoch [60/150], Loss: 12643.2795\n",
            "Epoch [70/150], Loss: 9461.1570\n",
            "Epoch [80/150], Loss: 9163.2722\n",
            "Epoch [90/150], Loss: 7564.3687\n",
            "Epoch [100/150], Loss: 9222.0203\n",
            "Epoch [110/150], Loss: 5104.8666\n",
            "Epoch [120/150], Loss: 5494.2979\n",
            "Epoch [130/150], Loss: 2943.3824\n",
            "Epoch [140/150], Loss: 6162.6989\n",
            "Epoch [150/150], Loss: 5644.3071\n",
            "Fold 1, RMSE: 50.981651306152344\n",
            "Epoch [10/150], Loss: 478512.2500\n",
            "Epoch [20/150], Loss: 58799.6348\n",
            "Epoch [30/150], Loss: 17590.3652\n",
            "Epoch [40/150], Loss: 20113.2173\n",
            "Epoch [50/150], Loss: 24321.2910\n",
            "Epoch [60/150], Loss: 19962.8582\n",
            "Epoch [70/150], Loss: 10250.9895\n",
            "Epoch [80/150], Loss: 13707.3760\n",
            "Epoch [90/150], Loss: 12945.2434\n",
            "Epoch [100/150], Loss: 9979.7703\n",
            "Epoch [110/150], Loss: 10611.2668\n",
            "Epoch [120/150], Loss: 11397.1716\n",
            "Epoch [130/150], Loss: 8329.6724\n",
            "Epoch [140/150], Loss: 7895.1540\n",
            "Epoch [150/150], Loss: 10431.3035\n",
            "Fold 2, RMSE: 64.0759048461914\n",
            "Epoch [10/150], Loss: 193699.5391\n",
            "Epoch [20/150], Loss: 23470.8076\n",
            "Epoch [30/150], Loss: 14869.4542\n",
            "Epoch [40/150], Loss: 14401.6389\n",
            "Epoch [50/150], Loss: 9512.9143\n",
            "Epoch [60/150], Loss: 12886.2100\n",
            "Epoch [70/150], Loss: 9592.3406\n",
            "Epoch [80/150], Loss: 7698.4597\n",
            "Epoch [90/150], Loss: 6384.5070\n",
            "Epoch [100/150], Loss: 9411.5990\n",
            "Epoch [110/150], Loss: 6058.2487\n",
            "Epoch [120/150], Loss: 8801.9666\n",
            "Epoch [130/150], Loss: 6604.1143\n",
            "Epoch [140/150], Loss: 6920.3555\n",
            "Epoch [150/150], Loss: 7605.7692\n",
            "Fold 3, RMSE: 89.62788391113281\n",
            "Epoch [10/150], Loss: 402258.0703\n",
            "Epoch [20/150], Loss: 54920.1416\n",
            "Epoch [30/150], Loss: 33823.5918\n",
            "Epoch [40/150], Loss: 21451.5298\n",
            "Epoch [50/150], Loss: 33992.7388\n",
            "Epoch [60/150], Loss: 17494.4180\n",
            "Epoch [70/150], Loss: 22197.3259\n",
            "Epoch [80/150], Loss: 20852.3652\n",
            "Epoch [90/150], Loss: 15051.3086\n",
            "Epoch [100/150], Loss: 16569.7468\n",
            "Epoch [110/150], Loss: 13507.5676\n",
            "Epoch [120/150], Loss: 15762.5793\n",
            "Epoch [130/150], Loss: 10857.3274\n",
            "Epoch [140/150], Loss: 11035.6001\n",
            "Epoch [150/150], Loss: 11864.3381\n",
            "Fold 4, RMSE: 34.317474365234375\n",
            "Epoch [10/150], Loss: 418946.6797\n",
            "Epoch [20/150], Loss: 71064.3232\n",
            "Epoch [30/150], Loss: 41455.7217\n",
            "Epoch [40/150], Loss: 25745.5044\n",
            "Epoch [50/150], Loss: 19124.3601\n",
            "Epoch [60/150], Loss: 21351.9927\n",
            "Epoch [70/150], Loss: 18344.6240\n",
            "Epoch [80/150], Loss: 12929.1128\n",
            "Epoch [90/150], Loss: 18119.8440\n",
            "Epoch [100/150], Loss: 15460.0349\n",
            "Epoch [110/150], Loss: 16590.8914\n",
            "Epoch [120/150], Loss: 12485.4119\n",
            "Epoch [130/150], Loss: 12602.0706\n",
            "Epoch [140/150], Loss: 9785.4528\n",
            "Epoch [150/150], Loss: 8498.3669\n",
            "Fold 5, RMSE: 48.120670318603516\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 57.42471694946289\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 33083.3022\n",
            "Epoch [20/100], Loss: 15624.3606\n",
            "Epoch [30/100], Loss: 15656.3970\n",
            "Epoch [40/100], Loss: 12717.4242\n",
            "Epoch [50/100], Loss: 11568.1802\n",
            "Epoch [60/100], Loss: 9178.1791\n",
            "Epoch [70/100], Loss: 10524.1351\n",
            "Epoch [80/100], Loss: 13292.5708\n",
            "Epoch [90/100], Loss: 13776.2410\n",
            "Epoch [100/100], Loss: 15628.9443\n",
            "Fold 1, RMSE: 46.43572235107422\n",
            "Epoch [10/100], Loss: 63758.8418\n",
            "Epoch [20/100], Loss: 27211.3213\n",
            "Epoch [30/100], Loss: 14552.2573\n",
            "Epoch [40/100], Loss: 13622.9395\n",
            "Epoch [50/100], Loss: 10704.0483\n",
            "Epoch [60/100], Loss: 16304.5422\n",
            "Epoch [70/100], Loss: 12679.6853\n",
            "Epoch [80/100], Loss: 11370.6885\n",
            "Epoch [90/100], Loss: 9858.5374\n",
            "Epoch [100/100], Loss: 10517.8843\n",
            "Fold 2, RMSE: 67.54564666748047\n",
            "Epoch [10/100], Loss: 35850.5693\n",
            "Epoch [20/100], Loss: 10706.0359\n",
            "Epoch [30/100], Loss: 14782.4404\n",
            "Epoch [40/100], Loss: 7711.4408\n",
            "Epoch [50/100], Loss: 11508.1125\n",
            "Epoch [60/100], Loss: 6961.1034\n",
            "Epoch [70/100], Loss: 12669.9708\n",
            "Epoch [80/100], Loss: 8026.3096\n",
            "Epoch [90/100], Loss: 10331.9946\n",
            "Epoch [100/100], Loss: 7260.9240\n",
            "Fold 3, RMSE: 88.49787139892578\n",
            "Epoch [10/100], Loss: 50116.7227\n",
            "Epoch [20/100], Loss: 13457.1299\n",
            "Epoch [30/100], Loss: 10991.0358\n",
            "Epoch [40/100], Loss: 11834.9551\n",
            "Epoch [50/100], Loss: 10691.3560\n",
            "Epoch [60/100], Loss: 10058.2322\n",
            "Epoch [70/100], Loss: 8404.3860\n",
            "Epoch [80/100], Loss: 8288.8147\n",
            "Epoch [90/100], Loss: 5450.9614\n",
            "Epoch [100/100], Loss: 4926.2740\n",
            "Fold 4, RMSE: 34.19912338256836\n",
            "Epoch [10/100], Loss: 27997.6016\n",
            "Epoch [20/100], Loss: 15112.1528\n",
            "Epoch [30/100], Loss: 12770.3774\n",
            "Epoch [40/100], Loss: 11232.2388\n",
            "Epoch [50/100], Loss: 11904.6904\n",
            "Epoch [60/100], Loss: 11592.6294\n",
            "Epoch [70/100], Loss: 10315.2429\n",
            "Epoch [80/100], Loss: 10561.9436\n",
            "Epoch [90/100], Loss: 10325.3021\n",
            "Epoch [100/100], Loss: 11076.4253\n",
            "Fold 5, RMSE: 47.90951156616211\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 56.91757507324219\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 37816.7886\n",
            "Epoch [20/150], Loss: 19757.5288\n",
            "Epoch [30/150], Loss: 13594.8301\n",
            "Epoch [40/150], Loss: 15580.9082\n",
            "Epoch [50/150], Loss: 12255.0291\n",
            "Epoch [60/150], Loss: 13107.9534\n",
            "Epoch [70/150], Loss: 14771.1196\n",
            "Epoch [80/150], Loss: 12327.0544\n",
            "Epoch [90/150], Loss: 12760.7568\n",
            "Epoch [100/150], Loss: 12225.2930\n",
            "Epoch [110/150], Loss: 16769.5442\n",
            "Epoch [120/150], Loss: 15008.8333\n",
            "Epoch [130/150], Loss: 11329.4216\n",
            "Epoch [140/150], Loss: 13447.8469\n",
            "Epoch [150/150], Loss: 11067.6324\n",
            "Fold 1, RMSE: 48.78270721435547\n",
            "Epoch [10/150], Loss: 44100.6841\n",
            "Epoch [20/150], Loss: 12286.0269\n",
            "Epoch [30/150], Loss: 12120.5518\n",
            "Epoch [40/150], Loss: 10035.7517\n",
            "Epoch [50/150], Loss: 10064.5635\n",
            "Epoch [60/150], Loss: 10133.6047\n",
            "Epoch [70/150], Loss: 10231.6719\n",
            "Epoch [80/150], Loss: 19775.7173\n",
            "Epoch [90/150], Loss: 10200.4673\n",
            "Epoch [100/150], Loss: 9016.9219\n",
            "Epoch [110/150], Loss: 15661.8689\n",
            "Epoch [120/150], Loss: 16203.4603\n",
            "Epoch [130/150], Loss: 10774.5190\n",
            "Epoch [140/150], Loss: 14184.2893\n",
            "Epoch [150/150], Loss: 8835.2119\n",
            "Fold 2, RMSE: 67.70700073242188\n",
            "Epoch [10/150], Loss: 34230.8140\n",
            "Epoch [20/150], Loss: 13040.2976\n",
            "Epoch [30/150], Loss: 9272.7427\n",
            "Epoch [40/150], Loss: 8089.2228\n",
            "Epoch [50/150], Loss: 10455.6089\n",
            "Epoch [60/150], Loss: 6414.7843\n",
            "Epoch [70/150], Loss: 9926.0996\n",
            "Epoch [80/150], Loss: 7160.3537\n",
            "Epoch [90/150], Loss: 7140.0144\n",
            "Epoch [100/150], Loss: 9383.2727\n",
            "Epoch [110/150], Loss: 9105.7148\n",
            "Epoch [120/150], Loss: 7897.1475\n",
            "Epoch [130/150], Loss: 7390.6714\n",
            "Epoch [140/150], Loss: 7063.7700\n",
            "Epoch [150/150], Loss: 10368.8937\n",
            "Fold 3, RMSE: 88.8421401977539\n",
            "Epoch [10/150], Loss: 24640.2537\n",
            "Epoch [20/150], Loss: 15708.7690\n",
            "Epoch [30/150], Loss: 12055.6348\n",
            "Epoch [40/150], Loss: 14420.5879\n",
            "Epoch [50/150], Loss: 12575.4817\n",
            "Epoch [60/150], Loss: 12614.8164\n",
            "Epoch [70/150], Loss: 13338.8140\n",
            "Epoch [80/150], Loss: 10312.9128\n",
            "Epoch [90/150], Loss: 13236.7710\n",
            "Epoch [100/150], Loss: 16714.0820\n",
            "Epoch [110/150], Loss: 11889.8126\n",
            "Epoch [120/150], Loss: 10673.5492\n",
            "Epoch [130/150], Loss: 12307.1682\n",
            "Epoch [140/150], Loss: 12212.5039\n",
            "Epoch [150/150], Loss: 11769.0154\n",
            "Fold 4, RMSE: 36.54023361206055\n",
            "Epoch [10/150], Loss: 29446.4824\n",
            "Epoch [20/150], Loss: 13934.1790\n",
            "Epoch [30/150], Loss: 9772.3840\n",
            "Epoch [40/150], Loss: 10308.3606\n",
            "Epoch [50/150], Loss: 17561.4646\n",
            "Epoch [60/150], Loss: 13064.4958\n",
            "Epoch [70/150], Loss: 9778.7318\n",
            "Epoch [80/150], Loss: 9970.5476\n",
            "Epoch [90/150], Loss: 13823.8770\n",
            "Epoch [100/150], Loss: 6919.1723\n",
            "Epoch [110/150], Loss: 6653.3485\n",
            "Epoch [120/150], Loss: 5698.3763\n",
            "Epoch [130/150], Loss: 6531.0015\n",
            "Epoch [140/150], Loss: 7579.3257\n",
            "Epoch [150/150], Loss: 4999.2526\n",
            "Fold 5, RMSE: 47.835601806640625\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 57.94153671264648\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 195079.9883\n",
            "Epoch [20/100], Loss: 21024.1636\n",
            "Epoch [30/100], Loss: 16815.0471\n",
            "Epoch [40/100], Loss: 12220.3530\n",
            "Epoch [50/100], Loss: 18958.2561\n",
            "Epoch [60/100], Loss: 10320.6879\n",
            "Epoch [70/100], Loss: 9795.9401\n",
            "Epoch [80/100], Loss: 10669.9189\n",
            "Epoch [90/100], Loss: 10977.9812\n",
            "Epoch [100/100], Loss: 8810.3733\n",
            "Fold 1, RMSE: 51.41316223144531\n",
            "Epoch [10/100], Loss: 394948.3438\n",
            "Epoch [20/100], Loss: 55850.5938\n",
            "Epoch [30/100], Loss: 22700.4106\n",
            "Epoch [40/100], Loss: 27187.4492\n",
            "Epoch [50/100], Loss: 21307.0713\n",
            "Epoch [60/100], Loss: 16355.6921\n",
            "Epoch [70/100], Loss: 17712.2600\n",
            "Epoch [80/100], Loss: 14989.0166\n",
            "Epoch [90/100], Loss: 13464.5050\n",
            "Epoch [100/100], Loss: 15204.5286\n",
            "Fold 2, RMSE: 69.1451644897461\n",
            "Epoch [10/100], Loss: 91972.9512\n",
            "Epoch [20/100], Loss: 21788.7256\n",
            "Epoch [30/100], Loss: 17153.3403\n",
            "Epoch [40/100], Loss: 8814.6982\n",
            "Epoch [50/100], Loss: 5890.5046\n",
            "Epoch [60/100], Loss: 7142.1418\n",
            "Epoch [70/100], Loss: 5999.3394\n",
            "Epoch [80/100], Loss: 7418.5402\n",
            "Epoch [90/100], Loss: 7829.4204\n",
            "Epoch [100/100], Loss: 5741.6581\n",
            "Fold 3, RMSE: 89.76715087890625\n",
            "Epoch [10/100], Loss: 1097867.9062\n",
            "Epoch [20/100], Loss: 314828.7109\n",
            "Epoch [30/100], Loss: 30985.0342\n",
            "Epoch [40/100], Loss: 36991.9253\n",
            "Epoch [50/100], Loss: 63761.9771\n",
            "Epoch [60/100], Loss: 24834.5667\n",
            "Epoch [70/100], Loss: 32784.0166\n",
            "Epoch [80/100], Loss: 27423.8105\n",
            "Epoch [90/100], Loss: 17214.7437\n",
            "Epoch [100/100], Loss: 22173.7515\n",
            "Fold 4, RMSE: 35.28927230834961\n",
            "Epoch [10/100], Loss: 312765.3125\n",
            "Epoch [20/100], Loss: 40536.5117\n",
            "Epoch [30/100], Loss: 35891.9380\n",
            "Epoch [40/100], Loss: 20220.7783\n",
            "Epoch [50/100], Loss: 24607.2866\n",
            "Epoch [60/100], Loss: 16416.9336\n",
            "Epoch [70/100], Loss: 19810.3230\n",
            "Epoch [80/100], Loss: 15287.0366\n",
            "Epoch [90/100], Loss: 11170.8748\n",
            "Epoch [100/100], Loss: 12393.6750\n",
            "Fold 5, RMSE: 46.04751968383789\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 58.332453918457034\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 176222.2695\n",
            "Epoch [20/150], Loss: 51854.4668\n",
            "Epoch [30/150], Loss: 15109.1384\n",
            "Epoch [40/150], Loss: 13827.7529\n",
            "Epoch [50/150], Loss: 19192.5415\n",
            "Epoch [60/150], Loss: 15872.6975\n",
            "Epoch [70/150], Loss: 10463.1794\n",
            "Epoch [80/150], Loss: 8784.7078\n",
            "Epoch [90/150], Loss: 10635.2764\n",
            "Epoch [100/150], Loss: 6267.8058\n",
            "Epoch [110/150], Loss: 6094.3418\n",
            "Epoch [120/150], Loss: 6806.0481\n",
            "Epoch [130/150], Loss: 4174.5890\n",
            "Epoch [140/150], Loss: 4965.5970\n",
            "Epoch [150/150], Loss: 6047.8501\n",
            "Fold 1, RMSE: 60.66120147705078\n",
            "Epoch [10/150], Loss: 780186.7500\n",
            "Epoch [20/150], Loss: 94143.5020\n",
            "Epoch [30/150], Loss: 36620.6855\n",
            "Epoch [40/150], Loss: 73395.5107\n",
            "Epoch [50/150], Loss: 20894.4011\n",
            "Epoch [60/150], Loss: 25603.8276\n",
            "Epoch [70/150], Loss: 20163.7769\n",
            "Epoch [80/150], Loss: 12689.3794\n",
            "Epoch [90/150], Loss: 13549.1885\n",
            "Epoch [100/150], Loss: 15557.9736\n",
            "Epoch [110/150], Loss: 13761.6091\n",
            "Epoch [120/150], Loss: 11455.6863\n",
            "Epoch [130/150], Loss: 11976.7427\n",
            "Epoch [140/150], Loss: 11821.8347\n",
            "Epoch [150/150], Loss: 16140.3262\n",
            "Fold 2, RMSE: 63.96311950683594\n",
            "Epoch [10/150], Loss: 221590.8125\n",
            "Epoch [20/150], Loss: 39568.5225\n",
            "Epoch [30/150], Loss: 19182.0151\n",
            "Epoch [40/150], Loss: 25239.8218\n",
            "Epoch [50/150], Loss: 11617.9285\n",
            "Epoch [60/150], Loss: 17704.5176\n",
            "Epoch [70/150], Loss: 14652.0874\n",
            "Epoch [80/150], Loss: 14285.6516\n",
            "Epoch [90/150], Loss: 9422.2114\n",
            "Epoch [100/150], Loss: 11972.9199\n",
            "Epoch [110/150], Loss: 9021.1658\n",
            "Epoch [120/150], Loss: 9458.9741\n",
            "Epoch [130/150], Loss: 9317.4650\n",
            "Epoch [140/150], Loss: 6654.1881\n",
            "Epoch [150/150], Loss: 11317.0515\n",
            "Fold 3, RMSE: 89.74323272705078\n",
            "Epoch [10/150], Loss: 27642.4751\n",
            "Epoch [20/150], Loss: 15558.1243\n",
            "Epoch [30/150], Loss: 14551.0195\n",
            "Epoch [40/150], Loss: 13895.0579\n",
            "Epoch [50/150], Loss: 13170.9919\n",
            "Epoch [60/150], Loss: 11766.3267\n",
            "Epoch [70/150], Loss: 9869.2986\n",
            "Epoch [80/150], Loss: 13364.3977\n",
            "Epoch [90/150], Loss: 13373.3455\n",
            "Epoch [100/150], Loss: 6643.8740\n",
            "Epoch [110/150], Loss: 9223.7881\n",
            "Epoch [120/150], Loss: 7659.7985\n",
            "Epoch [130/150], Loss: 9336.6967\n",
            "Epoch [140/150], Loss: 5457.8357\n",
            "Epoch [150/150], Loss: 4064.3258\n",
            "Fold 4, RMSE: 41.341548919677734\n",
            "Epoch [10/150], Loss: 407936.6797\n",
            "Epoch [20/150], Loss: 58975.9521\n",
            "Epoch [30/150], Loss: 26665.8706\n",
            "Epoch [40/150], Loss: 23681.6724\n",
            "Epoch [50/150], Loss: 35769.6167\n",
            "Epoch [60/150], Loss: 15703.8474\n",
            "Epoch [70/150], Loss: 17733.8818\n",
            "Epoch [80/150], Loss: 12684.2816\n",
            "Epoch [90/150], Loss: 15992.9604\n",
            "Epoch [100/150], Loss: 12423.1442\n",
            "Epoch [110/150], Loss: 16125.1567\n",
            "Epoch [120/150], Loss: 15849.2156\n",
            "Epoch [130/150], Loss: 9384.2677\n",
            "Epoch [140/150], Loss: 11787.9208\n",
            "Epoch [150/150], Loss: 11978.5054\n",
            "Fold 5, RMSE: 48.25946807861328\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 60.793714141845705\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 67324.3096\n",
            "Epoch [20/100], Loss: 14896.0374\n",
            "Epoch [30/100], Loss: 10654.2734\n",
            "Epoch [40/100], Loss: 13378.2830\n",
            "Epoch [50/100], Loss: 13854.8096\n",
            "Epoch [60/100], Loss: 11342.6978\n",
            "Epoch [70/100], Loss: 10692.5266\n",
            "Epoch [80/100], Loss: 14825.0061\n",
            "Epoch [90/100], Loss: 12447.7712\n",
            "Epoch [100/100], Loss: 12590.6519\n",
            "Fold 1, RMSE: 44.70918273925781\n",
            "Epoch [10/100], Loss: 68906.3301\n",
            "Epoch [20/100], Loss: 16837.6846\n",
            "Epoch [30/100], Loss: 22902.9004\n",
            "Epoch [40/100], Loss: 13394.3608\n",
            "Epoch [50/100], Loss: 14835.0240\n",
            "Epoch [60/100], Loss: 9998.5087\n",
            "Epoch [70/100], Loss: 10103.5626\n",
            "Epoch [80/100], Loss: 12832.7258\n",
            "Epoch [90/100], Loss: 12835.7937\n",
            "Epoch [100/100], Loss: 11104.0796\n",
            "Fold 2, RMSE: 71.41632080078125\n",
            "Epoch [10/100], Loss: 114501.1123\n",
            "Epoch [20/100], Loss: 11896.3921\n",
            "Epoch [30/100], Loss: 10325.7900\n",
            "Epoch [40/100], Loss: 16406.2405\n",
            "Epoch [50/100], Loss: 7533.6012\n",
            "Epoch [60/100], Loss: 9712.1792\n",
            "Epoch [70/100], Loss: 7416.3345\n",
            "Epoch [80/100], Loss: 12159.2538\n",
            "Epoch [90/100], Loss: 14349.7554\n",
            "Epoch [100/100], Loss: 6808.1005\n",
            "Fold 3, RMSE: 89.59925079345703\n",
            "Epoch [10/100], Loss: 39393.1553\n",
            "Epoch [20/100], Loss: 12203.3608\n",
            "Epoch [30/100], Loss: 13929.9167\n",
            "Epoch [40/100], Loss: 14198.1431\n",
            "Epoch [50/100], Loss: 13790.0132\n",
            "Epoch [60/100], Loss: 10574.1807\n",
            "Epoch [70/100], Loss: 11927.0256\n",
            "Epoch [80/100], Loss: 13609.3040\n",
            "Epoch [90/100], Loss: 8343.5283\n",
            "Epoch [100/100], Loss: 7489.9819\n",
            "Fold 4, RMSE: 34.36104965209961\n",
            "Epoch [10/100], Loss: 51501.3672\n",
            "Epoch [20/100], Loss: 23205.2930\n",
            "Epoch [30/100], Loss: 12790.8049\n",
            "Epoch [40/100], Loss: 17360.9565\n",
            "Epoch [50/100], Loss: 13403.9055\n",
            "Epoch [60/100], Loss: 14252.7505\n",
            "Epoch [70/100], Loss: 12827.9177\n",
            "Epoch [80/100], Loss: 11784.0668\n",
            "Epoch [90/100], Loss: 12406.1741\n",
            "Epoch [100/100], Loss: 12061.2615\n",
            "Fold 5, RMSE: 47.933349609375\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 57.60383071899414\n",
            "Training with neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 36200.3252\n",
            "Epoch [20/150], Loss: 11971.6636\n",
            "Epoch [30/150], Loss: 11631.7676\n",
            "Epoch [40/150], Loss: 10761.2366\n",
            "Epoch [50/150], Loss: 12178.8618\n",
            "Epoch [60/150], Loss: 10629.6389\n",
            "Epoch [70/150], Loss: 10460.6708\n",
            "Epoch [80/150], Loss: 18490.2041\n",
            "Epoch [90/150], Loss: 11374.6638\n",
            "Epoch [100/150], Loss: 9886.3003\n",
            "Epoch [110/150], Loss: 9274.8398\n",
            "Epoch [120/150], Loss: 9613.5818\n",
            "Epoch [130/150], Loss: 10741.9795\n",
            "Epoch [140/150], Loss: 9583.4496\n",
            "Epoch [150/150], Loss: 17417.7537\n",
            "Fold 1, RMSE: 48.622249603271484\n",
            "Epoch [10/150], Loss: 64526.9482\n",
            "Epoch [20/150], Loss: 12955.7512\n",
            "Epoch [30/150], Loss: 19578.8469\n",
            "Epoch [40/150], Loss: 13110.4734\n",
            "Epoch [50/150], Loss: 14831.4673\n",
            "Epoch [60/150], Loss: 9396.6711\n",
            "Epoch [70/150], Loss: 16497.7788\n",
            "Epoch [80/150], Loss: 13420.0479\n",
            "Epoch [90/150], Loss: 18136.0898\n",
            "Epoch [100/150], Loss: 12079.6512\n",
            "Epoch [110/150], Loss: 9116.7844\n",
            "Epoch [120/150], Loss: 13873.6641\n",
            "Epoch [130/150], Loss: 16846.0499\n",
            "Epoch [140/150], Loss: 11155.8181\n",
            "Epoch [150/150], Loss: 10148.4893\n",
            "Fold 2, RMSE: 71.6017837524414\n",
            "Epoch [10/150], Loss: 17856.5352\n",
            "Epoch [20/150], Loss: 12151.1632\n",
            "Epoch [30/150], Loss: 6799.3185\n",
            "Epoch [40/150], Loss: 6959.8024\n",
            "Epoch [50/150], Loss: 6147.9333\n",
            "Epoch [60/150], Loss: 5497.5833\n",
            "Epoch [70/150], Loss: 4945.4942\n",
            "Epoch [80/150], Loss: 5769.6337\n",
            "Epoch [90/150], Loss: 4089.6735\n",
            "Epoch [100/150], Loss: 4818.5301\n",
            "Epoch [110/150], Loss: 3051.2300\n",
            "Epoch [120/150], Loss: 2025.2481\n",
            "Epoch [130/150], Loss: 1137.4711\n",
            "Epoch [140/150], Loss: 1577.6568\n",
            "Epoch [150/150], Loss: 2281.4193\n",
            "Fold 3, RMSE: 93.9095230102539\n",
            "Epoch [10/150], Loss: 51041.6562\n",
            "Epoch [20/150], Loss: 22077.4028\n",
            "Epoch [30/150], Loss: 18671.2500\n",
            "Epoch [40/150], Loss: 11502.0475\n",
            "Epoch [50/150], Loss: 12888.8506\n",
            "Epoch [60/150], Loss: 13712.2140\n",
            "Epoch [70/150], Loss: 12972.1797\n",
            "Epoch [80/150], Loss: 12204.5320\n",
            "Epoch [90/150], Loss: 9934.4236\n",
            "Epoch [100/150], Loss: 13545.8491\n",
            "Epoch [110/150], Loss: 13505.1716\n",
            "Epoch [120/150], Loss: 16898.4053\n",
            "Epoch [130/150], Loss: 13949.6704\n",
            "Epoch [140/150], Loss: 12484.3721\n",
            "Epoch [150/150], Loss: 10593.1416\n",
            "Fold 4, RMSE: 34.85291290283203\n",
            "Epoch [10/150], Loss: 51544.5625\n",
            "Epoch [20/150], Loss: 18540.2856\n",
            "Epoch [30/150], Loss: 22953.7471\n",
            "Epoch [40/150], Loss: 14440.9778\n",
            "Epoch [50/150], Loss: 13565.0073\n",
            "Epoch [60/150], Loss: 9947.5349\n",
            "Epoch [70/150], Loss: 18379.9080\n",
            "Epoch [80/150], Loss: 14603.9583\n",
            "Epoch [90/150], Loss: 17760.5753\n",
            "Epoch [100/150], Loss: 12470.5808\n",
            "Epoch [110/150], Loss: 9929.3094\n",
            "Epoch [120/150], Loss: 12133.4021\n",
            "Epoch [130/150], Loss: 10141.1509\n",
            "Epoch [140/150], Loss: 11576.1479\n",
            "Epoch [150/150], Loss: 12700.7097\n",
            "Fold 5, RMSE: 47.749027252197266\n",
            "Avg RMSE for neurons=80, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 59.34709930419922\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 11920.2203\n",
            "Epoch [20/100], Loss: 11136.2651\n",
            "Epoch [30/100], Loss: 6338.6274\n",
            "Epoch [40/100], Loss: 6726.6599\n",
            "Epoch [50/100], Loss: 5541.5398\n",
            "Epoch [60/100], Loss: 6718.6348\n",
            "Epoch [70/100], Loss: 5646.1500\n",
            "Epoch [80/100], Loss: 1633.6168\n",
            "Epoch [90/100], Loss: 2158.8054\n",
            "Epoch [100/100], Loss: 4615.4530\n",
            "Fold 1, RMSE: 61.27201843261719\n",
            "Epoch [10/100], Loss: 18611.7607\n",
            "Epoch [20/100], Loss: 11837.8394\n",
            "Epoch [30/100], Loss: 9790.9789\n",
            "Epoch [40/100], Loss: 9585.0686\n",
            "Epoch [50/100], Loss: 6460.5723\n",
            "Epoch [60/100], Loss: 5601.4000\n",
            "Epoch [70/100], Loss: 4365.6390\n",
            "Epoch [80/100], Loss: 1693.0074\n",
            "Epoch [90/100], Loss: 4121.2619\n",
            "Epoch [100/100], Loss: 3541.8776\n",
            "Fold 2, RMSE: 64.2651138305664\n",
            "Epoch [10/100], Loss: 20263.6252\n",
            "Epoch [20/100], Loss: 11154.3771\n",
            "Epoch [30/100], Loss: 14011.5867\n",
            "Epoch [40/100], Loss: 11253.5562\n",
            "Epoch [50/100], Loss: 11617.4026\n",
            "Epoch [60/100], Loss: 14310.3313\n",
            "Epoch [70/100], Loss: 13053.2126\n",
            "Epoch [80/100], Loss: 13492.1079\n",
            "Epoch [90/100], Loss: 11561.6559\n",
            "Epoch [100/100], Loss: 13365.5291\n",
            "Fold 3, RMSE: 109.69561004638672\n",
            "Epoch [10/100], Loss: 15610.9702\n",
            "Epoch [20/100], Loss: 20633.1810\n",
            "Epoch [30/100], Loss: 16385.8566\n",
            "Epoch [40/100], Loss: 12388.6277\n",
            "Epoch [50/100], Loss: 7524.6703\n",
            "Epoch [60/100], Loss: 11539.3640\n",
            "Epoch [70/100], Loss: 6047.0682\n",
            "Epoch [80/100], Loss: 4797.7733\n",
            "Epoch [90/100], Loss: 1974.9907\n",
            "Epoch [100/100], Loss: 4950.1596\n",
            "Fold 4, RMSE: 43.97675704956055\n",
            "Epoch [10/100], Loss: 14103.5913\n",
            "Epoch [20/100], Loss: 9452.1047\n",
            "Epoch [30/100], Loss: 14332.1104\n",
            "Epoch [40/100], Loss: 6041.2666\n",
            "Epoch [50/100], Loss: 9063.5408\n",
            "Epoch [60/100], Loss: 8333.7228\n",
            "Epoch [70/100], Loss: 8312.2597\n",
            "Epoch [80/100], Loss: 9166.7124\n",
            "Epoch [90/100], Loss: 7755.6340\n",
            "Epoch [100/100], Loss: 8366.6672\n",
            "Fold 5, RMSE: 50.60017776489258\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 65.9619354248047\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16830.3916\n",
            "Epoch [20/150], Loss: 20950.8345\n",
            "Epoch [30/150], Loss: 4779.2250\n",
            "Epoch [40/150], Loss: 5674.3626\n",
            "Epoch [50/150], Loss: 5395.5006\n",
            "Epoch [60/150], Loss: 3417.5231\n",
            "Epoch [70/150], Loss: 9591.3950\n",
            "Epoch [80/150], Loss: 4044.0844\n",
            "Epoch [90/150], Loss: 4890.2209\n",
            "Epoch [100/150], Loss: 9848.5920\n",
            "Epoch [110/150], Loss: 8537.3312\n",
            "Epoch [120/150], Loss: 4894.0670\n",
            "Epoch [130/150], Loss: 4006.2916\n",
            "Epoch [140/150], Loss: 7963.5430\n",
            "Epoch [150/150], Loss: 10314.4644\n",
            "Fold 1, RMSE: 57.67144775390625\n",
            "Epoch [10/150], Loss: 13182.3134\n",
            "Epoch [20/150], Loss: 11224.9060\n",
            "Epoch [30/150], Loss: 10767.1294\n",
            "Epoch [40/150], Loss: 10352.8889\n",
            "Epoch [50/150], Loss: 6392.4742\n",
            "Epoch [60/150], Loss: 6665.5757\n",
            "Epoch [70/150], Loss: 8312.4098\n",
            "Epoch [80/150], Loss: 3137.2168\n",
            "Epoch [90/150], Loss: 4470.3439\n",
            "Epoch [100/150], Loss: 3400.6981\n",
            "Epoch [110/150], Loss: 2963.5824\n",
            "Epoch [120/150], Loss: 2543.0565\n",
            "Epoch [130/150], Loss: 1695.7906\n",
            "Epoch [140/150], Loss: 2171.5630\n",
            "Epoch [150/150], Loss: 1936.1175\n",
            "Fold 2, RMSE: 59.73583221435547\n",
            "Epoch [10/150], Loss: 13186.1665\n",
            "Epoch [20/150], Loss: 6233.3000\n",
            "Epoch [30/150], Loss: 9396.9578\n",
            "Epoch [40/150], Loss: 6820.8640\n",
            "Epoch [50/150], Loss: 6884.6617\n",
            "Epoch [60/150], Loss: 4806.0568\n",
            "Epoch [70/150], Loss: 4305.0239\n",
            "Epoch [80/150], Loss: 4282.0314\n",
            "Epoch [90/150], Loss: 6835.7496\n",
            "Epoch [100/150], Loss: 2746.8912\n",
            "Epoch [110/150], Loss: 2063.4031\n",
            "Epoch [120/150], Loss: 2768.2919\n",
            "Epoch [130/150], Loss: 3290.2006\n",
            "Epoch [140/150], Loss: 2447.6537\n",
            "Epoch [150/150], Loss: 3896.4891\n",
            "Fold 3, RMSE: 97.21932983398438\n",
            "Epoch [10/150], Loss: 13955.7188\n",
            "Epoch [20/150], Loss: 13044.7126\n",
            "Epoch [30/150], Loss: 23476.4304\n",
            "Epoch [40/150], Loss: 9444.8359\n",
            "Epoch [50/150], Loss: 7734.4083\n",
            "Epoch [60/150], Loss: 16201.8069\n",
            "Epoch [70/150], Loss: 6534.5496\n",
            "Epoch [80/150], Loss: 10291.2639\n",
            "Epoch [90/150], Loss: 7127.8724\n",
            "Epoch [100/150], Loss: 5447.4735\n",
            "Epoch [110/150], Loss: 10123.8441\n",
            "Epoch [120/150], Loss: 8897.0437\n",
            "Epoch [130/150], Loss: 11756.9857\n",
            "Epoch [140/150], Loss: 12640.0879\n",
            "Epoch [150/150], Loss: 6108.6329\n",
            "Fold 4, RMSE: 44.724510192871094\n",
            "Epoch [10/150], Loss: 15719.7275\n",
            "Epoch [20/150], Loss: 11293.1489\n",
            "Epoch [30/150], Loss: 9400.1528\n",
            "Epoch [40/150], Loss: 4914.7551\n",
            "Epoch [50/150], Loss: 8571.0507\n",
            "Epoch [60/150], Loss: 3833.1154\n",
            "Epoch [70/150], Loss: 4445.0091\n",
            "Epoch [80/150], Loss: 2852.5398\n",
            "Epoch [90/150], Loss: 2058.7997\n",
            "Epoch [100/150], Loss: 2645.7900\n",
            "Epoch [110/150], Loss: 3492.4157\n",
            "Epoch [120/150], Loss: 5866.6587\n",
            "Epoch [130/150], Loss: 5757.0659\n",
            "Epoch [140/150], Loss: 5513.9341\n",
            "Epoch [150/150], Loss: 4815.0452\n",
            "Fold 5, RMSE: 44.98988723754883\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 60.868201446533206\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14120.8112\n",
            "Epoch [20/100], Loss: 8407.2817\n",
            "Epoch [30/100], Loss: 7492.0500\n",
            "Epoch [40/100], Loss: 3955.0034\n",
            "Epoch [50/100], Loss: 5063.0842\n",
            "Epoch [60/100], Loss: 2479.2616\n",
            "Epoch [70/100], Loss: 4033.2186\n",
            "Epoch [80/100], Loss: 3907.0394\n",
            "Epoch [90/100], Loss: 2596.9261\n",
            "Epoch [100/100], Loss: 2629.8322\n",
            "Fold 1, RMSE: 56.33993148803711\n",
            "Epoch [10/100], Loss: 10027.8524\n",
            "Epoch [20/100], Loss: 9899.0787\n",
            "Epoch [30/100], Loss: 7329.2129\n",
            "Epoch [40/100], Loss: 1975.5320\n",
            "Epoch [50/100], Loss: 6652.2417\n",
            "Epoch [60/100], Loss: 10825.7748\n",
            "Epoch [70/100], Loss: 4920.9186\n",
            "Epoch [80/100], Loss: 5557.4201\n",
            "Epoch [90/100], Loss: 7363.2562\n",
            "Epoch [100/100], Loss: 7111.6510\n",
            "Fold 2, RMSE: 70.68232727050781\n",
            "Epoch [10/100], Loss: 10071.6238\n",
            "Epoch [20/100], Loss: 4880.7505\n",
            "Epoch [30/100], Loss: 6374.6720\n",
            "Epoch [40/100], Loss: 3932.4590\n",
            "Epoch [50/100], Loss: 4013.3070\n",
            "Epoch [60/100], Loss: 4370.6167\n",
            "Epoch [70/100], Loss: 7526.1875\n",
            "Epoch [80/100], Loss: 2793.1017\n",
            "Epoch [90/100], Loss: 2758.8185\n",
            "Epoch [100/100], Loss: 3018.5432\n",
            "Fold 3, RMSE: 91.81855010986328\n",
            "Epoch [10/100], Loss: 20341.1714\n",
            "Epoch [20/100], Loss: 18184.8318\n",
            "Epoch [30/100], Loss: 5902.6299\n",
            "Epoch [40/100], Loss: 8413.9744\n",
            "Epoch [50/100], Loss: 4813.7694\n",
            "Epoch [60/100], Loss: 8762.1621\n",
            "Epoch [70/100], Loss: 3380.4307\n",
            "Epoch [80/100], Loss: 2265.5190\n",
            "Epoch [90/100], Loss: 2753.3391\n",
            "Epoch [100/100], Loss: 2144.3314\n",
            "Fold 4, RMSE: 41.23680114746094\n",
            "Epoch [10/100], Loss: 14351.6863\n",
            "Epoch [20/100], Loss: 12386.9214\n",
            "Epoch [30/100], Loss: 6461.1592\n",
            "Epoch [40/100], Loss: 5527.1505\n",
            "Epoch [50/100], Loss: 6643.2178\n",
            "Epoch [60/100], Loss: 4717.3082\n",
            "Epoch [70/100], Loss: 4918.2042\n",
            "Epoch [80/100], Loss: 4060.6093\n",
            "Epoch [90/100], Loss: 2620.5641\n",
            "Epoch [100/100], Loss: 12057.8865\n",
            "Fold 5, RMSE: 44.377017974853516\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 60.890925598144534\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 11472.1393\n",
            "Epoch [20/150], Loss: 10293.4639\n",
            "Epoch [30/150], Loss: 6721.4441\n",
            "Epoch [40/150], Loss: 4308.8837\n",
            "Epoch [50/150], Loss: 4189.9517\n",
            "Epoch [60/150], Loss: 3744.3953\n",
            "Epoch [70/150], Loss: 2902.3065\n",
            "Epoch [80/150], Loss: 4285.6321\n",
            "Epoch [90/150], Loss: 2635.4897\n",
            "Epoch [100/150], Loss: 2964.8353\n",
            "Epoch [110/150], Loss: 3267.5459\n",
            "Epoch [120/150], Loss: 1984.7687\n",
            "Epoch [130/150], Loss: 3457.1831\n",
            "Epoch [140/150], Loss: 3020.5922\n",
            "Epoch [150/150], Loss: 2120.9895\n",
            "Fold 1, RMSE: 56.75699996948242\n",
            "Epoch [10/150], Loss: 12133.4150\n",
            "Epoch [20/150], Loss: 11169.4565\n",
            "Epoch [30/150], Loss: 5776.8262\n",
            "Epoch [40/150], Loss: 4161.8749\n",
            "Epoch [50/150], Loss: 5255.1588\n",
            "Epoch [60/150], Loss: 3725.2419\n",
            "Epoch [70/150], Loss: 4129.5417\n",
            "Epoch [80/150], Loss: 2141.1786\n",
            "Epoch [90/150], Loss: 5200.2995\n",
            "Epoch [100/150], Loss: 2159.9027\n",
            "Epoch [110/150], Loss: 1700.6032\n",
            "Epoch [120/150], Loss: 4823.0033\n",
            "Epoch [130/150], Loss: 2995.0130\n",
            "Epoch [140/150], Loss: 4843.7256\n",
            "Epoch [150/150], Loss: 2386.2739\n",
            "Fold 2, RMSE: 60.404117584228516\n",
            "Epoch [10/150], Loss: 11415.0129\n",
            "Epoch [20/150], Loss: 9754.6045\n",
            "Epoch [30/150], Loss: 6737.3812\n",
            "Epoch [40/150], Loss: 2927.4284\n",
            "Epoch [50/150], Loss: 3481.7012\n",
            "Epoch [60/150], Loss: 3392.5106\n",
            "Epoch [70/150], Loss: 2634.3237\n",
            "Epoch [80/150], Loss: 1723.7764\n",
            "Epoch [90/150], Loss: 3330.3896\n",
            "Epoch [100/150], Loss: 2630.7288\n",
            "Epoch [110/150], Loss: 2445.7343\n",
            "Epoch [120/150], Loss: 1206.6398\n",
            "Epoch [130/150], Loss: 2593.8982\n",
            "Epoch [140/150], Loss: 883.6513\n",
            "Epoch [150/150], Loss: 733.0723\n",
            "Fold 3, RMSE: 95.80862426757812\n",
            "Epoch [10/150], Loss: 10329.1511\n",
            "Epoch [20/150], Loss: 10654.4717\n",
            "Epoch [30/150], Loss: 5575.6205\n",
            "Epoch [40/150], Loss: 5606.4364\n",
            "Epoch [50/150], Loss: 2924.2682\n",
            "Epoch [60/150], Loss: 8111.9645\n",
            "Epoch [70/150], Loss: 8790.7456\n",
            "Epoch [80/150], Loss: 4333.2940\n",
            "Epoch [90/150], Loss: 5848.8911\n",
            "Epoch [100/150], Loss: 6226.8780\n",
            "Epoch [110/150], Loss: 2914.0167\n",
            "Epoch [120/150], Loss: 1627.6772\n",
            "Epoch [130/150], Loss: 2453.9598\n",
            "Epoch [140/150], Loss: 4801.5584\n",
            "Epoch [150/150], Loss: 4245.8576\n",
            "Fold 4, RMSE: 41.08576965332031\n",
            "Epoch [10/150], Loss: 12129.6028\n",
            "Epoch [20/150], Loss: 10085.2356\n",
            "Epoch [30/150], Loss: 5071.8050\n",
            "Epoch [40/150], Loss: 4352.6945\n",
            "Epoch [50/150], Loss: 2505.6779\n",
            "Epoch [60/150], Loss: 3943.6932\n",
            "Epoch [70/150], Loss: 2297.3092\n",
            "Epoch [80/150], Loss: 7221.0690\n",
            "Epoch [90/150], Loss: 5652.8435\n",
            "Epoch [100/150], Loss: 2071.6449\n",
            "Epoch [110/150], Loss: 1382.0194\n",
            "Epoch [120/150], Loss: 2165.9964\n",
            "Epoch [130/150], Loss: 1534.7823\n",
            "Epoch [140/150], Loss: 2449.4275\n",
            "Epoch [150/150], Loss: 1572.5230\n",
            "Fold 5, RMSE: 44.663204193115234\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 59.743743133544925\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 22258.2036\n",
            "Epoch [20/100], Loss: 18135.9197\n",
            "Epoch [30/100], Loss: 13213.7190\n",
            "Epoch [40/100], Loss: 11560.0447\n",
            "Epoch [50/100], Loss: 3476.6249\n",
            "Epoch [60/100], Loss: 8033.9359\n",
            "Epoch [70/100], Loss: 4722.4801\n",
            "Epoch [80/100], Loss: 6992.9315\n",
            "Epoch [90/100], Loss: 7323.9531\n",
            "Epoch [100/100], Loss: 9760.1754\n",
            "Fold 1, RMSE: 57.12673568725586\n",
            "Epoch [10/100], Loss: 13691.2158\n",
            "Epoch [20/100], Loss: 11188.7491\n",
            "Epoch [30/100], Loss: 9681.7988\n",
            "Epoch [40/100], Loss: 8073.9089\n",
            "Epoch [50/100], Loss: 7975.3964\n",
            "Epoch [60/100], Loss: 3529.4097\n",
            "Epoch [70/100], Loss: 3359.2439\n",
            "Epoch [80/100], Loss: 3865.6877\n",
            "Epoch [90/100], Loss: 3684.8597\n",
            "Epoch [100/100], Loss: 4125.6224\n",
            "Fold 2, RMSE: 74.03204345703125\n",
            "Epoch [10/100], Loss: 10280.1418\n",
            "Epoch [20/100], Loss: 6908.1075\n",
            "Epoch [30/100], Loss: 6962.2479\n",
            "Epoch [40/100], Loss: 5677.0952\n",
            "Epoch [50/100], Loss: 5692.0625\n",
            "Epoch [60/100], Loss: 2254.9341\n",
            "Epoch [70/100], Loss: 4926.6867\n",
            "Epoch [80/100], Loss: 2375.7393\n",
            "Epoch [90/100], Loss: 2707.1838\n",
            "Epoch [100/100], Loss: 9181.9868\n",
            "Fold 3, RMSE: 90.82333374023438\n",
            "Epoch [10/100], Loss: 13886.5703\n",
            "Epoch [20/100], Loss: 10434.5286\n",
            "Epoch [30/100], Loss: 11612.5178\n",
            "Epoch [40/100], Loss: 5400.1498\n",
            "Epoch [50/100], Loss: 8349.4111\n",
            "Epoch [60/100], Loss: 7790.2360\n",
            "Epoch [70/100], Loss: 5236.6719\n",
            "Epoch [80/100], Loss: 6478.6021\n",
            "Epoch [90/100], Loss: 6360.9143\n",
            "Epoch [100/100], Loss: 4002.5586\n",
            "Fold 4, RMSE: 40.79080581665039\n",
            "Epoch [10/100], Loss: 10377.8290\n",
            "Epoch [20/100], Loss: 10440.5061\n",
            "Epoch [30/100], Loss: 8008.3506\n",
            "Epoch [40/100], Loss: 3543.9883\n",
            "Epoch [50/100], Loss: 5955.8824\n",
            "Epoch [60/100], Loss: 7053.6157\n",
            "Epoch [70/100], Loss: 4502.1937\n",
            "Epoch [80/100], Loss: 2782.7516\n",
            "Epoch [90/100], Loss: 5938.8339\n",
            "Epoch [100/100], Loss: 4904.9441\n",
            "Fold 5, RMSE: 45.13507080078125\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 61.581597900390626\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 13508.9863\n",
            "Epoch [20/150], Loss: 9510.6155\n",
            "Epoch [30/150], Loss: 6562.1884\n",
            "Epoch [40/150], Loss: 5923.3544\n",
            "Epoch [50/150], Loss: 4901.6267\n",
            "Epoch [60/150], Loss: 3415.9056\n",
            "Epoch [70/150], Loss: 5706.8778\n",
            "Epoch [80/150], Loss: 2933.0767\n",
            "Epoch [90/150], Loss: 2343.4078\n",
            "Epoch [100/150], Loss: 1802.9983\n",
            "Epoch [110/150], Loss: 2891.6876\n",
            "Epoch [120/150], Loss: 4731.5114\n",
            "Epoch [130/150], Loss: 997.4097\n",
            "Epoch [140/150], Loss: 1780.6301\n",
            "Epoch [150/150], Loss: 6595.3788\n",
            "Fold 1, RMSE: 58.99396896362305\n",
            "Epoch [10/150], Loss: 17415.7969\n",
            "Epoch [20/150], Loss: 19104.2976\n",
            "Epoch [30/150], Loss: 5893.6228\n",
            "Epoch [40/150], Loss: 10654.1573\n",
            "Epoch [50/150], Loss: 7830.6390\n",
            "Epoch [60/150], Loss: 4555.1300\n",
            "Epoch [70/150], Loss: 3518.0336\n",
            "Epoch [80/150], Loss: 5336.4017\n",
            "Epoch [90/150], Loss: 6379.6491\n",
            "Epoch [100/150], Loss: 3629.6415\n",
            "Epoch [110/150], Loss: 10035.2856\n",
            "Epoch [120/150], Loss: 2891.3002\n",
            "Epoch [130/150], Loss: 2145.4021\n",
            "Epoch [140/150], Loss: 3092.2574\n",
            "Epoch [150/150], Loss: 1992.4705\n",
            "Fold 2, RMSE: 66.81543731689453\n",
            "Epoch [10/150], Loss: 11578.8240\n",
            "Epoch [20/150], Loss: 9145.1680\n",
            "Epoch [30/150], Loss: 7846.0432\n",
            "Epoch [40/150], Loss: 4339.9150\n",
            "Epoch [50/150], Loss: 6019.3914\n",
            "Epoch [60/150], Loss: 5758.4212\n",
            "Epoch [70/150], Loss: 3945.9286\n",
            "Epoch [80/150], Loss: 3408.6393\n",
            "Epoch [90/150], Loss: 2041.5872\n",
            "Epoch [100/150], Loss: 7539.2346\n",
            "Epoch [110/150], Loss: 3883.6038\n",
            "Epoch [120/150], Loss: 3535.5350\n",
            "Epoch [130/150], Loss: 2782.3892\n",
            "Epoch [140/150], Loss: 3610.5327\n",
            "Epoch [150/150], Loss: 4940.6932\n",
            "Fold 3, RMSE: 91.87232971191406\n",
            "Epoch [10/150], Loss: 16417.0933\n",
            "Epoch [20/150], Loss: 12306.5220\n",
            "Epoch [30/150], Loss: 14541.7090\n",
            "Epoch [40/150], Loss: 10799.2334\n",
            "Epoch [50/150], Loss: 4446.6292\n",
            "Epoch [60/150], Loss: 6356.6160\n",
            "Epoch [70/150], Loss: 7187.2441\n",
            "Epoch [80/150], Loss: 4420.6670\n",
            "Epoch [90/150], Loss: 5522.2867\n",
            "Epoch [100/150], Loss: 3243.1104\n",
            "Epoch [110/150], Loss: 1995.9742\n",
            "Epoch [120/150], Loss: 3104.0823\n",
            "Epoch [130/150], Loss: 2178.6856\n",
            "Epoch [140/150], Loss: 2316.0604\n",
            "Epoch [150/150], Loss: 1537.6310\n",
            "Fold 4, RMSE: 42.37921905517578\n",
            "Epoch [10/150], Loss: 14020.8105\n",
            "Epoch [20/150], Loss: 9261.3433\n",
            "Epoch [30/150], Loss: 7516.7126\n",
            "Epoch [40/150], Loss: 8716.5411\n",
            "Epoch [50/150], Loss: 9430.4211\n",
            "Epoch [60/150], Loss: 6313.2979\n",
            "Epoch [70/150], Loss: 4646.8738\n",
            "Epoch [80/150], Loss: 3832.9708\n",
            "Epoch [90/150], Loss: 2395.7022\n",
            "Epoch [100/150], Loss: 1669.0467\n",
            "Epoch [110/150], Loss: 6263.0720\n",
            "Epoch [120/150], Loss: 2350.0256\n",
            "Epoch [130/150], Loss: 3116.0602\n",
            "Epoch [140/150], Loss: 3422.3971\n",
            "Epoch [150/150], Loss: 3629.5164\n",
            "Fold 5, RMSE: 44.47222900390625\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 60.906636810302736\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 10821.6240\n",
            "Epoch [20/100], Loss: 8115.2627\n",
            "Epoch [30/100], Loss: 4179.3175\n",
            "Epoch [40/100], Loss: 8199.6787\n",
            "Epoch [50/100], Loss: 5937.8795\n",
            "Epoch [60/100], Loss: 2871.9786\n",
            "Epoch [70/100], Loss: 3992.6226\n",
            "Epoch [80/100], Loss: 3037.7609\n",
            "Epoch [90/100], Loss: 2783.9109\n",
            "Epoch [100/100], Loss: 1598.4296\n",
            "Fold 1, RMSE: 60.04574966430664\n",
            "Epoch [10/100], Loss: 9040.4929\n",
            "Epoch [20/100], Loss: 7005.1974\n",
            "Epoch [30/100], Loss: 3542.6546\n",
            "Epoch [40/100], Loss: 4629.6140\n",
            "Epoch [50/100], Loss: 5023.3008\n",
            "Epoch [60/100], Loss: 5881.1703\n",
            "Epoch [70/100], Loss: 3329.5010\n",
            "Epoch [80/100], Loss: 4593.9509\n",
            "Epoch [90/100], Loss: 10276.3004\n",
            "Epoch [100/100], Loss: 5466.2198\n",
            "Fold 2, RMSE: 65.26160430908203\n",
            "Epoch [10/100], Loss: 10604.0032\n",
            "Epoch [20/100], Loss: 5104.1064\n",
            "Epoch [30/100], Loss: 4856.5093\n",
            "Epoch [40/100], Loss: 5294.9036\n",
            "Epoch [50/100], Loss: 4600.1994\n",
            "Epoch [60/100], Loss: 3072.8147\n",
            "Epoch [70/100], Loss: 2568.9958\n",
            "Epoch [80/100], Loss: 2482.1882\n",
            "Epoch [90/100], Loss: 2223.2849\n",
            "Epoch [100/100], Loss: 2958.3275\n",
            "Fold 3, RMSE: 91.4176025390625\n",
            "Epoch [10/100], Loss: 14425.8162\n",
            "Epoch [20/100], Loss: 7481.7031\n",
            "Epoch [30/100], Loss: 8099.0410\n",
            "Epoch [40/100], Loss: 4072.3142\n",
            "Epoch [50/100], Loss: 4647.7954\n",
            "Epoch [60/100], Loss: 6352.4180\n",
            "Epoch [70/100], Loss: 3366.5417\n",
            "Epoch [80/100], Loss: 3491.9255\n",
            "Epoch [90/100], Loss: 5298.8424\n",
            "Epoch [100/100], Loss: 5337.1131\n",
            "Fold 4, RMSE: 43.76451110839844\n",
            "Epoch [10/100], Loss: 13784.7852\n",
            "Epoch [20/100], Loss: 10442.3428\n",
            "Epoch [30/100], Loss: 10970.7321\n",
            "Epoch [40/100], Loss: 9053.3918\n",
            "Epoch [50/100], Loss: 3616.2833\n",
            "Epoch [60/100], Loss: 3608.0274\n",
            "Epoch [70/100], Loss: 3476.2291\n",
            "Epoch [80/100], Loss: 2817.9377\n",
            "Epoch [90/100], Loss: 4329.4730\n",
            "Epoch [100/100], Loss: 3457.4784\n",
            "Fold 5, RMSE: 44.81380081176758\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 61.06065368652344\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 10986.4331\n",
            "Epoch [20/150], Loss: 10420.3224\n",
            "Epoch [30/150], Loss: 4730.3092\n",
            "Epoch [40/150], Loss: 5420.0288\n",
            "Epoch [50/150], Loss: 4671.6906\n",
            "Epoch [60/150], Loss: 5301.4337\n",
            "Epoch [70/150], Loss: 3069.2310\n",
            "Epoch [80/150], Loss: 4320.6593\n",
            "Epoch [90/150], Loss: 2309.0258\n",
            "Epoch [100/150], Loss: 2348.9245\n",
            "Epoch [110/150], Loss: 2167.7296\n",
            "Epoch [120/150], Loss: 6065.1592\n",
            "Epoch [130/150], Loss: 3418.4163\n",
            "Epoch [140/150], Loss: 2684.5509\n",
            "Epoch [150/150], Loss: 1426.7437\n",
            "Fold 1, RMSE: 59.59471130371094\n",
            "Epoch [10/150], Loss: 10652.5627\n",
            "Epoch [20/150], Loss: 9488.8230\n",
            "Epoch [30/150], Loss: 7004.0820\n",
            "Epoch [40/150], Loss: 4823.5291\n",
            "Epoch [50/150], Loss: 4484.8021\n",
            "Epoch [60/150], Loss: 11687.6270\n",
            "Epoch [70/150], Loss: 4770.4228\n",
            "Epoch [80/150], Loss: 1699.3739\n",
            "Epoch [90/150], Loss: 2174.5130\n",
            "Epoch [100/150], Loss: 6107.2381\n",
            "Epoch [110/150], Loss: 3045.4316\n",
            "Epoch [120/150], Loss: 1166.0483\n",
            "Epoch [130/150], Loss: 2185.0124\n",
            "Epoch [140/150], Loss: 3448.4412\n",
            "Epoch [150/150], Loss: 2606.6101\n",
            "Fold 2, RMSE: 64.7066421508789\n",
            "Epoch [10/150], Loss: 7077.7325\n",
            "Epoch [20/150], Loss: 6008.5297\n",
            "Epoch [30/150], Loss: 4174.2151\n",
            "Epoch [40/150], Loss: 6790.1183\n",
            "Epoch [50/150], Loss: 3601.8660\n",
            "Epoch [60/150], Loss: 1761.0128\n",
            "Epoch [70/150], Loss: 4611.9489\n",
            "Epoch [80/150], Loss: 2935.3355\n",
            "Epoch [90/150], Loss: 1216.7569\n",
            "Epoch [100/150], Loss: 2693.6442\n",
            "Epoch [110/150], Loss: 866.1238\n",
            "Epoch [120/150], Loss: 1231.4079\n",
            "Epoch [130/150], Loss: 1579.6860\n",
            "Epoch [140/150], Loss: 887.1053\n",
            "Epoch [150/150], Loss: 1088.4736\n",
            "Fold 3, RMSE: 88.68524169921875\n",
            "Epoch [10/150], Loss: 16806.0637\n",
            "Epoch [20/150], Loss: 13586.3486\n",
            "Epoch [30/150], Loss: 7604.4861\n",
            "Epoch [40/150], Loss: 5530.8879\n",
            "Epoch [50/150], Loss: 2014.7383\n",
            "Epoch [60/150], Loss: 6043.3598\n",
            "Epoch [70/150], Loss: 3994.2719\n",
            "Epoch [80/150], Loss: 4120.6632\n",
            "Epoch [90/150], Loss: 2893.9594\n",
            "Epoch [100/150], Loss: 3758.5588\n",
            "Epoch [110/150], Loss: 1804.6094\n",
            "Epoch [120/150], Loss: 3337.5001\n",
            "Epoch [130/150], Loss: 2534.5051\n",
            "Epoch [140/150], Loss: 4190.1242\n",
            "Epoch [150/150], Loss: 1494.8731\n",
            "Fold 4, RMSE: 40.84499740600586\n",
            "Epoch [10/150], Loss: 19659.7446\n",
            "Epoch [20/150], Loss: 12439.9136\n",
            "Epoch [30/150], Loss: 4260.1794\n",
            "Epoch [40/150], Loss: 8779.2417\n",
            "Epoch [50/150], Loss: 5498.0352\n",
            "Epoch [60/150], Loss: 5194.1682\n",
            "Epoch [70/150], Loss: 8124.9404\n",
            "Epoch [80/150], Loss: 4720.2954\n",
            "Epoch [90/150], Loss: 2107.2386\n",
            "Epoch [100/150], Loss: 2924.7798\n",
            "Epoch [110/150], Loss: 2837.4367\n",
            "Epoch [120/150], Loss: 2592.8658\n",
            "Epoch [130/150], Loss: 2590.8910\n",
            "Epoch [140/150], Loss: 4553.8644\n",
            "Epoch [150/150], Loss: 4035.8040\n",
            "Fold 5, RMSE: 44.425472259521484\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 59.65141296386719\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 13662.5515\n",
            "Epoch [20/100], Loss: 10925.8936\n",
            "Epoch [30/100], Loss: 3706.0467\n",
            "Epoch [40/100], Loss: 6750.8703\n",
            "Epoch [50/100], Loss: 4528.1252\n",
            "Epoch [60/100], Loss: 2618.9815\n",
            "Epoch [70/100], Loss: 4595.8747\n",
            "Epoch [80/100], Loss: 1788.5670\n",
            "Epoch [90/100], Loss: 2203.8804\n",
            "Epoch [100/100], Loss: 1689.9390\n",
            "Fold 1, RMSE: 57.8777961730957\n",
            "Epoch [10/100], Loss: 10686.2238\n",
            "Epoch [20/100], Loss: 10770.5649\n",
            "Epoch [30/100], Loss: 4202.1639\n",
            "Epoch [40/100], Loss: 7343.5411\n",
            "Epoch [50/100], Loss: 2902.5352\n",
            "Epoch [60/100], Loss: 5759.4169\n",
            "Epoch [70/100], Loss: 4400.4005\n",
            "Epoch [80/100], Loss: 3504.0125\n",
            "Epoch [90/100], Loss: 5627.0114\n",
            "Epoch [100/100], Loss: 3957.0319\n",
            "Fold 2, RMSE: 67.4413833618164\n",
            "Epoch [10/100], Loss: 12878.8506\n",
            "Epoch [20/100], Loss: 8337.3855\n",
            "Epoch [30/100], Loss: 8594.7692\n",
            "Epoch [40/100], Loss: 5851.8475\n",
            "Epoch [50/100], Loss: 4520.2661\n",
            "Epoch [60/100], Loss: 3383.4543\n",
            "Epoch [70/100], Loss: 4238.2150\n",
            "Epoch [80/100], Loss: 2088.6272\n",
            "Epoch [90/100], Loss: 2901.3056\n",
            "Epoch [100/100], Loss: 3182.0363\n",
            "Fold 3, RMSE: 93.37641906738281\n",
            "Epoch [10/100], Loss: 15376.6074\n",
            "Epoch [20/100], Loss: 11817.2935\n",
            "Epoch [30/100], Loss: 7701.9596\n",
            "Epoch [40/100], Loss: 7849.6189\n",
            "Epoch [50/100], Loss: 3986.6335\n",
            "Epoch [60/100], Loss: 3002.6366\n",
            "Epoch [70/100], Loss: 6094.0101\n",
            "Epoch [80/100], Loss: 3671.6621\n",
            "Epoch [90/100], Loss: 3595.8135\n",
            "Epoch [100/100], Loss: 2644.0423\n",
            "Fold 4, RMSE: 43.91921615600586\n",
            "Epoch [10/100], Loss: 20839.4680\n",
            "Epoch [20/100], Loss: 12972.0496\n",
            "Epoch [30/100], Loss: 9613.1213\n",
            "Epoch [40/100], Loss: 8587.2817\n",
            "Epoch [50/100], Loss: 4977.4434\n",
            "Epoch [60/100], Loss: 4791.3592\n",
            "Epoch [70/100], Loss: 3665.8190\n",
            "Epoch [80/100], Loss: 5352.2393\n",
            "Epoch [90/100], Loss: 3822.3004\n",
            "Epoch [100/100], Loss: 4447.8390\n",
            "Fold 5, RMSE: 44.06946563720703\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 61.336856079101565\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 11469.4440\n",
            "Epoch [20/150], Loss: 12952.9089\n",
            "Epoch [30/150], Loss: 6267.7252\n",
            "Epoch [40/150], Loss: 6079.6523\n",
            "Epoch [50/150], Loss: 5549.3315\n",
            "Epoch [60/150], Loss: 4722.3840\n",
            "Epoch [70/150], Loss: 4147.9551\n",
            "Epoch [80/150], Loss: 3829.9912\n",
            "Epoch [90/150], Loss: 1882.9900\n",
            "Epoch [100/150], Loss: 2865.5298\n",
            "Epoch [110/150], Loss: 5643.0930\n",
            "Epoch [120/150], Loss: 2894.3213\n",
            "Epoch [130/150], Loss: 4871.7618\n",
            "Epoch [140/150], Loss: 2280.9271\n",
            "Epoch [150/150], Loss: 3594.2192\n",
            "Fold 1, RMSE: 57.13496398925781\n",
            "Epoch [10/150], Loss: 13750.0244\n",
            "Epoch [20/150], Loss: 11785.6411\n",
            "Epoch [30/150], Loss: 9695.6055\n",
            "Epoch [40/150], Loss: 11092.6013\n",
            "Epoch [50/150], Loss: 3446.9944\n",
            "Epoch [60/150], Loss: 2582.9081\n",
            "Epoch [70/150], Loss: 3074.1401\n",
            "Epoch [80/150], Loss: 4690.9037\n",
            "Epoch [90/150], Loss: 4401.2896\n",
            "Epoch [100/150], Loss: 5243.2206\n",
            "Epoch [110/150], Loss: 4219.4019\n",
            "Epoch [120/150], Loss: 1511.0238\n",
            "Epoch [130/150], Loss: 6346.5248\n",
            "Epoch [140/150], Loss: 4611.4950\n",
            "Epoch [150/150], Loss: 7152.8966\n",
            "Fold 2, RMSE: 66.63143157958984\n",
            "Epoch [10/150], Loss: 8215.9924\n",
            "Epoch [20/150], Loss: 7648.2577\n",
            "Epoch [30/150], Loss: 4663.4025\n",
            "Epoch [40/150], Loss: 4983.0236\n",
            "Epoch [50/150], Loss: 3726.7720\n",
            "Epoch [60/150], Loss: 4518.3672\n",
            "Epoch [70/150], Loss: 3356.8219\n",
            "Epoch [80/150], Loss: 2656.6923\n",
            "Epoch [90/150], Loss: 3707.1126\n",
            "Epoch [100/150], Loss: 2230.6161\n",
            "Epoch [110/150], Loss: 1956.6701\n",
            "Epoch [120/150], Loss: 4050.0409\n",
            "Epoch [130/150], Loss: 3155.1624\n",
            "Epoch [140/150], Loss: 5087.2397\n",
            "Epoch [150/150], Loss: 1743.3053\n",
            "Fold 3, RMSE: 99.17914581298828\n",
            "Epoch [10/150], Loss: 23489.8672\n",
            "Epoch [20/150], Loss: 13111.7449\n",
            "Epoch [30/150], Loss: 10869.6541\n",
            "Epoch [40/150], Loss: 7912.8977\n",
            "Epoch [50/150], Loss: 3456.7289\n",
            "Epoch [60/150], Loss: 5438.1122\n",
            "Epoch [70/150], Loss: 3603.1453\n",
            "Epoch [80/150], Loss: 3586.6951\n",
            "Epoch [90/150], Loss: 6899.5101\n",
            "Epoch [100/150], Loss: 3803.0663\n",
            "Epoch [110/150], Loss: 5921.1621\n",
            "Epoch [120/150], Loss: 4359.8003\n",
            "Epoch [130/150], Loss: 2906.5933\n",
            "Epoch [140/150], Loss: 4039.5861\n",
            "Epoch [150/150], Loss: 4837.0104\n",
            "Fold 4, RMSE: 41.14532470703125\n",
            "Epoch [10/150], Loss: 15743.0035\n",
            "Epoch [20/150], Loss: 15298.9814\n",
            "Epoch [30/150], Loss: 12020.3992\n",
            "Epoch [40/150], Loss: 12790.9585\n",
            "Epoch [50/150], Loss: 8486.7971\n",
            "Epoch [60/150], Loss: 8898.5879\n",
            "Epoch [70/150], Loss: 8251.6471\n",
            "Epoch [80/150], Loss: 4693.8751\n",
            "Epoch [90/150], Loss: 7871.9724\n",
            "Epoch [100/150], Loss: 3809.3193\n",
            "Epoch [110/150], Loss: 7697.5929\n",
            "Epoch [120/150], Loss: 9123.0461\n",
            "Epoch [130/150], Loss: 7572.6814\n",
            "Epoch [140/150], Loss: 4527.6110\n",
            "Epoch [150/150], Loss: 7194.1274\n",
            "Fold 5, RMSE: 44.61055374145508\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 61.74028396606445\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 17398.4207\n",
            "Epoch [20/100], Loss: 9293.9609\n",
            "Epoch [30/100], Loss: 7625.4366\n",
            "Epoch [40/100], Loss: 5341.4348\n",
            "Epoch [50/100], Loss: 5021.5918\n",
            "Epoch [60/100], Loss: 2839.5480\n",
            "Epoch [70/100], Loss: 3480.2437\n",
            "Epoch [80/100], Loss: 5255.0242\n",
            "Epoch [90/100], Loss: 2709.5190\n",
            "Epoch [100/100], Loss: 2096.6567\n",
            "Fold 1, RMSE: 59.27922058105469\n",
            "Epoch [10/100], Loss: 7820.7903\n",
            "Epoch [20/100], Loss: 8790.7830\n",
            "Epoch [30/100], Loss: 6448.8788\n",
            "Epoch [40/100], Loss: 6309.2828\n",
            "Epoch [50/100], Loss: 3383.3387\n",
            "Epoch [60/100], Loss: 4778.9283\n",
            "Epoch [70/100], Loss: 4685.4667\n",
            "Epoch [80/100], Loss: 7408.2912\n",
            "Epoch [90/100], Loss: 3742.3917\n",
            "Epoch [100/100], Loss: 2766.8761\n",
            "Fold 2, RMSE: 69.66004943847656\n",
            "Epoch [10/100], Loss: 9715.2747\n",
            "Epoch [20/100], Loss: 8165.7070\n",
            "Epoch [30/100], Loss: 3695.0307\n",
            "Epoch [40/100], Loss: 3922.0712\n",
            "Epoch [50/100], Loss: 2970.5009\n",
            "Epoch [60/100], Loss: 1310.3929\n",
            "Epoch [70/100], Loss: 1098.7579\n",
            "Epoch [80/100], Loss: 935.7669\n",
            "Epoch [90/100], Loss: 1780.4151\n",
            "Epoch [100/100], Loss: 7932.5282\n",
            "Fold 3, RMSE: 97.7026138305664\n",
            "Epoch [10/100], Loss: 20589.7659\n",
            "Epoch [20/100], Loss: 9976.7405\n",
            "Epoch [30/100], Loss: 11391.7847\n",
            "Epoch [40/100], Loss: 4615.2847\n",
            "Epoch [50/100], Loss: 4562.6483\n",
            "Epoch [60/100], Loss: 6501.0565\n",
            "Epoch [70/100], Loss: 7463.7797\n",
            "Epoch [80/100], Loss: 2993.7162\n",
            "Epoch [90/100], Loss: 4500.1023\n",
            "Epoch [100/100], Loss: 4132.4939\n",
            "Fold 4, RMSE: 43.6815299987793\n",
            "Epoch [10/100], Loss: 17816.8142\n",
            "Epoch [20/100], Loss: 9425.2549\n",
            "Epoch [30/100], Loss: 6439.8348\n",
            "Epoch [40/100], Loss: 4958.8563\n",
            "Epoch [50/100], Loss: 6910.4867\n",
            "Epoch [60/100], Loss: 3274.0083\n",
            "Epoch [70/100], Loss: 2971.0601\n",
            "Epoch [80/100], Loss: 2572.7831\n",
            "Epoch [90/100], Loss: 4188.2869\n",
            "Epoch [100/100], Loss: 1745.3214\n",
            "Fold 5, RMSE: 43.8344612121582\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 62.83157501220703\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13304.2163\n",
            "Epoch [20/150], Loss: 13750.7688\n",
            "Epoch [30/150], Loss: 8368.6444\n",
            "Epoch [40/150], Loss: 4879.3409\n",
            "Epoch [50/150], Loss: 5793.4221\n",
            "Epoch [60/150], Loss: 5766.4208\n",
            "Epoch [70/150], Loss: 2052.7358\n",
            "Epoch [80/150], Loss: 3308.1812\n",
            "Epoch [90/150], Loss: 3170.3533\n",
            "Epoch [100/150], Loss: 1856.7914\n",
            "Epoch [110/150], Loss: 1726.6033\n",
            "Epoch [120/150], Loss: 3138.1570\n",
            "Epoch [130/150], Loss: 4026.6419\n",
            "Epoch [140/150], Loss: 2028.4991\n",
            "Epoch [150/150], Loss: 1022.9675\n",
            "Fold 1, RMSE: 56.75556182861328\n",
            "Epoch [10/150], Loss: 10687.7974\n",
            "Epoch [20/150], Loss: 9070.1577\n",
            "Epoch [30/150], Loss: 4786.2056\n",
            "Epoch [40/150], Loss: 5795.4270\n",
            "Epoch [50/150], Loss: 4754.1837\n",
            "Epoch [60/150], Loss: 3910.8540\n",
            "Epoch [70/150], Loss: 4690.9362\n",
            "Epoch [80/150], Loss: 3247.0834\n",
            "Epoch [90/150], Loss: 4241.5006\n",
            "Epoch [100/150], Loss: 7962.3096\n",
            "Epoch [110/150], Loss: 2781.4262\n",
            "Epoch [120/150], Loss: 2280.6037\n",
            "Epoch [130/150], Loss: 3570.4277\n",
            "Epoch [140/150], Loss: 3490.5515\n",
            "Epoch [150/150], Loss: 3948.4002\n",
            "Fold 2, RMSE: 62.47298049926758\n",
            "Epoch [10/150], Loss: 7603.5557\n",
            "Epoch [20/150], Loss: 4262.4546\n",
            "Epoch [30/150], Loss: 4321.6434\n",
            "Epoch [40/150], Loss: 2519.0346\n",
            "Epoch [50/150], Loss: 6289.9196\n",
            "Epoch [60/150], Loss: 5051.9535\n",
            "Epoch [70/150], Loss: 3664.1218\n",
            "Epoch [80/150], Loss: 3122.7294\n",
            "Epoch [90/150], Loss: 2864.5568\n",
            "Epoch [100/150], Loss: 1486.3753\n",
            "Epoch [110/150], Loss: 3369.9127\n",
            "Epoch [120/150], Loss: 2842.0005\n",
            "Epoch [130/150], Loss: 1116.2750\n",
            "Epoch [140/150], Loss: 2191.4196\n",
            "Epoch [150/150], Loss: 2086.5299\n",
            "Fold 3, RMSE: 91.82614135742188\n",
            "Epoch [10/150], Loss: 12035.4373\n",
            "Epoch [20/150], Loss: 9505.6042\n",
            "Epoch [30/150], Loss: 9952.9711\n",
            "Epoch [40/150], Loss: 9531.2842\n",
            "Epoch [50/150], Loss: 8814.7004\n",
            "Epoch [60/150], Loss: 7470.5101\n",
            "Epoch [70/150], Loss: 2391.8425\n",
            "Epoch [80/150], Loss: 4647.3022\n",
            "Epoch [90/150], Loss: 3921.6603\n",
            "Epoch [100/150], Loss: 1415.8349\n",
            "Epoch [110/150], Loss: 2761.5697\n",
            "Epoch [120/150], Loss: 5247.1213\n",
            "Epoch [130/150], Loss: 3637.4485\n",
            "Epoch [140/150], Loss: 3463.9030\n",
            "Epoch [150/150], Loss: 7143.0551\n",
            "Fold 4, RMSE: 45.008182525634766\n",
            "Epoch [10/150], Loss: 16136.7991\n",
            "Epoch [20/150], Loss: 14517.0747\n",
            "Epoch [30/150], Loss: 4841.8281\n",
            "Epoch [40/150], Loss: 4558.9662\n",
            "Epoch [50/150], Loss: 3492.9428\n",
            "Epoch [60/150], Loss: 4518.3729\n",
            "Epoch [70/150], Loss: 4287.6829\n",
            "Epoch [80/150], Loss: 2390.5881\n",
            "Epoch [90/150], Loss: 1492.1936\n",
            "Epoch [100/150], Loss: 2022.4813\n",
            "Epoch [110/150], Loss: 2518.7155\n",
            "Epoch [120/150], Loss: 1695.7717\n",
            "Epoch [130/150], Loss: 3126.3869\n",
            "Epoch [140/150], Loss: 1470.6884\n",
            "Epoch [150/150], Loss: 1153.4610\n",
            "Fold 5, RMSE: 44.8797721862793\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 60.18852767944336\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 16630.8638\n",
            "Epoch [20/100], Loss: 14450.3973\n",
            "Epoch [30/100], Loss: 13373.6096\n",
            "Epoch [40/100], Loss: 15421.9854\n",
            "Epoch [50/100], Loss: 14357.5635\n",
            "Epoch [60/100], Loss: 23416.7725\n",
            "Epoch [70/100], Loss: 7912.7932\n",
            "Epoch [80/100], Loss: 7100.4417\n",
            "Epoch [90/100], Loss: 13415.1238\n",
            "Epoch [100/100], Loss: 9898.5364\n",
            "Fold 1, RMSE: 67.54065704345703\n",
            "Epoch [10/100], Loss: 21127.4951\n",
            "Epoch [20/100], Loss: 10988.8254\n",
            "Epoch [30/100], Loss: 6305.1187\n",
            "Epoch [40/100], Loss: 6620.6904\n",
            "Epoch [50/100], Loss: 9717.3055\n",
            "Epoch [60/100], Loss: 4848.8644\n",
            "Epoch [70/100], Loss: 5066.2169\n",
            "Epoch [80/100], Loss: 4568.7106\n",
            "Epoch [90/100], Loss: 2624.9717\n",
            "Epoch [100/100], Loss: 7072.4779\n",
            "Fold 2, RMSE: 64.1379623413086\n",
            "Epoch [10/100], Loss: 15472.2256\n",
            "Epoch [20/100], Loss: 12060.8640\n",
            "Epoch [30/100], Loss: 12385.7122\n",
            "Epoch [40/100], Loss: 14512.9124\n",
            "Epoch [50/100], Loss: 14560.2998\n",
            "Epoch [60/100], Loss: 14343.3757\n",
            "Epoch [70/100], Loss: 16061.8054\n",
            "Epoch [80/100], Loss: 18088.5081\n",
            "Epoch [90/100], Loss: 14245.2266\n",
            "Epoch [100/100], Loss: 12144.8511\n",
            "Fold 3, RMSE: 109.72798156738281\n",
            "Epoch [10/100], Loss: 26328.9482\n",
            "Epoch [20/100], Loss: 14696.5911\n",
            "Epoch [30/100], Loss: 10475.0476\n",
            "Epoch [40/100], Loss: 9454.6295\n",
            "Epoch [50/100], Loss: 4698.8367\n",
            "Epoch [60/100], Loss: 7296.9957\n",
            "Epoch [70/100], Loss: 7967.9709\n",
            "Epoch [80/100], Loss: 6289.4786\n",
            "Epoch [90/100], Loss: 2501.2223\n",
            "Epoch [100/100], Loss: 4586.5155\n",
            "Fold 4, RMSE: 38.000892639160156\n",
            "Epoch [10/100], Loss: 19417.3276\n",
            "Epoch [20/100], Loss: 17072.9453\n",
            "Epoch [30/100], Loss: 21273.4629\n",
            "Epoch [40/100], Loss: 18962.1758\n",
            "Epoch [50/100], Loss: 26271.2139\n",
            "Epoch [60/100], Loss: 26510.9294\n",
            "Epoch [70/100], Loss: 26285.0828\n",
            "Epoch [80/100], Loss: 17864.0337\n",
            "Epoch [90/100], Loss: 17616.2112\n",
            "Epoch [100/100], Loss: 17194.5387\n",
            "Fold 5, RMSE: 57.776668548583984\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 67.43683242797852\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 15048.7637\n",
            "Epoch [20/150], Loss: 13526.7043\n",
            "Epoch [30/150], Loss: 10772.5974\n",
            "Epoch [40/150], Loss: 6441.8542\n",
            "Epoch [50/150], Loss: 10291.3381\n",
            "Epoch [60/150], Loss: 14116.0725\n",
            "Epoch [70/150], Loss: 6288.7448\n",
            "Epoch [80/150], Loss: 9172.6269\n",
            "Epoch [90/150], Loss: 8336.9131\n",
            "Epoch [100/150], Loss: 3209.2637\n",
            "Epoch [110/150], Loss: 9041.1481\n",
            "Epoch [120/150], Loss: 10482.4478\n",
            "Epoch [130/150], Loss: 11439.0238\n",
            "Epoch [140/150], Loss: 12054.1399\n",
            "Epoch [150/150], Loss: 5086.7063\n",
            "Fold 1, RMSE: 60.3607292175293\n",
            "Epoch [10/150], Loss: 25787.9785\n",
            "Epoch [20/150], Loss: 9999.4026\n",
            "Epoch [30/150], Loss: 8801.6622\n",
            "Epoch [40/150], Loss: 7797.6117\n",
            "Epoch [50/150], Loss: 5482.5515\n",
            "Epoch [60/150], Loss: 8155.1748\n",
            "Epoch [70/150], Loss: 4613.3416\n",
            "Epoch [80/150], Loss: 1996.3253\n",
            "Epoch [90/150], Loss: 2707.2527\n",
            "Epoch [100/150], Loss: 4145.1790\n",
            "Epoch [110/150], Loss: 5538.1528\n",
            "Epoch [120/150], Loss: 2671.6417\n",
            "Epoch [130/150], Loss: 2424.5793\n",
            "Epoch [140/150], Loss: 1985.4044\n",
            "Epoch [150/150], Loss: 4036.9519\n",
            "Fold 2, RMSE: 65.2508544921875\n",
            "Epoch [10/150], Loss: 15071.5151\n",
            "Epoch [20/150], Loss: 11669.7595\n",
            "Epoch [30/150], Loss: 8976.1182\n",
            "Epoch [40/150], Loss: 8653.1729\n",
            "Epoch [50/150], Loss: 5597.8954\n",
            "Epoch [60/150], Loss: 5070.6266\n",
            "Epoch [70/150], Loss: 6494.0615\n",
            "Epoch [80/150], Loss: 5867.6970\n",
            "Epoch [90/150], Loss: 4060.2867\n",
            "Epoch [100/150], Loss: 3456.7601\n",
            "Epoch [110/150], Loss: 4238.0134\n",
            "Epoch [120/150], Loss: 2319.1092\n",
            "Epoch [130/150], Loss: 2973.4571\n",
            "Epoch [140/150], Loss: 5002.7258\n",
            "Epoch [150/150], Loss: 6302.1041\n",
            "Fold 3, RMSE: 101.92623138427734\n",
            "Epoch [10/150], Loss: 16137.3652\n",
            "Epoch [20/150], Loss: 10742.9313\n",
            "Epoch [30/150], Loss: 13804.3794\n",
            "Epoch [40/150], Loss: 12939.2729\n",
            "Epoch [50/150], Loss: 6419.5006\n",
            "Epoch [60/150], Loss: 4713.1725\n",
            "Epoch [70/150], Loss: 9193.8304\n",
            "Epoch [80/150], Loss: 4507.9200\n",
            "Epoch [90/150], Loss: 11145.0311\n",
            "Epoch [100/150], Loss: 4649.6783\n",
            "Epoch [110/150], Loss: 8751.4149\n",
            "Epoch [120/150], Loss: 5827.1542\n",
            "Epoch [130/150], Loss: 8993.3374\n",
            "Epoch [140/150], Loss: 6264.1625\n",
            "Epoch [150/150], Loss: 6361.5834\n",
            "Fold 4, RMSE: 48.93914794921875\n",
            "Epoch [10/150], Loss: 17105.6406\n",
            "Epoch [20/150], Loss: 11595.7681\n",
            "Epoch [30/150], Loss: 12241.1943\n",
            "Epoch [40/150], Loss: 12527.8729\n",
            "Epoch [50/150], Loss: 6087.9955\n",
            "Epoch [60/150], Loss: 4669.3599\n",
            "Epoch [70/150], Loss: 6123.5088\n",
            "Epoch [80/150], Loss: 13257.6833\n",
            "Epoch [90/150], Loss: 11569.2441\n",
            "Epoch [100/150], Loss: 4816.3575\n",
            "Epoch [110/150], Loss: 4639.8287\n",
            "Epoch [120/150], Loss: 4226.1168\n",
            "Epoch [130/150], Loss: 4239.6479\n",
            "Epoch [140/150], Loss: 4699.4098\n",
            "Epoch [150/150], Loss: 4558.8015\n",
            "Fold 5, RMSE: 44.75331115722656\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 64.2460548400879\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 11267.9879\n",
            "Epoch [20/100], Loss: 9755.3428\n",
            "Epoch [30/100], Loss: 6034.8208\n",
            "Epoch [40/100], Loss: 6003.8438\n",
            "Epoch [50/100], Loss: 9008.1201\n",
            "Epoch [60/100], Loss: 6129.7746\n",
            "Epoch [70/100], Loss: 6647.7303\n",
            "Epoch [80/100], Loss: 8804.5913\n",
            "Epoch [90/100], Loss: 4994.9200\n",
            "Epoch [100/100], Loss: 8604.1282\n",
            "Fold 1, RMSE: 58.947513580322266\n",
            "Epoch [10/100], Loss: 10219.0204\n",
            "Epoch [20/100], Loss: 11418.6028\n",
            "Epoch [30/100], Loss: 9099.0734\n",
            "Epoch [40/100], Loss: 5457.3932\n",
            "Epoch [50/100], Loss: 8440.9923\n",
            "Epoch [60/100], Loss: 3465.0201\n",
            "Epoch [70/100], Loss: 2246.0074\n",
            "Epoch [80/100], Loss: 7172.0525\n",
            "Epoch [90/100], Loss: 2978.1546\n",
            "Epoch [100/100], Loss: 1383.7171\n",
            "Fold 2, RMSE: 74.72927856445312\n",
            "Epoch [10/100], Loss: 11393.7729\n",
            "Epoch [20/100], Loss: 7018.4674\n",
            "Epoch [30/100], Loss: 7549.6870\n",
            "Epoch [40/100], Loss: 3087.5340\n",
            "Epoch [50/100], Loss: 5551.4182\n",
            "Epoch [60/100], Loss: 4378.8056\n",
            "Epoch [70/100], Loss: 3520.5732\n",
            "Epoch [80/100], Loss: 3728.5878\n",
            "Epoch [90/100], Loss: 4342.3896\n",
            "Epoch [100/100], Loss: 4946.2861\n",
            "Fold 3, RMSE: 97.88872528076172\n",
            "Epoch [10/100], Loss: 15196.2886\n",
            "Epoch [20/100], Loss: 14087.2537\n",
            "Epoch [30/100], Loss: 9021.6428\n",
            "Epoch [40/100], Loss: 10577.0042\n",
            "Epoch [50/100], Loss: 9860.6101\n",
            "Epoch [60/100], Loss: 10712.4094\n",
            "Epoch [70/100], Loss: 9027.3553\n",
            "Epoch [80/100], Loss: 9331.6062\n",
            "Epoch [90/100], Loss: 6551.9625\n",
            "Epoch [100/100], Loss: 6934.0878\n",
            "Fold 4, RMSE: 44.50960922241211\n",
            "Epoch [10/100], Loss: 14315.6716\n",
            "Epoch [20/100], Loss: 14403.0476\n",
            "Epoch [30/100], Loss: 10338.3704\n",
            "Epoch [40/100], Loss: 8936.0309\n",
            "Epoch [50/100], Loss: 8270.9236\n",
            "Epoch [60/100], Loss: 5921.0115\n",
            "Epoch [70/100], Loss: 2812.2471\n",
            "Epoch [80/100], Loss: 2592.0688\n",
            "Epoch [90/100], Loss: 5460.9033\n",
            "Epoch [100/100], Loss: 2485.5369\n",
            "Fold 5, RMSE: 44.88075256347656\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 64.19117584228516\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13270.6011\n",
            "Epoch [20/150], Loss: 7301.6820\n",
            "Epoch [30/150], Loss: 15531.3157\n",
            "Epoch [40/150], Loss: 5803.8995\n",
            "Epoch [50/150], Loss: 10417.8357\n",
            "Epoch [60/150], Loss: 3691.8591\n",
            "Epoch [70/150], Loss: 4760.9105\n",
            "Epoch [80/150], Loss: 2758.2480\n",
            "Epoch [90/150], Loss: 4097.9822\n",
            "Epoch [100/150], Loss: 2064.1685\n",
            "Epoch [110/150], Loss: 2190.1639\n",
            "Epoch [120/150], Loss: 1307.6055\n",
            "Epoch [130/150], Loss: 3408.4902\n",
            "Epoch [140/150], Loss: 9093.9304\n",
            "Epoch [150/150], Loss: 7780.3158\n",
            "Fold 1, RMSE: 58.996299743652344\n",
            "Epoch [10/150], Loss: 15786.8599\n",
            "Epoch [20/150], Loss: 16199.4177\n",
            "Epoch [30/150], Loss: 5245.4208\n",
            "Epoch [40/150], Loss: 6065.3159\n",
            "Epoch [50/150], Loss: 5491.6427\n",
            "Epoch [60/150], Loss: 4282.6350\n",
            "Epoch [70/150], Loss: 3155.1965\n",
            "Epoch [80/150], Loss: 1998.6873\n",
            "Epoch [90/150], Loss: 4415.9661\n",
            "Epoch [100/150], Loss: 3046.6987\n",
            "Epoch [110/150], Loss: 5622.1092\n",
            "Epoch [120/150], Loss: 1821.7287\n",
            "Epoch [130/150], Loss: 3299.2661\n",
            "Epoch [140/150], Loss: 1987.3062\n",
            "Epoch [150/150], Loss: 3580.4707\n",
            "Fold 2, RMSE: 67.29057312011719\n",
            "Epoch [10/150], Loss: 12428.5349\n",
            "Epoch [20/150], Loss: 7390.3461\n",
            "Epoch [30/150], Loss: 9555.1991\n",
            "Epoch [40/150], Loss: 5797.9111\n",
            "Epoch [50/150], Loss: 6206.1348\n",
            "Epoch [60/150], Loss: 2962.9682\n",
            "Epoch [70/150], Loss: 2669.3488\n",
            "Epoch [80/150], Loss: 3117.6266\n",
            "Epoch [90/150], Loss: 3619.0638\n",
            "Epoch [100/150], Loss: 2405.3152\n",
            "Epoch [110/150], Loss: 4495.6209\n",
            "Epoch [120/150], Loss: 4173.6000\n",
            "Epoch [130/150], Loss: 1999.2175\n",
            "Epoch [140/150], Loss: 2685.0172\n",
            "Epoch [150/150], Loss: 3274.4883\n",
            "Fold 3, RMSE: 91.89404296875\n",
            "Epoch [10/150], Loss: 15303.2263\n",
            "Epoch [20/150], Loss: 7349.2208\n",
            "Epoch [30/150], Loss: 8824.8682\n",
            "Epoch [40/150], Loss: 10908.6257\n",
            "Epoch [50/150], Loss: 7985.4388\n",
            "Epoch [60/150], Loss: 6110.5533\n",
            "Epoch [70/150], Loss: 3068.0742\n",
            "Epoch [80/150], Loss: 6210.7902\n",
            "Epoch [90/150], Loss: 3450.3632\n",
            "Epoch [100/150], Loss: 3739.8915\n",
            "Epoch [110/150], Loss: 2099.7563\n",
            "Epoch [120/150], Loss: 2991.0713\n",
            "Epoch [130/150], Loss: 1502.7234\n",
            "Epoch [140/150], Loss: 2989.3957\n",
            "Epoch [150/150], Loss: 6213.9637\n",
            "Fold 4, RMSE: 42.74961471557617\n",
            "Epoch [10/150], Loss: 11839.5066\n",
            "Epoch [20/150], Loss: 12279.3423\n",
            "Epoch [30/150], Loss: 9770.1763\n",
            "Epoch [40/150], Loss: 18620.1051\n",
            "Epoch [50/150], Loss: 8860.5084\n",
            "Epoch [60/150], Loss: 3763.7222\n",
            "Epoch [70/150], Loss: 7284.7649\n",
            "Epoch [80/150], Loss: 3134.0843\n",
            "Epoch [90/150], Loss: 8092.7872\n",
            "Epoch [100/150], Loss: 12548.9612\n",
            "Epoch [110/150], Loss: 8715.5806\n",
            "Epoch [120/150], Loss: 1615.0618\n",
            "Epoch [130/150], Loss: 3521.8260\n",
            "Epoch [140/150], Loss: 2837.6648\n",
            "Epoch [150/150], Loss: 4507.8320\n",
            "Fold 5, RMSE: 44.90137481689453\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 61.16638107299805\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 21012.6204\n",
            "Epoch [20/100], Loss: 15143.0671\n",
            "Epoch [30/100], Loss: 12727.0573\n",
            "Epoch [40/100], Loss: 13113.1926\n",
            "Epoch [50/100], Loss: 10141.3096\n",
            "Epoch [60/100], Loss: 6565.7472\n",
            "Epoch [70/100], Loss: 10310.0497\n",
            "Epoch [80/100], Loss: 3830.1013\n",
            "Epoch [90/100], Loss: 8514.5978\n",
            "Epoch [100/100], Loss: 7670.1694\n",
            "Fold 1, RMSE: 59.348533630371094\n",
            "Epoch [10/100], Loss: 25665.9216\n",
            "Epoch [20/100], Loss: 25419.2915\n",
            "Epoch [30/100], Loss: 18424.2495\n",
            "Epoch [40/100], Loss: 18807.5205\n",
            "Epoch [50/100], Loss: 19597.2161\n",
            "Epoch [60/100], Loss: 15127.4258\n",
            "Epoch [70/100], Loss: 18402.5635\n",
            "Epoch [80/100], Loss: 19934.7393\n",
            "Epoch [90/100], Loss: 17591.3564\n",
            "Epoch [100/100], Loss: 24154.0383\n",
            "Fold 2, RMSE: 87.20883178710938\n",
            "Epoch [10/100], Loss: 10826.9265\n",
            "Epoch [20/100], Loss: 8095.9298\n",
            "Epoch [30/100], Loss: 5760.6505\n",
            "Epoch [40/100], Loss: 5648.2112\n",
            "Epoch [50/100], Loss: 5338.7586\n",
            "Epoch [60/100], Loss: 4887.8088\n",
            "Epoch [70/100], Loss: 9240.2983\n",
            "Epoch [80/100], Loss: 4159.0399\n",
            "Epoch [90/100], Loss: 2288.5076\n",
            "Epoch [100/100], Loss: 3753.4104\n",
            "Fold 3, RMSE: 90.79784393310547\n",
            "Epoch [10/100], Loss: 14610.8910\n",
            "Epoch [20/100], Loss: 13676.7332\n",
            "Epoch [30/100], Loss: 17352.9335\n",
            "Epoch [40/100], Loss: 9507.5890\n",
            "Epoch [50/100], Loss: 4509.3724\n",
            "Epoch [60/100], Loss: 5911.1242\n",
            "Epoch [70/100], Loss: 7256.3397\n",
            "Epoch [80/100], Loss: 5601.7940\n",
            "Epoch [90/100], Loss: 6334.3696\n",
            "Epoch [100/100], Loss: 5346.0422\n",
            "Fold 4, RMSE: 41.067604064941406\n",
            "Epoch [10/100], Loss: 30102.6099\n",
            "Epoch [20/100], Loss: 18383.5073\n",
            "Epoch [30/100], Loss: 7281.7568\n",
            "Epoch [40/100], Loss: 10048.6213\n",
            "Epoch [50/100], Loss: 7224.6425\n",
            "Epoch [60/100], Loss: 6228.9188\n",
            "Epoch [70/100], Loss: 4892.6984\n",
            "Epoch [80/100], Loss: 2485.5979\n",
            "Epoch [90/100], Loss: 3911.8356\n",
            "Epoch [100/100], Loss: 2114.7440\n",
            "Fold 5, RMSE: 50.80366516113281\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 65.84529571533203\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 25037.0464\n",
            "Epoch [20/150], Loss: 14678.2417\n",
            "Epoch [30/150], Loss: 8407.2720\n",
            "Epoch [40/150], Loss: 15415.1394\n",
            "Epoch [50/150], Loss: 6600.8328\n",
            "Epoch [60/150], Loss: 3341.8108\n",
            "Epoch [70/150], Loss: 7427.3846\n",
            "Epoch [80/150], Loss: 4681.5092\n",
            "Epoch [90/150], Loss: 5163.2642\n",
            "Epoch [100/150], Loss: 2014.0522\n",
            "Epoch [110/150], Loss: 2548.7175\n",
            "Epoch [120/150], Loss: 2296.4586\n",
            "Epoch [130/150], Loss: 3486.3682\n",
            "Epoch [140/150], Loss: 5277.6993\n",
            "Epoch [150/150], Loss: 3571.0840\n",
            "Fold 1, RMSE: 57.51449966430664\n",
            "Epoch [10/150], Loss: 36885.3164\n",
            "Epoch [20/150], Loss: 15459.5867\n",
            "Epoch [30/150], Loss: 17162.0132\n",
            "Epoch [40/150], Loss: 15913.5225\n",
            "Epoch [50/150], Loss: 16658.1787\n",
            "Epoch [60/150], Loss: 18987.2192\n",
            "Epoch [70/150], Loss: 17124.5896\n",
            "Epoch [80/150], Loss: 16003.2419\n",
            "Epoch [90/150], Loss: 26351.1230\n",
            "Epoch [100/150], Loss: 18034.5518\n",
            "Epoch [110/150], Loss: 23693.0027\n",
            "Epoch [120/150], Loss: 15419.1177\n",
            "Epoch [130/150], Loss: 15519.4395\n",
            "Epoch [140/150], Loss: 17719.1550\n",
            "Epoch [150/150], Loss: 18366.1257\n",
            "Fold 2, RMSE: 87.1997299194336\n",
            "Epoch [10/150], Loss: 23792.1641\n",
            "Epoch [20/150], Loss: 9280.3145\n",
            "Epoch [30/150], Loss: 7456.1892\n",
            "Epoch [40/150], Loss: 6002.7706\n",
            "Epoch [50/150], Loss: 6537.5571\n",
            "Epoch [60/150], Loss: 3474.8738\n",
            "Epoch [70/150], Loss: 2655.9183\n",
            "Epoch [80/150], Loss: 2104.4485\n",
            "Epoch [90/150], Loss: 3229.5646\n",
            "Epoch [100/150], Loss: 4007.8248\n",
            "Epoch [110/150], Loss: 3264.5025\n",
            "Epoch [120/150], Loss: 7250.1697\n",
            "Epoch [130/150], Loss: 2471.4545\n",
            "Epoch [140/150], Loss: 3781.5032\n",
            "Epoch [150/150], Loss: 1747.2251\n",
            "Fold 3, RMSE: 91.54252624511719\n",
            "Epoch [10/150], Loss: 36072.4580\n",
            "Epoch [20/150], Loss: 19846.4717\n",
            "Epoch [30/150], Loss: 11201.3046\n",
            "Epoch [40/150], Loss: 10885.9246\n",
            "Epoch [50/150], Loss: 3613.5010\n",
            "Epoch [60/150], Loss: 11320.7585\n",
            "Epoch [70/150], Loss: 11340.0283\n",
            "Epoch [80/150], Loss: 5566.8962\n",
            "Epoch [90/150], Loss: 9304.1227\n",
            "Epoch [100/150], Loss: 5565.9000\n",
            "Epoch [110/150], Loss: 6852.8099\n",
            "Epoch [120/150], Loss: 5773.9945\n",
            "Epoch [130/150], Loss: 5212.8668\n",
            "Epoch [140/150], Loss: 4383.2057\n",
            "Epoch [150/150], Loss: 4245.7243\n",
            "Fold 4, RMSE: 44.69660568237305\n",
            "Epoch [10/150], Loss: 23767.6528\n",
            "Epoch [20/150], Loss: 19169.4414\n",
            "Epoch [30/150], Loss: 21512.3108\n",
            "Epoch [40/150], Loss: 18270.5706\n",
            "Epoch [50/150], Loss: 18175.1992\n",
            "Epoch [60/150], Loss: 16966.4113\n",
            "Epoch [70/150], Loss: 18530.9036\n",
            "Epoch [80/150], Loss: 20175.9487\n",
            "Epoch [90/150], Loss: 19175.1172\n",
            "Epoch [100/150], Loss: 19137.1543\n",
            "Epoch [110/150], Loss: 19004.6948\n",
            "Epoch [120/150], Loss: 16971.5148\n",
            "Epoch [130/150], Loss: 20846.1602\n",
            "Epoch [140/150], Loss: 17544.2910\n",
            "Epoch [150/150], Loss: 18233.7275\n",
            "Fold 5, RMSE: 57.808414459228516\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 67.75235519409179\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14647.4707\n",
            "Epoch [20/100], Loss: 8339.1974\n",
            "Epoch [30/100], Loss: 4067.5413\n",
            "Epoch [40/100], Loss: 7474.6691\n",
            "Epoch [50/100], Loss: 7355.3499\n",
            "Epoch [60/100], Loss: 6198.5485\n",
            "Epoch [70/100], Loss: 7028.9583\n",
            "Epoch [80/100], Loss: 3370.9988\n",
            "Epoch [90/100], Loss: 3018.5706\n",
            "Epoch [100/100], Loss: 7499.4846\n",
            "Fold 1, RMSE: 56.52444839477539\n",
            "Epoch [10/100], Loss: 20591.6353\n",
            "Epoch [20/100], Loss: 11666.3660\n",
            "Epoch [30/100], Loss: 10912.8291\n",
            "Epoch [40/100], Loss: 9567.3932\n",
            "Epoch [50/100], Loss: 9791.7870\n",
            "Epoch [60/100], Loss: 6207.7177\n",
            "Epoch [70/100], Loss: 8034.0004\n",
            "Epoch [80/100], Loss: 3466.5562\n",
            "Epoch [90/100], Loss: 4157.3160\n",
            "Epoch [100/100], Loss: 4460.2734\n",
            "Fold 2, RMSE: 75.06086730957031\n",
            "Epoch [10/100], Loss: 9687.9005\n",
            "Epoch [20/100], Loss: 7232.2338\n",
            "Epoch [30/100], Loss: 7825.3140\n",
            "Epoch [40/100], Loss: 3730.9734\n",
            "Epoch [50/100], Loss: 5697.9691\n",
            "Epoch [60/100], Loss: 2072.0933\n",
            "Epoch [70/100], Loss: 3785.7430\n",
            "Epoch [80/100], Loss: 2584.1188\n",
            "Epoch [90/100], Loss: 4163.7853\n",
            "Epoch [100/100], Loss: 8851.0686\n",
            "Fold 3, RMSE: 96.56217193603516\n",
            "Epoch [10/100], Loss: 16915.4807\n",
            "Epoch [20/100], Loss: 17932.9626\n",
            "Epoch [30/100], Loss: 4624.6641\n",
            "Epoch [40/100], Loss: 8346.3933\n",
            "Epoch [50/100], Loss: 6143.0082\n",
            "Epoch [60/100], Loss: 4552.0065\n",
            "Epoch [70/100], Loss: 5051.4188\n",
            "Epoch [80/100], Loss: 7978.4357\n",
            "Epoch [90/100], Loss: 3185.9518\n",
            "Epoch [100/100], Loss: 4029.3274\n",
            "Fold 4, RMSE: 42.58381271362305\n",
            "Epoch [10/100], Loss: 14433.7209\n",
            "Epoch [20/100], Loss: 11028.5995\n",
            "Epoch [30/100], Loss: 4242.1877\n",
            "Epoch [40/100], Loss: 3738.1695\n",
            "Epoch [50/100], Loss: 2840.5938\n",
            "Epoch [60/100], Loss: 5710.4925\n",
            "Epoch [70/100], Loss: 8875.7188\n",
            "Epoch [80/100], Loss: 7057.7125\n",
            "Epoch [90/100], Loss: 8885.5911\n",
            "Epoch [100/100], Loss: 6204.7232\n",
            "Fold 5, RMSE: 45.44891357421875\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 63.23604278564453\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14272.9336\n",
            "Epoch [20/150], Loss: 9597.6123\n",
            "Epoch [30/150], Loss: 6590.5669\n",
            "Epoch [40/150], Loss: 6332.2500\n",
            "Epoch [50/150], Loss: 4702.2552\n",
            "Epoch [60/150], Loss: 6537.0762\n",
            "Epoch [70/150], Loss: 6563.5814\n",
            "Epoch [80/150], Loss: 4603.1480\n",
            "Epoch [90/150], Loss: 6178.9188\n",
            "Epoch [100/150], Loss: 5022.1716\n",
            "Epoch [110/150], Loss: 5128.3571\n",
            "Epoch [120/150], Loss: 3102.2731\n",
            "Epoch [130/150], Loss: 3914.8169\n",
            "Epoch [140/150], Loss: 5673.2234\n",
            "Epoch [150/150], Loss: 2758.3934\n",
            "Fold 1, RMSE: 57.8505744934082\n",
            "Epoch [10/150], Loss: 23068.0232\n",
            "Epoch [20/150], Loss: 18559.9229\n",
            "Epoch [30/150], Loss: 10511.6865\n",
            "Epoch [40/150], Loss: 6401.0292\n",
            "Epoch [50/150], Loss: 11287.2854\n",
            "Epoch [60/150], Loss: 2602.4569\n",
            "Epoch [70/150], Loss: 3536.0201\n",
            "Epoch [80/150], Loss: 6815.2438\n",
            "Epoch [90/150], Loss: 2229.4645\n",
            "Epoch [100/150], Loss: 3227.5775\n",
            "Epoch [110/150], Loss: 1885.4123\n",
            "Epoch [120/150], Loss: 3570.9355\n",
            "Epoch [130/150], Loss: 3391.2234\n",
            "Epoch [140/150], Loss: 5323.1127\n",
            "Epoch [150/150], Loss: 8158.7412\n",
            "Fold 2, RMSE: 70.62882995605469\n",
            "Epoch [10/150], Loss: 9406.2308\n",
            "Epoch [20/150], Loss: 8773.4720\n",
            "Epoch [30/150], Loss: 4765.5444\n",
            "Epoch [40/150], Loss: 4408.9507\n",
            "Epoch [50/150], Loss: 3988.5201\n",
            "Epoch [60/150], Loss: 3641.8726\n",
            "Epoch [70/150], Loss: 3210.4940\n",
            "Epoch [80/150], Loss: 4131.0966\n",
            "Epoch [90/150], Loss: 7730.9683\n",
            "Epoch [100/150], Loss: 3494.1564\n",
            "Epoch [110/150], Loss: 4253.0027\n",
            "Epoch [120/150], Loss: 2819.2375\n",
            "Epoch [130/150], Loss: 2960.0665\n",
            "Epoch [140/150], Loss: 4547.1326\n",
            "Epoch [150/150], Loss: 2791.7278\n",
            "Fold 3, RMSE: 98.48323059082031\n",
            "Epoch [10/150], Loss: 14627.3975\n",
            "Epoch [20/150], Loss: 11449.0043\n",
            "Epoch [30/150], Loss: 4926.2195\n",
            "Epoch [40/150], Loss: 6712.9647\n",
            "Epoch [50/150], Loss: 11279.0930\n",
            "Epoch [60/150], Loss: 6908.2488\n",
            "Epoch [70/150], Loss: 6308.1182\n",
            "Epoch [80/150], Loss: 6302.7221\n",
            "Epoch [90/150], Loss: 6073.7622\n",
            "Epoch [100/150], Loss: 1836.5033\n",
            "Epoch [110/150], Loss: 1522.6851\n",
            "Epoch [120/150], Loss: 1666.3939\n",
            "Epoch [130/150], Loss: 1717.4119\n",
            "Epoch [140/150], Loss: 1891.0353\n",
            "Epoch [150/150], Loss: 4005.6779\n",
            "Fold 4, RMSE: 45.10179901123047\n",
            "Epoch [10/150], Loss: 13169.4604\n",
            "Epoch [20/150], Loss: 9019.2537\n",
            "Epoch [30/150], Loss: 7441.7474\n",
            "Epoch [40/150], Loss: 5763.0778\n",
            "Epoch [50/150], Loss: 4981.0325\n",
            "Epoch [60/150], Loss: 3794.2584\n",
            "Epoch [70/150], Loss: 4096.5936\n",
            "Epoch [80/150], Loss: 9003.8713\n",
            "Epoch [90/150], Loss: 1317.3555\n",
            "Epoch [100/150], Loss: 10781.4154\n",
            "Epoch [110/150], Loss: 4574.6985\n",
            "Epoch [120/150], Loss: 3061.2582\n",
            "Epoch [130/150], Loss: 12865.1531\n",
            "Epoch [140/150], Loss: 5146.7772\n",
            "Epoch [150/150], Loss: 2413.1039\n",
            "Fold 5, RMSE: 45.97067642211914\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 63.60702209472656\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 15920.5001\n",
            "Epoch [20/100], Loss: 12437.0938\n",
            "Epoch [30/100], Loss: 11561.0244\n",
            "Epoch [40/100], Loss: 8502.9688\n",
            "Epoch [50/100], Loss: 7906.4603\n",
            "Epoch [60/100], Loss: 11033.9419\n",
            "Epoch [70/100], Loss: 6019.6808\n",
            "Epoch [80/100], Loss: 5825.1044\n",
            "Epoch [90/100], Loss: 4773.8022\n",
            "Epoch [100/100], Loss: 6845.4686\n",
            "Fold 1, RMSE: 64.24015808105469\n",
            "Epoch [10/100], Loss: 21644.7693\n",
            "Epoch [20/100], Loss: 13865.8220\n",
            "Epoch [30/100], Loss: 13510.8877\n",
            "Epoch [40/100], Loss: 10278.7075\n",
            "Epoch [50/100], Loss: 9192.3593\n",
            "Epoch [60/100], Loss: 4018.4944\n",
            "Epoch [70/100], Loss: 7459.0653\n",
            "Epoch [80/100], Loss: 3447.4078\n",
            "Epoch [90/100], Loss: 4654.8830\n",
            "Epoch [100/100], Loss: 3007.7358\n",
            "Fold 2, RMSE: 73.37904357910156\n",
            "Epoch [10/100], Loss: 13938.4817\n",
            "Epoch [20/100], Loss: 8162.2212\n",
            "Epoch [30/100], Loss: 7285.2502\n",
            "Epoch [40/100], Loss: 4665.8903\n",
            "Epoch [50/100], Loss: 5466.9241\n",
            "Epoch [60/100], Loss: 5747.8912\n",
            "Epoch [70/100], Loss: 5231.5801\n",
            "Epoch [80/100], Loss: 6566.7856\n",
            "Epoch [90/100], Loss: 4031.3201\n",
            "Epoch [100/100], Loss: 4242.9631\n",
            "Fold 3, RMSE: 96.88060760498047\n",
            "Epoch [10/100], Loss: 24029.0981\n",
            "Epoch [20/100], Loss: 22771.1213\n",
            "Epoch [30/100], Loss: 10195.2303\n",
            "Epoch [40/100], Loss: 12378.2900\n",
            "Epoch [50/100], Loss: 6736.5076\n",
            "Epoch [60/100], Loss: 4667.5684\n",
            "Epoch [70/100], Loss: 5020.7549\n",
            "Epoch [80/100], Loss: 9518.0105\n",
            "Epoch [90/100], Loss: 6438.1753\n",
            "Epoch [100/100], Loss: 3081.8388\n",
            "Fold 4, RMSE: 42.19318771362305\n",
            "Epoch [10/100], Loss: 19027.7554\n",
            "Epoch [20/100], Loss: 12293.6572\n",
            "Epoch [30/100], Loss: 8068.9126\n",
            "Epoch [40/100], Loss: 6999.9445\n",
            "Epoch [50/100], Loss: 7499.3572\n",
            "Epoch [60/100], Loss: 2614.1810\n",
            "Epoch [70/100], Loss: 4838.4429\n",
            "Epoch [80/100], Loss: 16500.2188\n",
            "Epoch [90/100], Loss: 4155.7700\n",
            "Epoch [100/100], Loss: 8985.6705\n",
            "Fold 5, RMSE: 45.800392150878906\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 64.49867782592773\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18650.4695\n",
            "Epoch [20/150], Loss: 12798.6802\n",
            "Epoch [30/150], Loss: 10413.5641\n",
            "Epoch [40/150], Loss: 5529.2048\n",
            "Epoch [50/150], Loss: 7575.7354\n",
            "Epoch [60/150], Loss: 7817.8177\n",
            "Epoch [70/150], Loss: 7288.8309\n",
            "Epoch [80/150], Loss: 8529.9087\n",
            "Epoch [90/150], Loss: 5854.9551\n",
            "Epoch [100/150], Loss: 2605.1043\n",
            "Epoch [110/150], Loss: 3383.2357\n",
            "Epoch [120/150], Loss: 3502.6580\n",
            "Epoch [130/150], Loss: 4377.8299\n",
            "Epoch [140/150], Loss: 7209.8881\n",
            "Epoch [150/150], Loss: 4017.4509\n",
            "Fold 1, RMSE: 59.16284942626953\n",
            "Epoch [10/150], Loss: 17066.4187\n",
            "Epoch [20/150], Loss: 18035.7031\n",
            "Epoch [30/150], Loss: 14479.2083\n",
            "Epoch [40/150], Loss: 26359.6482\n",
            "Epoch [50/150], Loss: 16363.8701\n",
            "Epoch [60/150], Loss: 15010.4775\n",
            "Epoch [70/150], Loss: 14154.0470\n",
            "Epoch [80/150], Loss: 15119.0388\n",
            "Epoch [90/150], Loss: 17539.0337\n",
            "Epoch [100/150], Loss: 18355.9585\n",
            "Epoch [110/150], Loss: 14845.3933\n",
            "Epoch [120/150], Loss: 19995.9675\n",
            "Epoch [130/150], Loss: 17680.4709\n",
            "Epoch [140/150], Loss: 17106.4199\n",
            "Epoch [150/150], Loss: 14983.2661\n",
            "Fold 2, RMSE: 86.82615661621094\n",
            "Epoch [10/150], Loss: 15004.1069\n",
            "Epoch [20/150], Loss: 8073.9348\n",
            "Epoch [30/150], Loss: 12742.6973\n",
            "Epoch [40/150], Loss: 7595.9612\n",
            "Epoch [50/150], Loss: 4305.7004\n",
            "Epoch [60/150], Loss: 2628.0742\n",
            "Epoch [70/150], Loss: 3755.8554\n",
            "Epoch [80/150], Loss: 7647.5153\n",
            "Epoch [90/150], Loss: 1540.6846\n",
            "Epoch [100/150], Loss: 6223.2901\n",
            "Epoch [110/150], Loss: 5926.9425\n",
            "Epoch [120/150], Loss: 2882.5154\n",
            "Epoch [130/150], Loss: 5335.6710\n",
            "Epoch [140/150], Loss: 3476.3577\n",
            "Epoch [150/150], Loss: 6635.7623\n",
            "Fold 3, RMSE: 93.0444107055664\n",
            "Epoch [10/150], Loss: 23961.9463\n",
            "Epoch [20/150], Loss: 11225.3960\n",
            "Epoch [30/150], Loss: 10591.1082\n",
            "Epoch [40/150], Loss: 7815.2358\n",
            "Epoch [50/150], Loss: 7137.3811\n",
            "Epoch [60/150], Loss: 5635.2274\n",
            "Epoch [70/150], Loss: 7345.9825\n",
            "Epoch [80/150], Loss: 2347.3644\n",
            "Epoch [90/150], Loss: 3080.8486\n",
            "Epoch [100/150], Loss: 3982.9583\n",
            "Epoch [110/150], Loss: 4479.0574\n",
            "Epoch [120/150], Loss: 5640.1171\n",
            "Epoch [130/150], Loss: 5528.1995\n",
            "Epoch [140/150], Loss: 4356.3123\n",
            "Epoch [150/150], Loss: 4632.8817\n",
            "Fold 4, RMSE: 41.254634857177734\n",
            "Epoch [10/150], Loss: 44287.5967\n",
            "Epoch [20/150], Loss: 12934.7070\n",
            "Epoch [30/150], Loss: 12920.5027\n",
            "Epoch [40/150], Loss: 8023.3488\n",
            "Epoch [50/150], Loss: 7057.4884\n",
            "Epoch [60/150], Loss: 3029.4101\n",
            "Epoch [70/150], Loss: 5741.0972\n",
            "Epoch [80/150], Loss: 3418.5574\n",
            "Epoch [90/150], Loss: 2636.4543\n",
            "Epoch [100/150], Loss: 3591.7365\n",
            "Epoch [110/150], Loss: 8106.9443\n",
            "Epoch [120/150], Loss: 4217.0684\n",
            "Epoch [130/150], Loss: 3070.6594\n",
            "Epoch [140/150], Loss: 3889.2346\n",
            "Epoch [150/150], Loss: 3672.3099\n",
            "Fold 5, RMSE: 46.94947052001953\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 65.44750442504883\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 11671.2954\n",
            "Epoch [20/100], Loss: 11157.1072\n",
            "Epoch [30/100], Loss: 4871.1812\n",
            "Epoch [40/100], Loss: 4732.7253\n",
            "Epoch [50/100], Loss: 4712.2207\n",
            "Epoch [60/100], Loss: 4263.3370\n",
            "Epoch [70/100], Loss: 2582.3016\n",
            "Epoch [80/100], Loss: 5861.0673\n",
            "Epoch [90/100], Loss: 5573.5131\n",
            "Epoch [100/100], Loss: 2834.1229\n",
            "Fold 1, RMSE: 57.12697982788086\n",
            "Epoch [10/100], Loss: 13340.3179\n",
            "Epoch [20/100], Loss: 8604.3696\n",
            "Epoch [30/100], Loss: 14571.7451\n",
            "Epoch [40/100], Loss: 8648.7065\n",
            "Epoch [50/100], Loss: 5248.1841\n",
            "Epoch [60/100], Loss: 6911.6772\n",
            "Epoch [70/100], Loss: 7033.2922\n",
            "Epoch [80/100], Loss: 6899.5018\n",
            "Epoch [90/100], Loss: 3023.0399\n",
            "Epoch [100/100], Loss: 4673.5938\n",
            "Fold 2, RMSE: 69.12287139892578\n",
            "Epoch [10/100], Loss: 7259.4266\n",
            "Epoch [20/100], Loss: 12125.6738\n",
            "Epoch [30/100], Loss: 22803.4005\n",
            "Epoch [40/100], Loss: 3349.3258\n",
            "Epoch [50/100], Loss: 4743.4636\n",
            "Epoch [60/100], Loss: 2195.3744\n",
            "Epoch [70/100], Loss: 4923.0474\n",
            "Epoch [80/100], Loss: 4770.9194\n",
            "Epoch [90/100], Loss: 4507.4879\n",
            "Epoch [100/100], Loss: 2816.2323\n",
            "Fold 3, RMSE: 98.67731475830078\n",
            "Epoch [10/100], Loss: 23782.2039\n",
            "Epoch [20/100], Loss: 19217.5479\n",
            "Epoch [30/100], Loss: 15926.6303\n",
            "Epoch [40/100], Loss: 5671.6521\n",
            "Epoch [50/100], Loss: 7915.4147\n",
            "Epoch [60/100], Loss: 5924.8655\n",
            "Epoch [70/100], Loss: 4123.8229\n",
            "Epoch [80/100], Loss: 3063.4686\n",
            "Epoch [90/100], Loss: 7676.8274\n",
            "Epoch [100/100], Loss: 6670.1248\n",
            "Fold 4, RMSE: 44.318477630615234\n",
            "Epoch [10/100], Loss: 23294.1562\n",
            "Epoch [20/100], Loss: 10305.8447\n",
            "Epoch [30/100], Loss: 8033.6467\n",
            "Epoch [40/100], Loss: 7980.2474\n",
            "Epoch [50/100], Loss: 4297.2030\n",
            "Epoch [60/100], Loss: 5027.4886\n",
            "Epoch [70/100], Loss: 4462.0830\n",
            "Epoch [80/100], Loss: 4429.9176\n",
            "Epoch [90/100], Loss: 5545.6083\n",
            "Epoch [100/100], Loss: 2307.3829\n",
            "Fold 5, RMSE: 44.06779098510742\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 62.66268692016602\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 10819.9203\n",
            "Epoch [20/150], Loss: 13649.6384\n",
            "Epoch [30/150], Loss: 4617.4327\n",
            "Epoch [40/150], Loss: 5134.9604\n",
            "Epoch [50/150], Loss: 5450.0210\n",
            "Epoch [60/150], Loss: 5131.8817\n",
            "Epoch [70/150], Loss: 3315.5655\n",
            "Epoch [80/150], Loss: 4340.1887\n",
            "Epoch [90/150], Loss: 3010.0386\n",
            "Epoch [100/150], Loss: 4069.6826\n",
            "Epoch [110/150], Loss: 2969.0952\n",
            "Epoch [120/150], Loss: 1114.1398\n",
            "Epoch [130/150], Loss: 1647.5184\n",
            "Epoch [140/150], Loss: 2376.3135\n",
            "Epoch [150/150], Loss: 2831.8047\n",
            "Fold 1, RMSE: 58.80573272705078\n",
            "Epoch [10/150], Loss: 15674.9651\n",
            "Epoch [20/150], Loss: 8554.0776\n",
            "Epoch [30/150], Loss: 9423.7716\n",
            "Epoch [40/150], Loss: 9175.6652\n",
            "Epoch [50/150], Loss: 5757.2253\n",
            "Epoch [60/150], Loss: 7854.1091\n",
            "Epoch [70/150], Loss: 6726.7173\n",
            "Epoch [80/150], Loss: 3553.8270\n",
            "Epoch [90/150], Loss: 3329.5759\n",
            "Epoch [100/150], Loss: 6605.9111\n",
            "Epoch [110/150], Loss: 5029.6787\n",
            "Epoch [120/150], Loss: 5087.5308\n",
            "Epoch [130/150], Loss: 6495.3566\n",
            "Epoch [140/150], Loss: 6062.8931\n",
            "Epoch [150/150], Loss: 2020.2219\n",
            "Fold 2, RMSE: 67.36549377441406\n",
            "Epoch [10/150], Loss: 8261.7683\n",
            "Epoch [20/150], Loss: 7233.6619\n",
            "Epoch [30/150], Loss: 7153.6249\n",
            "Epoch [40/150], Loss: 4557.0858\n",
            "Epoch [50/150], Loss: 3714.0392\n",
            "Epoch [60/150], Loss: 2378.2932\n",
            "Epoch [70/150], Loss: 1619.0084\n",
            "Epoch [80/150], Loss: 3936.1276\n",
            "Epoch [90/150], Loss: 1541.7614\n",
            "Epoch [100/150], Loss: 1897.7294\n",
            "Epoch [110/150], Loss: 3017.2248\n",
            "Epoch [120/150], Loss: 5223.0450\n",
            "Epoch [130/150], Loss: 2754.7241\n",
            "Epoch [140/150], Loss: 3418.2613\n",
            "Epoch [150/150], Loss: 1298.1877\n",
            "Fold 3, RMSE: 93.50906372070312\n",
            "Epoch [10/150], Loss: 18053.5386\n",
            "Epoch [20/150], Loss: 13316.3931\n",
            "Epoch [30/150], Loss: 10460.3987\n",
            "Epoch [40/150], Loss: 7973.9623\n",
            "Epoch [50/150], Loss: 6175.1384\n",
            "Epoch [60/150], Loss: 5223.1942\n",
            "Epoch [70/150], Loss: 5330.4479\n",
            "Epoch [80/150], Loss: 5609.5056\n",
            "Epoch [90/150], Loss: 3657.7115\n",
            "Epoch [100/150], Loss: 2092.2037\n",
            "Epoch [110/150], Loss: 2474.5586\n",
            "Epoch [120/150], Loss: 1744.0966\n",
            "Epoch [130/150], Loss: 1538.4411\n",
            "Epoch [140/150], Loss: 1434.6632\n",
            "Epoch [150/150], Loss: 1944.6821\n",
            "Fold 4, RMSE: 42.88106918334961\n",
            "Epoch [10/150], Loss: 14605.8845\n",
            "Epoch [20/150], Loss: 9805.1401\n",
            "Epoch [30/150], Loss: 5861.3215\n",
            "Epoch [40/150], Loss: 4605.8015\n",
            "Epoch [50/150], Loss: 9010.4314\n",
            "Epoch [60/150], Loss: 4523.5671\n",
            "Epoch [70/150], Loss: 1903.5926\n",
            "Epoch [80/150], Loss: 4513.9709\n",
            "Epoch [90/150], Loss: 1914.9986\n",
            "Epoch [100/150], Loss: 2222.6962\n",
            "Epoch [110/150], Loss: 2528.7941\n",
            "Epoch [120/150], Loss: 2229.1620\n",
            "Epoch [130/150], Loss: 2403.5993\n",
            "Epoch [140/150], Loss: 3850.8293\n",
            "Epoch [150/150], Loss: 1270.5243\n",
            "Fold 5, RMSE: 44.4955940246582\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 61.411390686035155\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 168206.8633\n",
            "Epoch [20/100], Loss: 40070.8037\n",
            "Epoch [30/100], Loss: 19602.9130\n",
            "Epoch [40/100], Loss: 17162.4136\n",
            "Epoch [50/100], Loss: 14545.9866\n",
            "Epoch [60/100], Loss: 14156.1897\n",
            "Epoch [70/100], Loss: 13196.4630\n",
            "Epoch [80/100], Loss: 6971.9829\n",
            "Epoch [90/100], Loss: 16132.3618\n",
            "Epoch [100/100], Loss: 8208.9652\n",
            "Fold 1, RMSE: 55.141056060791016\n",
            "Epoch [10/100], Loss: 22208.4009\n",
            "Epoch [20/100], Loss: 18651.2915\n",
            "Epoch [30/100], Loss: 17217.9980\n",
            "Epoch [40/100], Loss: 15426.3589\n",
            "Epoch [50/100], Loss: 20674.7456\n",
            "Epoch [60/100], Loss: 14417.0392\n",
            "Epoch [70/100], Loss: 14432.6282\n",
            "Epoch [80/100], Loss: 15079.2766\n",
            "Epoch [90/100], Loss: 17657.2769\n",
            "Epoch [100/100], Loss: 14614.1366\n",
            "Fold 2, RMSE: 87.23233795166016\n",
            "Epoch [10/100], Loss: 18388.0420\n",
            "Epoch [20/100], Loss: 17909.7710\n",
            "Epoch [30/100], Loss: 11888.5406\n",
            "Epoch [40/100], Loss: 16636.6343\n",
            "Epoch [50/100], Loss: 14077.3411\n",
            "Epoch [60/100], Loss: 20666.2678\n",
            "Epoch [70/100], Loss: 11427.3384\n",
            "Epoch [80/100], Loss: 15547.3452\n",
            "Epoch [90/100], Loss: 16879.6072\n",
            "Epoch [100/100], Loss: 16330.5884\n",
            "Fold 3, RMSE: 109.6714096069336\n",
            "Epoch [10/100], Loss: 22573.9399\n",
            "Epoch [20/100], Loss: 19803.5112\n",
            "Epoch [30/100], Loss: 28201.6855\n",
            "Epoch [40/100], Loss: 23153.8228\n",
            "Epoch [50/100], Loss: 17501.2776\n",
            "Epoch [60/100], Loss: 16205.1689\n",
            "Epoch [70/100], Loss: 17928.6233\n",
            "Epoch [80/100], Loss: 30618.2961\n",
            "Epoch [90/100], Loss: 20538.3711\n",
            "Epoch [100/100], Loss: 21092.8665\n",
            "Fold 4, RMSE: 50.362422943115234\n",
            "Epoch [10/100], Loss: 28077.0850\n",
            "Epoch [20/100], Loss: 15817.4927\n",
            "Epoch [30/100], Loss: 17416.5103\n",
            "Epoch [40/100], Loss: 16917.1489\n",
            "Epoch [50/100], Loss: 16507.7007\n",
            "Epoch [60/100], Loss: 19873.3374\n",
            "Epoch [70/100], Loss: 17329.4497\n",
            "Epoch [80/100], Loss: 16740.1958\n",
            "Epoch [90/100], Loss: 16800.8394\n",
            "Epoch [100/100], Loss: 17412.9697\n",
            "Fold 5, RMSE: 57.863887786865234\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 72.05422286987304\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 20935.2759\n",
            "Epoch [20/150], Loss: 13987.1674\n",
            "Epoch [30/150], Loss: 11779.9614\n",
            "Epoch [40/150], Loss: 12307.4067\n",
            "Epoch [50/150], Loss: 10965.5237\n",
            "Epoch [60/150], Loss: 6290.4845\n",
            "Epoch [70/150], Loss: 7549.2933\n",
            "Epoch [80/150], Loss: 7307.6881\n",
            "Epoch [90/150], Loss: 3876.9821\n",
            "Epoch [100/150], Loss: 3059.5236\n",
            "Epoch [110/150], Loss: 9084.4968\n",
            "Epoch [120/150], Loss: 3006.7239\n",
            "Epoch [130/150], Loss: 14637.3418\n",
            "Epoch [140/150], Loss: 5756.3916\n",
            "Epoch [150/150], Loss: 3602.7935\n",
            "Fold 1, RMSE: 58.113868713378906\n",
            "Epoch [10/150], Loss: 118818.0762\n",
            "Epoch [20/150], Loss: 26909.3296\n",
            "Epoch [30/150], Loss: 18208.7380\n",
            "Epoch [40/150], Loss: 18299.6372\n",
            "Epoch [50/150], Loss: 16457.2080\n",
            "Epoch [60/150], Loss: 14175.2407\n",
            "Epoch [70/150], Loss: 15888.0376\n",
            "Epoch [80/150], Loss: 17000.5420\n",
            "Epoch [90/150], Loss: 15627.3135\n",
            "Epoch [100/150], Loss: 14425.2705\n",
            "Epoch [110/150], Loss: 10851.7406\n",
            "Epoch [120/150], Loss: 11820.0840\n",
            "Epoch [130/150], Loss: 11871.7446\n",
            "Epoch [140/150], Loss: 7147.0145\n",
            "Epoch [150/150], Loss: 7348.6265\n",
            "Fold 2, RMSE: 64.09809875488281\n",
            "Epoch [10/150], Loss: 19985.2974\n",
            "Epoch [20/150], Loss: 13071.7993\n",
            "Epoch [30/150], Loss: 15649.3933\n",
            "Epoch [40/150], Loss: 11988.2908\n",
            "Epoch [50/150], Loss: 18108.8511\n",
            "Epoch [60/150], Loss: 13782.0869\n",
            "Epoch [70/150], Loss: 11432.9274\n",
            "Epoch [80/150], Loss: 13893.7729\n",
            "Epoch [90/150], Loss: 12531.4714\n",
            "Epoch [100/150], Loss: 12578.1982\n",
            "Epoch [110/150], Loss: 13445.5686\n",
            "Epoch [120/150], Loss: 14825.7046\n",
            "Epoch [130/150], Loss: 12978.2217\n",
            "Epoch [140/150], Loss: 12208.7358\n",
            "Epoch [150/150], Loss: 11655.7823\n",
            "Fold 3, RMSE: 109.65357971191406\n",
            "Epoch [10/150], Loss: 23716.6992\n",
            "Epoch [20/150], Loss: 20138.0884\n",
            "Epoch [30/150], Loss: 22227.1860\n",
            "Epoch [40/150], Loss: 23041.7451\n",
            "Epoch [50/150], Loss: 25603.6011\n",
            "Epoch [60/150], Loss: 20765.1462\n",
            "Epoch [70/150], Loss: 18965.6865\n",
            "Epoch [80/150], Loss: 17307.6252\n",
            "Epoch [90/150], Loss: 20434.2603\n",
            "Epoch [100/150], Loss: 19251.1602\n",
            "Epoch [110/150], Loss: 28820.2104\n",
            "Epoch [120/150], Loss: 18774.5635\n",
            "Epoch [130/150], Loss: 29159.4277\n",
            "Epoch [140/150], Loss: 18054.2363\n",
            "Epoch [150/150], Loss: 22299.5469\n",
            "Fold 4, RMSE: 54.12909698486328\n",
            "Epoch [10/150], Loss: 134277.1133\n",
            "Epoch [20/150], Loss: 18140.8586\n",
            "Epoch [30/150], Loss: 12675.9023\n",
            "Epoch [40/150], Loss: 11803.6516\n",
            "Epoch [50/150], Loss: 13394.8262\n",
            "Epoch [60/150], Loss: 9458.5924\n",
            "Epoch [70/150], Loss: 9574.0970\n",
            "Epoch [80/150], Loss: 14008.0734\n",
            "Epoch [90/150], Loss: 11189.0137\n",
            "Epoch [100/150], Loss: 8217.6554\n",
            "Epoch [110/150], Loss: 13290.6592\n",
            "Epoch [120/150], Loss: 17479.8751\n",
            "Epoch [130/150], Loss: 10853.7854\n",
            "Epoch [140/150], Loss: 9016.0594\n",
            "Epoch [150/150], Loss: 6404.5431\n",
            "Fold 5, RMSE: 46.337242126464844\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 66.46637725830078\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 52650.5156\n",
            "Epoch [20/100], Loss: 18023.7776\n",
            "Epoch [30/100], Loss: 15571.5271\n",
            "Epoch [40/100], Loss: 16634.7354\n",
            "Epoch [50/100], Loss: 15848.5728\n",
            "Epoch [60/100], Loss: 12668.3567\n",
            "Epoch [70/100], Loss: 12173.7960\n",
            "Epoch [80/100], Loss: 14073.6714\n",
            "Epoch [90/100], Loss: 16681.8647\n",
            "Epoch [100/100], Loss: 12247.5919\n",
            "Fold 1, RMSE: 53.54241943359375\n",
            "Epoch [10/100], Loss: 33030.4434\n",
            "Epoch [20/100], Loss: 14192.2449\n",
            "Epoch [30/100], Loss: 9458.5286\n",
            "Epoch [40/100], Loss: 9317.6794\n",
            "Epoch [50/100], Loss: 12225.4629\n",
            "Epoch [60/100], Loss: 12881.2043\n",
            "Epoch [70/100], Loss: 10460.8479\n",
            "Epoch [80/100], Loss: 8247.8157\n",
            "Epoch [90/100], Loss: 8393.5427\n",
            "Epoch [100/100], Loss: 9207.8672\n",
            "Fold 2, RMSE: 69.34439086914062\n",
            "Epoch [10/100], Loss: 40199.3301\n",
            "Epoch [20/100], Loss: 12725.8625\n",
            "Epoch [30/100], Loss: 8658.9103\n",
            "Epoch [40/100], Loss: 12101.9817\n",
            "Epoch [50/100], Loss: 9187.0134\n",
            "Epoch [60/100], Loss: 9986.6675\n",
            "Epoch [70/100], Loss: 9730.7244\n",
            "Epoch [80/100], Loss: 7606.6538\n",
            "Epoch [90/100], Loss: 7148.4133\n",
            "Epoch [100/100], Loss: 6929.9033\n",
            "Fold 3, RMSE: 96.8203353881836\n",
            "Epoch [10/100], Loss: 39522.7861\n",
            "Epoch [20/100], Loss: 25707.2549\n",
            "Epoch [30/100], Loss: 14521.5969\n",
            "Epoch [40/100], Loss: 13169.4026\n",
            "Epoch [50/100], Loss: 11350.3503\n",
            "Epoch [60/100], Loss: 11935.5620\n",
            "Epoch [70/100], Loss: 9482.9317\n",
            "Epoch [80/100], Loss: 12275.8337\n",
            "Epoch [90/100], Loss: 8708.8257\n",
            "Epoch [100/100], Loss: 11142.5205\n",
            "Fold 4, RMSE: 38.4430046081543\n",
            "Epoch [10/100], Loss: 14044.7581\n",
            "Epoch [20/100], Loss: 10937.4760\n",
            "Epoch [30/100], Loss: 17822.3730\n",
            "Epoch [40/100], Loss: 10952.5906\n",
            "Epoch [50/100], Loss: 14765.8428\n",
            "Epoch [60/100], Loss: 12483.5989\n",
            "Epoch [70/100], Loss: 11323.7925\n",
            "Epoch [80/100], Loss: 10012.8574\n",
            "Epoch [90/100], Loss: 7992.7125\n",
            "Epoch [100/100], Loss: 8353.5582\n",
            "Fold 5, RMSE: 44.771976470947266\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 60.584425354003905\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 41752.3457\n",
            "Epoch [20/150], Loss: 12474.9343\n",
            "Epoch [30/150], Loss: 11994.8303\n",
            "Epoch [40/150], Loss: 14296.1375\n",
            "Epoch [50/150], Loss: 12853.5762\n",
            "Epoch [60/150], Loss: 14543.0630\n",
            "Epoch [70/150], Loss: 11277.2190\n",
            "Epoch [80/150], Loss: 19308.6235\n",
            "Epoch [90/150], Loss: 11884.7556\n",
            "Epoch [100/150], Loss: 17062.0515\n",
            "Epoch [110/150], Loss: 11854.5996\n",
            "Epoch [120/150], Loss: 13796.0547\n",
            "Epoch [130/150], Loss: 11952.9028\n",
            "Epoch [140/150], Loss: 9495.5334\n",
            "Epoch [150/150], Loss: 13725.7441\n",
            "Fold 1, RMSE: 53.047210693359375\n",
            "Epoch [10/150], Loss: 13122.3519\n",
            "Epoch [20/150], Loss: 18997.7532\n",
            "Epoch [30/150], Loss: 19052.4072\n",
            "Epoch [40/150], Loss: 15509.8342\n",
            "Epoch [50/150], Loss: 17732.5591\n",
            "Epoch [60/150], Loss: 21632.7651\n",
            "Epoch [70/150], Loss: 11797.7029\n",
            "Epoch [80/150], Loss: 13734.3567\n",
            "Epoch [90/150], Loss: 11871.1023\n",
            "Epoch [100/150], Loss: 13999.0693\n",
            "Epoch [110/150], Loss: 11038.2087\n",
            "Epoch [120/150], Loss: 8763.4568\n",
            "Epoch [130/150], Loss: 10924.1128\n",
            "Epoch [140/150], Loss: 6855.9327\n",
            "Epoch [150/150], Loss: 6575.0250\n",
            "Fold 2, RMSE: 65.89844512939453\n",
            "Epoch [10/150], Loss: 11935.3459\n",
            "Epoch [20/150], Loss: 14327.2300\n",
            "Epoch [30/150], Loss: 12758.7861\n",
            "Epoch [40/150], Loss: 14098.4131\n",
            "Epoch [50/150], Loss: 14684.7175\n",
            "Epoch [60/150], Loss: 9750.2535\n",
            "Epoch [70/150], Loss: 7913.0985\n",
            "Epoch [80/150], Loss: 6901.0200\n",
            "Epoch [90/150], Loss: 8370.3245\n",
            "Epoch [100/150], Loss: 7322.1102\n",
            "Epoch [110/150], Loss: 7338.6431\n",
            "Epoch [120/150], Loss: 7662.3748\n",
            "Epoch [130/150], Loss: 12493.8292\n",
            "Epoch [140/150], Loss: 8849.9170\n",
            "Epoch [150/150], Loss: 6508.1478\n",
            "Fold 3, RMSE: 86.95796966552734\n",
            "Epoch [10/150], Loss: 21023.5107\n",
            "Epoch [20/150], Loss: 11173.7358\n",
            "Epoch [30/150], Loss: 9015.8448\n",
            "Epoch [40/150], Loss: 9135.0800\n",
            "Epoch [50/150], Loss: 10641.1794\n",
            "Epoch [60/150], Loss: 8135.7433\n",
            "Epoch [70/150], Loss: 3378.8491\n",
            "Epoch [80/150], Loss: 4667.0888\n",
            "Epoch [90/150], Loss: 8418.5647\n",
            "Epoch [100/150], Loss: 4655.4683\n",
            "Epoch [110/150], Loss: 7070.8664\n",
            "Epoch [120/150], Loss: 7786.3876\n",
            "Epoch [130/150], Loss: 6836.8184\n",
            "Epoch [140/150], Loss: 10099.0103\n",
            "Epoch [150/150], Loss: 11530.3284\n",
            "Fold 4, RMSE: 36.96727752685547\n",
            "Epoch [10/150], Loss: 16708.1060\n",
            "Epoch [20/150], Loss: 13775.7346\n",
            "Epoch [30/150], Loss: 16563.2947\n",
            "Epoch [40/150], Loss: 9175.8077\n",
            "Epoch [50/150], Loss: 9771.6069\n",
            "Epoch [60/150], Loss: 8712.9319\n",
            "Epoch [70/150], Loss: 7317.1963\n",
            "Epoch [80/150], Loss: 7939.3777\n",
            "Epoch [90/150], Loss: 18597.4668\n",
            "Epoch [100/150], Loss: 3867.9047\n",
            "Epoch [110/150], Loss: 8078.8250\n",
            "Epoch [120/150], Loss: 5536.3712\n",
            "Epoch [130/150], Loss: 3808.6671\n",
            "Epoch [140/150], Loss: 7149.8525\n",
            "Epoch [150/150], Loss: 4721.5094\n",
            "Fold 5, RMSE: 44.81620407104492\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 57.53742141723633\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 235274.4814\n",
            "Epoch [20/100], Loss: 41660.2905\n",
            "Epoch [30/100], Loss: 25872.7632\n",
            "Epoch [40/100], Loss: 22094.7803\n",
            "Epoch [50/100], Loss: 19270.4856\n",
            "Epoch [60/100], Loss: 12808.5992\n",
            "Epoch [70/100], Loss: 14317.7003\n",
            "Epoch [80/100], Loss: 12846.4534\n",
            "Epoch [90/100], Loss: 14444.3052\n",
            "Epoch [100/100], Loss: 13957.3115\n",
            "Fold 1, RMSE: 50.44049072265625\n",
            "Epoch [10/100], Loss: 502826.5156\n",
            "Epoch [20/100], Loss: 33735.3223\n",
            "Epoch [30/100], Loss: 31197.0757\n",
            "Epoch [40/100], Loss: 21098.8525\n",
            "Epoch [50/100], Loss: 18367.5645\n",
            "Epoch [60/100], Loss: 17800.8530\n",
            "Epoch [70/100], Loss: 10776.4233\n",
            "Epoch [80/100], Loss: 26985.1968\n",
            "Epoch [90/100], Loss: 10489.1367\n",
            "Epoch [100/100], Loss: 11948.8450\n",
            "Fold 2, RMSE: 71.9830322265625\n",
            "Epoch [10/100], Loss: 277704.7812\n",
            "Epoch [20/100], Loss: 29645.8335\n",
            "Epoch [30/100], Loss: 23692.0410\n",
            "Epoch [40/100], Loss: 16863.6227\n",
            "Epoch [50/100], Loss: 17856.4131\n",
            "Epoch [60/100], Loss: 12671.2390\n",
            "Epoch [70/100], Loss: 13025.6475\n",
            "Epoch [80/100], Loss: 9774.3932\n",
            "Epoch [90/100], Loss: 11979.7358\n",
            "Epoch [100/100], Loss: 8892.7297\n",
            "Fold 3, RMSE: 92.00516510009766\n",
            "Epoch [10/100], Loss: 886370.1406\n",
            "Epoch [20/100], Loss: 48817.7046\n",
            "Epoch [30/100], Loss: 47753.5679\n",
            "Epoch [40/100], Loss: 42464.7588\n",
            "Epoch [50/100], Loss: 27533.0127\n",
            "Epoch [60/100], Loss: 20686.4438\n",
            "Epoch [70/100], Loss: 22684.4160\n",
            "Epoch [80/100], Loss: 15914.8599\n",
            "Epoch [90/100], Loss: 23308.6572\n",
            "Epoch [100/100], Loss: 14562.6580\n",
            "Fold 4, RMSE: 38.74985885620117\n",
            "Epoch [10/100], Loss: 738800.7656\n",
            "Epoch [20/100], Loss: 77601.0508\n",
            "Epoch [30/100], Loss: 44023.2207\n",
            "Epoch [40/100], Loss: 27270.2886\n",
            "Epoch [50/100], Loss: 29428.0117\n",
            "Epoch [60/100], Loss: 12039.2202\n",
            "Epoch [70/100], Loss: 16603.3979\n",
            "Epoch [80/100], Loss: 15149.7268\n",
            "Epoch [90/100], Loss: 11985.4534\n",
            "Epoch [100/100], Loss: 10118.4822\n",
            "Fold 5, RMSE: 47.25185775756836\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 60.086080932617186\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 171768.6953\n",
            "Epoch [20/150], Loss: 30620.9893\n",
            "Epoch [30/150], Loss: 20914.9141\n",
            "Epoch [40/150], Loss: 13937.1577\n",
            "Epoch [50/150], Loss: 19896.6521\n",
            "Epoch [60/150], Loss: 24402.7644\n",
            "Epoch [70/150], Loss: 11235.5234\n",
            "Epoch [80/150], Loss: 12198.4011\n",
            "Epoch [90/150], Loss: 14702.0591\n",
            "Epoch [100/150], Loss: 16930.3623\n",
            "Epoch [110/150], Loss: 19983.4846\n",
            "Epoch [120/150], Loss: 12418.4744\n",
            "Epoch [130/150], Loss: 11295.4219\n",
            "Epoch [140/150], Loss: 12910.8788\n",
            "Epoch [150/150], Loss: 11656.2826\n",
            "Fold 1, RMSE: 56.83577346801758\n",
            "Epoch [10/150], Loss: 176720.6133\n",
            "Epoch [20/150], Loss: 34086.2339\n",
            "Epoch [30/150], Loss: 20403.0884\n",
            "Epoch [40/150], Loss: 21884.2056\n",
            "Epoch [50/150], Loss: 12453.7230\n",
            "Epoch [60/150], Loss: 19276.4309\n",
            "Epoch [70/150], Loss: 13570.0225\n",
            "Epoch [80/150], Loss: 19310.0444\n",
            "Epoch [90/150], Loss: 12353.7378\n",
            "Epoch [100/150], Loss: 13006.0320\n",
            "Epoch [110/150], Loss: 13818.5000\n",
            "Epoch [120/150], Loss: 13464.6188\n",
            "Epoch [130/150], Loss: 13204.3345\n",
            "Epoch [140/150], Loss: 10580.8661\n",
            "Epoch [150/150], Loss: 19217.6248\n",
            "Fold 2, RMSE: 71.12586975097656\n",
            "Epoch [10/150], Loss: 161916.0469\n",
            "Epoch [20/150], Loss: 18753.3696\n",
            "Epoch [30/150], Loss: 24055.7451\n",
            "Epoch [40/150], Loss: 17662.2778\n",
            "Epoch [50/150], Loss: 12425.8506\n",
            "Epoch [60/150], Loss: 11983.2300\n",
            "Epoch [70/150], Loss: 8441.4364\n",
            "Epoch [80/150], Loss: 10511.0007\n",
            "Epoch [90/150], Loss: 11837.0562\n",
            "Epoch [100/150], Loss: 7832.7974\n",
            "Epoch [110/150], Loss: 6437.1143\n",
            "Epoch [120/150], Loss: 8083.3315\n",
            "Epoch [130/150], Loss: 6652.9264\n",
            "Epoch [140/150], Loss: 7058.8817\n",
            "Epoch [150/150], Loss: 5698.9971\n",
            "Fold 3, RMSE: 92.17184448242188\n",
            "Epoch [10/150], Loss: 671891.0156\n",
            "Epoch [20/150], Loss: 118287.2539\n",
            "Epoch [30/150], Loss: 87900.9541\n",
            "Epoch [40/150], Loss: 28349.1145\n",
            "Epoch [50/150], Loss: 27193.1890\n",
            "Epoch [60/150], Loss: 21772.8945\n",
            "Epoch [70/150], Loss: 22892.9312\n",
            "Epoch [80/150], Loss: 16859.6589\n",
            "Epoch [90/150], Loss: 18072.8984\n",
            "Epoch [100/150], Loss: 14356.0378\n",
            "Epoch [110/150], Loss: 17485.9673\n",
            "Epoch [120/150], Loss: 11911.4128\n",
            "Epoch [130/150], Loss: 14963.7278\n",
            "Epoch [140/150], Loss: 20983.8477\n",
            "Epoch [150/150], Loss: 11952.4308\n",
            "Fold 4, RMSE: 38.177635192871094\n",
            "Epoch [10/150], Loss: 20650.7471\n",
            "Epoch [20/150], Loss: 24785.1392\n",
            "Epoch [30/150], Loss: 20157.0107\n",
            "Epoch [40/150], Loss: 18640.6997\n",
            "Epoch [50/150], Loss: 23121.6196\n",
            "Epoch [60/150], Loss: 24995.6206\n",
            "Epoch [70/150], Loss: 17300.5393\n",
            "Epoch [80/150], Loss: 20233.9805\n",
            "Epoch [90/150], Loss: 20898.4443\n",
            "Epoch [100/150], Loss: 17346.4900\n",
            "Epoch [110/150], Loss: 22387.4023\n",
            "Epoch [120/150], Loss: 18234.1824\n",
            "Epoch [130/150], Loss: 18460.4502\n",
            "Epoch [140/150], Loss: 18439.0869\n",
            "Epoch [150/150], Loss: 18500.0288\n",
            "Fold 5, RMSE: 57.6428337097168\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 63.190791320800784\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 60688.9019\n",
            "Epoch [20/100], Loss: 18361.5569\n",
            "Epoch [30/100], Loss: 15904.0605\n",
            "Epoch [40/100], Loss: 14198.1152\n",
            "Epoch [50/100], Loss: 11248.3871\n",
            "Epoch [60/100], Loss: 14070.0605\n",
            "Epoch [70/100], Loss: 15314.8215\n",
            "Epoch [80/100], Loss: 13654.1309\n",
            "Epoch [90/100], Loss: 15720.6602\n",
            "Epoch [100/100], Loss: 12520.5264\n",
            "Fold 1, RMSE: 49.68385696411133\n",
            "Epoch [10/100], Loss: 75604.0625\n",
            "Epoch [20/100], Loss: 12664.9395\n",
            "Epoch [30/100], Loss: 18186.0227\n",
            "Epoch [40/100], Loss: 11334.3899\n",
            "Epoch [50/100], Loss: 20288.4729\n",
            "Epoch [60/100], Loss: 10224.1241\n",
            "Epoch [70/100], Loss: 10431.5479\n",
            "Epoch [80/100], Loss: 13007.6350\n",
            "Epoch [90/100], Loss: 9011.5436\n",
            "Epoch [100/100], Loss: 9758.7408\n",
            "Fold 2, RMSE: 73.03270721435547\n",
            "Epoch [10/100], Loss: 35062.3867\n",
            "Epoch [20/100], Loss: 16848.6848\n",
            "Epoch [30/100], Loss: 12216.4001\n",
            "Epoch [40/100], Loss: 9661.0146\n",
            "Epoch [50/100], Loss: 12124.2656\n",
            "Epoch [60/100], Loss: 8429.1691\n",
            "Epoch [70/100], Loss: 10621.1992\n",
            "Epoch [80/100], Loss: 11442.5078\n",
            "Epoch [90/100], Loss: 9743.8950\n",
            "Epoch [100/100], Loss: 7353.7195\n",
            "Fold 3, RMSE: 94.9431381225586\n",
            "Epoch [10/100], Loss: 71235.3047\n",
            "Epoch [20/100], Loss: 19336.8174\n",
            "Epoch [30/100], Loss: 16905.9026\n",
            "Epoch [40/100], Loss: 13637.6094\n",
            "Epoch [50/100], Loss: 15343.9014\n",
            "Epoch [60/100], Loss: 16428.0042\n",
            "Epoch [70/100], Loss: 13734.3459\n",
            "Epoch [80/100], Loss: 15196.2678\n",
            "Epoch [90/100], Loss: 16269.8796\n",
            "Epoch [100/100], Loss: 11177.7792\n",
            "Fold 4, RMSE: 38.51882553100586\n",
            "Epoch [10/100], Loss: 70942.8047\n",
            "Epoch [20/100], Loss: 15303.7030\n",
            "Epoch [30/100], Loss: 12559.3320\n",
            "Epoch [40/100], Loss: 11873.2499\n",
            "Epoch [50/100], Loss: 18789.5212\n",
            "Epoch [60/100], Loss: 14184.8269\n",
            "Epoch [70/100], Loss: 10485.1086\n",
            "Epoch [80/100], Loss: 16528.5994\n",
            "Epoch [90/100], Loss: 12688.6709\n",
            "Epoch [100/100], Loss: 11542.2629\n",
            "Fold 5, RMSE: 48.98183059692383\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 61.03207168579102\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 33553.2036\n",
            "Epoch [20/150], Loss: 19422.8140\n",
            "Epoch [30/150], Loss: 12876.7797\n",
            "Epoch [40/150], Loss: 16195.9922\n",
            "Epoch [50/150], Loss: 14947.2393\n",
            "Epoch [60/150], Loss: 17804.0786\n",
            "Epoch [70/150], Loss: 11539.0265\n",
            "Epoch [80/150], Loss: 9832.0548\n",
            "Epoch [90/150], Loss: 16863.4412\n",
            "Epoch [100/150], Loss: 10248.9167\n",
            "Epoch [110/150], Loss: 19749.5980\n",
            "Epoch [120/150], Loss: 8293.5325\n",
            "Epoch [130/150], Loss: 13003.5337\n",
            "Epoch [140/150], Loss: 8860.8958\n",
            "Epoch [150/150], Loss: 11082.5480\n",
            "Fold 1, RMSE: 60.600399017333984\n",
            "Epoch [10/150], Loss: 48265.2803\n",
            "Epoch [20/150], Loss: 16450.9709\n",
            "Epoch [30/150], Loss: 16715.2710\n",
            "Epoch [40/150], Loss: 17088.5767\n",
            "Epoch [50/150], Loss: 10178.2759\n",
            "Epoch [60/150], Loss: 16519.7920\n",
            "Epoch [70/150], Loss: 14893.8711\n",
            "Epoch [80/150], Loss: 12031.5488\n",
            "Epoch [90/150], Loss: 12423.7886\n",
            "Epoch [100/150], Loss: 10100.6150\n",
            "Epoch [110/150], Loss: 11430.1472\n",
            "Epoch [120/150], Loss: 9179.1042\n",
            "Epoch [130/150], Loss: 11020.7588\n",
            "Epoch [140/150], Loss: 9715.5526\n",
            "Epoch [150/150], Loss: 9653.4873\n",
            "Fold 2, RMSE: 72.21094512939453\n",
            "Epoch [10/150], Loss: 47125.1963\n",
            "Epoch [20/150], Loss: 7332.9791\n",
            "Epoch [30/150], Loss: 11454.3031\n",
            "Epoch [40/150], Loss: 10413.4741\n",
            "Epoch [50/150], Loss: 10716.7522\n",
            "Epoch [60/150], Loss: 12214.8538\n",
            "Epoch [70/150], Loss: 7374.6636\n",
            "Epoch [80/150], Loss: 8407.7856\n",
            "Epoch [90/150], Loss: 7874.2612\n",
            "Epoch [100/150], Loss: 8449.3711\n",
            "Epoch [110/150], Loss: 8410.9121\n",
            "Epoch [120/150], Loss: 10374.0769\n",
            "Epoch [130/150], Loss: 8928.4453\n",
            "Epoch [140/150], Loss: 6687.5678\n",
            "Epoch [150/150], Loss: 9989.2268\n",
            "Fold 3, RMSE: 91.76615905761719\n",
            "Epoch [10/150], Loss: 83432.4297\n",
            "Epoch [20/150], Loss: 18566.9399\n",
            "Epoch [30/150], Loss: 18484.6348\n",
            "Epoch [40/150], Loss: 13612.4059\n",
            "Epoch [50/150], Loss: 14396.8455\n",
            "Epoch [60/150], Loss: 15877.0962\n",
            "Epoch [70/150], Loss: 12545.6154\n",
            "Epoch [80/150], Loss: 14235.7793\n",
            "Epoch [90/150], Loss: 15784.0305\n",
            "Epoch [100/150], Loss: 17772.5269\n",
            "Epoch [110/150], Loss: 14943.9104\n",
            "Epoch [120/150], Loss: 16456.5239\n",
            "Epoch [130/150], Loss: 13429.5906\n",
            "Epoch [140/150], Loss: 12640.3057\n",
            "Epoch [150/150], Loss: 12013.2626\n",
            "Fold 4, RMSE: 40.63764572143555\n",
            "Epoch [10/150], Loss: 144679.7695\n",
            "Epoch [20/150], Loss: 29377.7993\n",
            "Epoch [30/150], Loss: 24620.7393\n",
            "Epoch [40/150], Loss: 11035.9293\n",
            "Epoch [50/150], Loss: 16549.9192\n",
            "Epoch [60/150], Loss: 16972.7393\n",
            "Epoch [70/150], Loss: 14275.4766\n",
            "Epoch [80/150], Loss: 15375.4675\n",
            "Epoch [90/150], Loss: 17043.6113\n",
            "Epoch [100/150], Loss: 13724.4600\n",
            "Epoch [110/150], Loss: 14514.7402\n",
            "Epoch [120/150], Loss: 13750.8464\n",
            "Epoch [130/150], Loss: 12192.0103\n",
            "Epoch [140/150], Loss: 11564.8860\n",
            "Epoch [150/150], Loss: 12575.4373\n",
            "Fold 5, RMSE: 48.44217300415039\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 62.73146438598633\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 218159.6777\n",
            "Epoch [20/100], Loss: 83080.2930\n",
            "Epoch [30/100], Loss: 21013.4556\n",
            "Epoch [40/100], Loss: 22161.8149\n",
            "Epoch [50/100], Loss: 17623.9771\n",
            "Epoch [60/100], Loss: 14985.9624\n",
            "Epoch [70/100], Loss: 20672.6121\n",
            "Epoch [80/100], Loss: 10676.0922\n",
            "Epoch [90/100], Loss: 17931.6738\n",
            "Epoch [100/100], Loss: 13449.7963\n",
            "Fold 1, RMSE: 47.637203216552734\n",
            "Epoch [10/100], Loss: 95558.2051\n",
            "Epoch [20/100], Loss: 17538.2905\n",
            "Epoch [30/100], Loss: 13502.2673\n",
            "Epoch [40/100], Loss: 9193.2955\n",
            "Epoch [50/100], Loss: 10741.7322\n",
            "Epoch [60/100], Loss: 11657.6687\n",
            "Epoch [70/100], Loss: 9573.6525\n",
            "Epoch [80/100], Loss: 8672.5043\n",
            "Epoch [90/100], Loss: 9398.0619\n",
            "Epoch [100/100], Loss: 9735.3340\n",
            "Fold 2, RMSE: 61.00522994995117\n",
            "Epoch [10/100], Loss: 69315.4258\n",
            "Epoch [20/100], Loss: 16450.3564\n",
            "Epoch [30/100], Loss: 13297.8762\n",
            "Epoch [40/100], Loss: 8763.1360\n",
            "Epoch [50/100], Loss: 8369.8297\n",
            "Epoch [60/100], Loss: 9633.1033\n",
            "Epoch [70/100], Loss: 9395.4270\n",
            "Epoch [80/100], Loss: 9435.8494\n",
            "Epoch [90/100], Loss: 9037.5122\n",
            "Epoch [100/100], Loss: 6940.7850\n",
            "Fold 3, RMSE: 92.54886627197266\n",
            "Epoch [10/100], Loss: 215654.4980\n",
            "Epoch [20/100], Loss: 29702.0586\n",
            "Epoch [30/100], Loss: 26031.1338\n",
            "Epoch [40/100], Loss: 16464.9297\n",
            "Epoch [50/100], Loss: 14951.4260\n",
            "Epoch [60/100], Loss: 14814.1428\n",
            "Epoch [70/100], Loss: 12318.4873\n",
            "Epoch [80/100], Loss: 12491.0413\n",
            "Epoch [90/100], Loss: 17771.4850\n",
            "Epoch [100/100], Loss: 9134.2828\n",
            "Fold 4, RMSE: 36.12560272216797\n",
            "Epoch [10/100], Loss: 390787.4375\n",
            "Epoch [20/100], Loss: 52833.8057\n",
            "Epoch [30/100], Loss: 16404.1580\n",
            "Epoch [40/100], Loss: 17926.7019\n",
            "Epoch [50/100], Loss: 28725.9556\n",
            "Epoch [60/100], Loss: 13441.1646\n",
            "Epoch [70/100], Loss: 17587.7361\n",
            "Epoch [80/100], Loss: 16768.1494\n",
            "Epoch [90/100], Loss: 19814.9280\n",
            "Epoch [100/100], Loss: 14004.9094\n",
            "Fold 5, RMSE: 49.3978157043457\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 57.342943572998045\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 117315.6660\n",
            "Epoch [20/150], Loss: 16596.4253\n",
            "Epoch [30/150], Loss: 13344.8252\n",
            "Epoch [40/150], Loss: 10875.2290\n",
            "Epoch [50/150], Loss: 12320.6287\n",
            "Epoch [60/150], Loss: 8557.4829\n",
            "Epoch [70/150], Loss: 6084.8241\n",
            "Epoch [80/150], Loss: 3560.4193\n",
            "Epoch [90/150], Loss: 5213.7378\n",
            "Epoch [100/150], Loss: 9680.7201\n",
            "Epoch [110/150], Loss: 11676.3789\n",
            "Epoch [120/150], Loss: 15933.9952\n",
            "Epoch [130/150], Loss: 8844.9781\n",
            "Epoch [140/150], Loss: 6006.1093\n",
            "Epoch [150/150], Loss: 3096.8484\n",
            "Fold 1, RMSE: 57.73439025878906\n",
            "Epoch [10/150], Loss: 544806.5625\n",
            "Epoch [20/150], Loss: 61682.1475\n",
            "Epoch [30/150], Loss: 28345.5801\n",
            "Epoch [40/150], Loss: 18582.3413\n",
            "Epoch [50/150], Loss: 28233.6421\n",
            "Epoch [60/150], Loss: 18431.4614\n",
            "Epoch [70/150], Loss: 16694.7246\n",
            "Epoch [80/150], Loss: 10477.3545\n",
            "Epoch [90/150], Loss: 10933.1396\n",
            "Epoch [100/150], Loss: 13944.4846\n",
            "Epoch [110/150], Loss: 17765.7139\n",
            "Epoch [120/150], Loss: 11815.1946\n",
            "Epoch [130/150], Loss: 14864.2190\n",
            "Epoch [140/150], Loss: 9501.9963\n",
            "Epoch [150/150], Loss: 11733.4526\n",
            "Fold 2, RMSE: 68.71411895751953\n",
            "Epoch [10/150], Loss: 131589.8652\n",
            "Epoch [20/150], Loss: 80489.9980\n",
            "Epoch [30/150], Loss: 28312.1167\n",
            "Epoch [40/150], Loss: 16669.7495\n",
            "Epoch [50/150], Loss: 15450.2356\n",
            "Epoch [60/150], Loss: 11298.2241\n",
            "Epoch [70/150], Loss: 8564.4365\n",
            "Epoch [80/150], Loss: 9750.3127\n",
            "Epoch [90/150], Loss: 8485.4414\n",
            "Epoch [100/150], Loss: 10952.0579\n",
            "Epoch [110/150], Loss: 6717.3779\n",
            "Epoch [120/150], Loss: 8136.4678\n",
            "Epoch [130/150], Loss: 6439.8733\n",
            "Epoch [140/150], Loss: 8751.9944\n",
            "Epoch [150/150], Loss: 7246.5132\n",
            "Fold 3, RMSE: 94.07682800292969\n",
            "Epoch [10/150], Loss: 168140.6680\n",
            "Epoch [20/150], Loss: 27048.6855\n",
            "Epoch [30/150], Loss: 14527.8525\n",
            "Epoch [40/150], Loss: 18928.5005\n",
            "Epoch [50/150], Loss: 12791.8558\n",
            "Epoch [60/150], Loss: 17654.0693\n",
            "Epoch [70/150], Loss: 18552.8601\n",
            "Epoch [80/150], Loss: 12089.2622\n",
            "Epoch [90/150], Loss: 11939.9888\n",
            "Epoch [100/150], Loss: 10247.1616\n",
            "Epoch [110/150], Loss: 11588.6079\n",
            "Epoch [120/150], Loss: 11816.4458\n",
            "Epoch [130/150], Loss: 12485.7218\n",
            "Epoch [140/150], Loss: 12055.2148\n",
            "Epoch [150/150], Loss: 7646.4208\n",
            "Fold 4, RMSE: 38.646873474121094\n",
            "Epoch [10/150], Loss: 287432.4219\n",
            "Epoch [20/150], Loss: 36394.0586\n",
            "Epoch [30/150], Loss: 24441.7529\n",
            "Epoch [40/150], Loss: 22123.3398\n",
            "Epoch [50/150], Loss: 15637.7793\n",
            "Epoch [60/150], Loss: 17593.9082\n",
            "Epoch [70/150], Loss: 12657.9854\n",
            "Epoch [80/150], Loss: 16345.5421\n",
            "Epoch [90/150], Loss: 12224.2260\n",
            "Epoch [100/150], Loss: 10245.8076\n",
            "Epoch [110/150], Loss: 14642.7490\n",
            "Epoch [120/150], Loss: 11323.0789\n",
            "Epoch [130/150], Loss: 9819.9589\n",
            "Epoch [140/150], Loss: 9081.2673\n",
            "Epoch [150/150], Loss: 12164.3333\n",
            "Fold 5, RMSE: 46.98942565917969\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 61.23232727050781\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 60429.0225\n",
            "Epoch [20/100], Loss: 28541.0039\n",
            "Epoch [30/100], Loss: 16664.6277\n",
            "Epoch [40/100], Loss: 23300.0171\n",
            "Epoch [50/100], Loss: 20924.1090\n",
            "Epoch [60/100], Loss: 16143.4248\n",
            "Epoch [70/100], Loss: 14519.3994\n",
            "Epoch [80/100], Loss: 13614.9846\n",
            "Epoch [90/100], Loss: 12742.5039\n",
            "Epoch [100/100], Loss: 21092.9849\n",
            "Fold 1, RMSE: 52.91999435424805\n",
            "Epoch [10/100], Loss: 20971.1660\n",
            "Epoch [20/100], Loss: 12324.2905\n",
            "Epoch [30/100], Loss: 10473.0870\n",
            "Epoch [40/100], Loss: 10166.9219\n",
            "Epoch [50/100], Loss: 10850.5986\n",
            "Epoch [60/100], Loss: 9890.6232\n",
            "Epoch [70/100], Loss: 9076.3977\n",
            "Epoch [80/100], Loss: 10454.3555\n",
            "Epoch [90/100], Loss: 9870.5854\n",
            "Epoch [100/100], Loss: 10940.7699\n",
            "Fold 2, RMSE: 72.76559448242188\n",
            "Epoch [10/100], Loss: 21954.4224\n",
            "Epoch [20/100], Loss: 10940.9260\n",
            "Epoch [30/100], Loss: 9559.1011\n",
            "Epoch [40/100], Loss: 6994.5399\n",
            "Epoch [50/100], Loss: 7424.6499\n",
            "Epoch [60/100], Loss: 9459.0786\n",
            "Epoch [70/100], Loss: 7119.7574\n",
            "Epoch [80/100], Loss: 8871.7341\n",
            "Epoch [90/100], Loss: 8749.8357\n",
            "Epoch [100/100], Loss: 7144.9727\n",
            "Fold 3, RMSE: 94.77918243408203\n",
            "Epoch [10/100], Loss: 42593.1807\n",
            "Epoch [20/100], Loss: 26838.4150\n",
            "Epoch [30/100], Loss: 15839.5709\n",
            "Epoch [40/100], Loss: 14532.5352\n",
            "Epoch [50/100], Loss: 13037.4221\n",
            "Epoch [60/100], Loss: 10539.1184\n",
            "Epoch [70/100], Loss: 19118.1738\n",
            "Epoch [80/100], Loss: 14304.0278\n",
            "Epoch [90/100], Loss: 10156.2288\n",
            "Epoch [100/100], Loss: 10994.3898\n",
            "Fold 4, RMSE: 38.41009521484375\n",
            "Epoch [10/100], Loss: 32526.0217\n",
            "Epoch [20/100], Loss: 11024.6952\n",
            "Epoch [30/100], Loss: 13094.0784\n",
            "Epoch [40/100], Loss: 13418.7915\n",
            "Epoch [50/100], Loss: 12822.1853\n",
            "Epoch [60/100], Loss: 9732.6450\n",
            "Epoch [70/100], Loss: 12851.2043\n",
            "Epoch [80/100], Loss: 10678.9746\n",
            "Epoch [90/100], Loss: 14984.7837\n",
            "Epoch [100/100], Loss: 13974.4604\n",
            "Fold 5, RMSE: 46.2786750793457\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 61.03070831298828\n",
            "Training with neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 31436.0303\n",
            "Epoch [20/150], Loss: 19738.4304\n",
            "Epoch [30/150], Loss: 13525.4485\n",
            "Epoch [40/150], Loss: 11358.5698\n",
            "Epoch [50/150], Loss: 10867.4241\n",
            "Epoch [60/150], Loss: 12172.8438\n",
            "Epoch [70/150], Loss: 14200.6641\n",
            "Epoch [80/150], Loss: 12627.7769\n",
            "Epoch [90/150], Loss: 11792.3462\n",
            "Epoch [100/150], Loss: 10115.9518\n",
            "Epoch [110/150], Loss: 18320.0420\n",
            "Epoch [120/150], Loss: 10068.2664\n",
            "Epoch [130/150], Loss: 9717.2384\n",
            "Epoch [140/150], Loss: 11210.6876\n",
            "Epoch [150/150], Loss: 10214.7633\n",
            "Fold 1, RMSE: 51.62005615234375\n",
            "Epoch [10/150], Loss: 14299.3347\n",
            "Epoch [20/150], Loss: 15041.1204\n",
            "Epoch [30/150], Loss: 9211.3522\n",
            "Epoch [40/150], Loss: 9905.2177\n",
            "Epoch [50/150], Loss: 5698.6782\n",
            "Epoch [60/150], Loss: 5725.5876\n",
            "Epoch [70/150], Loss: 5944.8289\n",
            "Epoch [80/150], Loss: 3527.1613\n",
            "Epoch [90/150], Loss: 4988.1392\n",
            "Epoch [100/150], Loss: 2841.6334\n",
            "Epoch [110/150], Loss: 3391.9523\n",
            "Epoch [120/150], Loss: 3857.1663\n",
            "Epoch [130/150], Loss: 15968.1943\n",
            "Epoch [140/150], Loss: 16700.9333\n",
            "Epoch [150/150], Loss: 18961.5005\n",
            "Fold 2, RMSE: 80.59122467041016\n",
            "Epoch [10/150], Loss: 66111.2422\n",
            "Epoch [20/150], Loss: 14727.0405\n",
            "Epoch [30/150], Loss: 8335.7332\n",
            "Epoch [40/150], Loss: 7873.4249\n",
            "Epoch [50/150], Loss: 9147.3091\n",
            "Epoch [60/150], Loss: 8179.5781\n",
            "Epoch [70/150], Loss: 12200.9617\n",
            "Epoch [80/150], Loss: 11690.3188\n",
            "Epoch [90/150], Loss: 6799.6866\n",
            "Epoch [100/150], Loss: 7555.0295\n",
            "Epoch [110/150], Loss: 8149.7689\n",
            "Epoch [120/150], Loss: 7047.9392\n",
            "Epoch [130/150], Loss: 8484.3657\n",
            "Epoch [140/150], Loss: 6050.0928\n",
            "Epoch [150/150], Loss: 8606.2668\n",
            "Fold 3, RMSE: 94.5693359375\n",
            "Epoch [10/150], Loss: 44455.1240\n",
            "Epoch [20/150], Loss: 14747.8000\n",
            "Epoch [30/150], Loss: 14856.9243\n",
            "Epoch [40/150], Loss: 10997.0151\n",
            "Epoch [50/150], Loss: 11187.6060\n",
            "Epoch [60/150], Loss: 11633.5078\n",
            "Epoch [70/150], Loss: 14233.6902\n",
            "Epoch [80/150], Loss: 15458.8799\n",
            "Epoch [90/150], Loss: 12161.6086\n",
            "Epoch [100/150], Loss: 13713.9138\n",
            "Epoch [110/150], Loss: 13304.7390\n",
            "Epoch [120/150], Loss: 11772.8798\n",
            "Epoch [130/150], Loss: 12835.9810\n",
            "Epoch [140/150], Loss: 19085.5901\n",
            "Epoch [150/150], Loss: 10526.6371\n",
            "Fold 4, RMSE: 34.92289733886719\n",
            "Epoch [10/150], Loss: 43907.2471\n",
            "Epoch [20/150], Loss: 16828.9526\n",
            "Epoch [30/150], Loss: 15230.3550\n",
            "Epoch [40/150], Loss: 10519.8491\n",
            "Epoch [50/150], Loss: 12598.7712\n",
            "Epoch [60/150], Loss: 13021.9116\n",
            "Epoch [70/150], Loss: 13306.8035\n",
            "Epoch [80/150], Loss: 12997.2642\n",
            "Epoch [90/150], Loss: 11998.2205\n",
            "Epoch [100/150], Loss: 11550.9473\n",
            "Epoch [110/150], Loss: 9662.0330\n",
            "Epoch [120/150], Loss: 14506.2864\n",
            "Epoch [130/150], Loss: 15952.4468\n",
            "Epoch [140/150], Loss: 9135.9015\n",
            "Epoch [150/150], Loss: 11398.4534\n",
            "Fold 5, RMSE: 48.901878356933594\n",
            "Avg RMSE for neurons=80, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 62.12107849121094\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 20474.6694\n",
            "Epoch [20/100], Loss: 19507.1602\n",
            "Epoch [30/100], Loss: 27454.6006\n",
            "Epoch [40/100], Loss: 30183.8328\n",
            "Epoch [50/100], Loss: 17436.4790\n",
            "Epoch [60/100], Loss: 18125.4946\n",
            "Epoch [70/100], Loss: 17225.2351\n",
            "Epoch [80/100], Loss: 16973.5657\n",
            "Epoch [90/100], Loss: 18625.2886\n",
            "Epoch [100/100], Loss: 19966.7290\n",
            "Fold 1, RMSE: 67.5533447265625\n",
            "Epoch [10/100], Loss: 17928.8618\n",
            "Epoch [20/100], Loss: 12873.4415\n",
            "Epoch [30/100], Loss: 13121.5354\n",
            "Epoch [40/100], Loss: 9751.3655\n",
            "Epoch [50/100], Loss: 8863.3754\n",
            "Epoch [60/100], Loss: 13964.8997\n",
            "Epoch [70/100], Loss: 15514.0986\n",
            "Epoch [80/100], Loss: 9499.4426\n",
            "Epoch [90/100], Loss: 9870.9425\n",
            "Epoch [100/100], Loss: 12779.5728\n",
            "Fold 2, RMSE: 70.06493377685547\n",
            "Epoch [10/100], Loss: 11572.3806\n",
            "Epoch [20/100], Loss: 15189.6556\n",
            "Epoch [30/100], Loss: 12161.0886\n",
            "Epoch [40/100], Loss: 14599.4304\n",
            "Epoch [50/100], Loss: 14139.0652\n",
            "Epoch [60/100], Loss: 14318.4521\n",
            "Epoch [70/100], Loss: 18360.1643\n",
            "Epoch [80/100], Loss: 17978.4690\n",
            "Epoch [90/100], Loss: 14406.8745\n",
            "Epoch [100/100], Loss: 16056.3027\n",
            "Fold 3, RMSE: 109.71309661865234\n",
            "Epoch [10/100], Loss: 18744.2434\n",
            "Epoch [20/100], Loss: 26980.8015\n",
            "Epoch [30/100], Loss: 11235.4188\n",
            "Epoch [40/100], Loss: 15569.0625\n",
            "Epoch [50/100], Loss: 19849.3218\n",
            "Epoch [60/100], Loss: 8986.5038\n",
            "Epoch [70/100], Loss: 8042.1036\n",
            "Epoch [80/100], Loss: 10716.4895\n",
            "Epoch [90/100], Loss: 12427.0548\n",
            "Epoch [100/100], Loss: 14294.0881\n",
            "Fold 4, RMSE: 51.297576904296875\n",
            "Epoch [10/100], Loss: 21477.8042\n",
            "Epoch [20/100], Loss: 23156.3027\n",
            "Epoch [30/100], Loss: 20300.1885\n",
            "Epoch [40/100], Loss: 20925.9136\n",
            "Epoch [50/100], Loss: 24039.0894\n",
            "Epoch [60/100], Loss: 13296.5940\n",
            "Epoch [70/100], Loss: 9715.5261\n",
            "Epoch [80/100], Loss: 9660.3796\n",
            "Epoch [90/100], Loss: 15876.9717\n",
            "Epoch [100/100], Loss: 22403.0122\n",
            "Fold 5, RMSE: 50.037353515625\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 69.73326110839844\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 17870.7573\n",
            "Epoch [20/150], Loss: 13529.7861\n",
            "Epoch [30/150], Loss: 12396.6550\n",
            "Epoch [40/150], Loss: 12842.3525\n",
            "Epoch [50/150], Loss: 12522.1628\n",
            "Epoch [60/150], Loss: 14165.7758\n",
            "Epoch [70/150], Loss: 8992.3534\n",
            "Epoch [80/150], Loss: 11402.8000\n",
            "Epoch [90/150], Loss: 6648.0793\n",
            "Epoch [100/150], Loss: 8792.0010\n",
            "Epoch [110/150], Loss: 10863.8469\n",
            "Epoch [120/150], Loss: 7366.5802\n",
            "Epoch [130/150], Loss: 7092.9312\n",
            "Epoch [140/150], Loss: 9556.5356\n",
            "Epoch [150/150], Loss: 17117.8516\n",
            "Fold 1, RMSE: 56.18775939941406\n",
            "Epoch [10/150], Loss: 16778.7334\n",
            "Epoch [20/150], Loss: 15670.4678\n",
            "Epoch [30/150], Loss: 16347.1787\n",
            "Epoch [40/150], Loss: 15869.0459\n",
            "Epoch [50/150], Loss: 19024.6074\n",
            "Epoch [60/150], Loss: 21763.2070\n",
            "Epoch [70/150], Loss: 14273.7462\n",
            "Epoch [80/150], Loss: 16471.9109\n",
            "Epoch [90/150], Loss: 16532.4287\n",
            "Epoch [100/150], Loss: 19883.4756\n",
            "Epoch [110/150], Loss: 14931.4492\n",
            "Epoch [120/150], Loss: 16720.0415\n",
            "Epoch [130/150], Loss: 16902.6221\n",
            "Epoch [140/150], Loss: 22108.2656\n",
            "Epoch [150/150], Loss: 25026.9607\n",
            "Fold 2, RMSE: 86.84978485107422\n",
            "Epoch [10/150], Loss: 14365.7493\n",
            "Epoch [20/150], Loss: 14785.2690\n",
            "Epoch [30/150], Loss: 12379.1956\n",
            "Epoch [40/150], Loss: 13683.2043\n",
            "Epoch [50/150], Loss: 12152.5850\n",
            "Epoch [60/150], Loss: 15577.1475\n",
            "Epoch [70/150], Loss: 12755.1465\n",
            "Epoch [80/150], Loss: 16788.6133\n",
            "Epoch [90/150], Loss: 13649.7598\n",
            "Epoch [100/150], Loss: 17375.8923\n",
            "Epoch [110/150], Loss: 17575.6462\n",
            "Epoch [120/150], Loss: 15238.9791\n",
            "Epoch [130/150], Loss: 14239.5576\n",
            "Epoch [140/150], Loss: 14286.3142\n",
            "Epoch [150/150], Loss: 17671.1685\n",
            "Fold 3, RMSE: 109.65093231201172\n",
            "Epoch [10/150], Loss: 16843.6409\n",
            "Epoch [20/150], Loss: 18806.6729\n",
            "Epoch [30/150], Loss: 18849.6145\n",
            "Epoch [40/150], Loss: 20773.6870\n",
            "Epoch [50/150], Loss: 18623.6589\n",
            "Epoch [60/150], Loss: 21445.5220\n",
            "Epoch [70/150], Loss: 17595.4957\n",
            "Epoch [80/150], Loss: 20540.9189\n",
            "Epoch [90/150], Loss: 19561.7720\n",
            "Epoch [100/150], Loss: 22586.2490\n",
            "Epoch [110/150], Loss: 22461.6995\n",
            "Epoch [120/150], Loss: 18940.0371\n",
            "Epoch [130/150], Loss: 24998.6489\n",
            "Epoch [140/150], Loss: 21178.6553\n",
            "Epoch [150/150], Loss: 27526.4351\n",
            "Fold 4, RMSE: 54.2979850769043\n",
            "Epoch [10/150], Loss: 19429.1914\n",
            "Epoch [20/150], Loss: 18298.8979\n",
            "Epoch [30/150], Loss: 12115.4551\n",
            "Epoch [40/150], Loss: 10455.9781\n",
            "Epoch [50/150], Loss: 22806.7874\n",
            "Epoch [60/150], Loss: 11292.1130\n",
            "Epoch [70/150], Loss: 13418.5729\n",
            "Epoch [80/150], Loss: 12516.2493\n",
            "Epoch [90/150], Loss: 11033.3259\n",
            "Epoch [100/150], Loss: 15069.6113\n",
            "Epoch [110/150], Loss: 19944.8528\n",
            "Epoch [120/150], Loss: 16386.2874\n",
            "Epoch [130/150], Loss: 11658.8219\n",
            "Epoch [140/150], Loss: 11120.9136\n",
            "Epoch [150/150], Loss: 7381.4627\n",
            "Fold 5, RMSE: 46.08505630493164\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 70.61430358886719\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 26808.3364\n",
            "Epoch [20/100], Loss: 16668.1990\n",
            "Epoch [30/100], Loss: 27400.7031\n",
            "Epoch [40/100], Loss: 19538.6245\n",
            "Epoch [50/100], Loss: 17990.2981\n",
            "Epoch [60/100], Loss: 16170.4895\n",
            "Epoch [70/100], Loss: 19000.5488\n",
            "Epoch [80/100], Loss: 16234.8210\n",
            "Epoch [90/100], Loss: 16981.6279\n",
            "Epoch [100/100], Loss: 16709.3818\n",
            "Fold 1, RMSE: 67.46215057373047\n",
            "Epoch [10/100], Loss: 25927.2061\n",
            "Epoch [20/100], Loss: 16592.0164\n",
            "Epoch [30/100], Loss: 14994.4163\n",
            "Epoch [40/100], Loss: 16218.1758\n",
            "Epoch [50/100], Loss: 23016.3994\n",
            "Epoch [60/100], Loss: 24409.7405\n",
            "Epoch [70/100], Loss: 17497.5615\n",
            "Epoch [80/100], Loss: 15708.1631\n",
            "Epoch [90/100], Loss: 15524.4351\n",
            "Epoch [100/100], Loss: 14345.0787\n",
            "Fold 2, RMSE: 87.08497619628906\n",
            "Epoch [10/100], Loss: 12180.6567\n",
            "Epoch [20/100], Loss: 11007.5835\n",
            "Epoch [30/100], Loss: 9353.7007\n",
            "Epoch [40/100], Loss: 9295.3753\n",
            "Epoch [50/100], Loss: 17406.8596\n",
            "Epoch [60/100], Loss: 10108.2246\n",
            "Epoch [70/100], Loss: 14169.3628\n",
            "Epoch [80/100], Loss: 14937.9307\n",
            "Epoch [90/100], Loss: 15652.1519\n",
            "Epoch [100/100], Loss: 14515.6665\n",
            "Fold 3, RMSE: 109.56584930419922\n",
            "Epoch [10/100], Loss: 23546.4519\n",
            "Epoch [20/100], Loss: 15915.7129\n",
            "Epoch [30/100], Loss: 15322.0461\n",
            "Epoch [40/100], Loss: 16642.3706\n",
            "Epoch [50/100], Loss: 17151.6077\n",
            "Epoch [60/100], Loss: 8121.9861\n",
            "Epoch [70/100], Loss: 7970.3301\n",
            "Epoch [80/100], Loss: 6884.5909\n",
            "Epoch [90/100], Loss: 14465.6392\n",
            "Epoch [100/100], Loss: 9510.3110\n",
            "Fold 4, RMSE: 47.894813537597656\n",
            "Epoch [10/100], Loss: 17003.3081\n",
            "Epoch [20/100], Loss: 17130.6841\n",
            "Epoch [30/100], Loss: 20242.9124\n",
            "Epoch [40/100], Loss: 16995.7468\n",
            "Epoch [50/100], Loss: 8135.8097\n",
            "Epoch [60/100], Loss: 12438.4751\n",
            "Epoch [70/100], Loss: 7674.9338\n",
            "Epoch [80/100], Loss: 5844.1500\n",
            "Epoch [90/100], Loss: 13258.5959\n",
            "Epoch [100/100], Loss: 11624.0439\n",
            "Fold 5, RMSE: 44.8062629699707\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 71.36281051635743\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15867.4421\n",
            "Epoch [20/150], Loss: 14385.7517\n",
            "Epoch [30/150], Loss: 7565.9681\n",
            "Epoch [40/150], Loss: 7049.7743\n",
            "Epoch [50/150], Loss: 16111.5640\n",
            "Epoch [60/150], Loss: 9526.2507\n",
            "Epoch [70/150], Loss: 6016.9865\n",
            "Epoch [80/150], Loss: 7601.3989\n",
            "Epoch [90/150], Loss: 9118.8643\n",
            "Epoch [100/150], Loss: 10029.6488\n",
            "Epoch [110/150], Loss: 10377.6599\n",
            "Epoch [120/150], Loss: 12154.5435\n",
            "Epoch [130/150], Loss: 6944.9858\n",
            "Epoch [140/150], Loss: 14273.7065\n",
            "Epoch [150/150], Loss: 7653.7871\n",
            "Fold 1, RMSE: 53.42793273925781\n",
            "Epoch [10/150], Loss: 14346.8418\n",
            "Epoch [20/150], Loss: 16567.2690\n",
            "Epoch [30/150], Loss: 14970.0657\n",
            "Epoch [40/150], Loss: 11394.5483\n",
            "Epoch [50/150], Loss: 18140.8135\n",
            "Epoch [60/150], Loss: 17587.0437\n",
            "Epoch [70/150], Loss: 9232.7830\n",
            "Epoch [80/150], Loss: 15434.0435\n",
            "Epoch [90/150], Loss: 14837.7893\n",
            "Epoch [100/150], Loss: 12162.7297\n",
            "Epoch [110/150], Loss: 7459.8612\n",
            "Epoch [120/150], Loss: 9827.7448\n",
            "Epoch [130/150], Loss: 9598.0535\n",
            "Epoch [140/150], Loss: 6407.3333\n",
            "Epoch [150/150], Loss: 12100.5291\n",
            "Fold 2, RMSE: 73.42367553710938\n",
            "Epoch [10/150], Loss: 9734.2861\n",
            "Epoch [20/150], Loss: 11371.7881\n",
            "Epoch [30/150], Loss: 10225.4706\n",
            "Epoch [40/150], Loss: 6531.7799\n",
            "Epoch [50/150], Loss: 10407.7676\n",
            "Epoch [60/150], Loss: 9415.7532\n",
            "Epoch [70/150], Loss: 7345.3844\n",
            "Epoch [80/150], Loss: 6889.9683\n",
            "Epoch [90/150], Loss: 8762.4534\n",
            "Epoch [100/150], Loss: 4192.0532\n",
            "Epoch [110/150], Loss: 8905.0009\n",
            "Epoch [120/150], Loss: 5335.9047\n",
            "Epoch [130/150], Loss: 7042.4603\n",
            "Epoch [140/150], Loss: 7840.1544\n",
            "Epoch [150/150], Loss: 12515.3435\n",
            "Fold 3, RMSE: 109.22901916503906\n",
            "Epoch [10/150], Loss: 15869.4719\n",
            "Epoch [20/150], Loss: 11463.6132\n",
            "Epoch [30/150], Loss: 14902.6196\n",
            "Epoch [40/150], Loss: 16011.2097\n",
            "Epoch [50/150], Loss: 17518.6963\n",
            "Epoch [60/150], Loss: 12126.9943\n",
            "Epoch [70/150], Loss: 12389.0708\n",
            "Epoch [80/150], Loss: 7538.2953\n",
            "Epoch [90/150], Loss: 5783.3308\n",
            "Epoch [100/150], Loss: 13739.5166\n",
            "Epoch [110/150], Loss: 11427.9392\n",
            "Epoch [120/150], Loss: 16133.2515\n",
            "Epoch [130/150], Loss: 16129.4644\n",
            "Epoch [140/150], Loss: 19707.4175\n",
            "Epoch [150/150], Loss: 12849.8181\n",
            "Fold 4, RMSE: 53.256675720214844\n",
            "Epoch [10/150], Loss: 16733.2212\n",
            "Epoch [20/150], Loss: 12920.1079\n",
            "Epoch [30/150], Loss: 13250.7688\n",
            "Epoch [40/150], Loss: 10677.8787\n",
            "Epoch [50/150], Loss: 12539.2231\n",
            "Epoch [60/150], Loss: 17966.3862\n",
            "Epoch [70/150], Loss: 7111.6578\n",
            "Epoch [80/150], Loss: 11889.5034\n",
            "Epoch [90/150], Loss: 11657.3965\n",
            "Epoch [100/150], Loss: 18888.7578\n",
            "Epoch [110/150], Loss: 18362.8838\n",
            "Epoch [120/150], Loss: 19494.2317\n",
            "Epoch [130/150], Loss: 9621.4170\n",
            "Epoch [140/150], Loss: 19331.7773\n",
            "Epoch [150/150], Loss: 10922.4297\n",
            "Fold 5, RMSE: 44.99910354614258\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 66.86728134155274\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 16283.1362\n",
            "Epoch [20/100], Loss: 15745.4348\n",
            "Epoch [30/100], Loss: 17620.6516\n",
            "Epoch [40/100], Loss: 11204.2607\n",
            "Epoch [50/100], Loss: 21888.2170\n",
            "Epoch [60/100], Loss: 13865.6140\n",
            "Epoch [70/100], Loss: 8853.2454\n",
            "Epoch [80/100], Loss: 13896.3950\n",
            "Epoch [90/100], Loss: 9851.0692\n",
            "Epoch [100/100], Loss: 10814.3420\n",
            "Fold 1, RMSE: 58.08069610595703\n",
            "Epoch [10/100], Loss: 17201.9587\n",
            "Epoch [20/100], Loss: 18145.7261\n",
            "Epoch [30/100], Loss: 14230.2058\n",
            "Epoch [40/100], Loss: 15415.6101\n",
            "Epoch [50/100], Loss: 19688.7588\n",
            "Epoch [60/100], Loss: 14096.5415\n",
            "Epoch [70/100], Loss: 15081.0935\n",
            "Epoch [80/100], Loss: 22074.9502\n",
            "Epoch [90/100], Loss: 16549.0439\n",
            "Epoch [100/100], Loss: 16073.9771\n",
            "Fold 2, RMSE: 87.3366928100586\n",
            "Epoch [10/100], Loss: 12444.6462\n",
            "Epoch [20/100], Loss: 11268.4761\n",
            "Epoch [30/100], Loss: 7070.4001\n",
            "Epoch [40/100], Loss: 9556.3989\n",
            "Epoch [50/100], Loss: 5562.7634\n",
            "Epoch [60/100], Loss: 6709.8886\n",
            "Epoch [70/100], Loss: 5563.8756\n",
            "Epoch [80/100], Loss: 9893.7644\n",
            "Epoch [90/100], Loss: 6899.6471\n",
            "Epoch [100/100], Loss: 9139.4146\n",
            "Fold 3, RMSE: 102.11314392089844\n",
            "Epoch [10/100], Loss: 19198.9233\n",
            "Epoch [20/100], Loss: 18085.4194\n",
            "Epoch [30/100], Loss: 9856.9170\n",
            "Epoch [40/100], Loss: 9501.0376\n",
            "Epoch [50/100], Loss: 13662.7810\n",
            "Epoch [60/100], Loss: 8800.8059\n",
            "Epoch [70/100], Loss: 11326.8735\n",
            "Epoch [80/100], Loss: 11847.2245\n",
            "Epoch [90/100], Loss: 11384.2915\n",
            "Epoch [100/100], Loss: 11446.1675\n",
            "Fold 4, RMSE: 45.8939323425293\n",
            "Epoch [10/100], Loss: 19920.8013\n",
            "Epoch [20/100], Loss: 19390.4482\n",
            "Epoch [30/100], Loss: 18635.0554\n",
            "Epoch [40/100], Loss: 15982.4341\n",
            "Epoch [50/100], Loss: 13983.0679\n",
            "Epoch [60/100], Loss: 14023.1746\n",
            "Epoch [70/100], Loss: 19241.6760\n",
            "Epoch [80/100], Loss: 9172.3407\n",
            "Epoch [90/100], Loss: 10039.1941\n",
            "Epoch [100/100], Loss: 12115.8994\n",
            "Fold 5, RMSE: 53.10710906982422\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 69.30631484985352\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 24987.8281\n",
            "Epoch [20/150], Loss: 18425.3911\n",
            "Epoch [30/150], Loss: 22403.4033\n",
            "Epoch [40/150], Loss: 26136.8628\n",
            "Epoch [50/150], Loss: 20684.2300\n",
            "Epoch [60/150], Loss: 17366.3733\n",
            "Epoch [70/150], Loss: 17094.9106\n",
            "Epoch [80/150], Loss: 16822.6982\n",
            "Epoch [90/150], Loss: 18689.4722\n",
            "Epoch [100/150], Loss: 19325.1880\n",
            "Epoch [110/150], Loss: 17295.9958\n",
            "Epoch [120/150], Loss: 18277.8896\n",
            "Epoch [130/150], Loss: 18598.2275\n",
            "Epoch [140/150], Loss: 19026.3442\n",
            "Epoch [150/150], Loss: 16761.9578\n",
            "Fold 1, RMSE: 67.47346496582031\n",
            "Epoch [10/150], Loss: 17468.2463\n",
            "Epoch [20/150], Loss: 14080.4297\n",
            "Epoch [30/150], Loss: 14013.1548\n",
            "Epoch [40/150], Loss: 11391.8022\n",
            "Epoch [50/150], Loss: 12697.1118\n",
            "Epoch [60/150], Loss: 18457.9662\n",
            "Epoch [70/150], Loss: 8579.8469\n",
            "Epoch [80/150], Loss: 13316.7500\n",
            "Epoch [90/150], Loss: 5522.3821\n",
            "Epoch [100/150], Loss: 10060.4780\n",
            "Epoch [110/150], Loss: 13080.9097\n",
            "Epoch [120/150], Loss: 11394.6755\n",
            "Epoch [130/150], Loss: 11282.0403\n",
            "Epoch [140/150], Loss: 13611.6748\n",
            "Epoch [150/150], Loss: 8008.4106\n",
            "Fold 2, RMSE: 74.4861831665039\n",
            "Epoch [10/150], Loss: 14215.6958\n",
            "Epoch [20/150], Loss: 15990.8379\n",
            "Epoch [30/150], Loss: 12294.5518\n",
            "Epoch [40/150], Loss: 14748.4126\n",
            "Epoch [50/150], Loss: 12811.5710\n",
            "Epoch [60/150], Loss: 16066.0461\n",
            "Epoch [70/150], Loss: 12964.7595\n",
            "Epoch [80/150], Loss: 12977.7251\n",
            "Epoch [90/150], Loss: 13091.9839\n",
            "Epoch [100/150], Loss: 11458.7621\n",
            "Epoch [110/150], Loss: 12250.9658\n",
            "Epoch [120/150], Loss: 17049.8916\n",
            "Epoch [130/150], Loss: 11613.2808\n",
            "Epoch [140/150], Loss: 12012.6360\n",
            "Epoch [150/150], Loss: 16396.7310\n",
            "Fold 3, RMSE: 109.73442840576172\n",
            "Epoch [10/150], Loss: 14583.0112\n",
            "Epoch [20/150], Loss: 11363.4658\n",
            "Epoch [30/150], Loss: 11380.7439\n",
            "Epoch [40/150], Loss: 13412.7112\n",
            "Epoch [50/150], Loss: 14591.4854\n",
            "Epoch [60/150], Loss: 8348.4277\n",
            "Epoch [70/150], Loss: 10387.7394\n",
            "Epoch [80/150], Loss: 7290.7629\n",
            "Epoch [90/150], Loss: 7089.6535\n",
            "Epoch [100/150], Loss: 5066.1658\n",
            "Epoch [110/150], Loss: 8935.3622\n",
            "Epoch [120/150], Loss: 5409.5758\n",
            "Epoch [130/150], Loss: 17836.2993\n",
            "Epoch [140/150], Loss: 13881.1313\n",
            "Epoch [150/150], Loss: 11482.4246\n",
            "Fold 4, RMSE: 41.3176383972168\n",
            "Epoch [10/150], Loss: 15731.6199\n",
            "Epoch [20/150], Loss: 14327.7993\n",
            "Epoch [30/150], Loss: 15915.7871\n",
            "Epoch [40/150], Loss: 10343.3474\n",
            "Epoch [50/150], Loss: 12516.2271\n",
            "Epoch [60/150], Loss: 16956.4995\n",
            "Epoch [70/150], Loss: 18921.0757\n",
            "Epoch [80/150], Loss: 11075.7771\n",
            "Epoch [90/150], Loss: 7708.6603\n",
            "Epoch [100/150], Loss: 10098.0404\n",
            "Epoch [110/150], Loss: 8632.1151\n",
            "Epoch [120/150], Loss: 12841.9205\n",
            "Epoch [130/150], Loss: 12323.4558\n",
            "Epoch [140/150], Loss: 12475.4341\n",
            "Epoch [150/150], Loss: 12097.8021\n",
            "Fold 5, RMSE: 45.03593444824219\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 67.60952987670899\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14157.7488\n",
            "Epoch [20/100], Loss: 13507.6235\n",
            "Epoch [30/100], Loss: 14514.1992\n",
            "Epoch [40/100], Loss: 13560.7083\n",
            "Epoch [50/100], Loss: 12298.1779\n",
            "Epoch [60/100], Loss: 11077.0886\n",
            "Epoch [70/100], Loss: 13572.3726\n",
            "Epoch [80/100], Loss: 9963.6208\n",
            "Epoch [90/100], Loss: 10273.4924\n",
            "Epoch [100/100], Loss: 11246.8102\n",
            "Fold 1, RMSE: 60.3841552734375\n",
            "Epoch [10/100], Loss: 18487.4678\n",
            "Epoch [20/100], Loss: 22666.6797\n",
            "Epoch [30/100], Loss: 21969.3192\n",
            "Epoch [40/100], Loss: 12307.3335\n",
            "Epoch [50/100], Loss: 14882.4341\n",
            "Epoch [60/100], Loss: 11730.9011\n",
            "Epoch [70/100], Loss: 10442.3472\n",
            "Epoch [80/100], Loss: 13097.1321\n",
            "Epoch [90/100], Loss: 6208.3652\n",
            "Epoch [100/100], Loss: 5281.3926\n",
            "Fold 2, RMSE: 78.77801513671875\n",
            "Epoch [10/100], Loss: 16875.1523\n",
            "Epoch [20/100], Loss: 9439.3809\n",
            "Epoch [30/100], Loss: 9033.0934\n",
            "Epoch [40/100], Loss: 11331.5947\n",
            "Epoch [50/100], Loss: 10640.5261\n",
            "Epoch [60/100], Loss: 8705.3044\n",
            "Epoch [70/100], Loss: 7145.7893\n",
            "Epoch [80/100], Loss: 5109.0125\n",
            "Epoch [90/100], Loss: 6944.1830\n",
            "Epoch [100/100], Loss: 5599.9316\n",
            "Fold 3, RMSE: 100.19086456298828\n",
            "Epoch [10/100], Loss: 21607.7522\n",
            "Epoch [20/100], Loss: 18980.8672\n",
            "Epoch [30/100], Loss: 20763.0378\n",
            "Epoch [40/100], Loss: 9669.6152\n",
            "Epoch [50/100], Loss: 10890.0065\n",
            "Epoch [60/100], Loss: 10648.6548\n",
            "Epoch [70/100], Loss: 17750.3276\n",
            "Epoch [80/100], Loss: 21329.1343\n",
            "Epoch [90/100], Loss: 17164.2906\n",
            "Epoch [100/100], Loss: 23849.6147\n",
            "Fold 4, RMSE: 54.37366485595703\n",
            "Epoch [10/100], Loss: 13241.4607\n",
            "Epoch [20/100], Loss: 16581.4609\n",
            "Epoch [30/100], Loss: 13920.4629\n",
            "Epoch [40/100], Loss: 13734.8804\n",
            "Epoch [50/100], Loss: 13389.2620\n",
            "Epoch [60/100], Loss: 15502.6373\n",
            "Epoch [70/100], Loss: 14331.9983\n",
            "Epoch [80/100], Loss: 20265.2397\n",
            "Epoch [90/100], Loss: 7853.3967\n",
            "Epoch [100/100], Loss: 8251.1561\n",
            "Fold 5, RMSE: 43.572235107421875\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 67.45978698730468\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 16700.2266\n",
            "Epoch [20/150], Loss: 17217.8115\n",
            "Epoch [30/150], Loss: 12296.1646\n",
            "Epoch [40/150], Loss: 8004.9874\n",
            "Epoch [50/150], Loss: 11124.8257\n",
            "Epoch [60/150], Loss: 13402.7864\n",
            "Epoch [70/150], Loss: 11989.0613\n",
            "Epoch [80/150], Loss: 14083.9084\n",
            "Epoch [90/150], Loss: 10547.5784\n",
            "Epoch [100/150], Loss: 8236.5184\n",
            "Epoch [110/150], Loss: 6488.6570\n",
            "Epoch [120/150], Loss: 11056.0681\n",
            "Epoch [130/150], Loss: 14320.2769\n",
            "Epoch [140/150], Loss: 8886.5339\n",
            "Epoch [150/150], Loss: 7676.5789\n",
            "Fold 1, RMSE: 56.48594284057617\n",
            "Epoch [10/150], Loss: 17406.2515\n",
            "Epoch [20/150], Loss: 13449.9399\n",
            "Epoch [30/150], Loss: 13390.0222\n",
            "Epoch [40/150], Loss: 8780.6945\n",
            "Epoch [50/150], Loss: 11542.4124\n",
            "Epoch [60/150], Loss: 14237.6475\n",
            "Epoch [70/150], Loss: 14950.6836\n",
            "Epoch [80/150], Loss: 14802.4958\n",
            "Epoch [90/150], Loss: 8171.0919\n",
            "Epoch [100/150], Loss: 6296.8479\n",
            "Epoch [110/150], Loss: 5335.6202\n",
            "Epoch [120/150], Loss: 17591.2283\n",
            "Epoch [130/150], Loss: 14014.6719\n",
            "Epoch [140/150], Loss: 13523.5552\n",
            "Epoch [150/150], Loss: 8164.5300\n",
            "Fold 2, RMSE: 80.39249420166016\n",
            "Epoch [10/150], Loss: 12930.0085\n",
            "Epoch [20/150], Loss: 10941.7026\n",
            "Epoch [30/150], Loss: 14874.6736\n",
            "Epoch [40/150], Loss: 8749.9250\n",
            "Epoch [50/150], Loss: 8802.8572\n",
            "Epoch [60/150], Loss: 7249.3713\n",
            "Epoch [70/150], Loss: 6393.3414\n",
            "Epoch [80/150], Loss: 5136.7295\n",
            "Epoch [90/150], Loss: 12406.6309\n",
            "Epoch [100/150], Loss: 4885.8149\n",
            "Epoch [110/150], Loss: 5168.0124\n",
            "Epoch [120/150], Loss: 8815.1359\n",
            "Epoch [130/150], Loss: 10946.2350\n",
            "Epoch [140/150], Loss: 10351.2678\n",
            "Epoch [150/150], Loss: 10858.7466\n",
            "Fold 3, RMSE: 103.06695556640625\n",
            "Epoch [10/150], Loss: 18270.7659\n",
            "Epoch [20/150], Loss: 16147.9043\n",
            "Epoch [30/150], Loss: 12846.1514\n",
            "Epoch [40/150], Loss: 12318.0321\n",
            "Epoch [50/150], Loss: 10204.6083\n",
            "Epoch [60/150], Loss: 9094.1925\n",
            "Epoch [70/150], Loss: 10332.3479\n",
            "Epoch [80/150], Loss: 11047.8020\n",
            "Epoch [90/150], Loss: 10431.6690\n",
            "Epoch [100/150], Loss: 8638.9902\n",
            "Epoch [110/150], Loss: 5456.4662\n",
            "Epoch [120/150], Loss: 10495.2456\n",
            "Epoch [130/150], Loss: 5357.2954\n",
            "Epoch [140/150], Loss: 11713.0281\n",
            "Epoch [150/150], Loss: 7177.3679\n",
            "Fold 4, RMSE: 41.830238342285156\n",
            "Epoch [10/150], Loss: 15029.7966\n",
            "Epoch [20/150], Loss: 15902.2996\n",
            "Epoch [30/150], Loss: 18921.4429\n",
            "Epoch [40/150], Loss: 25768.5103\n",
            "Epoch [50/150], Loss: 12723.8843\n",
            "Epoch [60/150], Loss: 23430.3477\n",
            "Epoch [70/150], Loss: 8191.6813\n",
            "Epoch [80/150], Loss: 7800.9161\n",
            "Epoch [90/150], Loss: 15148.7240\n",
            "Epoch [100/150], Loss: 15720.1147\n",
            "Epoch [110/150], Loss: 9891.0330\n",
            "Epoch [120/150], Loss: 13902.8205\n",
            "Epoch [130/150], Loss: 9260.9045\n",
            "Epoch [140/150], Loss: 6734.2585\n",
            "Epoch [150/150], Loss: 8218.0743\n",
            "Fold 5, RMSE: 50.30184555053711\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 66.41549530029297\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 13381.6194\n",
            "Epoch [20/100], Loss: 13547.5713\n",
            "Epoch [30/100], Loss: 12796.9358\n",
            "Epoch [40/100], Loss: 9000.5920\n",
            "Epoch [50/100], Loss: 10329.7603\n",
            "Epoch [60/100], Loss: 9585.2214\n",
            "Epoch [70/100], Loss: 8635.0088\n",
            "Epoch [80/100], Loss: 13226.5818\n",
            "Epoch [90/100], Loss: 10106.0842\n",
            "Epoch [100/100], Loss: 6566.4270\n",
            "Fold 1, RMSE: 55.422508239746094\n",
            "Epoch [10/100], Loss: 13660.0791\n",
            "Epoch [20/100], Loss: 11413.2590\n",
            "Epoch [30/100], Loss: 11202.8517\n",
            "Epoch [40/100], Loss: 9834.3994\n",
            "Epoch [50/100], Loss: 13305.3228\n",
            "Epoch [60/100], Loss: 12033.6360\n",
            "Epoch [70/100], Loss: 10939.3302\n",
            "Epoch [80/100], Loss: 7049.0442\n",
            "Epoch [90/100], Loss: 5209.6990\n",
            "Epoch [100/100], Loss: 10669.9670\n",
            "Fold 2, RMSE: 67.47554779052734\n",
            "Epoch [10/100], Loss: 11050.8248\n",
            "Epoch [20/100], Loss: 14085.7812\n",
            "Epoch [30/100], Loss: 12123.7708\n",
            "Epoch [40/100], Loss: 11407.2754\n",
            "Epoch [50/100], Loss: 16883.5767\n",
            "Epoch [60/100], Loss: 14469.8069\n",
            "Epoch [70/100], Loss: 13166.5759\n",
            "Epoch [80/100], Loss: 14046.5872\n",
            "Epoch [90/100], Loss: 12848.3462\n",
            "Epoch [100/100], Loss: 17764.8149\n",
            "Fold 3, RMSE: 109.85408782958984\n",
            "Epoch [10/100], Loss: 18638.0615\n",
            "Epoch [20/100], Loss: 21710.3257\n",
            "Epoch [30/100], Loss: 15776.0737\n",
            "Epoch [40/100], Loss: 14715.3215\n",
            "Epoch [50/100], Loss: 15786.5640\n",
            "Epoch [60/100], Loss: 7180.2212\n",
            "Epoch [70/100], Loss: 13401.5371\n",
            "Epoch [80/100], Loss: 7030.5706\n",
            "Epoch [90/100], Loss: 13543.9016\n",
            "Epoch [100/100], Loss: 20534.2207\n",
            "Fold 4, RMSE: 48.257408142089844\n",
            "Epoch [10/100], Loss: 13964.5398\n",
            "Epoch [20/100], Loss: 20248.8975\n",
            "Epoch [30/100], Loss: 15661.2222\n",
            "Epoch [40/100], Loss: 13269.6675\n",
            "Epoch [50/100], Loss: 11470.9106\n",
            "Epoch [60/100], Loss: 12457.5422\n",
            "Epoch [70/100], Loss: 7039.3791\n",
            "Epoch [80/100], Loss: 16549.3677\n",
            "Epoch [90/100], Loss: 19611.7188\n",
            "Epoch [100/100], Loss: 25079.6152\n",
            "Fold 5, RMSE: 57.896968841552734\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 67.78130416870117\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 26772.3657\n",
            "Epoch [20/150], Loss: 19625.4399\n",
            "Epoch [30/150], Loss: 13337.7104\n",
            "Epoch [40/150], Loss: 22359.1123\n",
            "Epoch [50/150], Loss: 13247.1610\n",
            "Epoch [60/150], Loss: 10110.3203\n",
            "Epoch [70/150], Loss: 18816.1848\n",
            "Epoch [80/150], Loss: 12881.0283\n",
            "Epoch [90/150], Loss: 15291.9355\n",
            "Epoch [100/150], Loss: 12105.1555\n",
            "Epoch [110/150], Loss: 11818.8417\n",
            "Epoch [120/150], Loss: 10321.7869\n",
            "Epoch [130/150], Loss: 9405.5147\n",
            "Epoch [140/150], Loss: 12264.9863\n",
            "Epoch [150/150], Loss: 8029.4253\n",
            "Fold 1, RMSE: 61.012638092041016\n",
            "Epoch [10/150], Loss: 15159.4176\n",
            "Epoch [20/150], Loss: 14061.7125\n",
            "Epoch [30/150], Loss: 14912.5283\n",
            "Epoch [40/150], Loss: 16726.7883\n",
            "Epoch [50/150], Loss: 16088.5974\n",
            "Epoch [60/150], Loss: 17083.3633\n",
            "Epoch [70/150], Loss: 18851.2915\n",
            "Epoch [80/150], Loss: 14970.6980\n",
            "Epoch [90/150], Loss: 14632.4750\n",
            "Epoch [100/150], Loss: 16354.8940\n",
            "Epoch [110/150], Loss: 14982.0430\n",
            "Epoch [120/150], Loss: 16559.6050\n",
            "Epoch [130/150], Loss: 17344.5789\n",
            "Epoch [140/150], Loss: 16287.4097\n",
            "Epoch [150/150], Loss: 22400.1538\n",
            "Fold 2, RMSE: 87.0908432006836\n",
            "Epoch [10/150], Loss: 15718.2451\n",
            "Epoch [20/150], Loss: 11095.9390\n",
            "Epoch [30/150], Loss: 11672.4891\n",
            "Epoch [40/150], Loss: 12227.9661\n",
            "Epoch [50/150], Loss: 13063.3613\n",
            "Epoch [60/150], Loss: 12338.3396\n",
            "Epoch [70/150], Loss: 12241.2019\n",
            "Epoch [80/150], Loss: 15282.2656\n",
            "Epoch [90/150], Loss: 10773.4037\n",
            "Epoch [100/150], Loss: 15696.2566\n",
            "Epoch [110/150], Loss: 12715.8042\n",
            "Epoch [120/150], Loss: 11259.7801\n",
            "Epoch [130/150], Loss: 16110.7981\n",
            "Epoch [140/150], Loss: 12717.4956\n",
            "Epoch [150/150], Loss: 12309.3782\n",
            "Fold 3, RMSE: 109.76622009277344\n",
            "Epoch [10/150], Loss: 16613.0352\n",
            "Epoch [20/150], Loss: 20303.5034\n",
            "Epoch [30/150], Loss: 14600.4615\n",
            "Epoch [40/150], Loss: 15340.6675\n",
            "Epoch [50/150], Loss: 10622.9077\n",
            "Epoch [60/150], Loss: 11341.7427\n",
            "Epoch [70/150], Loss: 9000.5547\n",
            "Epoch [80/150], Loss: 16671.1394\n",
            "Epoch [90/150], Loss: 10171.8995\n",
            "Epoch [100/150], Loss: 9351.3872\n",
            "Epoch [110/150], Loss: 12050.7673\n",
            "Epoch [120/150], Loss: 22823.4802\n",
            "Epoch [130/150], Loss: 8605.9165\n",
            "Epoch [140/150], Loss: 8948.4874\n",
            "Epoch [150/150], Loss: 9555.2893\n",
            "Fold 4, RMSE: 46.32986831665039\n",
            "Epoch [10/150], Loss: 16168.5261\n",
            "Epoch [20/150], Loss: 16078.7698\n",
            "Epoch [30/150], Loss: 13716.4651\n",
            "Epoch [40/150], Loss: 19839.3606\n",
            "Epoch [50/150], Loss: 17078.2527\n",
            "Epoch [60/150], Loss: 13823.7271\n",
            "Epoch [70/150], Loss: 11234.0057\n",
            "Epoch [80/150], Loss: 21511.2070\n",
            "Epoch [90/150], Loss: 8995.4796\n",
            "Epoch [100/150], Loss: 18270.8613\n",
            "Epoch [110/150], Loss: 11359.2170\n",
            "Epoch [120/150], Loss: 11274.5306\n",
            "Epoch [130/150], Loss: 12602.6045\n",
            "Epoch [140/150], Loss: 10533.9366\n",
            "Epoch [150/150], Loss: 12454.6240\n",
            "Fold 5, RMSE: 47.43076705932617\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 70.32606735229493\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14928.3789\n",
            "Epoch [20/100], Loss: 16493.6931\n",
            "Epoch [30/100], Loss: 15476.2673\n",
            "Epoch [40/100], Loss: 9151.7186\n",
            "Epoch [50/100], Loss: 9776.8499\n",
            "Epoch [60/100], Loss: 10152.6143\n",
            "Epoch [70/100], Loss: 14954.4900\n",
            "Epoch [80/100], Loss: 12302.5667\n",
            "Epoch [90/100], Loss: 9224.1499\n",
            "Epoch [100/100], Loss: 6188.6597\n",
            "Fold 1, RMSE: 54.1862678527832\n",
            "Epoch [10/100], Loss: 15901.5312\n",
            "Epoch [20/100], Loss: 16451.0640\n",
            "Epoch [30/100], Loss: 7633.3303\n",
            "Epoch [40/100], Loss: 8843.8936\n",
            "Epoch [50/100], Loss: 11154.0575\n",
            "Epoch [60/100], Loss: 22405.4761\n",
            "Epoch [70/100], Loss: 11010.1375\n",
            "Epoch [80/100], Loss: 10889.5583\n",
            "Epoch [90/100], Loss: 9166.9537\n",
            "Epoch [100/100], Loss: 17369.5942\n",
            "Fold 2, RMSE: 83.1169662475586\n",
            "Epoch [10/100], Loss: 9845.7365\n",
            "Epoch [20/100], Loss: 13081.2385\n",
            "Epoch [30/100], Loss: 10952.0703\n",
            "Epoch [40/100], Loss: 5874.2084\n",
            "Epoch [50/100], Loss: 7783.1443\n",
            "Epoch [60/100], Loss: 5791.8764\n",
            "Epoch [70/100], Loss: 6992.6638\n",
            "Epoch [80/100], Loss: 13320.5454\n",
            "Epoch [90/100], Loss: 9146.2884\n",
            "Epoch [100/100], Loss: 4941.8551\n",
            "Fold 3, RMSE: 102.21155548095703\n",
            "Epoch [10/100], Loss: 17383.9258\n",
            "Epoch [20/100], Loss: 17873.0112\n",
            "Epoch [30/100], Loss: 11107.1758\n",
            "Epoch [40/100], Loss: 13143.5088\n",
            "Epoch [50/100], Loss: 12432.6606\n",
            "Epoch [60/100], Loss: 8653.0544\n",
            "Epoch [70/100], Loss: 8269.2174\n",
            "Epoch [80/100], Loss: 12165.7166\n",
            "Epoch [90/100], Loss: 9817.3058\n",
            "Epoch [100/100], Loss: 18141.1582\n",
            "Fold 4, RMSE: 44.34480667114258\n",
            "Epoch [10/100], Loss: 19846.5237\n",
            "Epoch [20/100], Loss: 17929.3218\n",
            "Epoch [30/100], Loss: 14643.1377\n",
            "Epoch [40/100], Loss: 9670.3381\n",
            "Epoch [50/100], Loss: 18952.9037\n",
            "Epoch [60/100], Loss: 9672.2625\n",
            "Epoch [70/100], Loss: 25599.0922\n",
            "Epoch [80/100], Loss: 19777.7744\n",
            "Epoch [90/100], Loss: 17986.7722\n",
            "Epoch [100/100], Loss: 18097.9805\n",
            "Fold 5, RMSE: 57.80074691772461\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 68.3320686340332\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 16114.3853\n",
            "Epoch [20/150], Loss: 19860.6233\n",
            "Epoch [30/150], Loss: 14994.7854\n",
            "Epoch [40/150], Loss: 5309.4015\n",
            "Epoch [50/150], Loss: 16649.3165\n",
            "Epoch [60/150], Loss: 8109.8047\n",
            "Epoch [70/150], Loss: 16718.6333\n",
            "Epoch [80/150], Loss: 23123.9199\n",
            "Epoch [90/150], Loss: 6330.3296\n",
            "Epoch [100/150], Loss: 5598.0743\n",
            "Epoch [110/150], Loss: 4746.7126\n",
            "Epoch [120/150], Loss: 5226.7526\n",
            "Epoch [130/150], Loss: 7951.0984\n",
            "Epoch [140/150], Loss: 4683.9622\n",
            "Epoch [150/150], Loss: 9818.3508\n",
            "Fold 1, RMSE: 59.39548873901367\n",
            "Epoch [10/150], Loss: 15675.5635\n",
            "Epoch [20/150], Loss: 11448.1750\n",
            "Epoch [30/150], Loss: 11125.7532\n",
            "Epoch [40/150], Loss: 12351.9268\n",
            "Epoch [50/150], Loss: 11728.3918\n",
            "Epoch [60/150], Loss: 11872.9828\n",
            "Epoch [70/150], Loss: 13536.5588\n",
            "Epoch [80/150], Loss: 9849.3694\n",
            "Epoch [90/150], Loss: 6853.4583\n",
            "Epoch [100/150], Loss: 12505.1724\n",
            "Epoch [110/150], Loss: 6429.9774\n",
            "Epoch [120/150], Loss: 14484.0076\n",
            "Epoch [130/150], Loss: 21969.7280\n",
            "Epoch [140/150], Loss: 14433.9418\n",
            "Epoch [150/150], Loss: 13794.8424\n",
            "Fold 2, RMSE: 86.94603729248047\n",
            "Epoch [10/150], Loss: 12530.0842\n",
            "Epoch [20/150], Loss: 11637.9224\n",
            "Epoch [30/150], Loss: 9272.4290\n",
            "Epoch [40/150], Loss: 10465.2952\n",
            "Epoch [50/150], Loss: 12968.0378\n",
            "Epoch [60/150], Loss: 11460.7336\n",
            "Epoch [70/150], Loss: 5570.2045\n",
            "Epoch [80/150], Loss: 12296.7030\n",
            "Epoch [90/150], Loss: 10188.1609\n",
            "Epoch [100/150], Loss: 7650.8644\n",
            "Epoch [110/150], Loss: 10100.0518\n",
            "Epoch [120/150], Loss: 7978.9011\n",
            "Epoch [130/150], Loss: 6085.8611\n",
            "Epoch [140/150], Loss: 3925.0264\n",
            "Epoch [150/150], Loss: 4332.7041\n",
            "Fold 3, RMSE: 96.94023132324219\n",
            "Epoch [10/150], Loss: 14982.6934\n",
            "Epoch [20/150], Loss: 17232.0532\n",
            "Epoch [30/150], Loss: 12641.9548\n",
            "Epoch [40/150], Loss: 17522.8713\n",
            "Epoch [50/150], Loss: 18880.4240\n",
            "Epoch [60/150], Loss: 14920.7717\n",
            "Epoch [70/150], Loss: 7792.8259\n",
            "Epoch [80/150], Loss: 8662.9568\n",
            "Epoch [90/150], Loss: 11530.9351\n",
            "Epoch [100/150], Loss: 14168.0597\n",
            "Epoch [110/150], Loss: 8594.2869\n",
            "Epoch [120/150], Loss: 9674.4587\n",
            "Epoch [130/150], Loss: 7507.2009\n",
            "Epoch [140/150], Loss: 6131.1814\n",
            "Epoch [150/150], Loss: 6840.6449\n",
            "Fold 4, RMSE: 47.31196975708008\n",
            "Epoch [10/150], Loss: 21367.3560\n",
            "Epoch [20/150], Loss: 18265.4292\n",
            "Epoch [30/150], Loss: 7563.2212\n",
            "Epoch [40/150], Loss: 17514.8218\n",
            "Epoch [50/150], Loss: 8745.2668\n",
            "Epoch [60/150], Loss: 6819.6548\n",
            "Epoch [70/150], Loss: 13911.9697\n",
            "Epoch [80/150], Loss: 8615.6328\n",
            "Epoch [90/150], Loss: 9592.9329\n",
            "Epoch [100/150], Loss: 6391.4735\n",
            "Epoch [110/150], Loss: 8719.4749\n",
            "Epoch [120/150], Loss: 5451.2230\n",
            "Epoch [130/150], Loss: 6611.5635\n",
            "Epoch [140/150], Loss: 5794.2284\n",
            "Epoch [150/150], Loss: 6732.1955\n",
            "Fold 5, RMSE: 46.30610656738281\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 67.37996673583984\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 22175.6140\n",
            "Epoch [20/100], Loss: 20980.0288\n",
            "Epoch [30/100], Loss: 24375.8188\n",
            "Epoch [40/100], Loss: 17398.0889\n",
            "Epoch [50/100], Loss: 26517.2632\n",
            "Epoch [60/100], Loss: 18247.2822\n",
            "Epoch [70/100], Loss: 16965.7859\n",
            "Epoch [80/100], Loss: 18626.5737\n",
            "Epoch [90/100], Loss: 16064.3110\n",
            "Epoch [100/100], Loss: 17072.7703\n",
            "Fold 1, RMSE: 67.44129180908203\n",
            "Epoch [10/100], Loss: 17979.3950\n",
            "Epoch [20/100], Loss: 16822.8357\n",
            "Epoch [30/100], Loss: 15748.6975\n",
            "Epoch [40/100], Loss: 15135.0159\n",
            "Epoch [50/100], Loss: 15570.9670\n",
            "Epoch [60/100], Loss: 14818.2910\n",
            "Epoch [70/100], Loss: 15222.6965\n",
            "Epoch [80/100], Loss: 25759.2773\n",
            "Epoch [90/100], Loss: 14518.5624\n",
            "Epoch [100/100], Loss: 14294.6046\n",
            "Fold 2, RMSE: 87.2668228149414\n",
            "Epoch [10/100], Loss: 12435.7908\n",
            "Epoch [20/100], Loss: 10111.2598\n",
            "Epoch [30/100], Loss: 12265.3960\n",
            "Epoch [40/100], Loss: 10674.8738\n",
            "Epoch [50/100], Loss: 10574.8586\n",
            "Epoch [60/100], Loss: 8380.8171\n",
            "Epoch [70/100], Loss: 9028.9321\n",
            "Epoch [80/100], Loss: 7363.7676\n",
            "Epoch [90/100], Loss: 8134.8636\n",
            "Epoch [100/100], Loss: 7056.4792\n",
            "Fold 3, RMSE: 100.61944580078125\n",
            "Epoch [10/100], Loss: 21615.6694\n",
            "Epoch [20/100], Loss: 18504.7246\n",
            "Epoch [30/100], Loss: 22688.4556\n",
            "Epoch [40/100], Loss: 18056.7815\n",
            "Epoch [50/100], Loss: 23756.7346\n",
            "Epoch [60/100], Loss: 27976.6182\n",
            "Epoch [70/100], Loss: 27669.0610\n",
            "Epoch [80/100], Loss: 21551.5239\n",
            "Epoch [90/100], Loss: 22204.0171\n",
            "Epoch [100/100], Loss: 29162.6074\n",
            "Fold 4, RMSE: 54.38728332519531\n",
            "Epoch [10/100], Loss: 19506.6865\n",
            "Epoch [20/100], Loss: 18313.8872\n",
            "Epoch [30/100], Loss: 17929.1006\n",
            "Epoch [40/100], Loss: 20841.0066\n",
            "Epoch [50/100], Loss: 17880.8137\n",
            "Epoch [60/100], Loss: 20553.0156\n",
            "Epoch [70/100], Loss: 23537.2446\n",
            "Epoch [80/100], Loss: 28785.9219\n",
            "Epoch [90/100], Loss: 19342.8257\n",
            "Epoch [100/100], Loss: 17748.2078\n",
            "Fold 5, RMSE: 57.83531951904297\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 73.5100326538086\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19061.5591\n",
            "Epoch [20/150], Loss: 28439.2527\n",
            "Epoch [30/150], Loss: 17056.9211\n",
            "Epoch [40/150], Loss: 24969.7715\n",
            "Epoch [50/150], Loss: 29469.7429\n",
            "Epoch [60/150], Loss: 19318.4146\n",
            "Epoch [70/150], Loss: 17218.3926\n",
            "Epoch [80/150], Loss: 18029.3989\n",
            "Epoch [90/150], Loss: 19526.3989\n",
            "Epoch [100/150], Loss: 17958.5828\n",
            "Epoch [110/150], Loss: 26910.7251\n",
            "Epoch [120/150], Loss: 17730.1123\n",
            "Epoch [130/150], Loss: 19476.2612\n",
            "Epoch [140/150], Loss: 17216.0396\n",
            "Epoch [150/150], Loss: 16158.5366\n",
            "Fold 1, RMSE: 67.09015655517578\n",
            "Epoch [10/150], Loss: 18994.2212\n",
            "Epoch [20/150], Loss: 19044.7065\n",
            "Epoch [30/150], Loss: 17043.2891\n",
            "Epoch [40/150], Loss: 17858.7197\n",
            "Epoch [50/150], Loss: 17399.0317\n",
            "Epoch [60/150], Loss: 15886.0693\n",
            "Epoch [70/150], Loss: 14935.4233\n",
            "Epoch [80/150], Loss: 15508.3687\n",
            "Epoch [90/150], Loss: 16374.5664\n",
            "Epoch [100/150], Loss: 13977.6223\n",
            "Epoch [110/150], Loss: 16995.3960\n",
            "Epoch [120/150], Loss: 17035.4238\n",
            "Epoch [130/150], Loss: 14739.1626\n",
            "Epoch [140/150], Loss: 15929.3562\n",
            "Epoch [150/150], Loss: 18145.8137\n",
            "Fold 2, RMSE: 87.01419067382812\n",
            "Epoch [10/150], Loss: 16646.9152\n",
            "Epoch [20/150], Loss: 15166.7705\n",
            "Epoch [30/150], Loss: 11409.0780\n",
            "Epoch [40/150], Loss: 16897.9275\n",
            "Epoch [50/150], Loss: 17545.8816\n",
            "Epoch [60/150], Loss: 12702.0352\n",
            "Epoch [70/150], Loss: 12888.4077\n",
            "Epoch [80/150], Loss: 15424.5911\n",
            "Epoch [90/150], Loss: 14301.7378\n",
            "Epoch [100/150], Loss: 15593.7075\n",
            "Epoch [110/150], Loss: 13406.9980\n",
            "Epoch [120/150], Loss: 14347.1382\n",
            "Epoch [130/150], Loss: 19321.7793\n",
            "Epoch [140/150], Loss: 13532.9658\n",
            "Epoch [150/150], Loss: 14232.4480\n",
            "Fold 3, RMSE: 109.56198120117188\n",
            "Epoch [10/150], Loss: 21997.0889\n",
            "Epoch [20/150], Loss: 20282.2034\n",
            "Epoch [30/150], Loss: 19997.0859\n",
            "Epoch [40/150], Loss: 14856.6622\n",
            "Epoch [50/150], Loss: 12800.9370\n",
            "Epoch [60/150], Loss: 12112.2275\n",
            "Epoch [70/150], Loss: 22027.2510\n",
            "Epoch [80/150], Loss: 15440.0896\n",
            "Epoch [90/150], Loss: 12962.2461\n",
            "Epoch [100/150], Loss: 12117.0959\n",
            "Epoch [110/150], Loss: 13315.1908\n",
            "Epoch [120/150], Loss: 16894.8276\n",
            "Epoch [130/150], Loss: 8622.8915\n",
            "Epoch [140/150], Loss: 16915.5239\n",
            "Epoch [150/150], Loss: 7576.6240\n",
            "Fold 4, RMSE: 53.2102165222168\n",
            "Epoch [10/150], Loss: 19400.0186\n",
            "Epoch [20/150], Loss: 21672.7197\n",
            "Epoch [30/150], Loss: 17918.0986\n",
            "Epoch [40/150], Loss: 16347.9319\n",
            "Epoch [50/150], Loss: 22840.1099\n",
            "Epoch [60/150], Loss: 17669.4146\n",
            "Epoch [70/150], Loss: 20453.3672\n",
            "Epoch [80/150], Loss: 19313.5503\n",
            "Epoch [90/150], Loss: 26854.1133\n",
            "Epoch [100/150], Loss: 21255.1978\n",
            "Epoch [110/150], Loss: 18725.4956\n",
            "Epoch [120/150], Loss: 31319.6018\n",
            "Epoch [130/150], Loss: 19696.5913\n",
            "Epoch [140/150], Loss: 16946.8105\n",
            "Epoch [150/150], Loss: 20700.0518\n",
            "Fold 5, RMSE: 57.663448333740234\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 74.90799865722656\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16225.2869\n",
            "Epoch [20/100], Loss: 19173.2920\n",
            "Epoch [30/100], Loss: 15352.0703\n",
            "Epoch [40/100], Loss: 12324.4868\n",
            "Epoch [50/100], Loss: 23068.0901\n",
            "Epoch [60/100], Loss: 15824.8955\n",
            "Epoch [70/100], Loss: 14351.7410\n",
            "Epoch [80/100], Loss: 13204.8528\n",
            "Epoch [90/100], Loss: 10508.6702\n",
            "Epoch [100/100], Loss: 15511.1716\n",
            "Fold 1, RMSE: 58.357059478759766\n",
            "Epoch [10/100], Loss: 24119.9226\n",
            "Epoch [20/100], Loss: 16521.9944\n",
            "Epoch [30/100], Loss: 15287.4558\n",
            "Epoch [40/100], Loss: 15464.7268\n",
            "Epoch [50/100], Loss: 13789.5757\n",
            "Epoch [60/100], Loss: 11653.1707\n",
            "Epoch [70/100], Loss: 13124.2904\n",
            "Epoch [80/100], Loss: 10707.7379\n",
            "Epoch [90/100], Loss: 14226.1796\n",
            "Epoch [100/100], Loss: 12774.6697\n",
            "Fold 2, RMSE: 85.90155792236328\n",
            "Epoch [10/100], Loss: 14766.1030\n",
            "Epoch [20/100], Loss: 14855.9385\n",
            "Epoch [30/100], Loss: 13212.0400\n",
            "Epoch [40/100], Loss: 12767.6860\n",
            "Epoch [50/100], Loss: 14582.8076\n",
            "Epoch [60/100], Loss: 14290.1855\n",
            "Epoch [70/100], Loss: 13021.3126\n",
            "Epoch [80/100], Loss: 12262.2537\n",
            "Epoch [90/100], Loss: 12103.1067\n",
            "Epoch [100/100], Loss: 13713.7090\n",
            "Fold 3, RMSE: 109.4126968383789\n",
            "Epoch [10/100], Loss: 16903.6841\n",
            "Epoch [20/100], Loss: 19828.2539\n",
            "Epoch [30/100], Loss: 22211.8857\n",
            "Epoch [40/100], Loss: 21332.5581\n",
            "Epoch [50/100], Loss: 26898.0576\n",
            "Epoch [60/100], Loss: 18369.0220\n",
            "Epoch [70/100], Loss: 16814.7949\n",
            "Epoch [80/100], Loss: 17239.8535\n",
            "Epoch [90/100], Loss: 23309.8823\n",
            "Epoch [100/100], Loss: 19661.6958\n",
            "Fold 4, RMSE: 54.08613967895508\n",
            "Epoch [10/100], Loss: 17087.1877\n",
            "Epoch [20/100], Loss: 19096.5161\n",
            "Epoch [30/100], Loss: 20257.6675\n",
            "Epoch [40/100], Loss: 24031.6836\n",
            "Epoch [50/100], Loss: 16292.6284\n",
            "Epoch [60/100], Loss: 16104.6006\n",
            "Epoch [70/100], Loss: 14562.8191\n",
            "Epoch [80/100], Loss: 10780.0337\n",
            "Epoch [90/100], Loss: 10712.9790\n",
            "Epoch [100/100], Loss: 12060.8157\n",
            "Fold 5, RMSE: 46.24333953857422\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 70.80015869140625\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 16832.0864\n",
            "Epoch [20/150], Loss: 17099.9517\n",
            "Epoch [30/150], Loss: 14002.8745\n",
            "Epoch [40/150], Loss: 17067.0366\n",
            "Epoch [50/150], Loss: 16048.9817\n",
            "Epoch [60/150], Loss: 23696.9722\n",
            "Epoch [70/150], Loss: 22111.3599\n",
            "Epoch [80/150], Loss: 15358.6826\n",
            "Epoch [90/150], Loss: 13571.9899\n",
            "Epoch [100/150], Loss: 11486.3580\n",
            "Epoch [110/150], Loss: 11140.4602\n",
            "Epoch [120/150], Loss: 6952.5593\n",
            "Epoch [130/150], Loss: 8660.1321\n",
            "Epoch [140/150], Loss: 10815.0930\n",
            "Epoch [150/150], Loss: 10510.2404\n",
            "Fold 1, RMSE: 58.68306350708008\n",
            "Epoch [10/150], Loss: 14978.0190\n",
            "Epoch [20/150], Loss: 14803.4105\n",
            "Epoch [30/150], Loss: 15288.4729\n",
            "Epoch [40/150], Loss: 14344.9500\n",
            "Epoch [50/150], Loss: 14682.5537\n",
            "Epoch [60/150], Loss: 18259.0054\n",
            "Epoch [70/150], Loss: 20367.3210\n",
            "Epoch [80/150], Loss: 17067.8176\n",
            "Epoch [90/150], Loss: 21123.6763\n",
            "Epoch [100/150], Loss: 14649.0005\n",
            "Epoch [110/150], Loss: 14286.4316\n",
            "Epoch [120/150], Loss: 17649.1057\n",
            "Epoch [130/150], Loss: 18861.1597\n",
            "Epoch [140/150], Loss: 15255.1313\n",
            "Epoch [150/150], Loss: 18011.8418\n",
            "Fold 2, RMSE: 86.59880828857422\n",
            "Epoch [10/150], Loss: 14921.5662\n",
            "Epoch [20/150], Loss: 14561.1748\n",
            "Epoch [30/150], Loss: 10968.1602\n",
            "Epoch [40/150], Loss: 15393.7432\n",
            "Epoch [50/150], Loss: 15483.0322\n",
            "Epoch [60/150], Loss: 11440.6956\n",
            "Epoch [70/150], Loss: 15085.5244\n",
            "Epoch [80/150], Loss: 14768.6636\n",
            "Epoch [90/150], Loss: 11824.3633\n",
            "Epoch [100/150], Loss: 12907.0720\n",
            "Epoch [110/150], Loss: 11556.9500\n",
            "Epoch [120/150], Loss: 11820.3468\n",
            "Epoch [130/150], Loss: 10826.5331\n",
            "Epoch [140/150], Loss: 13284.5659\n",
            "Epoch [150/150], Loss: 11228.3082\n",
            "Fold 3, RMSE: 108.88671112060547\n",
            "Epoch [10/150], Loss: 21033.9763\n",
            "Epoch [20/150], Loss: 20903.9856\n",
            "Epoch [30/150], Loss: 16584.8601\n",
            "Epoch [40/150], Loss: 12286.8788\n",
            "Epoch [50/150], Loss: 17584.9705\n",
            "Epoch [60/150], Loss: 19692.5828\n",
            "Epoch [70/150], Loss: 8370.2609\n",
            "Epoch [80/150], Loss: 18461.6870\n",
            "Epoch [90/150], Loss: 14639.7410\n",
            "Epoch [100/150], Loss: 10681.0342\n",
            "Epoch [110/150], Loss: 10833.8345\n",
            "Epoch [120/150], Loss: 13986.4778\n",
            "Epoch [130/150], Loss: 14892.1851\n",
            "Epoch [140/150], Loss: 8009.8601\n",
            "Epoch [150/150], Loss: 9181.6671\n",
            "Fold 4, RMSE: 50.15810012817383\n",
            "Epoch [10/150], Loss: 18766.3442\n",
            "Epoch [20/150], Loss: 22116.4365\n",
            "Epoch [30/150], Loss: 22241.1870\n",
            "Epoch [40/150], Loss: 23915.6538\n",
            "Epoch [50/150], Loss: 18675.5437\n",
            "Epoch [60/150], Loss: 20913.6616\n",
            "Epoch [70/150], Loss: 25671.4590\n",
            "Epoch [80/150], Loss: 13898.4194\n",
            "Epoch [90/150], Loss: 18034.1035\n",
            "Epoch [100/150], Loss: 16434.2393\n",
            "Epoch [110/150], Loss: 18058.7771\n",
            "Epoch [120/150], Loss: 19745.5093\n",
            "Epoch [130/150], Loss: 18797.1260\n",
            "Epoch [140/150], Loss: 18281.5913\n",
            "Epoch [150/150], Loss: 17188.4514\n",
            "Fold 5, RMSE: 57.039268493652344\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 72.27319030761718\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17759.8264\n",
            "Epoch [20/100], Loss: 18870.1499\n",
            "Epoch [30/100], Loss: 20570.1504\n",
            "Epoch [40/100], Loss: 16734.2007\n",
            "Epoch [50/100], Loss: 15841.8580\n",
            "Epoch [60/100], Loss: 19448.3843\n",
            "Epoch [70/100], Loss: 19917.0581\n",
            "Epoch [80/100], Loss: 18308.6636\n",
            "Epoch [90/100], Loss: 15963.4214\n",
            "Epoch [100/100], Loss: 16361.4233\n",
            "Fold 1, RMSE: 59.87926483154297\n",
            "Epoch [10/100], Loss: 28685.8101\n",
            "Epoch [20/100], Loss: 20090.3901\n",
            "Epoch [30/100], Loss: 12631.5625\n",
            "Epoch [40/100], Loss: 17801.4460\n",
            "Epoch [50/100], Loss: 25786.7964\n",
            "Epoch [60/100], Loss: 16256.4790\n",
            "Epoch [70/100], Loss: 16359.2783\n",
            "Epoch [80/100], Loss: 14674.9890\n",
            "Epoch [90/100], Loss: 16347.6328\n",
            "Epoch [100/100], Loss: 19994.2734\n",
            "Fold 2, RMSE: 86.53315734863281\n",
            "Epoch [10/100], Loss: 16068.7151\n",
            "Epoch [20/100], Loss: 16207.3152\n",
            "Epoch [30/100], Loss: 15879.4712\n",
            "Epoch [40/100], Loss: 10918.6428\n",
            "Epoch [50/100], Loss: 15088.3997\n",
            "Epoch [60/100], Loss: 17297.0679\n",
            "Epoch [70/100], Loss: 11817.0321\n",
            "Epoch [80/100], Loss: 14626.1069\n",
            "Epoch [90/100], Loss: 15437.7451\n",
            "Epoch [100/100], Loss: 15805.7136\n",
            "Fold 3, RMSE: 109.67312622070312\n",
            "Epoch [10/100], Loss: 22842.7261\n",
            "Epoch [20/100], Loss: 20646.5156\n",
            "Epoch [30/100], Loss: 20830.0918\n",
            "Epoch [40/100], Loss: 22671.9644\n",
            "Epoch [50/100], Loss: 17304.6675\n",
            "Epoch [60/100], Loss: 24350.4956\n",
            "Epoch [70/100], Loss: 21057.2295\n",
            "Epoch [80/100], Loss: 19312.6147\n",
            "Epoch [90/100], Loss: 25890.9863\n",
            "Epoch [100/100], Loss: 20889.4673\n",
            "Fold 4, RMSE: 54.336883544921875\n",
            "Epoch [10/100], Loss: 20446.8474\n",
            "Epoch [20/100], Loss: 17802.5015\n",
            "Epoch [30/100], Loss: 22591.6753\n",
            "Epoch [40/100], Loss: 19808.5981\n",
            "Epoch [50/100], Loss: 17860.2395\n",
            "Epoch [60/100], Loss: 17655.2529\n",
            "Epoch [70/100], Loss: 19706.5303\n",
            "Epoch [80/100], Loss: 17494.6104\n",
            "Epoch [90/100], Loss: 16843.5304\n",
            "Epoch [100/100], Loss: 20917.5591\n",
            "Fold 5, RMSE: 57.78839874267578\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 73.64216613769531\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18879.5703\n",
            "Epoch [20/150], Loss: 19162.3657\n",
            "Epoch [30/150], Loss: 16470.2438\n",
            "Epoch [40/150], Loss: 20054.1523\n",
            "Epoch [50/150], Loss: 16434.2468\n",
            "Epoch [60/150], Loss: 17422.4500\n",
            "Epoch [70/150], Loss: 23880.9268\n",
            "Epoch [80/150], Loss: 16670.9683\n",
            "Epoch [90/150], Loss: 15968.5083\n",
            "Epoch [100/150], Loss: 25190.0972\n",
            "Epoch [110/150], Loss: 18256.7461\n",
            "Epoch [120/150], Loss: 24748.4316\n",
            "Epoch [130/150], Loss: 20733.9834\n",
            "Epoch [140/150], Loss: 21276.4041\n",
            "Epoch [150/150], Loss: 15919.9617\n",
            "Fold 1, RMSE: 67.36474609375\n",
            "Epoch [10/150], Loss: 15849.3403\n",
            "Epoch [20/150], Loss: 21853.4745\n",
            "Epoch [30/150], Loss: 16960.8467\n",
            "Epoch [40/150], Loss: 19677.2510\n",
            "Epoch [50/150], Loss: 16579.3071\n",
            "Epoch [60/150], Loss: 14355.7510\n",
            "Epoch [70/150], Loss: 19534.6479\n",
            "Epoch [80/150], Loss: 16403.2036\n",
            "Epoch [90/150], Loss: 19022.5894\n",
            "Epoch [100/150], Loss: 27059.3572\n",
            "Epoch [110/150], Loss: 14223.8125\n",
            "Epoch [120/150], Loss: 14593.2028\n",
            "Epoch [130/150], Loss: 18903.3164\n",
            "Epoch [140/150], Loss: 16278.6067\n",
            "Epoch [150/150], Loss: 16228.1909\n",
            "Fold 2, RMSE: 86.95711517333984\n",
            "Epoch [10/150], Loss: 12312.5867\n",
            "Epoch [20/150], Loss: 13802.0337\n",
            "Epoch [30/150], Loss: 8485.6952\n",
            "Epoch [40/150], Loss: 9833.4006\n",
            "Epoch [50/150], Loss: 7392.4227\n",
            "Epoch [60/150], Loss: 10588.4351\n",
            "Epoch [70/150], Loss: 14102.6294\n",
            "Epoch [80/150], Loss: 14462.2690\n",
            "Epoch [90/150], Loss: 13518.0901\n",
            "Epoch [100/150], Loss: 14014.1426\n",
            "Epoch [110/150], Loss: 13895.4087\n",
            "Epoch [120/150], Loss: 14309.6968\n",
            "Epoch [130/150], Loss: 15029.0107\n",
            "Epoch [140/150], Loss: 12508.0576\n",
            "Epoch [150/150], Loss: 12918.9087\n",
            "Fold 3, RMSE: 109.64088439941406\n",
            "Epoch [10/150], Loss: 18777.8193\n",
            "Epoch [20/150], Loss: 21035.4580\n",
            "Epoch [30/150], Loss: 17590.4272\n",
            "Epoch [40/150], Loss: 19421.2324\n",
            "Epoch [50/150], Loss: 18744.7620\n",
            "Epoch [60/150], Loss: 27412.7317\n",
            "Epoch [70/150], Loss: 23214.9883\n",
            "Epoch [80/150], Loss: 17283.3694\n",
            "Epoch [90/150], Loss: 27020.3672\n",
            "Epoch [100/150], Loss: 19383.6191\n",
            "Epoch [110/150], Loss: 18735.4109\n",
            "Epoch [120/150], Loss: 18583.4268\n",
            "Epoch [130/150], Loss: 16990.8479\n",
            "Epoch [140/150], Loss: 17893.0908\n",
            "Epoch [150/150], Loss: 29286.2529\n",
            "Fold 4, RMSE: 54.16946792602539\n",
            "Epoch [10/150], Loss: 20650.8164\n",
            "Epoch [20/150], Loss: 17929.4854\n",
            "Epoch [30/150], Loss: 19595.7334\n",
            "Epoch [40/150], Loss: 29533.2292\n",
            "Epoch [50/150], Loss: 15170.7338\n",
            "Epoch [60/150], Loss: 18401.1133\n",
            "Epoch [70/150], Loss: 15843.1201\n",
            "Epoch [80/150], Loss: 15908.7859\n",
            "Epoch [90/150], Loss: 9481.5922\n",
            "Epoch [100/150], Loss: 12638.1838\n",
            "Epoch [110/150], Loss: 10548.0717\n",
            "Epoch [120/150], Loss: 17139.8163\n",
            "Epoch [130/150], Loss: 11229.6552\n",
            "Epoch [140/150], Loss: 11512.7004\n",
            "Epoch [150/150], Loss: 9755.5042\n",
            "Fold 5, RMSE: 44.928955078125\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 72.61223373413085\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16650.4302\n",
            "Epoch [20/100], Loss: 23032.7131\n",
            "Epoch [30/100], Loss: 17740.7446\n",
            "Epoch [40/100], Loss: 18295.6523\n",
            "Epoch [50/100], Loss: 11930.0054\n",
            "Epoch [60/100], Loss: 11749.2485\n",
            "Epoch [70/100], Loss: 13457.8640\n",
            "Epoch [80/100], Loss: 13799.9346\n",
            "Epoch [90/100], Loss: 12118.2041\n",
            "Epoch [100/100], Loss: 12396.1984\n",
            "Fold 1, RMSE: 54.1525764465332\n",
            "Epoch [10/100], Loss: 16834.2368\n",
            "Epoch [20/100], Loss: 13596.2996\n",
            "Epoch [30/100], Loss: 11966.3018\n",
            "Epoch [40/100], Loss: 9180.9067\n",
            "Epoch [50/100], Loss: 10704.6400\n",
            "Epoch [60/100], Loss: 13309.8082\n",
            "Epoch [70/100], Loss: 15935.5690\n",
            "Epoch [80/100], Loss: 11470.7444\n",
            "Epoch [90/100], Loss: 10373.9746\n",
            "Epoch [100/100], Loss: 9263.1382\n",
            "Fold 2, RMSE: 74.65335845947266\n",
            "Epoch [10/100], Loss: 11594.3145\n",
            "Epoch [20/100], Loss: 12195.1143\n",
            "Epoch [30/100], Loss: 12690.0200\n",
            "Epoch [40/100], Loss: 12677.9922\n",
            "Epoch [50/100], Loss: 13431.0015\n",
            "Epoch [60/100], Loss: 11282.0938\n",
            "Epoch [70/100], Loss: 10643.8622\n",
            "Epoch [80/100], Loss: 13107.2983\n",
            "Epoch [90/100], Loss: 12168.2175\n",
            "Epoch [100/100], Loss: 11834.9915\n",
            "Fold 3, RMSE: 109.28428649902344\n",
            "Epoch [10/100], Loss: 17464.8955\n",
            "Epoch [20/100], Loss: 17888.1084\n",
            "Epoch [30/100], Loss: 21725.5764\n",
            "Epoch [40/100], Loss: 14852.8174\n",
            "Epoch [50/100], Loss: 25540.9863\n",
            "Epoch [60/100], Loss: 20571.7026\n",
            "Epoch [70/100], Loss: 12697.3748\n",
            "Epoch [80/100], Loss: 14503.0359\n",
            "Epoch [90/100], Loss: 12191.4294\n",
            "Epoch [100/100], Loss: 25227.3003\n",
            "Fold 4, RMSE: 42.08236312866211\n",
            "Epoch [10/100], Loss: 17467.5579\n",
            "Epoch [20/100], Loss: 19699.9014\n",
            "Epoch [30/100], Loss: 17444.4722\n",
            "Epoch [40/100], Loss: 24616.2598\n",
            "Epoch [50/100], Loss: 18769.1560\n",
            "Epoch [60/100], Loss: 18213.1147\n",
            "Epoch [70/100], Loss: 17357.9929\n",
            "Epoch [80/100], Loss: 17327.3901\n",
            "Epoch [90/100], Loss: 17781.3152\n",
            "Epoch [100/100], Loss: 19183.3452\n",
            "Fold 5, RMSE: 57.54977798461914\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 67.5444725036621\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14199.8081\n",
            "Epoch [20/150], Loss: 20867.1223\n",
            "Epoch [30/150], Loss: 16031.8733\n",
            "Epoch [40/150], Loss: 15161.6490\n",
            "Epoch [50/150], Loss: 10647.9009\n",
            "Epoch [60/150], Loss: 15545.0737\n",
            "Epoch [70/150], Loss: 20568.9497\n",
            "Epoch [80/150], Loss: 27812.5762\n",
            "Epoch [90/150], Loss: 17267.2661\n",
            "Epoch [100/150], Loss: 9797.4888\n",
            "Epoch [110/150], Loss: 8031.1816\n",
            "Epoch [120/150], Loss: 17489.3821\n",
            "Epoch [130/150], Loss: 13193.7551\n",
            "Epoch [140/150], Loss: 7493.5827\n",
            "Epoch [150/150], Loss: 9057.9149\n",
            "Fold 1, RMSE: 58.19520950317383\n",
            "Epoch [10/150], Loss: 22496.0215\n",
            "Epoch [20/150], Loss: 19282.2131\n",
            "Epoch [30/150], Loss: 15104.1558\n",
            "Epoch [40/150], Loss: 17232.2622\n",
            "Epoch [50/150], Loss: 15085.2917\n",
            "Epoch [60/150], Loss: 14731.7122\n",
            "Epoch [70/150], Loss: 13817.1707\n",
            "Epoch [80/150], Loss: 13928.9253\n",
            "Epoch [90/150], Loss: 17569.0872\n",
            "Epoch [100/150], Loss: 16469.3799\n",
            "Epoch [110/150], Loss: 15129.9937\n",
            "Epoch [120/150], Loss: 17029.0220\n",
            "Epoch [130/150], Loss: 21500.9810\n",
            "Epoch [140/150], Loss: 14186.9905\n",
            "Epoch [150/150], Loss: 17402.3950\n",
            "Fold 2, RMSE: 85.91556549072266\n",
            "Epoch [10/150], Loss: 11004.9285\n",
            "Epoch [20/150], Loss: 13501.1011\n",
            "Epoch [30/150], Loss: 10680.5693\n",
            "Epoch [40/150], Loss: 12441.1064\n",
            "Epoch [50/150], Loss: 13180.9292\n",
            "Epoch [60/150], Loss: 10694.0796\n",
            "Epoch [70/150], Loss: 15006.1238\n",
            "Epoch [80/150], Loss: 14546.3354\n",
            "Epoch [90/150], Loss: 13867.7891\n",
            "Epoch [100/150], Loss: 9419.6971\n",
            "Epoch [110/150], Loss: 12052.3264\n",
            "Epoch [120/150], Loss: 8751.5138\n",
            "Epoch [130/150], Loss: 9668.1331\n",
            "Epoch [140/150], Loss: 8845.0812\n",
            "Epoch [150/150], Loss: 9283.2379\n",
            "Fold 3, RMSE: 108.96363830566406\n",
            "Epoch [10/150], Loss: 27740.1895\n",
            "Epoch [20/150], Loss: 13635.4098\n",
            "Epoch [30/150], Loss: 12641.9971\n",
            "Epoch [40/150], Loss: 10555.4707\n",
            "Epoch [50/150], Loss: 17539.7488\n",
            "Epoch [60/150], Loss: 21120.4924\n",
            "Epoch [70/150], Loss: 15294.4946\n",
            "Epoch [80/150], Loss: 15077.9436\n",
            "Epoch [90/150], Loss: 15322.2144\n",
            "Epoch [100/150], Loss: 19395.6909\n",
            "Epoch [110/150], Loss: 12645.6572\n",
            "Epoch [120/150], Loss: 7569.6595\n",
            "Epoch [130/150], Loss: 9201.7606\n",
            "Epoch [140/150], Loss: 7435.0757\n",
            "Epoch [150/150], Loss: 5741.2042\n",
            "Fold 4, RMSE: 45.7147331237793\n",
            "Epoch [10/150], Loss: 19060.1729\n",
            "Epoch [20/150], Loss: 11818.1277\n",
            "Epoch [30/150], Loss: 21184.9744\n",
            "Epoch [40/150], Loss: 21354.9436\n",
            "Epoch [50/150], Loss: 11899.4062\n",
            "Epoch [60/150], Loss: 7984.9644\n",
            "Epoch [70/150], Loss: 4992.5320\n",
            "Epoch [80/150], Loss: 11163.3433\n",
            "Epoch [90/150], Loss: 15426.2681\n",
            "Epoch [100/150], Loss: 15852.2188\n",
            "Epoch [110/150], Loss: 5554.2090\n",
            "Epoch [120/150], Loss: 14083.4385\n",
            "Epoch [130/150], Loss: 19269.4525\n",
            "Epoch [140/150], Loss: 15459.3530\n",
            "Epoch [150/150], Loss: 11685.2865\n",
            "Fold 5, RMSE: 47.677127838134766\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 69.29325485229492\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 20528.3154\n",
            "Epoch [20/100], Loss: 16347.5632\n",
            "Epoch [30/100], Loss: 16718.5737\n",
            "Epoch [40/100], Loss: 24029.4595\n",
            "Epoch [50/100], Loss: 17158.0420\n",
            "Epoch [60/100], Loss: 16551.8547\n",
            "Epoch [70/100], Loss: 21261.1941\n",
            "Epoch [80/100], Loss: 20246.7534\n",
            "Epoch [90/100], Loss: 21006.4961\n",
            "Epoch [100/100], Loss: 17284.7783\n",
            "Fold 1, RMSE: 67.4638900756836\n",
            "Epoch [10/100], Loss: 17496.2642\n",
            "Epoch [20/100], Loss: 16375.2891\n",
            "Epoch [30/100], Loss: 14030.6912\n",
            "Epoch [40/100], Loss: 16851.1270\n",
            "Epoch [50/100], Loss: 24643.5017\n",
            "Epoch [60/100], Loss: 18246.5686\n",
            "Epoch [70/100], Loss: 15170.2468\n",
            "Epoch [80/100], Loss: 14739.7970\n",
            "Epoch [90/100], Loss: 14522.1038\n",
            "Epoch [100/100], Loss: 26386.6982\n",
            "Fold 2, RMSE: 87.29161071777344\n",
            "Epoch [10/100], Loss: 18102.9250\n",
            "Epoch [20/100], Loss: 14109.5828\n",
            "Epoch [30/100], Loss: 13888.6807\n",
            "Epoch [40/100], Loss: 11527.1926\n",
            "Epoch [50/100], Loss: 11480.3427\n",
            "Epoch [60/100], Loss: 16207.4084\n",
            "Epoch [70/100], Loss: 12361.7129\n",
            "Epoch [80/100], Loss: 14949.4980\n",
            "Epoch [90/100], Loss: 11508.6971\n",
            "Epoch [100/100], Loss: 15101.0398\n",
            "Fold 3, RMSE: 109.59161376953125\n",
            "Epoch [10/100], Loss: 17496.4780\n",
            "Epoch [20/100], Loss: 16797.3213\n",
            "Epoch [30/100], Loss: 12347.0723\n",
            "Epoch [40/100], Loss: 19027.5557\n",
            "Epoch [50/100], Loss: 19306.2251\n",
            "Epoch [60/100], Loss: 17233.3993\n",
            "Epoch [70/100], Loss: 26814.0459\n",
            "Epoch [80/100], Loss: 22219.3987\n",
            "Epoch [90/100], Loss: 20061.1445\n",
            "Epoch [100/100], Loss: 24689.3472\n",
            "Fold 4, RMSE: 54.25175476074219\n",
            "Epoch [10/100], Loss: 23774.4717\n",
            "Epoch [20/100], Loss: 21088.0156\n",
            "Epoch [30/100], Loss: 11757.8167\n",
            "Epoch [40/100], Loss: 14670.7773\n",
            "Epoch [50/100], Loss: 7032.4949\n",
            "Epoch [60/100], Loss: 14512.0061\n",
            "Epoch [70/100], Loss: 6854.2296\n",
            "Epoch [80/100], Loss: 15269.2524\n",
            "Epoch [90/100], Loss: 19533.3574\n",
            "Epoch [100/100], Loss: 11654.7405\n",
            "Fold 5, RMSE: 44.73386001586914\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 72.66654586791992\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18920.7314\n",
            "Epoch [20/150], Loss: 29591.8806\n",
            "Epoch [30/150], Loss: 16330.3846\n",
            "Epoch [40/150], Loss: 20065.5483\n",
            "Epoch [50/150], Loss: 22409.3726\n",
            "Epoch [60/150], Loss: 17137.2712\n",
            "Epoch [70/150], Loss: 16735.1963\n",
            "Epoch [80/150], Loss: 20027.8750\n",
            "Epoch [90/150], Loss: 16976.9314\n",
            "Epoch [100/150], Loss: 15612.9061\n",
            "Epoch [110/150], Loss: 18746.0640\n",
            "Epoch [120/150], Loss: 20340.3955\n",
            "Epoch [130/150], Loss: 18528.2354\n",
            "Epoch [140/150], Loss: 17882.5579\n",
            "Epoch [150/150], Loss: 20564.4287\n",
            "Fold 1, RMSE: 67.22056579589844\n",
            "Epoch [10/150], Loss: 15554.4102\n",
            "Epoch [20/150], Loss: 25433.9968\n",
            "Epoch [30/150], Loss: 14295.7980\n",
            "Epoch [40/150], Loss: 14737.6002\n",
            "Epoch [50/150], Loss: 22829.5588\n",
            "Epoch [60/150], Loss: 19135.9788\n",
            "Epoch [70/150], Loss: 14969.9709\n",
            "Epoch [80/150], Loss: 20105.8416\n",
            "Epoch [90/150], Loss: 25201.9883\n",
            "Epoch [100/150], Loss: 16646.9067\n",
            "Epoch [110/150], Loss: 15058.3091\n",
            "Epoch [120/150], Loss: 17205.0085\n",
            "Epoch [130/150], Loss: 13577.2566\n",
            "Epoch [140/150], Loss: 18271.7388\n",
            "Epoch [150/150], Loss: 17034.4038\n",
            "Fold 2, RMSE: 87.09618377685547\n",
            "Epoch [10/150], Loss: 12255.1953\n",
            "Epoch [20/150], Loss: 15947.5636\n",
            "Epoch [30/150], Loss: 16479.4893\n",
            "Epoch [40/150], Loss: 11705.4094\n",
            "Epoch [50/150], Loss: 11520.1958\n",
            "Epoch [60/150], Loss: 12251.6453\n",
            "Epoch [70/150], Loss: 11445.5159\n",
            "Epoch [80/150], Loss: 14846.4099\n",
            "Epoch [90/150], Loss: 13540.7991\n",
            "Epoch [100/150], Loss: 13914.7693\n",
            "Epoch [110/150], Loss: 17398.3210\n",
            "Epoch [120/150], Loss: 12479.2524\n",
            "Epoch [130/150], Loss: 13126.3745\n",
            "Epoch [140/150], Loss: 14910.2061\n",
            "Epoch [150/150], Loss: 13052.1624\n",
            "Fold 3, RMSE: 109.34188079833984\n",
            "Epoch [10/150], Loss: 16828.4937\n",
            "Epoch [20/150], Loss: 14310.3496\n",
            "Epoch [30/150], Loss: 15281.1497\n",
            "Epoch [40/150], Loss: 15385.4902\n",
            "Epoch [50/150], Loss: 14021.7559\n",
            "Epoch [60/150], Loss: 18170.7139\n",
            "Epoch [70/150], Loss: 11534.2867\n",
            "Epoch [80/150], Loss: 9222.1590\n",
            "Epoch [90/150], Loss: 7670.1038\n",
            "Epoch [100/150], Loss: 15139.0037\n",
            "Epoch [110/150], Loss: 14442.1616\n",
            "Epoch [120/150], Loss: 7971.0377\n",
            "Epoch [130/150], Loss: 9457.7689\n",
            "Epoch [140/150], Loss: 16334.2837\n",
            "Epoch [150/150], Loss: 14760.1472\n",
            "Fold 4, RMSE: 52.00305938720703\n",
            "Epoch [10/150], Loss: 17904.1350\n",
            "Epoch [20/150], Loss: 22209.7646\n",
            "Epoch [30/150], Loss: 25620.5127\n",
            "Epoch [40/150], Loss: 17756.3713\n",
            "Epoch [50/150], Loss: 20914.5581\n",
            "Epoch [60/150], Loss: 17827.5872\n",
            "Epoch [70/150], Loss: 18754.1245\n",
            "Epoch [80/150], Loss: 23630.4482\n",
            "Epoch [90/150], Loss: 18692.4531\n",
            "Epoch [100/150], Loss: 24919.8862\n",
            "Epoch [110/150], Loss: 21365.9941\n",
            "Epoch [120/150], Loss: 21019.4858\n",
            "Epoch [130/150], Loss: 18009.5503\n",
            "Epoch [140/150], Loss: 17466.4856\n",
            "Epoch [150/150], Loss: 20289.8364\n",
            "Fold 5, RMSE: 57.81940460205078\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 74.69621887207032\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 21524.1182\n",
            "Epoch [20/100], Loss: 20734.6118\n",
            "Epoch [30/100], Loss: 20392.0049\n",
            "Epoch [40/100], Loss: 21226.8677\n",
            "Epoch [50/100], Loss: 15974.9681\n",
            "Epoch [60/100], Loss: 21125.0237\n",
            "Epoch [70/100], Loss: 19863.8315\n",
            "Epoch [80/100], Loss: 15432.6294\n",
            "Epoch [90/100], Loss: 18665.2915\n",
            "Epoch [100/100], Loss: 17268.9375\n",
            "Fold 1, RMSE: 67.24806213378906\n",
            "Epoch [10/100], Loss: 17176.8140\n",
            "Epoch [20/100], Loss: 14764.2136\n",
            "Epoch [30/100], Loss: 18131.5918\n",
            "Epoch [40/100], Loss: 15074.6814\n",
            "Epoch [50/100], Loss: 14609.3496\n",
            "Epoch [60/100], Loss: 16664.8662\n",
            "Epoch [70/100], Loss: 15477.9817\n",
            "Epoch [80/100], Loss: 19471.5229\n",
            "Epoch [90/100], Loss: 13567.8203\n",
            "Epoch [100/100], Loss: 15044.7065\n",
            "Fold 2, RMSE: 86.89523315429688\n",
            "Epoch [10/100], Loss: 13754.0688\n",
            "Epoch [20/100], Loss: 9555.1338\n",
            "Epoch [30/100], Loss: 12689.7014\n",
            "Epoch [40/100], Loss: 13796.1814\n",
            "Epoch [50/100], Loss: 10258.4377\n",
            "Epoch [60/100], Loss: 9341.7938\n",
            "Epoch [70/100], Loss: 5756.4294\n",
            "Epoch [80/100], Loss: 7270.9984\n",
            "Epoch [90/100], Loss: 5831.9174\n",
            "Epoch [100/100], Loss: 6227.9667\n",
            "Fold 3, RMSE: 97.63846588134766\n",
            "Epoch [10/100], Loss: 23866.8730\n",
            "Epoch [20/100], Loss: 20090.2371\n",
            "Epoch [30/100], Loss: 17987.1631\n",
            "Epoch [40/100], Loss: 16505.7290\n",
            "Epoch [50/100], Loss: 16769.6477\n",
            "Epoch [60/100], Loss: 23655.6450\n",
            "Epoch [70/100], Loss: 21081.3652\n",
            "Epoch [80/100], Loss: 16931.7598\n",
            "Epoch [90/100], Loss: 18592.8928\n",
            "Epoch [100/100], Loss: 19650.4448\n",
            "Fold 4, RMSE: 53.75095748901367\n",
            "Epoch [10/100], Loss: 18671.9858\n",
            "Epoch [20/100], Loss: 25333.2739\n",
            "Epoch [30/100], Loss: 13684.4785\n",
            "Epoch [40/100], Loss: 13249.6267\n",
            "Epoch [50/100], Loss: 13240.9631\n",
            "Epoch [60/100], Loss: 11646.7395\n",
            "Epoch [70/100], Loss: 12387.7139\n",
            "Epoch [80/100], Loss: 12620.4648\n",
            "Epoch [90/100], Loss: 16681.4500\n",
            "Epoch [100/100], Loss: 23186.4639\n",
            "Fold 5, RMSE: 57.79369354248047\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 72.66528244018555\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 17763.5166\n",
            "Epoch [20/150], Loss: 12290.6663\n",
            "Epoch [30/150], Loss: 12298.1177\n",
            "Epoch [40/150], Loss: 13865.4316\n",
            "Epoch [50/150], Loss: 15142.4048\n",
            "Epoch [60/150], Loss: 16198.6309\n",
            "Epoch [70/150], Loss: 17341.5525\n",
            "Epoch [80/150], Loss: 10321.2566\n",
            "Epoch [90/150], Loss: 14950.8765\n",
            "Epoch [100/150], Loss: 16370.5647\n",
            "Epoch [110/150], Loss: 15222.3013\n",
            "Epoch [120/150], Loss: 18661.6774\n",
            "Epoch [130/150], Loss: 15978.1887\n",
            "Epoch [140/150], Loss: 11000.9983\n",
            "Epoch [150/150], Loss: 14508.4824\n",
            "Fold 1, RMSE: 56.069007873535156\n",
            "Epoch [10/150], Loss: 16240.3325\n",
            "Epoch [20/150], Loss: 12777.1782\n",
            "Epoch [30/150], Loss: 10419.0002\n",
            "Epoch [40/150], Loss: 11727.9404\n",
            "Epoch [50/150], Loss: 6624.5911\n",
            "Epoch [60/150], Loss: 10270.7842\n",
            "Epoch [70/150], Loss: 13070.3076\n",
            "Epoch [80/150], Loss: 16304.0923\n",
            "Epoch [90/150], Loss: 12085.3745\n",
            "Epoch [100/150], Loss: 12512.0564\n",
            "Epoch [110/150], Loss: 15002.4617\n",
            "Epoch [120/150], Loss: 10818.5920\n",
            "Epoch [130/150], Loss: 12399.2803\n",
            "Epoch [140/150], Loss: 8835.7288\n",
            "Epoch [150/150], Loss: 8153.2432\n",
            "Fold 2, RMSE: 74.65411376953125\n",
            "Epoch [10/150], Loss: 15932.1050\n",
            "Epoch [20/150], Loss: 14317.0640\n",
            "Epoch [30/150], Loss: 18258.1907\n",
            "Epoch [40/150], Loss: 15245.4899\n",
            "Epoch [50/150], Loss: 15774.3379\n",
            "Epoch [60/150], Loss: 15263.2949\n",
            "Epoch [70/150], Loss: 14365.9541\n",
            "Epoch [80/150], Loss: 10671.5330\n",
            "Epoch [90/150], Loss: 13375.6116\n",
            "Epoch [100/150], Loss: 12253.6167\n",
            "Epoch [110/150], Loss: 11195.2061\n",
            "Epoch [120/150], Loss: 12771.2190\n",
            "Epoch [130/150], Loss: 13881.0510\n",
            "Epoch [140/150], Loss: 11888.1687\n",
            "Epoch [150/150], Loss: 11494.1511\n",
            "Fold 3, RMSE: 108.65695190429688\n",
            "Epoch [10/150], Loss: 24372.9900\n",
            "Epoch [20/150], Loss: 17458.4351\n",
            "Epoch [30/150], Loss: 12202.3359\n",
            "Epoch [40/150], Loss: 12901.4709\n",
            "Epoch [50/150], Loss: 13257.7344\n",
            "Epoch [60/150], Loss: 17387.6167\n",
            "Epoch [70/150], Loss: 20467.9956\n",
            "Epoch [80/150], Loss: 17495.2808\n",
            "Epoch [90/150], Loss: 15388.5598\n",
            "Epoch [100/150], Loss: 9915.0732\n",
            "Epoch [110/150], Loss: 10767.3906\n",
            "Epoch [120/150], Loss: 14017.3489\n",
            "Epoch [130/150], Loss: 7337.7249\n",
            "Epoch [140/150], Loss: 12742.7140\n",
            "Epoch [150/150], Loss: 10877.9420\n",
            "Fold 4, RMSE: 51.51581954956055\n",
            "Epoch [10/150], Loss: 16566.9733\n",
            "Epoch [20/150], Loss: 17503.8271\n",
            "Epoch [30/150], Loss: 17379.7349\n",
            "Epoch [40/150], Loss: 19987.0352\n",
            "Epoch [50/150], Loss: 26669.1265\n",
            "Epoch [60/150], Loss: 21561.9214\n",
            "Epoch [70/150], Loss: 19831.0381\n",
            "Epoch [80/150], Loss: 19484.9546\n",
            "Epoch [90/150], Loss: 15842.8982\n",
            "Epoch [100/150], Loss: 19898.0571\n",
            "Epoch [110/150], Loss: 17603.7148\n",
            "Epoch [120/150], Loss: 19046.8281\n",
            "Epoch [130/150], Loss: 22790.8477\n",
            "Epoch [140/150], Loss: 17438.3552\n",
            "Epoch [150/150], Loss: 18185.4619\n",
            "Fold 5, RMSE: 57.24597930908203\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 69.62837448120118\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17588.2644\n",
            "Epoch [20/100], Loss: 18624.3960\n",
            "Epoch [30/100], Loss: 27555.4053\n",
            "Epoch [40/100], Loss: 19061.3613\n",
            "Epoch [50/100], Loss: 16628.7706\n",
            "Epoch [60/100], Loss: 17519.4094\n",
            "Epoch [70/100], Loss: 16616.7845\n",
            "Epoch [80/100], Loss: 16387.4788\n",
            "Epoch [90/100], Loss: 17502.9531\n",
            "Epoch [100/100], Loss: 18044.2375\n",
            "Fold 1, RMSE: 67.43553161621094\n",
            "Epoch [10/100], Loss: 18936.5010\n",
            "Epoch [20/100], Loss: 17097.8877\n",
            "Epoch [30/100], Loss: 17348.3340\n",
            "Epoch [40/100], Loss: 18485.9971\n",
            "Epoch [50/100], Loss: 18773.8147\n",
            "Epoch [60/100], Loss: 14487.5945\n",
            "Epoch [70/100], Loss: 17452.3823\n",
            "Epoch [80/100], Loss: 18507.2446\n",
            "Epoch [90/100], Loss: 16054.1240\n",
            "Epoch [100/100], Loss: 14856.7395\n",
            "Fold 2, RMSE: 86.43377685546875\n",
            "Epoch [10/100], Loss: 14059.8013\n",
            "Epoch [20/100], Loss: 16700.0042\n",
            "Epoch [30/100], Loss: 11950.9678\n",
            "Epoch [40/100], Loss: 11373.9635\n",
            "Epoch [50/100], Loss: 16975.3848\n",
            "Epoch [60/100], Loss: 11551.4401\n",
            "Epoch [70/100], Loss: 15474.4287\n",
            "Epoch [80/100], Loss: 11685.2468\n",
            "Epoch [90/100], Loss: 12248.1479\n",
            "Epoch [100/100], Loss: 11500.9639\n",
            "Fold 3, RMSE: 109.7464370727539\n",
            "Epoch [10/100], Loss: 19276.3418\n",
            "Epoch [20/100], Loss: 18500.0049\n",
            "Epoch [30/100], Loss: 19413.4648\n",
            "Epoch [40/100], Loss: 20876.7891\n",
            "Epoch [50/100], Loss: 22705.0088\n",
            "Epoch [60/100], Loss: 24795.1904\n",
            "Epoch [70/100], Loss: 20667.6802\n",
            "Epoch [80/100], Loss: 20978.4619\n",
            "Epoch [90/100], Loss: 20609.0757\n",
            "Epoch [100/100], Loss: 18450.3372\n",
            "Fold 4, RMSE: 53.98928451538086\n",
            "Epoch [10/100], Loss: 19782.7451\n",
            "Epoch [20/100], Loss: 22543.9836\n",
            "Epoch [30/100], Loss: 28398.8154\n",
            "Epoch [40/100], Loss: 28080.0542\n",
            "Epoch [50/100], Loss: 18507.4634\n",
            "Epoch [60/100], Loss: 22548.9053\n",
            "Epoch [70/100], Loss: 19226.9839\n",
            "Epoch [80/100], Loss: 19260.8887\n",
            "Epoch [90/100], Loss: 22850.5674\n",
            "Epoch [100/100], Loss: 22215.4697\n",
            "Fold 5, RMSE: 57.83610153198242\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 75.08822631835938\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 251241.4219\n",
            "Epoch [20/150], Loss: 22260.7505\n",
            "Epoch [30/150], Loss: 16837.4648\n",
            "Epoch [40/150], Loss: 18583.5195\n",
            "Epoch [50/150], Loss: 15478.1768\n",
            "Epoch [60/150], Loss: 16609.3767\n",
            "Epoch [70/150], Loss: 14458.9449\n",
            "Epoch [80/150], Loss: 23073.6479\n",
            "Epoch [90/150], Loss: 12879.8257\n",
            "Epoch [100/150], Loss: 25346.3784\n",
            "Epoch [110/150], Loss: 12439.2156\n",
            "Epoch [120/150], Loss: 12623.5864\n",
            "Epoch [130/150], Loss: 20383.4546\n",
            "Epoch [140/150], Loss: 11562.3889\n",
            "Epoch [150/150], Loss: 11849.9354\n",
            "Fold 1, RMSE: 56.98536682128906\n",
            "Epoch [10/150], Loss: 27333.6135\n",
            "Epoch [20/150], Loss: 16856.8281\n",
            "Epoch [30/150], Loss: 15832.7944\n",
            "Epoch [40/150], Loss: 14857.0203\n",
            "Epoch [50/150], Loss: 14081.6116\n",
            "Epoch [60/150], Loss: 19452.8394\n",
            "Epoch [70/150], Loss: 18418.7456\n",
            "Epoch [80/150], Loss: 16708.1733\n",
            "Epoch [90/150], Loss: 15547.2117\n",
            "Epoch [100/150], Loss: 16242.4966\n",
            "Epoch [110/150], Loss: 14255.6371\n",
            "Epoch [120/150], Loss: 22510.8818\n",
            "Epoch [130/150], Loss: 16775.4939\n",
            "Epoch [140/150], Loss: 14637.5266\n",
            "Epoch [150/150], Loss: 14245.7191\n",
            "Fold 2, RMSE: 86.51488494873047\n",
            "Epoch [10/150], Loss: 57240.7734\n",
            "Epoch [20/150], Loss: 17647.9048\n",
            "Epoch [30/150], Loss: 14409.5337\n",
            "Epoch [40/150], Loss: 9577.6833\n",
            "Epoch [50/150], Loss: 13023.2573\n",
            "Epoch [60/150], Loss: 14388.9297\n",
            "Epoch [70/150], Loss: 12894.9893\n",
            "Epoch [80/150], Loss: 13652.3943\n",
            "Epoch [90/150], Loss: 10209.8450\n",
            "Epoch [100/150], Loss: 13524.8838\n",
            "Epoch [110/150], Loss: 13545.2866\n",
            "Epoch [120/150], Loss: 14968.7664\n",
            "Epoch [130/150], Loss: 10952.4272\n",
            "Epoch [140/150], Loss: 12055.5891\n",
            "Epoch [150/150], Loss: 13682.2083\n",
            "Fold 3, RMSE: 99.459228515625\n",
            "Epoch [10/150], Loss: 18021.0852\n",
            "Epoch [20/150], Loss: 19849.4595\n",
            "Epoch [30/150], Loss: 20456.1426\n",
            "Epoch [40/150], Loss: 19512.5176\n",
            "Epoch [50/150], Loss: 20457.3047\n",
            "Epoch [60/150], Loss: 22732.7339\n",
            "Epoch [70/150], Loss: 21422.0225\n",
            "Epoch [80/150], Loss: 22142.3765\n",
            "Epoch [90/150], Loss: 21847.9531\n",
            "Epoch [100/150], Loss: 23049.3457\n",
            "Epoch [110/150], Loss: 20130.9072\n",
            "Epoch [120/150], Loss: 19486.7427\n",
            "Epoch [130/150], Loss: 21674.6567\n",
            "Epoch [140/150], Loss: 17103.8146\n",
            "Epoch [150/150], Loss: 19956.7876\n",
            "Fold 4, RMSE: 54.295623779296875\n",
            "Epoch [10/150], Loss: 21047.9067\n",
            "Epoch [20/150], Loss: 18013.7275\n",
            "Epoch [30/150], Loss: 21488.2285\n",
            "Epoch [40/150], Loss: 18649.3250\n",
            "Epoch [50/150], Loss: 21491.7065\n",
            "Epoch [60/150], Loss: 29164.0381\n",
            "Epoch [70/150], Loss: 21585.1611\n",
            "Epoch [80/150], Loss: 17944.2144\n",
            "Epoch [90/150], Loss: 22015.4648\n",
            "Epoch [100/150], Loss: 20359.8225\n",
            "Epoch [110/150], Loss: 22317.9390\n",
            "Epoch [120/150], Loss: 17057.6462\n",
            "Epoch [130/150], Loss: 21390.0215\n",
            "Epoch [140/150], Loss: 16786.8485\n",
            "Epoch [150/150], Loss: 18825.6577\n",
            "Fold 5, RMSE: 57.76243209838867\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 71.00350723266601\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 19904.3855\n",
            "Epoch [20/100], Loss: 17605.2305\n",
            "Epoch [30/100], Loss: 19583.6333\n",
            "Epoch [40/100], Loss: 19788.0674\n",
            "Epoch [50/100], Loss: 15190.5934\n",
            "Epoch [60/100], Loss: 18542.7998\n",
            "Epoch [70/100], Loss: 26510.3398\n",
            "Epoch [80/100], Loss: 16608.9214\n",
            "Epoch [90/100], Loss: 23411.2217\n",
            "Epoch [100/100], Loss: 18542.9214\n",
            "Fold 1, RMSE: 66.9400405883789\n",
            "Epoch [10/100], Loss: 15633.1504\n",
            "Epoch [20/100], Loss: 17646.9863\n",
            "Epoch [30/100], Loss: 19342.7004\n",
            "Epoch [40/100], Loss: 17321.9131\n",
            "Epoch [50/100], Loss: 16413.1763\n",
            "Epoch [60/100], Loss: 20133.9243\n",
            "Epoch [70/100], Loss: 15412.9644\n",
            "Epoch [80/100], Loss: 14011.5479\n",
            "Epoch [90/100], Loss: 18172.4650\n",
            "Epoch [100/100], Loss: 15047.8555\n",
            "Fold 2, RMSE: 85.51731872558594\n",
            "Epoch [10/100], Loss: 14320.1130\n",
            "Epoch [20/100], Loss: 16662.6875\n",
            "Epoch [30/100], Loss: 15847.7010\n",
            "Epoch [40/100], Loss: 12152.0035\n",
            "Epoch [50/100], Loss: 15146.5464\n",
            "Epoch [60/100], Loss: 14040.5718\n",
            "Epoch [70/100], Loss: 11948.6885\n",
            "Epoch [80/100], Loss: 12465.6885\n",
            "Epoch [90/100], Loss: 10736.3807\n",
            "Epoch [100/100], Loss: 14593.2786\n",
            "Fold 3, RMSE: 106.99916076660156\n",
            "Epoch [10/100], Loss: 22858.2202\n",
            "Epoch [20/100], Loss: 28104.5547\n",
            "Epoch [30/100], Loss: 19895.6699\n",
            "Epoch [40/100], Loss: 18031.5645\n",
            "Epoch [50/100], Loss: 18947.7517\n",
            "Epoch [60/100], Loss: 20993.5298\n",
            "Epoch [70/100], Loss: 18276.1326\n",
            "Epoch [80/100], Loss: 20579.6982\n",
            "Epoch [90/100], Loss: 20871.2542\n",
            "Epoch [100/100], Loss: 17003.9197\n",
            "Fold 4, RMSE: 53.1035270690918\n",
            "Epoch [10/100], Loss: 72358.0703\n",
            "Epoch [20/100], Loss: 16697.9771\n",
            "Epoch [30/100], Loss: 18341.4785\n",
            "Epoch [40/100], Loss: 13920.2744\n",
            "Epoch [50/100], Loss: 17587.5840\n",
            "Epoch [60/100], Loss: 12654.6836\n",
            "Epoch [70/100], Loss: 14280.5623\n",
            "Epoch [80/100], Loss: 11407.3293\n",
            "Epoch [90/100], Loss: 12949.6736\n",
            "Epoch [100/100], Loss: 14212.3074\n",
            "Fold 5, RMSE: 48.818572998046875\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 72.27572402954101\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 17806.2266\n",
            "Epoch [20/150], Loss: 16559.0839\n",
            "Epoch [30/150], Loss: 20174.9429\n",
            "Epoch [40/150], Loss: 17747.2729\n",
            "Epoch [50/150], Loss: 20385.9956\n",
            "Epoch [60/150], Loss: 16925.8220\n",
            "Epoch [70/150], Loss: 27785.4604\n",
            "Epoch [80/150], Loss: 15587.0902\n",
            "Epoch [90/150], Loss: 15935.7023\n",
            "Epoch [100/150], Loss: 18481.4995\n",
            "Epoch [110/150], Loss: 17484.2212\n",
            "Epoch [120/150], Loss: 16472.5232\n",
            "Epoch [130/150], Loss: 21201.5586\n",
            "Epoch [140/150], Loss: 15480.5231\n",
            "Epoch [150/150], Loss: 17597.7756\n",
            "Fold 1, RMSE: 65.7103271484375\n",
            "Epoch [10/150], Loss: 19505.7881\n",
            "Epoch [20/150], Loss: 18169.7377\n",
            "Epoch [30/150], Loss: 14628.4294\n",
            "Epoch [40/150], Loss: 13936.3490\n",
            "Epoch [50/150], Loss: 14629.4266\n",
            "Epoch [60/150], Loss: 15602.7087\n",
            "Epoch [70/150], Loss: 15327.5720\n",
            "Epoch [80/150], Loss: 17331.4348\n",
            "Epoch [90/150], Loss: 18403.7351\n",
            "Epoch [100/150], Loss: 15501.2520\n",
            "Epoch [110/150], Loss: 16367.2285\n",
            "Epoch [120/150], Loss: 13498.4370\n",
            "Epoch [130/150], Loss: 15401.7788\n",
            "Epoch [140/150], Loss: 13100.1470\n",
            "Epoch [150/150], Loss: 11932.4474\n",
            "Fold 2, RMSE: 76.30790710449219\n",
            "Epoch [10/150], Loss: 13841.5244\n",
            "Epoch [20/150], Loss: 14412.2949\n",
            "Epoch [30/150], Loss: 12706.5095\n",
            "Epoch [40/150], Loss: 14288.7749\n",
            "Epoch [50/150], Loss: 12363.7009\n",
            "Epoch [60/150], Loss: 13741.2937\n",
            "Epoch [70/150], Loss: 20727.4167\n",
            "Epoch [80/150], Loss: 10839.9471\n",
            "Epoch [90/150], Loss: 11711.3221\n",
            "Epoch [100/150], Loss: 11980.0156\n",
            "Epoch [110/150], Loss: 13312.6316\n",
            "Epoch [120/150], Loss: 13783.3777\n",
            "Epoch [130/150], Loss: 15603.6121\n",
            "Epoch [140/150], Loss: 12234.9502\n",
            "Epoch [150/150], Loss: 10056.5863\n",
            "Fold 3, RMSE: 106.24369812011719\n",
            "Epoch [10/150], Loss: 62478.3145\n",
            "Epoch [20/150], Loss: 17864.7583\n",
            "Epoch [30/150], Loss: 17553.2710\n",
            "Epoch [40/150], Loss: 17469.7788\n",
            "Epoch [50/150], Loss: 12823.4268\n",
            "Epoch [60/150], Loss: 16216.4800\n",
            "Epoch [70/150], Loss: 16420.0383\n",
            "Epoch [80/150], Loss: 16741.4402\n",
            "Epoch [90/150], Loss: 20259.8835\n",
            "Epoch [100/150], Loss: 14404.2661\n",
            "Epoch [110/150], Loss: 15340.1921\n",
            "Epoch [120/150], Loss: 19330.7271\n",
            "Epoch [130/150], Loss: 13014.6840\n",
            "Epoch [140/150], Loss: 13739.4600\n",
            "Epoch [150/150], Loss: 13186.7756\n",
            "Fold 4, RMSE: 46.33818817138672\n",
            "Epoch [10/150], Loss: 19071.5830\n",
            "Epoch [20/150], Loss: 17253.5913\n",
            "Epoch [30/150], Loss: 19430.2349\n",
            "Epoch [40/150], Loss: 22580.6704\n",
            "Epoch [50/150], Loss: 20969.8145\n",
            "Epoch [60/150], Loss: 18646.9658\n",
            "Epoch [70/150], Loss: 17800.5366\n",
            "Epoch [80/150], Loss: 19018.9883\n",
            "Epoch [90/150], Loss: 27105.0220\n",
            "Epoch [100/150], Loss: 22065.7388\n",
            "Epoch [110/150], Loss: 20179.3567\n",
            "Epoch [120/150], Loss: 17578.4814\n",
            "Epoch [130/150], Loss: 22108.3071\n",
            "Epoch [140/150], Loss: 15528.2178\n",
            "Epoch [150/150], Loss: 23603.5837\n",
            "Fold 5, RMSE: 53.26318359375\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 69.57266082763672\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 18692.2240\n",
            "Epoch [20/100], Loss: 20125.3101\n",
            "Epoch [30/100], Loss: 20957.7349\n",
            "Epoch [40/100], Loss: 26605.1379\n",
            "Epoch [50/100], Loss: 18485.6729\n",
            "Epoch [60/100], Loss: 18236.1392\n",
            "Epoch [70/100], Loss: 17426.7915\n",
            "Epoch [80/100], Loss: 18467.9688\n",
            "Epoch [90/100], Loss: 19608.6604\n",
            "Epoch [100/100], Loss: 19339.0879\n",
            "Fold 1, RMSE: 67.41775512695312\n",
            "Epoch [10/100], Loss: 18529.9388\n",
            "Epoch [20/100], Loss: 17676.5833\n",
            "Epoch [30/100], Loss: 15280.6584\n",
            "Epoch [40/100], Loss: 17060.6772\n",
            "Epoch [50/100], Loss: 18231.1292\n",
            "Epoch [60/100], Loss: 23931.2554\n",
            "Epoch [70/100], Loss: 17294.2007\n",
            "Epoch [80/100], Loss: 17374.0659\n",
            "Epoch [90/100], Loss: 16683.8105\n",
            "Epoch [100/100], Loss: 17767.7295\n",
            "Fold 2, RMSE: 87.25354766845703\n",
            "Epoch [10/100], Loss: 91991.4355\n",
            "Epoch [20/100], Loss: 17877.3337\n",
            "Epoch [30/100], Loss: 18388.0255\n",
            "Epoch [40/100], Loss: 15655.9004\n",
            "Epoch [50/100], Loss: 13729.3733\n",
            "Epoch [60/100], Loss: 13802.0012\n",
            "Epoch [70/100], Loss: 12946.1975\n",
            "Epoch [80/100], Loss: 13095.4409\n",
            "Epoch [90/100], Loss: 14106.2273\n",
            "Epoch [100/100], Loss: 9961.1201\n",
            "Fold 3, RMSE: 99.53816986083984\n",
            "Epoch [10/100], Loss: 63457.3418\n",
            "Epoch [20/100], Loss: 22127.1914\n",
            "Epoch [30/100], Loss: 17967.3145\n",
            "Epoch [40/100], Loss: 25202.7944\n",
            "Epoch [50/100], Loss: 21845.6401\n",
            "Epoch [60/100], Loss: 25718.9126\n",
            "Epoch [70/100], Loss: 18860.5674\n",
            "Epoch [80/100], Loss: 20962.0503\n",
            "Epoch [90/100], Loss: 21082.7109\n",
            "Epoch [100/100], Loss: 18384.0896\n",
            "Fold 4, RMSE: 54.582176208496094\n",
            "Epoch [10/100], Loss: 189184.0625\n",
            "Epoch [20/100], Loss: 24395.2192\n",
            "Epoch [30/100], Loss: 17233.6467\n",
            "Epoch [40/100], Loss: 15531.5088\n",
            "Epoch [50/100], Loss: 17869.2876\n",
            "Epoch [60/100], Loss: 14921.8665\n",
            "Epoch [70/100], Loss: 14930.3048\n",
            "Epoch [80/100], Loss: 12210.9094\n",
            "Epoch [90/100], Loss: 12619.9592\n",
            "Epoch [100/100], Loss: 15987.0693\n",
            "Fold 5, RMSE: 49.96240234375\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 71.75081024169921\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18056.8938\n",
            "Epoch [20/150], Loss: 16853.5837\n",
            "Epoch [30/150], Loss: 28401.5605\n",
            "Epoch [40/150], Loss: 25091.9502\n",
            "Epoch [50/150], Loss: 16516.5518\n",
            "Epoch [60/150], Loss: 28756.4241\n",
            "Epoch [70/150], Loss: 23950.8184\n",
            "Epoch [80/150], Loss: 17600.3857\n",
            "Epoch [90/150], Loss: 18185.8003\n",
            "Epoch [100/150], Loss: 17138.5991\n",
            "Epoch [110/150], Loss: 16647.2473\n",
            "Epoch [120/150], Loss: 17152.0581\n",
            "Epoch [130/150], Loss: 21142.9634\n",
            "Epoch [140/150], Loss: 20950.8179\n",
            "Epoch [150/150], Loss: 17259.5640\n",
            "Fold 1, RMSE: 67.36260986328125\n",
            "Epoch [10/150], Loss: 340177.4688\n",
            "Epoch [20/150], Loss: 16261.2319\n",
            "Epoch [30/150], Loss: 16799.0103\n",
            "Epoch [40/150], Loss: 15758.8762\n",
            "Epoch [50/150], Loss: 21325.8633\n",
            "Epoch [60/150], Loss: 17437.0090\n",
            "Epoch [70/150], Loss: 14254.2366\n",
            "Epoch [80/150], Loss: 13918.5170\n",
            "Epoch [90/150], Loss: 13694.2700\n",
            "Epoch [100/150], Loss: 20147.5396\n",
            "Epoch [110/150], Loss: 23031.1184\n",
            "Epoch [120/150], Loss: 21438.1904\n",
            "Epoch [130/150], Loss: 14647.9690\n",
            "Epoch [140/150], Loss: 15665.8347\n",
            "Epoch [150/150], Loss: 13052.3613\n",
            "Fold 2, RMSE: 81.21044921875\n",
            "Epoch [10/150], Loss: 551492.1406\n",
            "Epoch [20/150], Loss: 78368.6055\n",
            "Epoch [30/150], Loss: 31163.6172\n",
            "Epoch [40/150], Loss: 14099.3485\n",
            "Epoch [50/150], Loss: 20462.1631\n",
            "Epoch [60/150], Loss: 11135.8853\n",
            "Epoch [70/150], Loss: 12989.8135\n",
            "Epoch [80/150], Loss: 10060.5824\n",
            "Epoch [90/150], Loss: 14296.5242\n",
            "Epoch [100/150], Loss: 8023.6864\n",
            "Epoch [110/150], Loss: 9728.8269\n",
            "Epoch [120/150], Loss: 12380.2808\n",
            "Epoch [130/150], Loss: 9303.2959\n",
            "Epoch [140/150], Loss: 12256.4744\n",
            "Epoch [150/150], Loss: 7906.8916\n",
            "Fold 3, RMSE: 97.3860855102539\n",
            "Epoch [10/150], Loss: 21000.5598\n",
            "Epoch [20/150], Loss: 22471.2881\n",
            "Epoch [30/150], Loss: 18970.4780\n",
            "Epoch [40/150], Loss: 20263.6602\n",
            "Epoch [50/150], Loss: 18396.9778\n",
            "Epoch [60/150], Loss: 16860.5281\n",
            "Epoch [70/150], Loss: 16530.9816\n",
            "Epoch [80/150], Loss: 27308.3745\n",
            "Epoch [90/150], Loss: 17161.5959\n",
            "Epoch [100/150], Loss: 26050.5649\n",
            "Epoch [110/150], Loss: 17511.6492\n",
            "Epoch [120/150], Loss: 17606.6423\n",
            "Epoch [130/150], Loss: 18436.2615\n",
            "Epoch [140/150], Loss: 26146.2749\n",
            "Epoch [150/150], Loss: 18842.5864\n",
            "Fold 4, RMSE: 54.27470016479492\n",
            "Epoch [10/150], Loss: 327749.4453\n",
            "Epoch [20/150], Loss: 51355.2949\n",
            "Epoch [30/150], Loss: 22059.6431\n",
            "Epoch [40/150], Loss: 22049.2900\n",
            "Epoch [50/150], Loss: 15270.5063\n",
            "Epoch [60/150], Loss: 14571.6443\n",
            "Epoch [70/150], Loss: 19187.4229\n",
            "Epoch [80/150], Loss: 17338.7695\n",
            "Epoch [90/150], Loss: 11970.9062\n",
            "Epoch [100/150], Loss: 18425.1177\n",
            "Epoch [110/150], Loss: 11674.0903\n",
            "Epoch [120/150], Loss: 10925.2797\n",
            "Epoch [130/150], Loss: 9580.0004\n",
            "Epoch [140/150], Loss: 12390.1100\n",
            "Epoch [150/150], Loss: 10618.1112\n",
            "Fold 5, RMSE: 48.73524856567383\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 69.79381866455078\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 172380.6133\n",
            "Epoch [20/100], Loss: 30603.0469\n",
            "Epoch [30/100], Loss: 18552.6177\n",
            "Epoch [40/100], Loss: 19480.9827\n",
            "Epoch [50/100], Loss: 21805.0659\n",
            "Epoch [60/100], Loss: 13567.0754\n",
            "Epoch [70/100], Loss: 21554.9597\n",
            "Epoch [80/100], Loss: 17623.8142\n",
            "Epoch [90/100], Loss: 13163.9531\n",
            "Epoch [100/100], Loss: 12637.7124\n",
            "Fold 1, RMSE: 57.582340240478516\n",
            "Epoch [10/100], Loss: 212652.0098\n",
            "Epoch [20/100], Loss: 24425.9053\n",
            "Epoch [30/100], Loss: 13910.8368\n",
            "Epoch [40/100], Loss: 15609.5999\n",
            "Epoch [50/100], Loss: 9668.9900\n",
            "Epoch [60/100], Loss: 17019.8782\n",
            "Epoch [70/100], Loss: 11145.4395\n",
            "Epoch [80/100], Loss: 10029.5654\n",
            "Epoch [90/100], Loss: 11993.3795\n",
            "Epoch [100/100], Loss: 14213.4087\n",
            "Fold 2, RMSE: 78.41616821289062\n",
            "Epoch [10/100], Loss: 14800.6111\n",
            "Epoch [20/100], Loss: 11529.1941\n",
            "Epoch [30/100], Loss: 12948.6595\n",
            "Epoch [40/100], Loss: 15429.1006\n",
            "Epoch [50/100], Loss: 16432.4967\n",
            "Epoch [60/100], Loss: 10009.5950\n",
            "Epoch [70/100], Loss: 10382.2532\n",
            "Epoch [80/100], Loss: 7110.6030\n",
            "Epoch [90/100], Loss: 12540.0264\n",
            "Epoch [100/100], Loss: 8846.6164\n",
            "Fold 3, RMSE: 102.76482391357422\n",
            "Epoch [10/100], Loss: 23047.6880\n",
            "Epoch [20/100], Loss: 18201.9553\n",
            "Epoch [30/100], Loss: 18095.7563\n",
            "Epoch [40/100], Loss: 28217.1577\n",
            "Epoch [50/100], Loss: 19569.7056\n",
            "Epoch [60/100], Loss: 24873.4287\n",
            "Epoch [70/100], Loss: 23432.3423\n",
            "Epoch [80/100], Loss: 29961.7747\n",
            "Epoch [90/100], Loss: 19694.8271\n",
            "Epoch [100/100], Loss: 17929.5571\n",
            "Fold 4, RMSE: 53.62269973754883\n",
            "Epoch [10/100], Loss: 19701.0000\n",
            "Epoch [20/100], Loss: 14807.9510\n",
            "Epoch [30/100], Loss: 14193.6006\n",
            "Epoch [40/100], Loss: 24478.3604\n",
            "Epoch [50/100], Loss: 18097.0471\n",
            "Epoch [60/100], Loss: 14717.6667\n",
            "Epoch [70/100], Loss: 15039.3425\n",
            "Epoch [80/100], Loss: 14026.3667\n",
            "Epoch [90/100], Loss: 13672.9263\n",
            "Epoch [100/100], Loss: 12266.5089\n",
            "Fold 5, RMSE: 49.324302673339844\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 68.34206695556641\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 23603.5374\n",
            "Epoch [20/150], Loss: 19242.0171\n",
            "Epoch [30/150], Loss: 18325.8677\n",
            "Epoch [40/150], Loss: 21581.9202\n",
            "Epoch [50/150], Loss: 24618.3008\n",
            "Epoch [60/150], Loss: 7569.3723\n",
            "Epoch [70/150], Loss: 11369.5242\n",
            "Epoch [80/150], Loss: 14658.0994\n",
            "Epoch [90/150], Loss: 13944.8594\n",
            "Epoch [100/150], Loss: 10842.0259\n",
            "Epoch [110/150], Loss: 7163.7819\n",
            "Epoch [120/150], Loss: 9186.4483\n",
            "Epoch [130/150], Loss: 12273.3616\n",
            "Epoch [140/150], Loss: 15004.9009\n",
            "Epoch [150/150], Loss: 19051.1909\n",
            "Fold 1, RMSE: 58.199501037597656\n",
            "Epoch [10/150], Loss: 13243.1569\n",
            "Epoch [20/150], Loss: 14843.8556\n",
            "Epoch [30/150], Loss: 16576.8647\n",
            "Epoch [40/150], Loss: 20287.8003\n",
            "Epoch [50/150], Loss: 16632.6707\n",
            "Epoch [60/150], Loss: 16193.6880\n",
            "Epoch [70/150], Loss: 16460.0737\n",
            "Epoch [80/150], Loss: 14854.3169\n",
            "Epoch [90/150], Loss: 10316.3124\n",
            "Epoch [100/150], Loss: 11941.1199\n",
            "Epoch [110/150], Loss: 14180.7529\n",
            "Epoch [120/150], Loss: 11542.1906\n",
            "Epoch [130/150], Loss: 10287.6602\n",
            "Epoch [140/150], Loss: 13251.0636\n",
            "Epoch [150/150], Loss: 10025.4387\n",
            "Fold 2, RMSE: 66.2722396850586\n",
            "Epoch [10/150], Loss: 21746.5132\n",
            "Epoch [20/150], Loss: 11040.8218\n",
            "Epoch [30/150], Loss: 10034.1509\n",
            "Epoch [40/150], Loss: 8935.0650\n",
            "Epoch [50/150], Loss: 9821.8219\n",
            "Epoch [60/150], Loss: 11936.8525\n",
            "Epoch [70/150], Loss: 9269.1586\n",
            "Epoch [80/150], Loss: 7406.4854\n",
            "Epoch [90/150], Loss: 9168.9641\n",
            "Epoch [100/150], Loss: 10306.2966\n",
            "Epoch [110/150], Loss: 9654.4495\n",
            "Epoch [120/150], Loss: 7484.2949\n",
            "Epoch [130/150], Loss: 6229.8848\n",
            "Epoch [140/150], Loss: 6234.5023\n",
            "Epoch [150/150], Loss: 9081.0564\n",
            "Fold 3, RMSE: 99.81763458251953\n",
            "Epoch [10/150], Loss: 110274.5664\n",
            "Epoch [20/150], Loss: 19230.7837\n",
            "Epoch [30/150], Loss: 33378.0059\n",
            "Epoch [40/150], Loss: 21897.6260\n",
            "Epoch [50/150], Loss: 17065.3569\n",
            "Epoch [60/150], Loss: 14114.2206\n",
            "Epoch [70/150], Loss: 13078.1705\n",
            "Epoch [80/150], Loss: 15123.9194\n",
            "Epoch [90/150], Loss: 18982.6833\n",
            "Epoch [100/150], Loss: 13807.5012\n",
            "Epoch [110/150], Loss: 9831.2952\n",
            "Epoch [120/150], Loss: 13444.5303\n",
            "Epoch [130/150], Loss: 9739.2875\n",
            "Epoch [140/150], Loss: 10895.4500\n",
            "Epoch [150/150], Loss: 11114.3401\n",
            "Fold 4, RMSE: 40.77293395996094\n",
            "Epoch [10/150], Loss: 23599.0173\n",
            "Epoch [20/150], Loss: 22961.5444\n",
            "Epoch [30/150], Loss: 29353.6021\n",
            "Epoch [40/150], Loss: 24457.2114\n",
            "Epoch [50/150], Loss: 18508.2266\n",
            "Epoch [60/150], Loss: 20384.3457\n",
            "Epoch [70/150], Loss: 19152.2173\n",
            "Epoch [80/150], Loss: 18320.2539\n",
            "Epoch [90/150], Loss: 17908.6990\n",
            "Epoch [100/150], Loss: 23321.8899\n",
            "Epoch [110/150], Loss: 23654.2346\n",
            "Epoch [120/150], Loss: 17744.1250\n",
            "Epoch [130/150], Loss: 15355.7268\n",
            "Epoch [140/150], Loss: 15159.5967\n",
            "Epoch [150/150], Loss: 19145.7983\n",
            "Fold 5, RMSE: 55.368831634521484\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 64.08622817993164\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 24356.8286\n",
            "Epoch [20/100], Loss: 18297.8391\n",
            "Epoch [30/100], Loss: 16353.9932\n",
            "Epoch [40/100], Loss: 18169.6116\n",
            "Epoch [50/100], Loss: 14315.8199\n",
            "Epoch [60/100], Loss: 13175.3723\n",
            "Epoch [70/100], Loss: 15285.7788\n",
            "Epoch [80/100], Loss: 21322.6116\n",
            "Epoch [90/100], Loss: 12211.5811\n",
            "Epoch [100/100], Loss: 12707.0725\n",
            "Fold 1, RMSE: 65.56391906738281\n",
            "Epoch [10/100], Loss: 51060.9160\n",
            "Epoch [20/100], Loss: 14443.9760\n",
            "Epoch [30/100], Loss: 21903.0137\n",
            "Epoch [40/100], Loss: 17791.0068\n",
            "Epoch [50/100], Loss: 23969.8911\n",
            "Epoch [60/100], Loss: 16611.8872\n",
            "Epoch [70/100], Loss: 16645.9980\n",
            "Epoch [80/100], Loss: 14817.2073\n",
            "Epoch [90/100], Loss: 17511.3252\n",
            "Epoch [100/100], Loss: 23104.6997\n",
            "Fold 2, RMSE: 87.45013427734375\n",
            "Epoch [10/100], Loss: 12024.5012\n",
            "Epoch [20/100], Loss: 18259.3071\n",
            "Epoch [30/100], Loss: 16573.5073\n",
            "Epoch [40/100], Loss: 12823.2083\n",
            "Epoch [50/100], Loss: 12146.7498\n",
            "Epoch [60/100], Loss: 12420.8777\n",
            "Epoch [70/100], Loss: 14183.3970\n",
            "Epoch [80/100], Loss: 13396.5862\n",
            "Epoch [90/100], Loss: 14325.1812\n",
            "Epoch [100/100], Loss: 13066.9023\n",
            "Fold 3, RMSE: 109.8624496459961\n",
            "Epoch [10/100], Loss: 212945.3281\n",
            "Epoch [20/100], Loss: 45285.0742\n",
            "Epoch [30/100], Loss: 28432.6941\n",
            "Epoch [40/100], Loss: 23714.7021\n",
            "Epoch [50/100], Loss: 31616.9839\n",
            "Epoch [60/100], Loss: 20675.4502\n",
            "Epoch [70/100], Loss: 22912.6584\n",
            "Epoch [80/100], Loss: 17326.3784\n",
            "Epoch [90/100], Loss: 16291.7075\n",
            "Epoch [100/100], Loss: 19307.8906\n",
            "Fold 4, RMSE: 41.32361602783203\n",
            "Epoch [10/100], Loss: 92108.4863\n",
            "Epoch [20/100], Loss: 20335.6108\n",
            "Epoch [30/100], Loss: 20558.2812\n",
            "Epoch [40/100], Loss: 11934.0352\n",
            "Epoch [50/100], Loss: 15652.5554\n",
            "Epoch [60/100], Loss: 21819.5381\n",
            "Epoch [70/100], Loss: 20336.4644\n",
            "Epoch [80/100], Loss: 16559.3774\n",
            "Epoch [90/100], Loss: 19768.9836\n",
            "Epoch [100/100], Loss: 17732.2954\n",
            "Fold 5, RMSE: 48.63254165649414\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 70.56653213500977\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 29341.6733\n",
            "Epoch [20/150], Loss: 28284.6816\n",
            "Epoch [30/150], Loss: 17756.0981\n",
            "Epoch [40/150], Loss: 22024.9812\n",
            "Epoch [50/150], Loss: 17767.2896\n",
            "Epoch [60/150], Loss: 16811.6060\n",
            "Epoch [70/150], Loss: 16260.2314\n",
            "Epoch [80/150], Loss: 26437.5640\n",
            "Epoch [90/150], Loss: 12913.7880\n",
            "Epoch [100/150], Loss: 17299.2227\n",
            "Epoch [110/150], Loss: 16932.7939\n",
            "Epoch [120/150], Loss: 15178.7048\n",
            "Epoch [130/150], Loss: 16147.2539\n",
            "Epoch [140/150], Loss: 23674.4065\n",
            "Epoch [150/150], Loss: 15903.0972\n",
            "Fold 1, RMSE: 64.91014862060547\n",
            "Epoch [10/150], Loss: 1208830.7188\n",
            "Epoch [20/150], Loss: 58375.6162\n",
            "Epoch [30/150], Loss: 29772.7163\n",
            "Epoch [40/150], Loss: 27611.4946\n",
            "Epoch [50/150], Loss: 21027.6904\n",
            "Epoch [60/150], Loss: 18863.3545\n",
            "Epoch [70/150], Loss: 15276.4404\n",
            "Epoch [80/150], Loss: 15148.8638\n",
            "Epoch [90/150], Loss: 21604.5955\n",
            "Epoch [100/150], Loss: 15531.3508\n",
            "Epoch [110/150], Loss: 12433.9837\n",
            "Epoch [120/150], Loss: 14340.4019\n",
            "Epoch [130/150], Loss: 18854.3735\n",
            "Epoch [140/150], Loss: 13903.8765\n",
            "Epoch [150/150], Loss: 13578.0371\n",
            "Fold 2, RMSE: 78.86534881591797\n",
            "Epoch [10/150], Loss: 434297.4453\n",
            "Epoch [20/150], Loss: 17619.0630\n",
            "Epoch [30/150], Loss: 17298.6169\n",
            "Epoch [40/150], Loss: 12755.2759\n",
            "Epoch [50/150], Loss: 15864.5356\n",
            "Epoch [60/150], Loss: 11894.5338\n",
            "Epoch [70/150], Loss: 20938.3857\n",
            "Epoch [80/150], Loss: 18332.8857\n",
            "Epoch [90/150], Loss: 13384.5955\n",
            "Epoch [100/150], Loss: 17555.9214\n",
            "Epoch [110/150], Loss: 11039.4158\n",
            "Epoch [120/150], Loss: 11476.4985\n",
            "Epoch [130/150], Loss: 11324.1042\n",
            "Epoch [140/150], Loss: 8617.2523\n",
            "Epoch [150/150], Loss: 11251.5974\n",
            "Fold 3, RMSE: 102.50888061523438\n",
            "Epoch [10/150], Loss: 228309.7266\n",
            "Epoch [20/150], Loss: 16324.8269\n",
            "Epoch [30/150], Loss: 17368.8486\n",
            "Epoch [40/150], Loss: 21008.1702\n",
            "Epoch [50/150], Loss: 20666.4595\n",
            "Epoch [60/150], Loss: 23649.0471\n",
            "Epoch [70/150], Loss: 19227.5103\n",
            "Epoch [80/150], Loss: 16699.3257\n",
            "Epoch [90/150], Loss: 19275.0923\n",
            "Epoch [100/150], Loss: 14932.0278\n",
            "Epoch [110/150], Loss: 21692.1389\n",
            "Epoch [120/150], Loss: 11364.6333\n",
            "Epoch [130/150], Loss: 14929.2056\n",
            "Epoch [140/150], Loss: 14427.8743\n",
            "Epoch [150/150], Loss: 10579.6386\n",
            "Fold 4, RMSE: 41.496097564697266\n",
            "Epoch [10/150], Loss: 653148.5312\n",
            "Epoch [20/150], Loss: 30651.8384\n",
            "Epoch [30/150], Loss: 34088.0884\n",
            "Epoch [40/150], Loss: 21614.3650\n",
            "Epoch [50/150], Loss: 13733.3396\n",
            "Epoch [60/150], Loss: 20206.2446\n",
            "Epoch [70/150], Loss: 17524.0039\n",
            "Epoch [80/150], Loss: 16401.1748\n",
            "Epoch [90/150], Loss: 14245.3088\n",
            "Epoch [100/150], Loss: 16709.7881\n",
            "Epoch [110/150], Loss: 17306.6096\n",
            "Epoch [120/150], Loss: 13832.8025\n",
            "Epoch [130/150], Loss: 11177.2867\n",
            "Epoch [140/150], Loss: 13528.3591\n",
            "Epoch [150/150], Loss: 14519.3936\n",
            "Fold 5, RMSE: 48.21136474609375\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 67.19836807250977\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 216905.6719\n",
            "Epoch [20/100], Loss: 116085.2441\n",
            "Epoch [30/100], Loss: 30517.4141\n",
            "Epoch [40/100], Loss: 24976.3950\n",
            "Epoch [50/100], Loss: 30064.1846\n",
            "Epoch [60/100], Loss: 19774.9097\n",
            "Epoch [70/100], Loss: 19387.7673\n",
            "Epoch [80/100], Loss: 21386.3423\n",
            "Epoch [90/100], Loss: 18136.0706\n",
            "Epoch [100/100], Loss: 19651.9438\n",
            "Fold 1, RMSE: 59.543846130371094\n",
            "Epoch [10/100], Loss: 90388.2031\n",
            "Epoch [20/100], Loss: 26460.7949\n",
            "Epoch [30/100], Loss: 16501.8174\n",
            "Epoch [40/100], Loss: 25127.3826\n",
            "Epoch [50/100], Loss: 14948.5857\n",
            "Epoch [60/100], Loss: 12720.3237\n",
            "Epoch [70/100], Loss: 12670.8701\n",
            "Epoch [80/100], Loss: 12305.8640\n",
            "Epoch [90/100], Loss: 11123.2784\n",
            "Epoch [100/100], Loss: 17661.1301\n",
            "Fold 2, RMSE: 79.98542022705078\n",
            "Epoch [10/100], Loss: 153064.4297\n",
            "Epoch [20/100], Loss: 41483.8604\n",
            "Epoch [30/100], Loss: 21133.9277\n",
            "Epoch [40/100], Loss: 18024.4150\n",
            "Epoch [50/100], Loss: 14398.9128\n",
            "Epoch [60/100], Loss: 15887.6873\n",
            "Epoch [70/100], Loss: 14650.6650\n",
            "Epoch [80/100], Loss: 13610.9600\n",
            "Epoch [90/100], Loss: 9389.2593\n",
            "Epoch [100/100], Loss: 10149.3679\n",
            "Fold 3, RMSE: 105.0810546875\n",
            "Epoch [10/100], Loss: 18538.8680\n",
            "Epoch [20/100], Loss: 19409.1172\n",
            "Epoch [30/100], Loss: 23778.6621\n",
            "Epoch [40/100], Loss: 22728.5684\n",
            "Epoch [50/100], Loss: 22328.3379\n",
            "Epoch [60/100], Loss: 16408.4211\n",
            "Epoch [70/100], Loss: 19142.3447\n",
            "Epoch [80/100], Loss: 13593.1453\n",
            "Epoch [90/100], Loss: 11301.9790\n",
            "Epoch [100/100], Loss: 15622.7593\n",
            "Fold 4, RMSE: 52.63071823120117\n",
            "Epoch [10/100], Loss: 18284.1719\n",
            "Epoch [20/100], Loss: 25295.5249\n",
            "Epoch [30/100], Loss: 17336.7350\n",
            "Epoch [40/100], Loss: 20846.9014\n",
            "Epoch [50/100], Loss: 26911.6350\n",
            "Epoch [60/100], Loss: 21233.9424\n",
            "Epoch [70/100], Loss: 20524.6621\n",
            "Epoch [80/100], Loss: 17753.8662\n",
            "Epoch [90/100], Loss: 15429.6978\n",
            "Epoch [100/100], Loss: 23043.7949\n",
            "Fold 5, RMSE: 54.3588981628418\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 70.31998748779297\n",
            "Training with neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 32860.4688\n",
            "Epoch [20/150], Loss: 18434.7695\n",
            "Epoch [30/150], Loss: 34371.9160\n",
            "Epoch [40/150], Loss: 17434.5493\n",
            "Epoch [50/150], Loss: 15988.5452\n",
            "Epoch [60/150], Loss: 13704.8557\n",
            "Epoch [70/150], Loss: 15928.5166\n",
            "Epoch [80/150], Loss: 20784.1621\n",
            "Epoch [90/150], Loss: 16844.1526\n",
            "Epoch [100/150], Loss: 13412.5183\n",
            "Epoch [110/150], Loss: 14285.7566\n",
            "Epoch [120/150], Loss: 11683.5195\n",
            "Epoch [130/150], Loss: 13962.0105\n",
            "Epoch [140/150], Loss: 12196.2791\n",
            "Epoch [150/150], Loss: 9812.4851\n",
            "Fold 1, RMSE: 55.72542953491211\n",
            "Epoch [10/150], Loss: 24785.6648\n",
            "Epoch [20/150], Loss: 18810.9988\n",
            "Epoch [30/150], Loss: 18493.2549\n",
            "Epoch [40/150], Loss: 20353.2827\n",
            "Epoch [50/150], Loss: 23546.1965\n",
            "Epoch [60/150], Loss: 16809.3455\n",
            "Epoch [70/150], Loss: 15370.7192\n",
            "Epoch [80/150], Loss: 13689.5892\n",
            "Epoch [90/150], Loss: 14298.6727\n",
            "Epoch [100/150], Loss: 16756.3501\n",
            "Epoch [110/150], Loss: 16520.3125\n",
            "Epoch [120/150], Loss: 18195.8340\n",
            "Epoch [130/150], Loss: 12801.9295\n",
            "Epoch [140/150], Loss: 15548.3423\n",
            "Epoch [150/150], Loss: 12003.9973\n",
            "Fold 2, RMSE: 81.94325256347656\n",
            "Epoch [10/150], Loss: 38466.1826\n",
            "Epoch [20/150], Loss: 10853.3435\n",
            "Epoch [30/150], Loss: 9496.7625\n",
            "Epoch [40/150], Loss: 14025.0874\n",
            "Epoch [50/150], Loss: 9474.7246\n",
            "Epoch [60/150], Loss: 17743.3040\n",
            "Epoch [70/150], Loss: 11729.3638\n",
            "Epoch [80/150], Loss: 8949.0620\n",
            "Epoch [90/150], Loss: 11971.4595\n",
            "Epoch [100/150], Loss: 8267.7811\n",
            "Epoch [110/150], Loss: 9471.1394\n",
            "Epoch [120/150], Loss: 8729.8127\n",
            "Epoch [130/150], Loss: 7283.0005\n",
            "Epoch [140/150], Loss: 5831.6339\n",
            "Epoch [150/150], Loss: 10913.3940\n",
            "Fold 3, RMSE: 108.4536361694336\n",
            "Epoch [10/150], Loss: 137002.7070\n",
            "Epoch [20/150], Loss: 23145.6694\n",
            "Epoch [30/150], Loss: 21683.4614\n",
            "Epoch [40/150], Loss: 15249.3286\n",
            "Epoch [50/150], Loss: 15347.7153\n",
            "Epoch [60/150], Loss: 15786.1118\n",
            "Epoch [70/150], Loss: 20932.9653\n",
            "Epoch [80/150], Loss: 17676.2720\n",
            "Epoch [90/150], Loss: 15319.2146\n",
            "Epoch [100/150], Loss: 14875.1226\n",
            "Epoch [110/150], Loss: 17088.1311\n",
            "Epoch [120/150], Loss: 13562.0374\n",
            "Epoch [130/150], Loss: 12607.5361\n",
            "Epoch [140/150], Loss: 14013.9497\n",
            "Epoch [150/150], Loss: 12175.2295\n",
            "Fold 4, RMSE: 42.444122314453125\n",
            "Epoch [10/150], Loss: 22656.8364\n",
            "Epoch [20/150], Loss: 25129.4634\n",
            "Epoch [30/150], Loss: 19857.8618\n",
            "Epoch [40/150], Loss: 17426.4644\n",
            "Epoch [50/150], Loss: 22200.3186\n",
            "Epoch [60/150], Loss: 19955.7881\n",
            "Epoch [70/150], Loss: 17029.0566\n",
            "Epoch [80/150], Loss: 18314.0811\n",
            "Epoch [90/150], Loss: 21843.0115\n",
            "Epoch [100/150], Loss: 21763.1401\n",
            "Epoch [110/150], Loss: 16527.0420\n",
            "Epoch [120/150], Loss: 15002.5181\n",
            "Epoch [130/150], Loss: 12452.7073\n",
            "Epoch [140/150], Loss: 17513.2764\n",
            "Epoch [150/150], Loss: 10897.8811\n",
            "Fold 5, RMSE: 47.48611831665039\n",
            "Avg RMSE for neurons=80, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 67.21051177978515\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14750.1670\n",
            "Epoch [20/100], Loss: 9911.6846\n",
            "Epoch [30/100], Loss: 8437.3539\n",
            "Epoch [40/100], Loss: 10103.1396\n",
            "Epoch [50/100], Loss: 3957.2388\n",
            "Epoch [60/100], Loss: 1504.7160\n",
            "Epoch [70/100], Loss: 2171.1013\n",
            "Epoch [80/100], Loss: 1857.2245\n",
            "Epoch [90/100], Loss: 1739.7250\n",
            "Epoch [100/100], Loss: 1880.0592\n",
            "Fold 1, RMSE: 76.35640716552734\n",
            "Epoch [10/100], Loss: 21241.5195\n",
            "Epoch [20/100], Loss: 15950.7850\n",
            "Epoch [30/100], Loss: 10420.7231\n",
            "Epoch [40/100], Loss: 4960.7026\n",
            "Epoch [50/100], Loss: 6209.6799\n",
            "Epoch [60/100], Loss: 1407.2452\n",
            "Epoch [70/100], Loss: 1705.0521\n",
            "Epoch [80/100], Loss: 2054.6838\n",
            "Epoch [90/100], Loss: 2574.7348\n",
            "Epoch [100/100], Loss: 2953.9165\n",
            "Fold 2, RMSE: 69.69891357421875\n",
            "Epoch [10/100], Loss: 21577.2222\n",
            "Epoch [20/100], Loss: 5696.6795\n",
            "Epoch [30/100], Loss: 4847.9598\n",
            "Epoch [40/100], Loss: 3150.0207\n",
            "Epoch [50/100], Loss: 4269.0826\n",
            "Epoch [60/100], Loss: 2078.0251\n",
            "Epoch [70/100], Loss: 2176.5973\n",
            "Epoch [80/100], Loss: 1422.9309\n",
            "Epoch [90/100], Loss: 1696.8606\n",
            "Epoch [100/100], Loss: 1241.4229\n",
            "Fold 3, RMSE: 92.92964172363281\n",
            "Epoch [10/100], Loss: 11775.2043\n",
            "Epoch [20/100], Loss: 17898.5627\n",
            "Epoch [30/100], Loss: 7901.6953\n",
            "Epoch [40/100], Loss: 8127.1494\n",
            "Epoch [50/100], Loss: 2946.1077\n",
            "Epoch [60/100], Loss: 3250.9896\n",
            "Epoch [70/100], Loss: 3694.2104\n",
            "Epoch [80/100], Loss: 2521.2978\n",
            "Epoch [90/100], Loss: 3388.6531\n",
            "Epoch [100/100], Loss: 2389.5433\n",
            "Fold 4, RMSE: 43.65713882446289\n",
            "Epoch [10/100], Loss: 12828.2593\n",
            "Epoch [20/100], Loss: 9837.7847\n",
            "Epoch [30/100], Loss: 4040.5341\n",
            "Epoch [40/100], Loss: 4073.4661\n",
            "Epoch [50/100], Loss: 3278.6433\n",
            "Epoch [60/100], Loss: 2874.6405\n",
            "Epoch [70/100], Loss: 4815.1274\n",
            "Epoch [80/100], Loss: 3078.1163\n",
            "Epoch [90/100], Loss: 2057.5722\n",
            "Epoch [100/100], Loss: 3313.2237\n",
            "Fold 5, RMSE: 45.79510498046875\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 65.68744125366212\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18594.9692\n",
            "Epoch [20/150], Loss: 7046.9359\n",
            "Epoch [30/150], Loss: 6899.8308\n",
            "Epoch [40/150], Loss: 3782.0109\n",
            "Epoch [50/150], Loss: 4142.7218\n",
            "Epoch [60/150], Loss: 3075.1664\n",
            "Epoch [70/150], Loss: 5737.9079\n",
            "Epoch [80/150], Loss: 1993.4918\n",
            "Epoch [90/150], Loss: 2343.7701\n",
            "Epoch [100/150], Loss: 2097.0871\n",
            "Epoch [110/150], Loss: 1812.8170\n",
            "Epoch [120/150], Loss: 2803.0991\n",
            "Epoch [130/150], Loss: 3163.1846\n",
            "Epoch [140/150], Loss: 3072.6801\n",
            "Epoch [150/150], Loss: 1791.6785\n",
            "Fold 1, RMSE: 59.16281509399414\n",
            "Epoch [10/150], Loss: 8446.9503\n",
            "Epoch [20/150], Loss: 8123.8486\n",
            "Epoch [30/150], Loss: 4811.0010\n",
            "Epoch [40/150], Loss: 3861.2705\n",
            "Epoch [50/150], Loss: 2327.2958\n",
            "Epoch [60/150], Loss: 1321.8043\n",
            "Epoch [70/150], Loss: 2583.3421\n",
            "Epoch [80/150], Loss: 2429.3755\n",
            "Epoch [90/150], Loss: 2678.4643\n",
            "Epoch [100/150], Loss: 3022.4355\n",
            "Epoch [110/150], Loss: 3335.7478\n",
            "Epoch [120/150], Loss: 1072.1953\n",
            "Epoch [130/150], Loss: 2147.6564\n",
            "Epoch [140/150], Loss: 633.4572\n",
            "Epoch [150/150], Loss: 1016.1736\n",
            "Fold 2, RMSE: 62.74968338012695\n",
            "Epoch [10/150], Loss: 10218.4908\n",
            "Epoch [20/150], Loss: 6558.3063\n",
            "Epoch [30/150], Loss: 4856.3057\n",
            "Epoch [40/150], Loss: 4735.8189\n",
            "Epoch [50/150], Loss: 2394.5766\n",
            "Epoch [60/150], Loss: 1794.3071\n",
            "Epoch [70/150], Loss: 2775.2887\n",
            "Epoch [80/150], Loss: 1923.2139\n",
            "Epoch [90/150], Loss: 1973.2652\n",
            "Epoch [100/150], Loss: 1271.6667\n",
            "Epoch [110/150], Loss: 2557.4727\n",
            "Epoch [120/150], Loss: 2789.6143\n",
            "Epoch [130/150], Loss: 2079.3559\n",
            "Epoch [140/150], Loss: 1157.0935\n",
            "Epoch [150/150], Loss: 2595.5576\n",
            "Fold 3, RMSE: 91.22544860839844\n",
            "Epoch [10/150], Loss: 18029.9946\n",
            "Epoch [20/150], Loss: 13757.6960\n",
            "Epoch [30/150], Loss: 7863.7028\n",
            "Epoch [40/150], Loss: 7469.7668\n",
            "Epoch [50/150], Loss: 7368.1624\n",
            "Epoch [60/150], Loss: 3763.4702\n",
            "Epoch [70/150], Loss: 2831.8044\n",
            "Epoch [80/150], Loss: 1551.0665\n",
            "Epoch [90/150], Loss: 1483.3110\n",
            "Epoch [100/150], Loss: 4469.8977\n",
            "Epoch [110/150], Loss: 2077.3984\n",
            "Epoch [120/150], Loss: 2233.4878\n",
            "Epoch [130/150], Loss: 1909.3298\n",
            "Epoch [140/150], Loss: 3746.5946\n",
            "Epoch [150/150], Loss: 1023.6601\n",
            "Fold 4, RMSE: 42.3752326965332\n",
            "Epoch [10/150], Loss: 15092.0127\n",
            "Epoch [20/150], Loss: 8743.5564\n",
            "Epoch [30/150], Loss: 6730.6287\n",
            "Epoch [40/150], Loss: 6700.1049\n",
            "Epoch [50/150], Loss: 3967.3838\n",
            "Epoch [60/150], Loss: 2097.3781\n",
            "Epoch [70/150], Loss: 1755.2293\n",
            "Epoch [80/150], Loss: 1943.5002\n",
            "Epoch [90/150], Loss: 1956.4846\n",
            "Epoch [100/150], Loss: 2600.6209\n",
            "Epoch [110/150], Loss: 3129.2794\n",
            "Epoch [120/150], Loss: 1914.2453\n",
            "Epoch [130/150], Loss: 2136.0695\n",
            "Epoch [140/150], Loss: 1941.8897\n",
            "Epoch [150/150], Loss: 1860.5640\n",
            "Fold 5, RMSE: 45.546539306640625\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 60.211943817138675\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 12035.5188\n",
            "Epoch [20/100], Loss: 8756.8296\n",
            "Epoch [30/100], Loss: 4402.1070\n",
            "Epoch [40/100], Loss: 2458.0053\n",
            "Epoch [50/100], Loss: 3707.0199\n",
            "Epoch [60/100], Loss: 1232.2347\n",
            "Epoch [70/100], Loss: 7870.7371\n",
            "Epoch [80/100], Loss: 5229.3594\n",
            "Epoch [90/100], Loss: 2902.7127\n",
            "Epoch [100/100], Loss: 967.8417\n",
            "Fold 1, RMSE: 58.83877944946289\n",
            "Epoch [10/100], Loss: 11627.8018\n",
            "Epoch [20/100], Loss: 7087.5818\n",
            "Epoch [30/100], Loss: 7231.5236\n",
            "Epoch [40/100], Loss: 2759.9771\n",
            "Epoch [50/100], Loss: 2758.5367\n",
            "Epoch [60/100], Loss: 1140.5151\n",
            "Epoch [70/100], Loss: 3006.7877\n",
            "Epoch [80/100], Loss: 3841.1898\n",
            "Epoch [90/100], Loss: 1969.1667\n",
            "Epoch [100/100], Loss: 847.1429\n",
            "Fold 2, RMSE: 64.80242156982422\n",
            "Epoch [10/100], Loss: 10140.8381\n",
            "Epoch [20/100], Loss: 6266.9680\n",
            "Epoch [30/100], Loss: 5404.3206\n",
            "Epoch [40/100], Loss: 3055.1890\n",
            "Epoch [50/100], Loss: 1515.4579\n",
            "Epoch [60/100], Loss: 2483.7260\n",
            "Epoch [70/100], Loss: 1267.7402\n",
            "Epoch [80/100], Loss: 796.2484\n",
            "Epoch [90/100], Loss: 2206.0674\n",
            "Epoch [100/100], Loss: 1897.3442\n",
            "Fold 3, RMSE: 90.18907165527344\n",
            "Epoch [10/100], Loss: 11316.8142\n",
            "Epoch [20/100], Loss: 7995.5264\n",
            "Epoch [30/100], Loss: 8850.5146\n",
            "Epoch [40/100], Loss: 1601.1213\n",
            "Epoch [50/100], Loss: 2435.5963\n",
            "Epoch [60/100], Loss: 1399.2839\n",
            "Epoch [70/100], Loss: 1191.8189\n",
            "Epoch [80/100], Loss: 1043.4406\n",
            "Epoch [90/100], Loss: 1163.1186\n",
            "Epoch [100/100], Loss: 1138.5469\n",
            "Fold 4, RMSE: 46.56863784790039\n",
            "Epoch [10/100], Loss: 12930.4683\n",
            "Epoch [20/100], Loss: 8371.0775\n",
            "Epoch [30/100], Loss: 4164.0575\n",
            "Epoch [40/100], Loss: 7066.6885\n",
            "Epoch [50/100], Loss: 1965.0363\n",
            "Epoch [60/100], Loss: 5467.5626\n",
            "Epoch [70/100], Loss: 2270.0054\n",
            "Epoch [80/100], Loss: 2443.4531\n",
            "Epoch [90/100], Loss: 1364.9949\n",
            "Epoch [100/100], Loss: 807.9258\n",
            "Fold 5, RMSE: 46.366275787353516\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 61.35303726196289\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 7811.0720\n",
            "Epoch [20/150], Loss: 4767.3508\n",
            "Epoch [30/150], Loss: 2661.9525\n",
            "Epoch [40/150], Loss: 3021.3622\n",
            "Epoch [50/150], Loss: 2194.5680\n",
            "Epoch [60/150], Loss: 2910.9461\n",
            "Epoch [70/150], Loss: 1383.9903\n",
            "Epoch [80/150], Loss: 3557.1494\n",
            "Epoch [90/150], Loss: 2267.6470\n",
            "Epoch [100/150], Loss: 1659.5928\n",
            "Epoch [110/150], Loss: 1434.0652\n",
            "Epoch [120/150], Loss: 1685.1421\n",
            "Epoch [130/150], Loss: 1832.6044\n",
            "Epoch [140/150], Loss: 1740.9342\n",
            "Epoch [150/150], Loss: 697.8537\n",
            "Fold 1, RMSE: 60.54255676269531\n",
            "Epoch [10/150], Loss: 11147.3063\n",
            "Epoch [20/150], Loss: 9136.8890\n",
            "Epoch [30/150], Loss: 5107.6447\n",
            "Epoch [40/150], Loss: 4302.4061\n",
            "Epoch [50/150], Loss: 2819.5839\n",
            "Epoch [60/150], Loss: 2797.9308\n",
            "Epoch [70/150], Loss: 3181.3531\n",
            "Epoch [80/150], Loss: 1140.0642\n",
            "Epoch [90/150], Loss: 3528.7347\n",
            "Epoch [100/150], Loss: 988.9143\n",
            "Epoch [110/150], Loss: 973.7129\n",
            "Epoch [120/150], Loss: 1722.8175\n",
            "Epoch [130/150], Loss: 1097.7613\n",
            "Epoch [140/150], Loss: 790.5562\n",
            "Epoch [150/150], Loss: 2683.4611\n",
            "Fold 2, RMSE: 68.9913101196289\n",
            "Epoch [10/150], Loss: 7626.5486\n",
            "Epoch [20/150], Loss: 5745.6807\n",
            "Epoch [30/150], Loss: 3835.4865\n",
            "Epoch [40/150], Loss: 3098.9024\n",
            "Epoch [50/150], Loss: 1336.9094\n",
            "Epoch [60/150], Loss: 2923.0335\n",
            "Epoch [70/150], Loss: 3158.3103\n",
            "Epoch [80/150], Loss: 2918.1620\n",
            "Epoch [90/150], Loss: 939.8139\n",
            "Epoch [100/150], Loss: 1045.2283\n",
            "Epoch [110/150], Loss: 1123.5101\n",
            "Epoch [120/150], Loss: 753.3940\n",
            "Epoch [130/150], Loss: 3429.3372\n",
            "Epoch [140/150], Loss: 3071.2615\n",
            "Epoch [150/150], Loss: 614.2926\n",
            "Fold 3, RMSE: 94.4260482788086\n",
            "Epoch [10/150], Loss: 11786.2749\n",
            "Epoch [20/150], Loss: 10633.4543\n",
            "Epoch [30/150], Loss: 5393.5009\n",
            "Epoch [40/150], Loss: 2818.7039\n",
            "Epoch [50/150], Loss: 2791.0883\n",
            "Epoch [60/150], Loss: 1624.0339\n",
            "Epoch [70/150], Loss: 3007.7205\n",
            "Epoch [80/150], Loss: 1296.3609\n",
            "Epoch [90/150], Loss: 700.8801\n",
            "Epoch [100/150], Loss: 613.8734\n",
            "Epoch [110/150], Loss: 1750.5252\n",
            "Epoch [120/150], Loss: 1400.4361\n",
            "Epoch [130/150], Loss: 914.6854\n",
            "Epoch [140/150], Loss: 938.7214\n",
            "Epoch [150/150], Loss: 727.9175\n",
            "Fold 4, RMSE: 43.86867904663086\n",
            "Epoch [10/150], Loss: 11812.0884\n",
            "Epoch [20/150], Loss: 7861.6709\n",
            "Epoch [30/150], Loss: 6662.3341\n",
            "Epoch [40/150], Loss: 2090.0520\n",
            "Epoch [50/150], Loss: 2535.1517\n",
            "Epoch [60/150], Loss: 2512.2384\n",
            "Epoch [70/150], Loss: 1979.4347\n",
            "Epoch [80/150], Loss: 1838.6838\n",
            "Epoch [90/150], Loss: 2220.7265\n",
            "Epoch [100/150], Loss: 1366.6297\n",
            "Epoch [110/150], Loss: 1253.7587\n",
            "Epoch [120/150], Loss: 1762.2934\n",
            "Epoch [130/150], Loss: 630.9389\n",
            "Epoch [140/150], Loss: 1090.0891\n",
            "Epoch [150/150], Loss: 579.9078\n",
            "Fold 5, RMSE: 45.79732894897461\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 62.72518463134766\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 10862.3828\n",
            "Epoch [20/100], Loss: 9982.1067\n",
            "Epoch [30/100], Loss: 3526.5491\n",
            "Epoch [40/100], Loss: 2701.9541\n",
            "Epoch [50/100], Loss: 3064.0220\n",
            "Epoch [60/100], Loss: 2581.6781\n",
            "Epoch [70/100], Loss: 2224.6748\n",
            "Epoch [80/100], Loss: 3810.2803\n",
            "Epoch [90/100], Loss: 2092.8304\n",
            "Epoch [100/100], Loss: 2300.7573\n",
            "Fold 1, RMSE: 59.00905990600586\n",
            "Epoch [10/100], Loss: 32065.8267\n",
            "Epoch [20/100], Loss: 14597.6787\n",
            "Epoch [30/100], Loss: 10521.2468\n",
            "Epoch [40/100], Loss: 5948.0183\n",
            "Epoch [50/100], Loss: 3607.9733\n",
            "Epoch [60/100], Loss: 4672.0940\n",
            "Epoch [70/100], Loss: 5120.3878\n",
            "Epoch [80/100], Loss: 1097.0526\n",
            "Epoch [90/100], Loss: 2505.9626\n",
            "Epoch [100/100], Loss: 2544.4791\n",
            "Fold 2, RMSE: 64.75344848632812\n",
            "Epoch [10/100], Loss: 16311.9648\n",
            "Epoch [20/100], Loss: 8174.5505\n",
            "Epoch [30/100], Loss: 3801.5587\n",
            "Epoch [40/100], Loss: 2788.8939\n",
            "Epoch [50/100], Loss: 2476.1186\n",
            "Epoch [60/100], Loss: 2203.8523\n",
            "Epoch [70/100], Loss: 2278.3203\n",
            "Epoch [80/100], Loss: 1873.4396\n",
            "Epoch [90/100], Loss: 3106.6403\n",
            "Epoch [100/100], Loss: 1820.9485\n",
            "Fold 3, RMSE: 89.63787841796875\n",
            "Epoch [10/100], Loss: 13434.8180\n",
            "Epoch [20/100], Loss: 7940.4073\n",
            "Epoch [30/100], Loss: 6116.8906\n",
            "Epoch [40/100], Loss: 3810.3682\n",
            "Epoch [50/100], Loss: 2060.9850\n",
            "Epoch [60/100], Loss: 1217.5554\n",
            "Epoch [70/100], Loss: 5106.3123\n",
            "Epoch [80/100], Loss: 3306.4004\n",
            "Epoch [90/100], Loss: 3206.8250\n",
            "Epoch [100/100], Loss: 4034.4406\n",
            "Fold 4, RMSE: 42.2379150390625\n",
            "Epoch [10/100], Loss: 25234.7612\n",
            "Epoch [20/100], Loss: 11767.9541\n",
            "Epoch [30/100], Loss: 12980.2324\n",
            "Epoch [40/100], Loss: 5453.6161\n",
            "Epoch [50/100], Loss: 5740.9158\n",
            "Epoch [60/100], Loss: 3374.1841\n",
            "Epoch [70/100], Loss: 4049.0696\n",
            "Epoch [80/100], Loss: 2344.7876\n",
            "Epoch [90/100], Loss: 1192.3081\n",
            "Epoch [100/100], Loss: 2369.5728\n",
            "Fold 5, RMSE: 48.905662536621094\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 60.908792877197264\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 13429.1007\n",
            "Epoch [20/150], Loss: 7647.2488\n",
            "Epoch [30/150], Loss: 7398.7147\n",
            "Epoch [40/150], Loss: 3615.1915\n",
            "Epoch [50/150], Loss: 1900.0898\n",
            "Epoch [60/150], Loss: 2348.5244\n",
            "Epoch [70/150], Loss: 4211.1240\n",
            "Epoch [80/150], Loss: 1472.4520\n",
            "Epoch [90/150], Loss: 1812.2507\n",
            "Epoch [100/150], Loss: 1228.4957\n",
            "Epoch [110/150], Loss: 2247.5096\n",
            "Epoch [120/150], Loss: 1508.4638\n",
            "Epoch [130/150], Loss: 1302.9933\n",
            "Epoch [140/150], Loss: 2493.6012\n",
            "Epoch [150/150], Loss: 1502.5020\n",
            "Fold 1, RMSE: 60.48146057128906\n",
            "Epoch [10/150], Loss: 28232.9004\n",
            "Epoch [20/150], Loss: 11475.4807\n",
            "Epoch [30/150], Loss: 6558.9740\n",
            "Epoch [40/150], Loss: 4737.1619\n",
            "Epoch [50/150], Loss: 2683.7982\n",
            "Epoch [60/150], Loss: 4565.3311\n",
            "Epoch [70/150], Loss: 2614.1758\n",
            "Epoch [80/150], Loss: 1785.4087\n",
            "Epoch [90/150], Loss: 1688.0065\n",
            "Epoch [100/150], Loss: 2884.7801\n",
            "Epoch [110/150], Loss: 1408.2966\n",
            "Epoch [120/150], Loss: 2326.8975\n",
            "Epoch [130/150], Loss: 2127.4442\n",
            "Epoch [140/150], Loss: 2047.3878\n",
            "Epoch [150/150], Loss: 940.3595\n",
            "Fold 2, RMSE: 64.53863525390625\n",
            "Epoch [10/150], Loss: 16940.8086\n",
            "Epoch [20/150], Loss: 5426.5977\n",
            "Epoch [30/150], Loss: 5164.8206\n",
            "Epoch [40/150], Loss: 5753.6719\n",
            "Epoch [50/150], Loss: 3366.8737\n",
            "Epoch [60/150], Loss: 1867.8985\n",
            "Epoch [70/150], Loss: 2615.5204\n",
            "Epoch [80/150], Loss: 4456.3251\n",
            "Epoch [90/150], Loss: 1535.4010\n",
            "Epoch [100/150], Loss: 994.6634\n",
            "Epoch [110/150], Loss: 2004.7691\n",
            "Epoch [120/150], Loss: 1670.9833\n",
            "Epoch [130/150], Loss: 1174.0118\n",
            "Epoch [140/150], Loss: 2040.8801\n",
            "Epoch [150/150], Loss: 1039.5635\n",
            "Fold 3, RMSE: 98.72377014160156\n",
            "Epoch [10/150], Loss: 20512.2026\n",
            "Epoch [20/150], Loss: 7858.4823\n",
            "Epoch [30/150], Loss: 7497.2341\n",
            "Epoch [40/150], Loss: 4932.0682\n",
            "Epoch [50/150], Loss: 3684.9565\n",
            "Epoch [60/150], Loss: 3066.4202\n",
            "Epoch [70/150], Loss: 2234.3936\n",
            "Epoch [80/150], Loss: 2149.7592\n",
            "Epoch [90/150], Loss: 2847.4383\n",
            "Epoch [100/150], Loss: 2416.2415\n",
            "Epoch [110/150], Loss: 1915.6986\n",
            "Epoch [120/150], Loss: 1707.0543\n",
            "Epoch [130/150], Loss: 1502.6343\n",
            "Epoch [140/150], Loss: 2488.7555\n",
            "Epoch [150/150], Loss: 2842.3124\n",
            "Fold 4, RMSE: 39.495548248291016\n",
            "Epoch [10/150], Loss: 16154.1904\n",
            "Epoch [20/150], Loss: 13265.3928\n",
            "Epoch [30/150], Loss: 8112.1630\n",
            "Epoch [40/150], Loss: 8877.3778\n",
            "Epoch [50/150], Loss: 4175.7094\n",
            "Epoch [60/150], Loss: 6507.0867\n",
            "Epoch [70/150], Loss: 2443.3716\n",
            "Epoch [80/150], Loss: 4429.4799\n",
            "Epoch [90/150], Loss: 1883.3268\n",
            "Epoch [100/150], Loss: 2294.8426\n",
            "Epoch [110/150], Loss: 3925.2369\n",
            "Epoch [120/150], Loss: 1224.2291\n",
            "Epoch [130/150], Loss: 3458.1617\n",
            "Epoch [140/150], Loss: 3304.0185\n",
            "Epoch [150/150], Loss: 1291.6970\n",
            "Fold 5, RMSE: 45.084922790527344\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 61.66486740112305\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 7977.3386\n",
            "Epoch [20/100], Loss: 5248.2620\n",
            "Epoch [30/100], Loss: 5716.5109\n",
            "Epoch [40/100], Loss: 1973.8716\n",
            "Epoch [50/100], Loss: 1737.6735\n",
            "Epoch [60/100], Loss: 2667.0453\n",
            "Epoch [70/100], Loss: 1640.5745\n",
            "Epoch [80/100], Loss: 2376.7897\n",
            "Epoch [90/100], Loss: 1360.3417\n",
            "Epoch [100/100], Loss: 2510.5717\n",
            "Fold 1, RMSE: 66.03958129882812\n",
            "Epoch [10/100], Loss: 9925.8044\n",
            "Epoch [20/100], Loss: 6042.5842\n",
            "Epoch [30/100], Loss: 4647.4205\n",
            "Epoch [40/100], Loss: 4236.3506\n",
            "Epoch [50/100], Loss: 1969.1354\n",
            "Epoch [60/100], Loss: 1288.2730\n",
            "Epoch [70/100], Loss: 1007.5946\n",
            "Epoch [80/100], Loss: 1447.4231\n",
            "Epoch [90/100], Loss: 4166.6827\n",
            "Epoch [100/100], Loss: 2204.9903\n",
            "Fold 2, RMSE: 63.86513900756836\n",
            "Epoch [10/100], Loss: 7756.3672\n",
            "Epoch [20/100], Loss: 5468.1118\n",
            "Epoch [30/100], Loss: 3655.3771\n",
            "Epoch [40/100], Loss: 2241.0626\n",
            "Epoch [50/100], Loss: 5111.6544\n",
            "Epoch [60/100], Loss: 1974.7791\n",
            "Epoch [70/100], Loss: 891.3888\n",
            "Epoch [80/100], Loss: 989.0504\n",
            "Epoch [90/100], Loss: 1496.4725\n",
            "Epoch [100/100], Loss: 3230.8286\n",
            "Fold 3, RMSE: 94.45333862304688\n",
            "Epoch [10/100], Loss: 19305.2385\n",
            "Epoch [20/100], Loss: 11607.8423\n",
            "Epoch [30/100], Loss: 6765.7643\n",
            "Epoch [40/100], Loss: 4956.5123\n",
            "Epoch [50/100], Loss: 3831.7194\n",
            "Epoch [60/100], Loss: 2486.4418\n",
            "Epoch [70/100], Loss: 5473.6965\n",
            "Epoch [80/100], Loss: 12657.8995\n",
            "Epoch [90/100], Loss: 1164.3666\n",
            "Epoch [100/100], Loss: 1622.4608\n",
            "Fold 4, RMSE: 44.03163528442383\n",
            "Epoch [10/100], Loss: 14749.5586\n",
            "Epoch [20/100], Loss: 7219.8558\n",
            "Epoch [30/100], Loss: 8691.2665\n",
            "Epoch [40/100], Loss: 3540.8511\n",
            "Epoch [50/100], Loss: 3978.3135\n",
            "Epoch [60/100], Loss: 1574.7125\n",
            "Epoch [70/100], Loss: 3674.6366\n",
            "Epoch [80/100], Loss: 1311.8339\n",
            "Epoch [90/100], Loss: 1549.5158\n",
            "Epoch [100/100], Loss: 2104.3772\n",
            "Fold 5, RMSE: 46.779884338378906\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 63.03391571044922\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13437.7307\n",
            "Epoch [20/150], Loss: 6563.7220\n",
            "Epoch [30/150], Loss: 6905.8208\n",
            "Epoch [40/150], Loss: 4385.4487\n",
            "Epoch [50/150], Loss: 2106.1016\n",
            "Epoch [60/150], Loss: 2518.6484\n",
            "Epoch [70/150], Loss: 3271.8604\n",
            "Epoch [80/150], Loss: 2250.3132\n",
            "Epoch [90/150], Loss: 1828.4507\n",
            "Epoch [100/150], Loss: 763.8303\n",
            "Epoch [110/150], Loss: 2130.2194\n",
            "Epoch [120/150], Loss: 946.7158\n",
            "Epoch [130/150], Loss: 1782.4854\n",
            "Epoch [140/150], Loss: 831.8462\n",
            "Epoch [150/150], Loss: 845.6250\n",
            "Fold 1, RMSE: 57.51512145996094\n",
            "Epoch [10/150], Loss: 10257.4199\n",
            "Epoch [20/150], Loss: 6631.6743\n",
            "Epoch [30/150], Loss: 6258.2671\n",
            "Epoch [40/150], Loss: 4732.7720\n",
            "Epoch [50/150], Loss: 2650.9141\n",
            "Epoch [60/150], Loss: 1742.9338\n",
            "Epoch [70/150], Loss: 1566.4758\n",
            "Epoch [80/150], Loss: 1425.0470\n",
            "Epoch [90/150], Loss: 1411.3832\n",
            "Epoch [100/150], Loss: 1833.8698\n",
            "Epoch [110/150], Loss: 651.2883\n",
            "Epoch [120/150], Loss: 915.9495\n",
            "Epoch [130/150], Loss: 5185.6805\n",
            "Epoch [140/150], Loss: 3238.7873\n",
            "Epoch [150/150], Loss: 1617.2776\n",
            "Fold 2, RMSE: 68.23631286621094\n",
            "Epoch [10/150], Loss: 9466.4460\n",
            "Epoch [20/150], Loss: 6488.5557\n",
            "Epoch [30/150], Loss: 4205.1043\n",
            "Epoch [40/150], Loss: 2361.5391\n",
            "Epoch [50/150], Loss: 2094.9367\n",
            "Epoch [60/150], Loss: 2356.1655\n",
            "Epoch [70/150], Loss: 3197.2159\n",
            "Epoch [80/150], Loss: 1719.0821\n",
            "Epoch [90/150], Loss: 1748.5045\n",
            "Epoch [100/150], Loss: 1426.4013\n",
            "Epoch [110/150], Loss: 718.7249\n",
            "Epoch [120/150], Loss: 885.4610\n",
            "Epoch [130/150], Loss: 2140.6880\n",
            "Epoch [140/150], Loss: 855.1654\n",
            "Epoch [150/150], Loss: 1749.6262\n",
            "Fold 3, RMSE: 90.62886810302734\n",
            "Epoch [10/150], Loss: 11481.3612\n",
            "Epoch [20/150], Loss: 10548.2861\n",
            "Epoch [30/150], Loss: 4899.6494\n",
            "Epoch [40/150], Loss: 2504.2982\n",
            "Epoch [50/150], Loss: 1968.2530\n",
            "Epoch [60/150], Loss: 3601.6489\n",
            "Epoch [70/150], Loss: 3560.2334\n",
            "Epoch [80/150], Loss: 2117.0802\n",
            "Epoch [90/150], Loss: 1638.1201\n",
            "Epoch [100/150], Loss: 1473.5352\n",
            "Epoch [110/150], Loss: 1454.7736\n",
            "Epoch [120/150], Loss: 2790.5555\n",
            "Epoch [130/150], Loss: 1173.7838\n",
            "Epoch [140/150], Loss: 2730.7397\n",
            "Epoch [150/150], Loss: 3063.7765\n",
            "Fold 4, RMSE: 42.248863220214844\n",
            "Epoch [10/150], Loss: 12298.8657\n",
            "Epoch [20/150], Loss: 8096.1805\n",
            "Epoch [30/150], Loss: 6164.9236\n",
            "Epoch [40/150], Loss: 2866.6279\n",
            "Epoch [50/150], Loss: 4172.9963\n",
            "Epoch [60/150], Loss: 2042.7498\n",
            "Epoch [70/150], Loss: 2080.6888\n",
            "Epoch [80/150], Loss: 1440.3752\n",
            "Epoch [90/150], Loss: 2049.8850\n",
            "Epoch [100/150], Loss: 1563.0105\n",
            "Epoch [110/150], Loss: 1868.7275\n",
            "Epoch [120/150], Loss: 1765.1369\n",
            "Epoch [130/150], Loss: 1507.5076\n",
            "Epoch [140/150], Loss: 640.5367\n",
            "Epoch [150/150], Loss: 609.0498\n",
            "Fold 5, RMSE: 43.874000549316406\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 60.500633239746094\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 16200.2838\n",
            "Epoch [20/100], Loss: 10731.3422\n",
            "Epoch [30/100], Loss: 3796.3642\n",
            "Epoch [40/100], Loss: 2902.5964\n",
            "Epoch [50/100], Loss: 2273.9345\n",
            "Epoch [60/100], Loss: 3979.0787\n",
            "Epoch [70/100], Loss: 4920.4189\n",
            "Epoch [80/100], Loss: 2247.6143\n",
            "Epoch [90/100], Loss: 2940.0648\n",
            "Epoch [100/100], Loss: 1699.0657\n",
            "Fold 1, RMSE: 58.18115997314453\n",
            "Epoch [10/100], Loss: 15958.9702\n",
            "Epoch [20/100], Loss: 15545.8450\n",
            "Epoch [30/100], Loss: 6512.1890\n",
            "Epoch [40/100], Loss: 4154.3602\n",
            "Epoch [50/100], Loss: 2710.8839\n",
            "Epoch [60/100], Loss: 3292.1376\n",
            "Epoch [70/100], Loss: 1462.9105\n",
            "Epoch [80/100], Loss: 2033.2338\n",
            "Epoch [90/100], Loss: 2449.8433\n",
            "Epoch [100/100], Loss: 1250.0443\n",
            "Fold 2, RMSE: 65.3933334350586\n",
            "Epoch [10/100], Loss: 18048.8958\n",
            "Epoch [20/100], Loss: 8203.5627\n",
            "Epoch [30/100], Loss: 5478.7551\n",
            "Epoch [40/100], Loss: 5907.3906\n",
            "Epoch [50/100], Loss: 4942.6517\n",
            "Epoch [60/100], Loss: 2312.5239\n",
            "Epoch [70/100], Loss: 2123.7753\n",
            "Epoch [80/100], Loss: 2200.9374\n",
            "Epoch [90/100], Loss: 1569.2289\n",
            "Epoch [100/100], Loss: 1030.5113\n",
            "Fold 3, RMSE: 94.88777923583984\n",
            "Epoch [10/100], Loss: 24486.6372\n",
            "Epoch [20/100], Loss: 10282.6677\n",
            "Epoch [30/100], Loss: 7988.6580\n",
            "Epoch [40/100], Loss: 5945.2365\n",
            "Epoch [50/100], Loss: 2754.9382\n",
            "Epoch [60/100], Loss: 3671.4846\n",
            "Epoch [70/100], Loss: 2724.1516\n",
            "Epoch [80/100], Loss: 1482.0564\n",
            "Epoch [90/100], Loss: 2952.4004\n",
            "Epoch [100/100], Loss: 1581.2907\n",
            "Fold 4, RMSE: 37.34095001220703\n",
            "Epoch [10/100], Loss: 22186.7666\n",
            "Epoch [20/100], Loss: 9789.3545\n",
            "Epoch [30/100], Loss: 6917.8741\n",
            "Epoch [40/100], Loss: 2522.6380\n",
            "Epoch [50/100], Loss: 3847.6316\n",
            "Epoch [60/100], Loss: 2090.5034\n",
            "Epoch [70/100], Loss: 2593.2950\n",
            "Epoch [80/100], Loss: 3714.1241\n",
            "Epoch [90/100], Loss: 1790.3948\n",
            "Epoch [100/100], Loss: 9103.6577\n",
            "Fold 5, RMSE: 44.50677490234375\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 60.06199951171875\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 12581.2983\n",
            "Epoch [20/150], Loss: 7578.2607\n",
            "Epoch [30/150], Loss: 5108.9250\n",
            "Epoch [40/150], Loss: 6410.2294\n",
            "Epoch [50/150], Loss: 2139.2651\n",
            "Epoch [60/150], Loss: 1990.8631\n",
            "Epoch [70/150], Loss: 2078.4330\n",
            "Epoch [80/150], Loss: 3001.3997\n",
            "Epoch [90/150], Loss: 1729.1663\n",
            "Epoch [100/150], Loss: 4681.3348\n",
            "Epoch [110/150], Loss: 870.9623\n",
            "Epoch [120/150], Loss: 1122.7477\n",
            "Epoch [130/150], Loss: 1951.1004\n",
            "Epoch [140/150], Loss: 1147.3058\n",
            "Epoch [150/150], Loss: 2274.9693\n",
            "Fold 1, RMSE: 60.92344284057617\n",
            "Epoch [10/150], Loss: 19724.0948\n",
            "Epoch [20/150], Loss: 9261.9413\n",
            "Epoch [30/150], Loss: 7250.0352\n",
            "Epoch [40/150], Loss: 2781.9170\n",
            "Epoch [50/150], Loss: 3644.3902\n",
            "Epoch [60/150], Loss: 2658.8883\n",
            "Epoch [70/150], Loss: 3874.0214\n",
            "Epoch [80/150], Loss: 2924.4196\n",
            "Epoch [90/150], Loss: 2663.7058\n",
            "Epoch [100/150], Loss: 2328.0174\n",
            "Epoch [110/150], Loss: 1507.8008\n",
            "Epoch [120/150], Loss: 1795.0230\n",
            "Epoch [130/150], Loss: 1912.9187\n",
            "Epoch [140/150], Loss: 1820.8171\n",
            "Epoch [150/150], Loss: 4768.6290\n",
            "Fold 2, RMSE: 57.114906311035156\n",
            "Epoch [10/150], Loss: 15326.9927\n",
            "Epoch [20/150], Loss: 9423.5593\n",
            "Epoch [30/150], Loss: 5121.3403\n",
            "Epoch [40/150], Loss: 4236.0658\n",
            "Epoch [50/150], Loss: 2933.9455\n",
            "Epoch [60/150], Loss: 2631.0024\n",
            "Epoch [70/150], Loss: 1492.1316\n",
            "Epoch [80/150], Loss: 2189.5917\n",
            "Epoch [90/150], Loss: 3222.7983\n",
            "Epoch [100/150], Loss: 1372.9252\n",
            "Epoch [110/150], Loss: 1711.2990\n",
            "Epoch [120/150], Loss: 3052.9396\n",
            "Epoch [130/150], Loss: 1265.7249\n",
            "Epoch [140/150], Loss: 2475.3965\n",
            "Epoch [150/150], Loss: 919.6218\n",
            "Fold 3, RMSE: 94.99507904052734\n",
            "Epoch [10/150], Loss: 21771.0029\n",
            "Epoch [20/150], Loss: 11653.2953\n",
            "Epoch [30/150], Loss: 8094.2991\n",
            "Epoch [40/150], Loss: 4590.0751\n",
            "Epoch [50/150], Loss: 4243.2645\n",
            "Epoch [60/150], Loss: 3734.5218\n",
            "Epoch [70/150], Loss: 5107.8947\n",
            "Epoch [80/150], Loss: 1989.4959\n",
            "Epoch [90/150], Loss: 4322.5427\n",
            "Epoch [100/150], Loss: 1373.6785\n",
            "Epoch [110/150], Loss: 3930.8507\n",
            "Epoch [120/150], Loss: 2957.5481\n",
            "Epoch [130/150], Loss: 2912.3784\n",
            "Epoch [140/150], Loss: 1834.5963\n",
            "Epoch [150/150], Loss: 1511.3639\n",
            "Fold 4, RMSE: 39.11404037475586\n",
            "Epoch [10/150], Loss: 15060.0840\n",
            "Epoch [20/150], Loss: 11938.3455\n",
            "Epoch [30/150], Loss: 7159.6383\n",
            "Epoch [40/150], Loss: 4723.7633\n",
            "Epoch [50/150], Loss: 3028.3250\n",
            "Epoch [60/150], Loss: 1810.7458\n",
            "Epoch [70/150], Loss: 2210.2537\n",
            "Epoch [80/150], Loss: 1495.8756\n",
            "Epoch [90/150], Loss: 1561.4856\n",
            "Epoch [100/150], Loss: 830.3521\n",
            "Epoch [110/150], Loss: 2276.5697\n",
            "Epoch [120/150], Loss: 2214.5270\n",
            "Epoch [130/150], Loss: 1091.5074\n",
            "Epoch [140/150], Loss: 1853.2507\n",
            "Epoch [150/150], Loss: 4296.4943\n",
            "Fold 5, RMSE: 44.944664001464844\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 59.418426513671875\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 10631.8533\n",
            "Epoch [20/100], Loss: 6135.6753\n",
            "Epoch [30/100], Loss: 3759.9393\n",
            "Epoch [40/100], Loss: 3150.1298\n",
            "Epoch [50/100], Loss: 2875.3412\n",
            "Epoch [60/100], Loss: 1710.7491\n",
            "Epoch [70/100], Loss: 2004.6481\n",
            "Epoch [80/100], Loss: 1509.3889\n",
            "Epoch [90/100], Loss: 1065.9340\n",
            "Epoch [100/100], Loss: 3425.6868\n",
            "Fold 1, RMSE: 56.70790481567383\n",
            "Epoch [10/100], Loss: 8787.9562\n",
            "Epoch [20/100], Loss: 8709.2219\n",
            "Epoch [30/100], Loss: 7379.1938\n",
            "Epoch [40/100], Loss: 2980.7512\n",
            "Epoch [50/100], Loss: 2897.1100\n",
            "Epoch [60/100], Loss: 2283.3538\n",
            "Epoch [70/100], Loss: 1868.4284\n",
            "Epoch [80/100], Loss: 3679.7434\n",
            "Epoch [90/100], Loss: 920.4474\n",
            "Epoch [100/100], Loss: 2811.0277\n",
            "Fold 2, RMSE: 60.18556594848633\n",
            "Epoch [10/100], Loss: 7296.1486\n",
            "Epoch [20/100], Loss: 4719.5946\n",
            "Epoch [30/100], Loss: 2486.5297\n",
            "Epoch [40/100], Loss: 2664.7877\n",
            "Epoch [50/100], Loss: 2676.3560\n",
            "Epoch [60/100], Loss: 1248.0868\n",
            "Epoch [70/100], Loss: 1681.8831\n",
            "Epoch [80/100], Loss: 1313.9704\n",
            "Epoch [90/100], Loss: 1766.5034\n",
            "Epoch [100/100], Loss: 625.0568\n",
            "Fold 3, RMSE: 92.90914916992188\n",
            "Epoch [10/100], Loss: 14467.6763\n",
            "Epoch [20/100], Loss: 10860.0024\n",
            "Epoch [30/100], Loss: 4300.7614\n",
            "Epoch [40/100], Loss: 4247.2333\n",
            "Epoch [50/100], Loss: 1786.1639\n",
            "Epoch [60/100], Loss: 3183.8242\n",
            "Epoch [70/100], Loss: 2612.6435\n",
            "Epoch [80/100], Loss: 2283.5067\n",
            "Epoch [90/100], Loss: 3336.5051\n",
            "Epoch [100/100], Loss: 2142.3935\n",
            "Fold 4, RMSE: 44.0792350769043\n",
            "Epoch [10/100], Loss: 9379.0992\n",
            "Epoch [20/100], Loss: 6623.7928\n",
            "Epoch [30/100], Loss: 5274.9089\n",
            "Epoch [40/100], Loss: 6110.6328\n",
            "Epoch [50/100], Loss: 3334.9891\n",
            "Epoch [60/100], Loss: 3313.6659\n",
            "Epoch [70/100], Loss: 3216.9602\n",
            "Epoch [80/100], Loss: 2954.9334\n",
            "Epoch [90/100], Loss: 2345.9502\n",
            "Epoch [100/100], Loss: 2852.2004\n",
            "Fold 5, RMSE: 51.59238815307617\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 61.0948486328125\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 17032.6438\n",
            "Epoch [20/150], Loss: 10049.8694\n",
            "Epoch [30/150], Loss: 4036.5156\n",
            "Epoch [40/150], Loss: 3115.3888\n",
            "Epoch [50/150], Loss: 2079.0009\n",
            "Epoch [60/150], Loss: 1880.1048\n",
            "Epoch [70/150], Loss: 1709.6002\n",
            "Epoch [80/150], Loss: 1480.7917\n",
            "Epoch [90/150], Loss: 569.7992\n",
            "Epoch [100/150], Loss: 1165.8693\n",
            "Epoch [110/150], Loss: 641.7156\n",
            "Epoch [120/150], Loss: 854.9506\n",
            "Epoch [130/150], Loss: 1751.0115\n",
            "Epoch [140/150], Loss: 1097.3269\n",
            "Epoch [150/150], Loss: 2828.1133\n",
            "Fold 1, RMSE: 56.94187927246094\n",
            "Epoch [10/150], Loss: 9175.1479\n",
            "Epoch [20/150], Loss: 8680.2670\n",
            "Epoch [30/150], Loss: 5965.8033\n",
            "Epoch [40/150], Loss: 1806.2450\n",
            "Epoch [50/150], Loss: 1716.7876\n",
            "Epoch [60/150], Loss: 2685.5286\n",
            "Epoch [70/150], Loss: 1864.1402\n",
            "Epoch [80/150], Loss: 1940.9659\n",
            "Epoch [90/150], Loss: 2070.2145\n",
            "Epoch [100/150], Loss: 1484.2658\n",
            "Epoch [110/150], Loss: 896.8032\n",
            "Epoch [120/150], Loss: 1083.1862\n",
            "Epoch [130/150], Loss: 863.1121\n",
            "Epoch [140/150], Loss: 825.3364\n",
            "Epoch [150/150], Loss: 702.7581\n",
            "Fold 2, RMSE: 63.299156188964844\n",
            "Epoch [10/150], Loss: 7772.1294\n",
            "Epoch [20/150], Loss: 7120.0168\n",
            "Epoch [30/150], Loss: 4613.6343\n",
            "Epoch [40/150], Loss: 3449.2629\n",
            "Epoch [50/150], Loss: 1470.6275\n",
            "Epoch [60/150], Loss: 7533.1927\n",
            "Epoch [70/150], Loss: 3513.3875\n",
            "Epoch [80/150], Loss: 1674.6661\n",
            "Epoch [90/150], Loss: 1652.9292\n",
            "Epoch [100/150], Loss: 1011.4686\n",
            "Epoch [110/150], Loss: 769.5772\n",
            "Epoch [120/150], Loss: 640.6933\n",
            "Epoch [130/150], Loss: 1216.5835\n",
            "Epoch [140/150], Loss: 590.5176\n",
            "Epoch [150/150], Loss: 871.0997\n",
            "Fold 3, RMSE: 90.91458129882812\n",
            "Epoch [10/150], Loss: 18708.3879\n",
            "Epoch [20/150], Loss: 12225.1873\n",
            "Epoch [30/150], Loss: 3458.9081\n",
            "Epoch [40/150], Loss: 6519.3777\n",
            "Epoch [50/150], Loss: 2874.7146\n",
            "Epoch [60/150], Loss: 2082.2238\n",
            "Epoch [70/150], Loss: 3732.6302\n",
            "Epoch [80/150], Loss: 1028.5367\n",
            "Epoch [90/150], Loss: 941.8551\n",
            "Epoch [100/150], Loss: 2303.0483\n",
            "Epoch [110/150], Loss: 1022.6470\n",
            "Epoch [120/150], Loss: 2436.8312\n",
            "Epoch [130/150], Loss: 1938.4616\n",
            "Epoch [140/150], Loss: 557.0978\n",
            "Epoch [150/150], Loss: 951.4951\n",
            "Fold 4, RMSE: 40.723445892333984\n",
            "Epoch [10/150], Loss: 8346.0887\n",
            "Epoch [20/150], Loss: 10584.1907\n",
            "Epoch [30/150], Loss: 5432.5234\n",
            "Epoch [40/150], Loss: 2552.2780\n",
            "Epoch [50/150], Loss: 2543.5184\n",
            "Epoch [60/150], Loss: 2438.9246\n",
            "Epoch [70/150], Loss: 4670.2851\n",
            "Epoch [80/150], Loss: 1877.1633\n",
            "Epoch [90/150], Loss: 1829.3309\n",
            "Epoch [100/150], Loss: 4529.8000\n",
            "Epoch [110/150], Loss: 3770.4149\n",
            "Epoch [120/150], Loss: 1381.1590\n",
            "Epoch [130/150], Loss: 1860.8109\n",
            "Epoch [140/150], Loss: 1247.5981\n",
            "Epoch [150/150], Loss: 1115.9279\n",
            "Fold 5, RMSE: 47.73823928833008\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 59.92346038818359\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 27524.1162\n",
            "Epoch [20/100], Loss: 14311.8640\n",
            "Epoch [30/100], Loss: 11026.4608\n",
            "Epoch [40/100], Loss: 6728.5798\n",
            "Epoch [50/100], Loss: 3835.5102\n",
            "Epoch [60/100], Loss: 8828.8281\n",
            "Epoch [70/100], Loss: 5794.2515\n",
            "Epoch [80/100], Loss: 1828.1909\n",
            "Epoch [90/100], Loss: 1338.4239\n",
            "Epoch [100/100], Loss: 8585.6010\n",
            "Fold 1, RMSE: 58.77741241455078\n",
            "Epoch [10/100], Loss: 40465.8555\n",
            "Epoch [20/100], Loss: 10601.4186\n",
            "Epoch [30/100], Loss: 7396.8150\n",
            "Epoch [40/100], Loss: 5001.6848\n",
            "Epoch [50/100], Loss: 6981.8641\n",
            "Epoch [60/100], Loss: 4614.1814\n",
            "Epoch [70/100], Loss: 11980.9968\n",
            "Epoch [80/100], Loss: 2486.1651\n",
            "Epoch [90/100], Loss: 1925.3332\n",
            "Epoch [100/100], Loss: 1513.0786\n",
            "Fold 2, RMSE: 66.2701416015625\n",
            "Epoch [10/100], Loss: 12957.4109\n",
            "Epoch [20/100], Loss: 9026.6333\n",
            "Epoch [30/100], Loss: 7387.3137\n",
            "Epoch [40/100], Loss: 7037.9429\n",
            "Epoch [50/100], Loss: 3762.1106\n",
            "Epoch [60/100], Loss: 5644.7439\n",
            "Epoch [70/100], Loss: 2706.2640\n",
            "Epoch [80/100], Loss: 2946.2830\n",
            "Epoch [90/100], Loss: 1201.7942\n",
            "Epoch [100/100], Loss: 1340.0782\n",
            "Fold 3, RMSE: 94.80382537841797\n",
            "Epoch [10/100], Loss: 26190.0452\n",
            "Epoch [20/100], Loss: 13684.8418\n",
            "Epoch [30/100], Loss: 9246.0610\n",
            "Epoch [40/100], Loss: 11621.0503\n",
            "Epoch [50/100], Loss: 5075.7727\n",
            "Epoch [60/100], Loss: 1995.2216\n",
            "Epoch [70/100], Loss: 2955.3724\n",
            "Epoch [80/100], Loss: 3238.8889\n",
            "Epoch [90/100], Loss: 9520.7260\n",
            "Epoch [100/100], Loss: 2692.9217\n",
            "Fold 4, RMSE: 42.32088851928711\n",
            "Epoch [10/100], Loss: 15972.4729\n",
            "Epoch [20/100], Loss: 9908.0031\n",
            "Epoch [30/100], Loss: 12281.7847\n",
            "Epoch [40/100], Loss: 6079.7140\n",
            "Epoch [50/100], Loss: 5589.0065\n",
            "Epoch [60/100], Loss: 3735.5573\n",
            "Epoch [70/100], Loss: 4818.2256\n",
            "Epoch [80/100], Loss: 5521.8968\n",
            "Epoch [90/100], Loss: 1953.3998\n",
            "Epoch [100/100], Loss: 3835.0629\n",
            "Fold 5, RMSE: 47.01153564453125\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 61.836760711669925\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 29446.0405\n",
            "Epoch [20/150], Loss: 13250.8457\n",
            "Epoch [30/150], Loss: 11206.6482\n",
            "Epoch [40/150], Loss: 8084.3536\n",
            "Epoch [50/150], Loss: 9054.0092\n",
            "Epoch [60/150], Loss: 4639.7087\n",
            "Epoch [70/150], Loss: 3086.4847\n",
            "Epoch [80/150], Loss: 4894.9542\n",
            "Epoch [90/150], Loss: 2291.3787\n",
            "Epoch [100/150], Loss: 2924.0553\n",
            "Epoch [110/150], Loss: 1495.1039\n",
            "Epoch [120/150], Loss: 1264.4488\n",
            "Epoch [130/150], Loss: 1757.0559\n",
            "Epoch [140/150], Loss: 884.9464\n",
            "Epoch [150/150], Loss: 2130.6759\n",
            "Fold 1, RMSE: 54.98590850830078\n",
            "Epoch [10/150], Loss: 16006.2739\n",
            "Epoch [20/150], Loss: 11834.9709\n",
            "Epoch [30/150], Loss: 14689.3516\n",
            "Epoch [40/150], Loss: 9374.8203\n",
            "Epoch [50/150], Loss: 6920.0690\n",
            "Epoch [60/150], Loss: 5973.1866\n",
            "Epoch [70/150], Loss: 3232.5250\n",
            "Epoch [80/150], Loss: 2696.6115\n",
            "Epoch [90/150], Loss: 3430.7111\n",
            "Epoch [100/150], Loss: 2473.4370\n",
            "Epoch [110/150], Loss: 2284.8386\n",
            "Epoch [120/150], Loss: 5107.2776\n",
            "Epoch [130/150], Loss: 2399.9279\n",
            "Epoch [140/150], Loss: 2233.5728\n",
            "Epoch [150/150], Loss: 1594.8617\n",
            "Fold 2, RMSE: 65.91654205322266\n",
            "Epoch [10/150], Loss: 73002.8877\n",
            "Epoch [20/150], Loss: 13789.8047\n",
            "Epoch [30/150], Loss: 9879.0190\n",
            "Epoch [40/150], Loss: 5553.1335\n",
            "Epoch [50/150], Loss: 7467.9404\n",
            "Epoch [60/150], Loss: 2653.7700\n",
            "Epoch [70/150], Loss: 2105.5487\n",
            "Epoch [80/150], Loss: 4529.8181\n",
            "Epoch [90/150], Loss: 1578.0292\n",
            "Epoch [100/150], Loss: 3450.2162\n",
            "Epoch [110/150], Loss: 1191.2915\n",
            "Epoch [120/150], Loss: 1090.6278\n",
            "Epoch [130/150], Loss: 702.1142\n",
            "Epoch [140/150], Loss: 3541.6445\n",
            "Epoch [150/150], Loss: 1228.7284\n",
            "Fold 3, RMSE: 95.69536590576172\n",
            "Epoch [10/150], Loss: 21606.5378\n",
            "Epoch [20/150], Loss: 12359.3669\n",
            "Epoch [30/150], Loss: 6493.1759\n",
            "Epoch [40/150], Loss: 3326.2122\n",
            "Epoch [50/150], Loss: 3909.9331\n",
            "Epoch [60/150], Loss: 3172.4116\n",
            "Epoch [70/150], Loss: 2003.4963\n",
            "Epoch [80/150], Loss: 3540.8801\n",
            "Epoch [90/150], Loss: 2805.0690\n",
            "Epoch [100/150], Loss: 4649.0048\n",
            "Epoch [110/150], Loss: 2579.2046\n",
            "Epoch [120/150], Loss: 2994.4338\n",
            "Epoch [130/150], Loss: 4503.6572\n",
            "Epoch [140/150], Loss: 6604.5344\n",
            "Epoch [150/150], Loss: 2764.6338\n",
            "Fold 4, RMSE: 44.83625411987305\n",
            "Epoch [10/150], Loss: 33004.3945\n",
            "Epoch [20/150], Loss: 14655.6489\n",
            "Epoch [30/150], Loss: 11446.4427\n",
            "Epoch [40/150], Loss: 11333.5884\n",
            "Epoch [50/150], Loss: 7595.0101\n",
            "Epoch [60/150], Loss: 10070.5032\n",
            "Epoch [70/150], Loss: 6406.7300\n",
            "Epoch [80/150], Loss: 6183.0131\n",
            "Epoch [90/150], Loss: 5468.8456\n",
            "Epoch [100/150], Loss: 4608.7904\n",
            "Epoch [110/150], Loss: 3352.2744\n",
            "Epoch [120/150], Loss: 5156.6816\n",
            "Epoch [130/150], Loss: 4313.6776\n",
            "Epoch [140/150], Loss: 6277.4301\n",
            "Epoch [150/150], Loss: 8436.6660\n",
            "Fold 5, RMSE: 45.716495513916016\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 61.43011322021484\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 11342.6317\n",
            "Epoch [20/100], Loss: 5025.8103\n",
            "Epoch [30/100], Loss: 4586.6581\n",
            "Epoch [40/100], Loss: 4161.5226\n",
            "Epoch [50/100], Loss: 3636.7683\n",
            "Epoch [60/100], Loss: 3446.4200\n",
            "Epoch [70/100], Loss: 1647.4895\n",
            "Epoch [80/100], Loss: 2414.0108\n",
            "Epoch [90/100], Loss: 2414.9801\n",
            "Epoch [100/100], Loss: 2253.5370\n",
            "Fold 1, RMSE: 53.52598190307617\n",
            "Epoch [10/100], Loss: 10556.5122\n",
            "Epoch [20/100], Loss: 13101.3049\n",
            "Epoch [30/100], Loss: 6197.3532\n",
            "Epoch [40/100], Loss: 3295.6005\n",
            "Epoch [50/100], Loss: 3515.0342\n",
            "Epoch [60/100], Loss: 4421.5428\n",
            "Epoch [70/100], Loss: 2062.4630\n",
            "Epoch [80/100], Loss: 2610.3304\n",
            "Epoch [90/100], Loss: 2839.4874\n",
            "Epoch [100/100], Loss: 2136.8524\n",
            "Fold 2, RMSE: 64.9614486694336\n",
            "Epoch [10/100], Loss: 9169.1760\n",
            "Epoch [20/100], Loss: 5371.2625\n",
            "Epoch [30/100], Loss: 4249.7191\n",
            "Epoch [40/100], Loss: 1583.0436\n",
            "Epoch [50/100], Loss: 3051.4112\n",
            "Epoch [60/100], Loss: 2492.1540\n",
            "Epoch [70/100], Loss: 6180.2653\n",
            "Epoch [80/100], Loss: 2386.4894\n",
            "Epoch [90/100], Loss: 2968.3412\n",
            "Epoch [100/100], Loss: 2352.3062\n",
            "Fold 3, RMSE: 92.88593292236328\n",
            "Epoch [10/100], Loss: 10930.3254\n",
            "Epoch [20/100], Loss: 5262.8448\n",
            "Epoch [30/100], Loss: 5771.8307\n",
            "Epoch [40/100], Loss: 3705.9375\n",
            "Epoch [50/100], Loss: 2959.1452\n",
            "Epoch [60/100], Loss: 4045.3968\n",
            "Epoch [70/100], Loss: 1854.4914\n",
            "Epoch [80/100], Loss: 1872.1907\n",
            "Epoch [90/100], Loss: 2380.8707\n",
            "Epoch [100/100], Loss: 1086.1266\n",
            "Fold 4, RMSE: 41.554229736328125\n",
            "Epoch [10/100], Loss: 18666.2905\n",
            "Epoch [20/100], Loss: 13097.6057\n",
            "Epoch [30/100], Loss: 5955.8289\n",
            "Epoch [40/100], Loss: 4006.5166\n",
            "Epoch [50/100], Loss: 4058.3422\n",
            "Epoch [60/100], Loss: 4294.0082\n",
            "Epoch [70/100], Loss: 1687.8160\n",
            "Epoch [80/100], Loss: 1404.6383\n",
            "Epoch [90/100], Loss: 1722.5081\n",
            "Epoch [100/100], Loss: 4289.7791\n",
            "Fold 5, RMSE: 45.70271301269531\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 59.726061248779295\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 17696.4426\n",
            "Epoch [20/150], Loss: 14532.4558\n",
            "Epoch [30/150], Loss: 4475.2108\n",
            "Epoch [40/150], Loss: 4119.6205\n",
            "Epoch [50/150], Loss: 3840.2057\n",
            "Epoch [60/150], Loss: 2736.1130\n",
            "Epoch [70/150], Loss: 1851.6995\n",
            "Epoch [80/150], Loss: 1705.4147\n",
            "Epoch [90/150], Loss: 2419.3574\n",
            "Epoch [100/150], Loss: 2267.2055\n",
            "Epoch [110/150], Loss: 2448.6318\n",
            "Epoch [120/150], Loss: 1721.6878\n",
            "Epoch [130/150], Loss: 1555.3839\n",
            "Epoch [140/150], Loss: 1913.4332\n",
            "Epoch [150/150], Loss: 2264.8380\n",
            "Fold 1, RMSE: 56.99517822265625\n",
            "Epoch [10/150], Loss: 11603.3015\n",
            "Epoch [20/150], Loss: 6917.8203\n",
            "Epoch [30/150], Loss: 5366.3359\n",
            "Epoch [40/150], Loss: 3057.5323\n",
            "Epoch [50/150], Loss: 6854.7305\n",
            "Epoch [60/150], Loss: 2427.1890\n",
            "Epoch [70/150], Loss: 1311.8539\n",
            "Epoch [80/150], Loss: 1214.7936\n",
            "Epoch [90/150], Loss: 2267.4651\n",
            "Epoch [100/150], Loss: 1757.9357\n",
            "Epoch [110/150], Loss: 3116.3819\n",
            "Epoch [120/150], Loss: 2017.6769\n",
            "Epoch [130/150], Loss: 3311.1538\n",
            "Epoch [140/150], Loss: 930.6714\n",
            "Epoch [150/150], Loss: 1025.4038\n",
            "Fold 2, RMSE: 64.37026977539062\n",
            "Epoch [10/150], Loss: 7423.3622\n",
            "Epoch [20/150], Loss: 4988.8672\n",
            "Epoch [30/150], Loss: 6118.2731\n",
            "Epoch [40/150], Loss: 3227.6217\n",
            "Epoch [50/150], Loss: 1943.6108\n",
            "Epoch [60/150], Loss: 1671.3966\n",
            "Epoch [70/150], Loss: 930.9454\n",
            "Epoch [80/150], Loss: 1714.2474\n",
            "Epoch [90/150], Loss: 1100.2740\n",
            "Epoch [100/150], Loss: 669.9235\n",
            "Epoch [110/150], Loss: 1989.9016\n",
            "Epoch [120/150], Loss: 663.2966\n",
            "Epoch [130/150], Loss: 757.7831\n",
            "Epoch [140/150], Loss: 997.7897\n",
            "Epoch [150/150], Loss: 809.6309\n",
            "Fold 3, RMSE: 93.58271026611328\n",
            "Epoch [10/150], Loss: 14043.8638\n",
            "Epoch [20/150], Loss: 9089.3152\n",
            "Epoch [30/150], Loss: 8120.2709\n",
            "Epoch [40/150], Loss: 10260.6156\n",
            "Epoch [50/150], Loss: 3310.1650\n",
            "Epoch [60/150], Loss: 2946.8252\n",
            "Epoch [70/150], Loss: 2419.9175\n",
            "Epoch [80/150], Loss: 3064.2849\n",
            "Epoch [90/150], Loss: 1739.6639\n",
            "Epoch [100/150], Loss: 914.7832\n",
            "Epoch [110/150], Loss: 2314.1823\n",
            "Epoch [120/150], Loss: 2870.7421\n",
            "Epoch [130/150], Loss: 1959.4774\n",
            "Epoch [140/150], Loss: 714.8278\n",
            "Epoch [150/150], Loss: 1801.2155\n",
            "Fold 4, RMSE: 41.39626693725586\n",
            "Epoch [10/150], Loss: 13481.3621\n",
            "Epoch [20/150], Loss: 13842.8721\n",
            "Epoch [30/150], Loss: 7615.6942\n",
            "Epoch [40/150], Loss: 6108.8729\n",
            "Epoch [50/150], Loss: 3024.4039\n",
            "Epoch [60/150], Loss: 2750.6883\n",
            "Epoch [70/150], Loss: 1886.2209\n",
            "Epoch [80/150], Loss: 1792.1321\n",
            "Epoch [90/150], Loss: 1623.1497\n",
            "Epoch [100/150], Loss: 2798.7607\n",
            "Epoch [110/150], Loss: 2121.1732\n",
            "Epoch [120/150], Loss: 2826.3525\n",
            "Epoch [130/150], Loss: 1276.0097\n",
            "Epoch [140/150], Loss: 630.6970\n",
            "Epoch [150/150], Loss: 1134.6608\n",
            "Fold 5, RMSE: 49.25088119506836\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 61.11906127929687\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 80232.0488\n",
            "Epoch [20/100], Loss: 16619.9023\n",
            "Epoch [30/100], Loss: 12939.1260\n",
            "Epoch [40/100], Loss: 8859.6188\n",
            "Epoch [50/100], Loss: 11557.4471\n",
            "Epoch [60/100], Loss: 5013.0126\n",
            "Epoch [70/100], Loss: 5139.6061\n",
            "Epoch [80/100], Loss: 2604.9479\n",
            "Epoch [90/100], Loss: 3101.5433\n",
            "Epoch [100/100], Loss: 3401.4299\n",
            "Fold 1, RMSE: 54.06809997558594\n",
            "Epoch [10/100], Loss: 60968.3438\n",
            "Epoch [20/100], Loss: 11826.2800\n",
            "Epoch [30/100], Loss: 8786.9690\n",
            "Epoch [40/100], Loss: 7147.1897\n",
            "Epoch [50/100], Loss: 4415.6653\n",
            "Epoch [60/100], Loss: 3936.9328\n",
            "Epoch [70/100], Loss: 4539.7741\n",
            "Epoch [80/100], Loss: 2499.6774\n",
            "Epoch [90/100], Loss: 5388.1337\n",
            "Epoch [100/100], Loss: 1823.7521\n",
            "Fold 2, RMSE: 66.4781494140625\n",
            "Epoch [10/100], Loss: 12788.4209\n",
            "Epoch [20/100], Loss: 9370.8900\n",
            "Epoch [30/100], Loss: 5457.5940\n",
            "Epoch [40/100], Loss: 3719.8735\n",
            "Epoch [50/100], Loss: 3381.0127\n",
            "Epoch [60/100], Loss: 3079.4026\n",
            "Epoch [70/100], Loss: 4148.3224\n",
            "Epoch [80/100], Loss: 3346.2612\n",
            "Epoch [90/100], Loss: 2619.8456\n",
            "Epoch [100/100], Loss: 1821.0591\n",
            "Fold 3, RMSE: 99.8322525024414\n",
            "Epoch [10/100], Loss: 13889.3298\n",
            "Epoch [20/100], Loss: 8074.3578\n",
            "Epoch [30/100], Loss: 6765.4133\n",
            "Epoch [40/100], Loss: 4160.0193\n",
            "Epoch [50/100], Loss: 6049.3240\n",
            "Epoch [60/100], Loss: 9787.6671\n",
            "Epoch [70/100], Loss: 2687.4045\n",
            "Epoch [80/100], Loss: 3192.8324\n",
            "Epoch [90/100], Loss: 6701.3164\n",
            "Epoch [100/100], Loss: 3497.3755\n",
            "Fold 4, RMSE: 46.09022903442383\n",
            "Epoch [10/100], Loss: 12722.8716\n",
            "Epoch [20/100], Loss: 7859.7716\n",
            "Epoch [30/100], Loss: 6230.6255\n",
            "Epoch [40/100], Loss: 3684.5704\n",
            "Epoch [50/100], Loss: 3398.1063\n",
            "Epoch [60/100], Loss: 1983.5216\n",
            "Epoch [70/100], Loss: 2834.0651\n",
            "Epoch [80/100], Loss: 6139.7204\n",
            "Epoch [90/100], Loss: 6091.7749\n",
            "Epoch [100/100], Loss: 1778.0693\n",
            "Fold 5, RMSE: 49.58797836303711\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 63.211341857910156\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 25982.6279\n",
            "Epoch [20/150], Loss: 11596.3357\n",
            "Epoch [30/150], Loss: 11420.1064\n",
            "Epoch [40/150], Loss: 10461.3220\n",
            "Epoch [50/150], Loss: 19363.2253\n",
            "Epoch [60/150], Loss: 4355.8068\n",
            "Epoch [70/150], Loss: 3531.2653\n",
            "Epoch [80/150], Loss: 6534.3978\n",
            "Epoch [90/150], Loss: 6322.4021\n",
            "Epoch [100/150], Loss: 2218.8748\n",
            "Epoch [110/150], Loss: 5211.3500\n",
            "Epoch [120/150], Loss: 6468.3859\n",
            "Epoch [130/150], Loss: 10907.2059\n",
            "Epoch [140/150], Loss: 3632.7536\n",
            "Epoch [150/150], Loss: 2173.4749\n",
            "Fold 1, RMSE: 60.12662887573242\n",
            "Epoch [10/150], Loss: 14544.0426\n",
            "Epoch [20/150], Loss: 12630.9646\n",
            "Epoch [30/150], Loss: 7158.5697\n",
            "Epoch [40/150], Loss: 7385.9487\n",
            "Epoch [50/150], Loss: 3668.6539\n",
            "Epoch [60/150], Loss: 4903.0952\n",
            "Epoch [70/150], Loss: 3010.2317\n",
            "Epoch [80/150], Loss: 2394.4529\n",
            "Epoch [90/150], Loss: 3928.8646\n",
            "Epoch [100/150], Loss: 2779.8437\n",
            "Epoch [110/150], Loss: 2319.7693\n",
            "Epoch [120/150], Loss: 1719.8321\n",
            "Epoch [130/150], Loss: 1168.6068\n",
            "Epoch [140/150], Loss: 807.2888\n",
            "Epoch [150/150], Loss: 1191.0893\n",
            "Fold 2, RMSE: 62.66172409057617\n",
            "Epoch [10/150], Loss: 25497.6465\n",
            "Epoch [20/150], Loss: 11858.9254\n",
            "Epoch [30/150], Loss: 8766.3402\n",
            "Epoch [40/150], Loss: 3815.5009\n",
            "Epoch [50/150], Loss: 2841.1955\n",
            "Epoch [60/150], Loss: 6791.6713\n",
            "Epoch [70/150], Loss: 2693.6135\n",
            "Epoch [80/150], Loss: 1866.5886\n",
            "Epoch [90/150], Loss: 3164.9356\n",
            "Epoch [100/150], Loss: 2111.2414\n",
            "Epoch [110/150], Loss: 3604.2296\n",
            "Epoch [120/150], Loss: 3255.6048\n",
            "Epoch [130/150], Loss: 1846.3354\n",
            "Epoch [140/150], Loss: 1804.9724\n",
            "Epoch [150/150], Loss: 2606.6243\n",
            "Fold 3, RMSE: 92.8767318725586\n",
            "Epoch [10/150], Loss: 58019.5068\n",
            "Epoch [20/150], Loss: 24834.8711\n",
            "Epoch [30/150], Loss: 8953.0394\n",
            "Epoch [40/150], Loss: 6755.9916\n",
            "Epoch [50/150], Loss: 4040.9169\n",
            "Epoch [60/150], Loss: 3394.8228\n",
            "Epoch [70/150], Loss: 2673.2582\n",
            "Epoch [80/150], Loss: 6069.9017\n",
            "Epoch [90/150], Loss: 4143.2783\n",
            "Epoch [100/150], Loss: 4424.4856\n",
            "Epoch [110/150], Loss: 4723.3390\n",
            "Epoch [120/150], Loss: 2246.4119\n",
            "Epoch [130/150], Loss: 1807.6803\n",
            "Epoch [140/150], Loss: 5071.1325\n",
            "Epoch [150/150], Loss: 2486.3786\n",
            "Fold 4, RMSE: 39.22486114501953\n",
            "Epoch [10/150], Loss: 37923.6445\n",
            "Epoch [20/150], Loss: 18566.4873\n",
            "Epoch [30/150], Loss: 9436.6643\n",
            "Epoch [40/150], Loss: 9357.5062\n",
            "Epoch [50/150], Loss: 8203.5757\n",
            "Epoch [60/150], Loss: 4139.5436\n",
            "Epoch [70/150], Loss: 4074.3441\n",
            "Epoch [80/150], Loss: 2649.3294\n",
            "Epoch [90/150], Loss: 4109.2530\n",
            "Epoch [100/150], Loss: 1194.5951\n",
            "Epoch [110/150], Loss: 2793.8367\n",
            "Epoch [120/150], Loss: 2436.4944\n",
            "Epoch [130/150], Loss: 2834.4315\n",
            "Epoch [140/150], Loss: 3750.5111\n",
            "Epoch [150/150], Loss: 2964.4431\n",
            "Fold 5, RMSE: 50.70195007324219\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 61.11837921142578\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16027.9141\n",
            "Epoch [20/100], Loss: 12240.0039\n",
            "Epoch [30/100], Loss: 8188.0276\n",
            "Epoch [40/100], Loss: 5514.8721\n",
            "Epoch [50/100], Loss: 3063.1653\n",
            "Epoch [60/100], Loss: 3473.1432\n",
            "Epoch [70/100], Loss: 2969.5593\n",
            "Epoch [80/100], Loss: 1728.7162\n",
            "Epoch [90/100], Loss: 1587.7194\n",
            "Epoch [100/100], Loss: 1124.3867\n",
            "Fold 1, RMSE: 59.10883712768555\n",
            "Epoch [10/100], Loss: 10462.7188\n",
            "Epoch [20/100], Loss: 12564.3904\n",
            "Epoch [30/100], Loss: 11557.5220\n",
            "Epoch [40/100], Loss: 4997.6340\n",
            "Epoch [50/100], Loss: 2405.0046\n",
            "Epoch [60/100], Loss: 3850.0811\n",
            "Epoch [70/100], Loss: 2487.8679\n",
            "Epoch [80/100], Loss: 1446.7897\n",
            "Epoch [90/100], Loss: 1337.7378\n",
            "Epoch [100/100], Loss: 2856.3845\n",
            "Fold 2, RMSE: 56.73638916015625\n",
            "Epoch [10/100], Loss: 8129.9205\n",
            "Epoch [20/100], Loss: 5208.7643\n",
            "Epoch [30/100], Loss: 5360.6604\n",
            "Epoch [40/100], Loss: 3054.9111\n",
            "Epoch [50/100], Loss: 2201.0223\n",
            "Epoch [60/100], Loss: 1005.7962\n",
            "Epoch [70/100], Loss: 2922.5166\n",
            "Epoch [80/100], Loss: 2456.4713\n",
            "Epoch [90/100], Loss: 2188.5994\n",
            "Epoch [100/100], Loss: 823.5950\n",
            "Fold 3, RMSE: 92.16695404052734\n",
            "Epoch [10/100], Loss: 16013.2336\n",
            "Epoch [20/100], Loss: 10479.7849\n",
            "Epoch [30/100], Loss: 14200.0222\n",
            "Epoch [40/100], Loss: 4205.9594\n",
            "Epoch [50/100], Loss: 2891.5446\n",
            "Epoch [60/100], Loss: 3633.8467\n",
            "Epoch [70/100], Loss: 2318.1443\n",
            "Epoch [80/100], Loss: 2022.4007\n",
            "Epoch [90/100], Loss: 2802.4456\n",
            "Epoch [100/100], Loss: 6089.6611\n",
            "Fold 4, RMSE: 36.232757568359375\n",
            "Epoch [10/100], Loss: 15172.8115\n",
            "Epoch [20/100], Loss: 10618.8579\n",
            "Epoch [30/100], Loss: 7077.6301\n",
            "Epoch [40/100], Loss: 4588.2842\n",
            "Epoch [50/100], Loss: 4200.9740\n",
            "Epoch [60/100], Loss: 5150.2274\n",
            "Epoch [70/100], Loss: 2741.5812\n",
            "Epoch [80/100], Loss: 1999.6123\n",
            "Epoch [90/100], Loss: 2922.3065\n",
            "Epoch [100/100], Loss: 3394.9234\n",
            "Fold 5, RMSE: 48.63357925415039\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 58.57570343017578\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15235.6040\n",
            "Epoch [20/150], Loss: 9752.0126\n",
            "Epoch [30/150], Loss: 4505.1976\n",
            "Epoch [40/150], Loss: 8169.5758\n",
            "Epoch [50/150], Loss: 6425.1031\n",
            "Epoch [60/150], Loss: 2563.8720\n",
            "Epoch [70/150], Loss: 2604.3090\n",
            "Epoch [80/150], Loss: 1292.9511\n",
            "Epoch [90/150], Loss: 2047.7087\n",
            "Epoch [100/150], Loss: 2660.9308\n",
            "Epoch [110/150], Loss: 1085.6545\n",
            "Epoch [120/150], Loss: 1387.5460\n",
            "Epoch [130/150], Loss: 1985.6703\n",
            "Epoch [140/150], Loss: 1063.0337\n",
            "Epoch [150/150], Loss: 390.7346\n",
            "Fold 1, RMSE: 58.00345230102539\n",
            "Epoch [10/150], Loss: 14235.6021\n",
            "Epoch [20/150], Loss: 5423.4727\n",
            "Epoch [30/150], Loss: 3589.4166\n",
            "Epoch [40/150], Loss: 3893.3041\n",
            "Epoch [50/150], Loss: 2953.8917\n",
            "Epoch [60/150], Loss: 2176.0547\n",
            "Epoch [70/150], Loss: 2090.3937\n",
            "Epoch [80/150], Loss: 936.4614\n",
            "Epoch [90/150], Loss: 3273.1276\n",
            "Epoch [100/150], Loss: 2171.2911\n",
            "Epoch [110/150], Loss: 2683.8456\n",
            "Epoch [120/150], Loss: 1774.3254\n",
            "Epoch [130/150], Loss: 2418.0107\n",
            "Epoch [140/150], Loss: 1509.3195\n",
            "Epoch [150/150], Loss: 2384.0126\n",
            "Fold 2, RMSE: 68.2488784790039\n",
            "Epoch [10/150], Loss: 8303.5600\n",
            "Epoch [20/150], Loss: 8942.3901\n",
            "Epoch [30/150], Loss: 4840.9543\n",
            "Epoch [40/150], Loss: 2368.1653\n",
            "Epoch [50/150], Loss: 2632.5718\n",
            "Epoch [60/150], Loss: 1256.0436\n",
            "Epoch [70/150], Loss: 2846.1098\n",
            "Epoch [80/150], Loss: 7387.8171\n",
            "Epoch [90/150], Loss: 3470.1118\n",
            "Epoch [100/150], Loss: 3561.1582\n",
            "Epoch [110/150], Loss: 2848.9301\n",
            "Epoch [120/150], Loss: 1845.3741\n",
            "Epoch [130/150], Loss: 1070.3715\n",
            "Epoch [140/150], Loss: 765.7966\n",
            "Epoch [150/150], Loss: 2356.1186\n",
            "Fold 3, RMSE: 93.06201171875\n",
            "Epoch [10/150], Loss: 25448.9492\n",
            "Epoch [20/150], Loss: 10044.6815\n",
            "Epoch [30/150], Loss: 14284.1362\n",
            "Epoch [40/150], Loss: 6382.0743\n",
            "Epoch [50/150], Loss: 4275.4240\n",
            "Epoch [60/150], Loss: 2858.5811\n",
            "Epoch [70/150], Loss: 2919.1440\n",
            "Epoch [80/150], Loss: 3554.6933\n",
            "Epoch [90/150], Loss: 8877.0266\n",
            "Epoch [100/150], Loss: 2542.6800\n",
            "Epoch [110/150], Loss: 2224.5944\n",
            "Epoch [120/150], Loss: 1945.7599\n",
            "Epoch [130/150], Loss: 1569.9916\n",
            "Epoch [140/150], Loss: 1781.5430\n",
            "Epoch [150/150], Loss: 1549.9843\n",
            "Fold 4, RMSE: 39.24772644042969\n",
            "Epoch [10/150], Loss: 17206.4697\n",
            "Epoch [20/150], Loss: 11068.5493\n",
            "Epoch [30/150], Loss: 7512.2220\n",
            "Epoch [40/150], Loss: 4880.4279\n",
            "Epoch [50/150], Loss: 2529.1271\n",
            "Epoch [60/150], Loss: 3643.9520\n",
            "Epoch [70/150], Loss: 2624.5369\n",
            "Epoch [80/150], Loss: 3805.9500\n",
            "Epoch [90/150], Loss: 2411.9212\n",
            "Epoch [100/150], Loss: 1621.9345\n",
            "Epoch [110/150], Loss: 2490.8045\n",
            "Epoch [120/150], Loss: 2535.9276\n",
            "Epoch [130/150], Loss: 4674.8281\n",
            "Epoch [140/150], Loss: 1478.7776\n",
            "Epoch [150/150], Loss: 1735.9619\n",
            "Fold 5, RMSE: 49.76644515991211\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 61.66570281982422\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 33300.4375\n",
            "Epoch [20/100], Loss: 14470.0317\n",
            "Epoch [30/100], Loss: 8834.5365\n",
            "Epoch [40/100], Loss: 8171.2035\n",
            "Epoch [50/100], Loss: 4873.2231\n",
            "Epoch [60/100], Loss: 4375.6736\n",
            "Epoch [70/100], Loss: 4105.8739\n",
            "Epoch [80/100], Loss: 2352.6226\n",
            "Epoch [90/100], Loss: 3253.3150\n",
            "Epoch [100/100], Loss: 2528.3094\n",
            "Fold 1, RMSE: 57.456268310546875\n",
            "Epoch [10/100], Loss: 31549.8037\n",
            "Epoch [20/100], Loss: 14069.8140\n",
            "Epoch [30/100], Loss: 16516.2732\n",
            "Epoch [40/100], Loss: 9346.8816\n",
            "Epoch [50/100], Loss: 11498.4539\n",
            "Epoch [60/100], Loss: 6988.9434\n",
            "Epoch [70/100], Loss: 5605.0371\n",
            "Epoch [80/100], Loss: 5337.0609\n",
            "Epoch [90/100], Loss: 4149.8953\n",
            "Epoch [100/100], Loss: 4604.3202\n",
            "Fold 2, RMSE: 58.60240936279297\n",
            "Epoch [10/100], Loss: 36500.2666\n",
            "Epoch [20/100], Loss: 15803.4857\n",
            "Epoch [30/100], Loss: 8959.0708\n",
            "Epoch [40/100], Loss: 7579.7485\n",
            "Epoch [50/100], Loss: 3034.4741\n",
            "Epoch [60/100], Loss: 3366.4065\n",
            "Epoch [70/100], Loss: 2356.9045\n",
            "Epoch [80/100], Loss: 1748.4138\n",
            "Epoch [90/100], Loss: 2207.0008\n",
            "Epoch [100/100], Loss: 1214.9890\n",
            "Fold 3, RMSE: 90.99353790283203\n",
            "Epoch [10/100], Loss: 18840.7974\n",
            "Epoch [20/100], Loss: 15699.7322\n",
            "Epoch [30/100], Loss: 6918.5349\n",
            "Epoch [40/100], Loss: 7855.0640\n",
            "Epoch [50/100], Loss: 5128.6482\n",
            "Epoch [60/100], Loss: 10276.3570\n",
            "Epoch [70/100], Loss: 3952.2824\n",
            "Epoch [80/100], Loss: 4766.3431\n",
            "Epoch [90/100], Loss: 5553.2808\n",
            "Epoch [100/100], Loss: 2460.0309\n",
            "Fold 4, RMSE: 39.666011810302734\n",
            "Epoch [10/100], Loss: 26716.9204\n",
            "Epoch [20/100], Loss: 11100.7855\n",
            "Epoch [30/100], Loss: 8378.8931\n",
            "Epoch [40/100], Loss: 3535.5461\n",
            "Epoch [50/100], Loss: 4289.0260\n",
            "Epoch [60/100], Loss: 3367.4925\n",
            "Epoch [70/100], Loss: 3780.0547\n",
            "Epoch [80/100], Loss: 3998.1219\n",
            "Epoch [90/100], Loss: 2546.6686\n",
            "Epoch [100/100], Loss: 1424.4882\n",
            "Fold 5, RMSE: 47.94829559326172\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 58.93330459594726\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 40281.3271\n",
            "Epoch [20/150], Loss: 10578.5410\n",
            "Epoch [30/150], Loss: 8134.3171\n",
            "Epoch [40/150], Loss: 6171.0287\n",
            "Epoch [50/150], Loss: 4981.9838\n",
            "Epoch [60/150], Loss: 3075.7081\n",
            "Epoch [70/150], Loss: 2178.7292\n",
            "Epoch [80/150], Loss: 1873.6255\n",
            "Epoch [90/150], Loss: 2816.3788\n",
            "Epoch [100/150], Loss: 1649.4476\n",
            "Epoch [110/150], Loss: 3634.5120\n",
            "Epoch [120/150], Loss: 3512.3027\n",
            "Epoch [130/150], Loss: 1321.6077\n",
            "Epoch [140/150], Loss: 3059.9539\n",
            "Epoch [150/150], Loss: 1190.2028\n",
            "Fold 1, RMSE: 61.59832763671875\n",
            "Epoch [10/150], Loss: 21246.3657\n",
            "Epoch [20/150], Loss: 10173.9966\n",
            "Epoch [30/150], Loss: 5673.0084\n",
            "Epoch [40/150], Loss: 4253.8120\n",
            "Epoch [50/150], Loss: 5048.1965\n",
            "Epoch [60/150], Loss: 3568.3153\n",
            "Epoch [70/150], Loss: 2422.4504\n",
            "Epoch [80/150], Loss: 2392.1369\n",
            "Epoch [90/150], Loss: 1975.3525\n",
            "Epoch [100/150], Loss: 2937.5127\n",
            "Epoch [110/150], Loss: 2412.4161\n",
            "Epoch [120/150], Loss: 6704.7190\n",
            "Epoch [130/150], Loss: 3429.2180\n",
            "Epoch [140/150], Loss: 3333.5679\n",
            "Epoch [150/150], Loss: 4263.6382\n",
            "Fold 2, RMSE: 71.1232681274414\n",
            "Epoch [10/150], Loss: 36525.9648\n",
            "Epoch [20/150], Loss: 9218.4987\n",
            "Epoch [30/150], Loss: 7179.7134\n",
            "Epoch [40/150], Loss: 6518.2046\n",
            "Epoch [50/150], Loss: 4812.9142\n",
            "Epoch [60/150], Loss: 6146.7379\n",
            "Epoch [70/150], Loss: 4456.5479\n",
            "Epoch [80/150], Loss: 2146.8932\n",
            "Epoch [90/150], Loss: 2003.1120\n",
            "Epoch [100/150], Loss: 2011.9611\n",
            "Epoch [110/150], Loss: 1957.2066\n",
            "Epoch [120/150], Loss: 1614.3077\n",
            "Epoch [130/150], Loss: 1361.1127\n",
            "Epoch [140/150], Loss: 5116.5395\n",
            "Epoch [150/150], Loss: 2342.7144\n",
            "Fold 3, RMSE: 89.64188385009766\n",
            "Epoch [10/150], Loss: 22951.5422\n",
            "Epoch [20/150], Loss: 9274.2201\n",
            "Epoch [30/150], Loss: 7971.3647\n",
            "Epoch [40/150], Loss: 7577.8170\n",
            "Epoch [50/150], Loss: 3656.2819\n",
            "Epoch [60/150], Loss: 1850.5168\n",
            "Epoch [70/150], Loss: 4618.9629\n",
            "Epoch [80/150], Loss: 10172.6244\n",
            "Epoch [90/150], Loss: 6823.2737\n",
            "Epoch [100/150], Loss: 5443.4677\n",
            "Epoch [110/150], Loss: 2830.4437\n",
            "Epoch [120/150], Loss: 1785.5781\n",
            "Epoch [130/150], Loss: 2439.3658\n",
            "Epoch [140/150], Loss: 2235.5559\n",
            "Epoch [150/150], Loss: 1316.0402\n",
            "Fold 4, RMSE: 40.10133361816406\n",
            "Epoch [10/150], Loss: 69824.8262\n",
            "Epoch [20/150], Loss: 13669.9309\n",
            "Epoch [30/150], Loss: 9188.6156\n",
            "Epoch [40/150], Loss: 6833.9272\n",
            "Epoch [50/150], Loss: 5162.6733\n",
            "Epoch [60/150], Loss: 4053.9378\n",
            "Epoch [70/150], Loss: 7359.4292\n",
            "Epoch [80/150], Loss: 4473.7841\n",
            "Epoch [90/150], Loss: 2701.6745\n",
            "Epoch [100/150], Loss: 3258.6696\n",
            "Epoch [110/150], Loss: 1630.0281\n",
            "Epoch [120/150], Loss: 2462.2558\n",
            "Epoch [130/150], Loss: 4611.5577\n",
            "Epoch [140/150], Loss: 1266.8773\n",
            "Epoch [150/150], Loss: 2254.8470\n",
            "Fold 5, RMSE: 44.66263198852539\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 61.425489044189455\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 15564.0876\n",
            "Epoch [20/100], Loss: 9983.5427\n",
            "Epoch [30/100], Loss: 8209.9082\n",
            "Epoch [40/100], Loss: 2935.2889\n",
            "Epoch [50/100], Loss: 5313.3378\n",
            "Epoch [60/100], Loss: 2498.7883\n",
            "Epoch [70/100], Loss: 2653.5657\n",
            "Epoch [80/100], Loss: 2127.1196\n",
            "Epoch [90/100], Loss: 1661.6435\n",
            "Epoch [100/100], Loss: 2227.9580\n",
            "Fold 1, RMSE: 56.858463287353516\n",
            "Epoch [10/100], Loss: 16012.8540\n",
            "Epoch [20/100], Loss: 8142.5696\n",
            "Epoch [30/100], Loss: 11564.8566\n",
            "Epoch [40/100], Loss: 6684.7832\n",
            "Epoch [50/100], Loss: 4311.6752\n",
            "Epoch [60/100], Loss: 2786.8870\n",
            "Epoch [70/100], Loss: 3048.7006\n",
            "Epoch [80/100], Loss: 5214.6998\n",
            "Epoch [90/100], Loss: 2335.2410\n",
            "Epoch [100/100], Loss: 4908.9315\n",
            "Fold 2, RMSE: 58.63494110107422\n",
            "Epoch [10/100], Loss: 8723.4077\n",
            "Epoch [20/100], Loss: 4794.2217\n",
            "Epoch [30/100], Loss: 4602.6677\n",
            "Epoch [40/100], Loss: 5141.6968\n",
            "Epoch [50/100], Loss: 2438.6677\n",
            "Epoch [60/100], Loss: 1604.5190\n",
            "Epoch [70/100], Loss: 2090.4755\n",
            "Epoch [80/100], Loss: 1461.5854\n",
            "Epoch [90/100], Loss: 1602.2976\n",
            "Epoch [100/100], Loss: 4437.7484\n",
            "Fold 3, RMSE: 86.62769317626953\n",
            "Epoch [10/100], Loss: 15577.1589\n",
            "Epoch [20/100], Loss: 9895.1506\n",
            "Epoch [30/100], Loss: 4557.3316\n",
            "Epoch [40/100], Loss: 2637.9537\n",
            "Epoch [50/100], Loss: 6298.3386\n",
            "Epoch [60/100], Loss: 2990.4548\n",
            "Epoch [70/100], Loss: 2299.1502\n",
            "Epoch [80/100], Loss: 1636.0923\n",
            "Epoch [90/100], Loss: 2523.6628\n",
            "Epoch [100/100], Loss: 2146.3849\n",
            "Fold 4, RMSE: 41.32008361816406\n",
            "Epoch [10/100], Loss: 13038.6460\n",
            "Epoch [20/100], Loss: 10299.8188\n",
            "Epoch [30/100], Loss: 6671.2546\n",
            "Epoch [40/100], Loss: 4805.9018\n",
            "Epoch [50/100], Loss: 2965.7535\n",
            "Epoch [60/100], Loss: 1921.2347\n",
            "Epoch [70/100], Loss: 1594.7062\n",
            "Epoch [80/100], Loss: 2780.1295\n",
            "Epoch [90/100], Loss: 1680.5482\n",
            "Epoch [100/100], Loss: 1655.2141\n",
            "Fold 5, RMSE: 44.875640869140625\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 57.66336441040039\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14584.0957\n",
            "Epoch [20/150], Loss: 10011.2957\n",
            "Epoch [30/150], Loss: 5029.7614\n",
            "Epoch [40/150], Loss: 2226.5635\n",
            "Epoch [50/150], Loss: 4564.9709\n",
            "Epoch [60/150], Loss: 2042.9386\n",
            "Epoch [70/150], Loss: 2084.0063\n",
            "Epoch [80/150], Loss: 3520.8349\n",
            "Epoch [90/150], Loss: 3423.3945\n",
            "Epoch [100/150], Loss: 1508.2833\n",
            "Epoch [110/150], Loss: 4395.0137\n",
            "Epoch [120/150], Loss: 1125.7861\n",
            "Epoch [130/150], Loss: 2017.9749\n",
            "Epoch [140/150], Loss: 1392.8701\n",
            "Epoch [150/150], Loss: 1840.5148\n",
            "Fold 1, RMSE: 62.493412017822266\n",
            "Epoch [10/150], Loss: 14339.9565\n",
            "Epoch [20/150], Loss: 7460.6395\n",
            "Epoch [30/150], Loss: 4915.3882\n",
            "Epoch [40/150], Loss: 2334.2891\n",
            "Epoch [50/150], Loss: 2984.7581\n",
            "Epoch [60/150], Loss: 2003.6740\n",
            "Epoch [70/150], Loss: 3976.7755\n",
            "Epoch [80/150], Loss: 1231.3409\n",
            "Epoch [90/150], Loss: 1864.0630\n",
            "Epoch [100/150], Loss: 1761.3809\n",
            "Epoch [110/150], Loss: 1351.1462\n",
            "Epoch [120/150], Loss: 3235.2607\n",
            "Epoch [130/150], Loss: 6463.3518\n",
            "Epoch [140/150], Loss: 2322.2426\n",
            "Epoch [150/150], Loss: 2075.2568\n",
            "Fold 2, RMSE: 67.25345611572266\n",
            "Epoch [10/150], Loss: 9375.6636\n",
            "Epoch [20/150], Loss: 6664.8540\n",
            "Epoch [30/150], Loss: 3501.1088\n",
            "Epoch [40/150], Loss: 4172.5290\n",
            "Epoch [50/150], Loss: 2721.6325\n",
            "Epoch [60/150], Loss: 1194.9459\n",
            "Epoch [70/150], Loss: 903.2274\n",
            "Epoch [80/150], Loss: 963.6958\n",
            "Epoch [90/150], Loss: 1706.6791\n",
            "Epoch [100/150], Loss: 2062.6422\n",
            "Epoch [110/150], Loss: 1576.1534\n",
            "Epoch [120/150], Loss: 823.9552\n",
            "Epoch [130/150], Loss: 722.5803\n",
            "Epoch [140/150], Loss: 908.1147\n",
            "Epoch [150/150], Loss: 2096.9747\n",
            "Fold 3, RMSE: 94.11957550048828\n",
            "Epoch [10/150], Loss: 15483.1289\n",
            "Epoch [20/150], Loss: 11484.3799\n",
            "Epoch [30/150], Loss: 8745.6369\n",
            "Epoch [40/150], Loss: 7879.4714\n",
            "Epoch [50/150], Loss: 3959.5143\n",
            "Epoch [60/150], Loss: 3740.4286\n",
            "Epoch [70/150], Loss: 3606.1984\n",
            "Epoch [80/150], Loss: 3755.9263\n",
            "Epoch [90/150], Loss: 1719.9430\n",
            "Epoch [100/150], Loss: 2649.8201\n",
            "Epoch [110/150], Loss: 1551.9538\n",
            "Epoch [120/150], Loss: 5849.6732\n",
            "Epoch [130/150], Loss: 3622.5424\n",
            "Epoch [140/150], Loss: 1716.0461\n",
            "Epoch [150/150], Loss: 4188.7810\n",
            "Fold 4, RMSE: 42.34356689453125\n",
            "Epoch [10/150], Loss: 21368.3223\n",
            "Epoch [20/150], Loss: 9840.5350\n",
            "Epoch [30/150], Loss: 10500.7366\n",
            "Epoch [40/150], Loss: 9791.3398\n",
            "Epoch [50/150], Loss: 5642.7670\n",
            "Epoch [60/150], Loss: 3416.5571\n",
            "Epoch [70/150], Loss: 4239.5652\n",
            "Epoch [80/150], Loss: 2765.1331\n",
            "Epoch [90/150], Loss: 3431.2874\n",
            "Epoch [100/150], Loss: 3077.4662\n",
            "Epoch [110/150], Loss: 2423.3586\n",
            "Epoch [120/150], Loss: 3349.4087\n",
            "Epoch [130/150], Loss: 2252.5112\n",
            "Epoch [140/150], Loss: 1930.8756\n",
            "Epoch [150/150], Loss: 4712.2534\n",
            "Fold 5, RMSE: 50.740638732910156\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 63.39012985229492\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 148716.0859\n",
            "Epoch [20/100], Loss: 13413.9459\n",
            "Epoch [30/100], Loss: 14649.2581\n",
            "Epoch [40/100], Loss: 8842.5021\n",
            "Epoch [50/100], Loss: 7408.0994\n",
            "Epoch [60/100], Loss: 6589.7438\n",
            "Epoch [70/100], Loss: 5971.1022\n",
            "Epoch [80/100], Loss: 6589.3499\n",
            "Epoch [90/100], Loss: 5966.3671\n",
            "Epoch [100/100], Loss: 5086.1150\n",
            "Fold 1, RMSE: 50.384765625\n",
            "Epoch [10/100], Loss: 377226.6406\n",
            "Epoch [20/100], Loss: 59135.7461\n",
            "Epoch [30/100], Loss: 19913.6523\n",
            "Epoch [40/100], Loss: 19905.4912\n",
            "Epoch [50/100], Loss: 14222.8364\n",
            "Epoch [60/100], Loss: 15671.5815\n",
            "Epoch [70/100], Loss: 10730.4357\n",
            "Epoch [80/100], Loss: 9386.3558\n",
            "Epoch [90/100], Loss: 11241.9727\n",
            "Epoch [100/100], Loss: 8693.4633\n",
            "Fold 2, RMSE: 65.80915069580078\n",
            "Epoch [10/100], Loss: 134136.2539\n",
            "Epoch [20/100], Loss: 30143.0850\n",
            "Epoch [30/100], Loss: 15657.9348\n",
            "Epoch [40/100], Loss: 26197.5557\n",
            "Epoch [50/100], Loss: 12626.1719\n",
            "Epoch [60/100], Loss: 13868.0188\n",
            "Epoch [70/100], Loss: 18204.4817\n",
            "Epoch [80/100], Loss: 15712.4224\n",
            "Epoch [90/100], Loss: 13230.3340\n",
            "Epoch [100/100], Loss: 13230.2338\n",
            "Fold 3, RMSE: 93.3582534790039\n",
            "Epoch [10/100], Loss: 1026269.5469\n",
            "Epoch [20/100], Loss: 115607.1191\n",
            "Epoch [30/100], Loss: 52075.5537\n",
            "Epoch [40/100], Loss: 37150.4375\n",
            "Epoch [50/100], Loss: 28916.9673\n",
            "Epoch [60/100], Loss: 31262.6235\n",
            "Epoch [70/100], Loss: 21397.8125\n",
            "Epoch [80/100], Loss: 15389.9963\n",
            "Epoch [90/100], Loss: 15917.8528\n",
            "Epoch [100/100], Loss: 14283.6230\n",
            "Fold 4, RMSE: 36.44163131713867\n",
            "Epoch [10/100], Loss: 124004.5820\n",
            "Epoch [20/100], Loss: 33704.0430\n",
            "Epoch [30/100], Loss: 19734.2378\n",
            "Epoch [40/100], Loss: 10776.5804\n",
            "Epoch [50/100], Loss: 11315.5648\n",
            "Epoch [60/100], Loss: 16265.9297\n",
            "Epoch [70/100], Loss: 11213.3989\n",
            "Epoch [80/100], Loss: 11155.9724\n",
            "Epoch [90/100], Loss: 9590.3030\n",
            "Epoch [100/100], Loss: 10180.0325\n",
            "Fold 5, RMSE: 45.896392822265625\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 58.378038787841795\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 524316.3281\n",
            "Epoch [20/150], Loss: 134360.0508\n",
            "Epoch [30/150], Loss: 55756.3945\n",
            "Epoch [40/150], Loss: 50758.5566\n",
            "Epoch [50/150], Loss: 24716.0459\n",
            "Epoch [60/150], Loss: 26288.4048\n",
            "Epoch [70/150], Loss: 18760.1558\n",
            "Epoch [80/150], Loss: 21706.7856\n",
            "Epoch [90/150], Loss: 23575.4688\n",
            "Epoch [100/150], Loss: 23873.0815\n",
            "Epoch [110/150], Loss: 19031.4783\n",
            "Epoch [120/150], Loss: 14417.2551\n",
            "Epoch [130/150], Loss: 18875.0938\n",
            "Epoch [140/150], Loss: 14333.3398\n",
            "Epoch [150/150], Loss: 11741.5144\n",
            "Fold 1, RMSE: 45.98093795776367\n",
            "Epoch [10/150], Loss: 143555.2422\n",
            "Epoch [20/150], Loss: 31657.9053\n",
            "Epoch [30/150], Loss: 16202.0322\n",
            "Epoch [40/150], Loss: 11736.1240\n",
            "Epoch [50/150], Loss: 12768.5204\n",
            "Epoch [60/150], Loss: 13110.0757\n",
            "Epoch [70/150], Loss: 17264.7507\n",
            "Epoch [80/150], Loss: 9948.3118\n",
            "Epoch [90/150], Loss: 9599.4432\n",
            "Epoch [100/150], Loss: 12298.0798\n",
            "Epoch [110/150], Loss: 17429.4525\n",
            "Epoch [120/150], Loss: 8008.3599\n",
            "Epoch [130/150], Loss: 8099.0188\n",
            "Epoch [140/150], Loss: 7957.9629\n",
            "Epoch [150/150], Loss: 6399.5555\n",
            "Fold 2, RMSE: 64.6866683959961\n",
            "Epoch [10/150], Loss: 231497.9023\n",
            "Epoch [20/150], Loss: 44048.3486\n",
            "Epoch [30/150], Loss: 24315.3574\n",
            "Epoch [40/150], Loss: 18137.1045\n",
            "Epoch [50/150], Loss: 19780.7837\n",
            "Epoch [60/150], Loss: 11182.6987\n",
            "Epoch [70/150], Loss: 10782.0776\n",
            "Epoch [80/150], Loss: 11810.1418\n",
            "Epoch [90/150], Loss: 9455.9598\n",
            "Epoch [100/150], Loss: 9627.4875\n",
            "Epoch [110/150], Loss: 10570.0400\n",
            "Epoch [120/150], Loss: 9947.1521\n",
            "Epoch [130/150], Loss: 11556.6453\n",
            "Epoch [140/150], Loss: 10681.3767\n",
            "Epoch [150/150], Loss: 12372.0178\n",
            "Fold 3, RMSE: 92.12773895263672\n",
            "Epoch [10/150], Loss: 289199.5156\n",
            "Epoch [20/150], Loss: 54700.7529\n",
            "Epoch [30/150], Loss: 26914.1724\n",
            "Epoch [40/150], Loss: 18018.1260\n",
            "Epoch [50/150], Loss: 14623.7820\n",
            "Epoch [60/150], Loss: 17053.5166\n",
            "Epoch [70/150], Loss: 14268.9768\n",
            "Epoch [80/150], Loss: 9337.4954\n",
            "Epoch [90/150], Loss: 11383.8555\n",
            "Epoch [100/150], Loss: 8921.0780\n",
            "Epoch [110/150], Loss: 4881.8985\n",
            "Epoch [120/150], Loss: 9209.9440\n",
            "Epoch [130/150], Loss: 9315.4548\n",
            "Epoch [140/150], Loss: 9400.2595\n",
            "Epoch [150/150], Loss: 3485.7645\n",
            "Fold 4, RMSE: 39.07647705078125\n",
            "Epoch [10/150], Loss: 783502.5781\n",
            "Epoch [20/150], Loss: 38846.4595\n",
            "Epoch [30/150], Loss: 27020.5132\n",
            "Epoch [40/150], Loss: 25260.3701\n",
            "Epoch [50/150], Loss: 15689.4568\n",
            "Epoch [60/150], Loss: 23259.0796\n",
            "Epoch [70/150], Loss: 13288.2708\n",
            "Epoch [80/150], Loss: 13779.8723\n",
            "Epoch [90/150], Loss: 11945.9795\n",
            "Epoch [100/150], Loss: 11740.7808\n",
            "Epoch [110/150], Loss: 9634.9314\n",
            "Epoch [120/150], Loss: 9849.7188\n",
            "Epoch [130/150], Loss: 8251.7930\n",
            "Epoch [140/150], Loss: 8062.3296\n",
            "Epoch [150/150], Loss: 7983.1938\n",
            "Fold 5, RMSE: 48.00144958496094\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 57.97465438842774\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18273.5146\n",
            "Epoch [20/100], Loss: 10723.5569\n",
            "Epoch [30/100], Loss: 7282.9429\n",
            "Epoch [40/100], Loss: 12218.1296\n",
            "Epoch [50/100], Loss: 9926.7584\n",
            "Epoch [60/100], Loss: 4535.8971\n",
            "Epoch [70/100], Loss: 3383.2386\n",
            "Epoch [80/100], Loss: 4131.8735\n",
            "Epoch [90/100], Loss: 5466.8726\n",
            "Epoch [100/100], Loss: 5246.6523\n",
            "Fold 1, RMSE: 58.113948822021484\n",
            "Epoch [10/100], Loss: 49545.1748\n",
            "Epoch [20/100], Loss: 11941.7571\n",
            "Epoch [30/100], Loss: 10143.6418\n",
            "Epoch [40/100], Loss: 10672.2646\n",
            "Epoch [50/100], Loss: 9303.6703\n",
            "Epoch [60/100], Loss: 8897.9432\n",
            "Epoch [70/100], Loss: 12245.6594\n",
            "Epoch [80/100], Loss: 9720.9998\n",
            "Epoch [90/100], Loss: 12413.8994\n",
            "Epoch [100/100], Loss: 9964.1772\n",
            "Fold 2, RMSE: 69.38094329833984\n",
            "Epoch [10/100], Loss: 11482.3153\n",
            "Epoch [20/100], Loss: 11555.5601\n",
            "Epoch [30/100], Loss: 8483.7764\n",
            "Epoch [40/100], Loss: 11505.4692\n",
            "Epoch [50/100], Loss: 7249.2632\n",
            "Epoch [60/100], Loss: 8094.4374\n",
            "Epoch [70/100], Loss: 9538.2385\n",
            "Epoch [80/100], Loss: 7090.9327\n",
            "Epoch [90/100], Loss: 8727.8990\n",
            "Epoch [100/100], Loss: 8784.1343\n",
            "Fold 3, RMSE: 91.6775894165039\n",
            "Epoch [10/100], Loss: 41481.2002\n",
            "Epoch [20/100], Loss: 19276.4360\n",
            "Epoch [30/100], Loss: 15869.9324\n",
            "Epoch [40/100], Loss: 12701.8152\n",
            "Epoch [50/100], Loss: 20735.2476\n",
            "Epoch [60/100], Loss: 13596.8862\n",
            "Epoch [70/100], Loss: 12692.2593\n",
            "Epoch [80/100], Loss: 16749.2744\n",
            "Epoch [90/100], Loss: 18677.2087\n",
            "Epoch [100/100], Loss: 11322.3152\n",
            "Fold 4, RMSE: 36.06097412109375\n",
            "Epoch [10/100], Loss: 38348.5376\n",
            "Epoch [20/100], Loss: 14373.9934\n",
            "Epoch [30/100], Loss: 16268.6814\n",
            "Epoch [40/100], Loss: 12471.5247\n",
            "Epoch [50/100], Loss: 9577.0122\n",
            "Epoch [60/100], Loss: 15386.1577\n",
            "Epoch [70/100], Loss: 13802.6084\n",
            "Epoch [80/100], Loss: 10683.4709\n",
            "Epoch [90/100], Loss: 10275.9720\n",
            "Epoch [100/100], Loss: 9991.1977\n",
            "Fold 5, RMSE: 48.355899810791016\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 60.71787109375\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 27850.5059\n",
            "Epoch [20/150], Loss: 15816.0649\n",
            "Epoch [30/150], Loss: 11927.3909\n",
            "Epoch [40/150], Loss: 14292.2158\n",
            "Epoch [50/150], Loss: 10920.4207\n",
            "Epoch [60/150], Loss: 16003.0007\n",
            "Epoch [70/150], Loss: 12936.3550\n",
            "Epoch [80/150], Loss: 11482.5808\n",
            "Epoch [90/150], Loss: 9763.9039\n",
            "Epoch [100/150], Loss: 13641.0637\n",
            "Epoch [110/150], Loss: 6722.7933\n",
            "Epoch [120/150], Loss: 9010.4128\n",
            "Epoch [130/150], Loss: 3628.4485\n",
            "Epoch [140/150], Loss: 4958.0349\n",
            "Epoch [150/150], Loss: 2741.6561\n",
            "Fold 1, RMSE: 55.60469436645508\n",
            "Epoch [10/150], Loss: 31067.0059\n",
            "Epoch [20/150], Loss: 11296.5387\n",
            "Epoch [30/150], Loss: 21198.6765\n",
            "Epoch [40/150], Loss: 10741.9856\n",
            "Epoch [50/150], Loss: 18441.7246\n",
            "Epoch [60/150], Loss: 9920.0488\n",
            "Epoch [70/150], Loss: 9281.6178\n",
            "Epoch [80/150], Loss: 17106.9370\n",
            "Epoch [90/150], Loss: 8401.5737\n",
            "Epoch [100/150], Loss: 14184.5908\n",
            "Epoch [110/150], Loss: 8260.0963\n",
            "Epoch [120/150], Loss: 10961.6211\n",
            "Epoch [130/150], Loss: 9950.9518\n",
            "Epoch [140/150], Loss: 7923.3875\n",
            "Epoch [150/150], Loss: 15429.2886\n",
            "Fold 2, RMSE: 69.87037658691406\n",
            "Epoch [10/150], Loss: 17062.3569\n",
            "Epoch [20/150], Loss: 7154.4066\n",
            "Epoch [30/150], Loss: 9356.6104\n",
            "Epoch [40/150], Loss: 8369.2356\n",
            "Epoch [50/150], Loss: 7447.0544\n",
            "Epoch [60/150], Loss: 7603.4025\n",
            "Epoch [70/150], Loss: 7409.6532\n",
            "Epoch [80/150], Loss: 7771.3579\n",
            "Epoch [90/150], Loss: 6987.6412\n",
            "Epoch [100/150], Loss: 10403.8087\n",
            "Epoch [110/150], Loss: 8717.5283\n",
            "Epoch [120/150], Loss: 6151.2883\n",
            "Epoch [130/150], Loss: 6261.4216\n",
            "Epoch [140/150], Loss: 8601.9211\n",
            "Epoch [150/150], Loss: 5490.0803\n",
            "Fold 3, RMSE: 88.67735290527344\n",
            "Epoch [10/150], Loss: 54333.4189\n",
            "Epoch [20/150], Loss: 22145.6626\n",
            "Epoch [30/150], Loss: 11507.1320\n",
            "Epoch [40/150], Loss: 13166.3855\n",
            "Epoch [50/150], Loss: 15385.7180\n",
            "Epoch [60/150], Loss: 16150.5881\n",
            "Epoch [70/150], Loss: 13124.7869\n",
            "Epoch [80/150], Loss: 11119.1010\n",
            "Epoch [90/150], Loss: 17372.7253\n",
            "Epoch [100/150], Loss: 11963.4446\n",
            "Epoch [110/150], Loss: 14855.8689\n",
            "Epoch [120/150], Loss: 10878.1752\n",
            "Epoch [130/150], Loss: 10319.9958\n",
            "Epoch [140/150], Loss: 9660.8597\n",
            "Epoch [150/150], Loss: 10982.8263\n",
            "Fold 4, RMSE: 35.7416877746582\n",
            "Epoch [10/150], Loss: 33693.5415\n",
            "Epoch [20/150], Loss: 14234.4343\n",
            "Epoch [30/150], Loss: 13028.8230\n",
            "Epoch [40/150], Loss: 12515.8389\n",
            "Epoch [50/150], Loss: 18254.6316\n",
            "Epoch [60/150], Loss: 12331.7598\n",
            "Epoch [70/150], Loss: 10422.4226\n",
            "Epoch [80/150], Loss: 14819.3979\n",
            "Epoch [90/150], Loss: 7277.6420\n",
            "Epoch [100/150], Loss: 11405.6633\n",
            "Epoch [110/150], Loss: 8317.3574\n",
            "Epoch [120/150], Loss: 8661.4329\n",
            "Epoch [130/150], Loss: 5366.3625\n",
            "Epoch [140/150], Loss: 5854.6859\n",
            "Epoch [150/150], Loss: 6897.0120\n",
            "Fold 5, RMSE: 47.36383819580078\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 59.45158996582031\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 393885.2109\n",
            "Epoch [20/100], Loss: 50451.2051\n",
            "Epoch [30/100], Loss: 36687.0303\n",
            "Epoch [40/100], Loss: 21133.1606\n",
            "Epoch [50/100], Loss: 24469.4150\n",
            "Epoch [60/100], Loss: 17591.6431\n",
            "Epoch [70/100], Loss: 17294.7900\n",
            "Epoch [80/100], Loss: 19757.2805\n",
            "Epoch [90/100], Loss: 26479.3540\n",
            "Epoch [100/100], Loss: 11884.1323\n",
            "Fold 1, RMSE: 42.703590393066406\n",
            "Epoch [10/100], Loss: 465699.4688\n",
            "Epoch [20/100], Loss: 42533.8711\n",
            "Epoch [30/100], Loss: 24845.7476\n",
            "Epoch [40/100], Loss: 22007.3877\n",
            "Epoch [50/100], Loss: 13831.2542\n",
            "Epoch [60/100], Loss: 12912.0474\n",
            "Epoch [70/100], Loss: 17132.9329\n",
            "Epoch [80/100], Loss: 10056.4268\n",
            "Epoch [90/100], Loss: 10943.8938\n",
            "Epoch [100/100], Loss: 12651.5059\n",
            "Fold 2, RMSE: 67.95448303222656\n",
            "Epoch [10/100], Loss: 273418.9219\n",
            "Epoch [20/100], Loss: 52607.4150\n",
            "Epoch [30/100], Loss: 32761.5811\n",
            "Epoch [40/100], Loss: 17579.6201\n",
            "Epoch [50/100], Loss: 15842.3765\n",
            "Epoch [60/100], Loss: 18148.3711\n",
            "Epoch [70/100], Loss: 14985.0818\n",
            "Epoch [80/100], Loss: 13189.4453\n",
            "Epoch [90/100], Loss: 13026.7827\n",
            "Epoch [100/100], Loss: 8979.4584\n",
            "Fold 3, RMSE: 91.85658264160156\n",
            "Epoch [10/100], Loss: 323015.0312\n",
            "Epoch [20/100], Loss: 37594.7217\n",
            "Epoch [30/100], Loss: 17787.2012\n",
            "Epoch [40/100], Loss: 12115.1299\n",
            "Epoch [50/100], Loss: 14306.2651\n",
            "Epoch [60/100], Loss: 11191.3625\n",
            "Epoch [70/100], Loss: 17060.1917\n",
            "Epoch [80/100], Loss: 9095.3287\n",
            "Epoch [90/100], Loss: 9675.2330\n",
            "Epoch [100/100], Loss: 8001.7782\n",
            "Fold 4, RMSE: 34.1553840637207\n",
            "Epoch [10/100], Loss: 415914.8984\n",
            "Epoch [20/100], Loss: 51836.6123\n",
            "Epoch [30/100], Loss: 29941.2266\n",
            "Epoch [40/100], Loss: 23471.2246\n",
            "Epoch [50/100], Loss: 17968.8574\n",
            "Epoch [60/100], Loss: 16651.0178\n",
            "Epoch [70/100], Loss: 9575.0496\n",
            "Epoch [80/100], Loss: 19650.2285\n",
            "Epoch [90/100], Loss: 14257.5591\n",
            "Epoch [100/100], Loss: 12171.1272\n",
            "Fold 5, RMSE: 46.53224563598633\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 56.64045715332031\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 373747.2891\n",
            "Epoch [20/150], Loss: 119831.2812\n",
            "Epoch [30/150], Loss: 29504.0117\n",
            "Epoch [40/150], Loss: 25166.5181\n",
            "Epoch [50/150], Loss: 13954.5178\n",
            "Epoch [60/150], Loss: 22062.4219\n",
            "Epoch [70/150], Loss: 20793.4912\n",
            "Epoch [80/150], Loss: 13890.1555\n",
            "Epoch [90/150], Loss: 13978.2483\n",
            "Epoch [100/150], Loss: 16698.1614\n",
            "Epoch [110/150], Loss: 27294.6572\n",
            "Epoch [120/150], Loss: 15061.8430\n",
            "Epoch [130/150], Loss: 15150.6226\n",
            "Epoch [140/150], Loss: 11618.8333\n",
            "Epoch [150/150], Loss: 16827.8955\n",
            "Fold 1, RMSE: 45.53064727783203\n",
            "Epoch [10/150], Loss: 249702.1016\n",
            "Epoch [20/150], Loss: 86962.6211\n",
            "Epoch [30/150], Loss: 38027.6123\n",
            "Epoch [40/150], Loss: 31944.2764\n",
            "Epoch [50/150], Loss: 27054.4551\n",
            "Epoch [60/150], Loss: 17498.8076\n",
            "Epoch [70/150], Loss: 14192.5525\n",
            "Epoch [80/150], Loss: 14376.0076\n",
            "Epoch [90/150], Loss: 12321.6089\n",
            "Epoch [100/150], Loss: 13105.0277\n",
            "Epoch [110/150], Loss: 15025.5471\n",
            "Epoch [120/150], Loss: 10175.1088\n",
            "Epoch [130/150], Loss: 11895.5902\n",
            "Epoch [140/150], Loss: 20790.9319\n",
            "Epoch [150/150], Loss: 11154.7834\n",
            "Fold 2, RMSE: 68.5041732788086\n",
            "Epoch [10/150], Loss: 273765.1953\n",
            "Epoch [20/150], Loss: 56656.2598\n",
            "Epoch [30/150], Loss: 34344.2588\n",
            "Epoch [40/150], Loss: 19176.8037\n",
            "Epoch [50/150], Loss: 23428.5220\n",
            "Epoch [60/150], Loss: 11156.2373\n",
            "Epoch [70/150], Loss: 11271.0637\n",
            "Epoch [80/150], Loss: 12564.3684\n",
            "Epoch [90/150], Loss: 14348.9438\n",
            "Epoch [100/150], Loss: 15890.0696\n",
            "Epoch [110/150], Loss: 13169.8945\n",
            "Epoch [120/150], Loss: 14775.6487\n",
            "Epoch [130/150], Loss: 7834.0253\n",
            "Epoch [140/150], Loss: 7364.2855\n",
            "Epoch [150/150], Loss: 7731.3802\n",
            "Fold 3, RMSE: 91.0383529663086\n",
            "Epoch [10/150], Loss: 384494.0781\n",
            "Epoch [20/150], Loss: 91930.2217\n",
            "Epoch [30/150], Loss: 52386.9570\n",
            "Epoch [40/150], Loss: 23720.3799\n",
            "Epoch [50/150], Loss: 23266.2930\n",
            "Epoch [60/150], Loss: 22940.6304\n",
            "Epoch [70/150], Loss: 21467.0220\n",
            "Epoch [80/150], Loss: 25404.7129\n",
            "Epoch [90/150], Loss: 16713.1182\n",
            "Epoch [100/150], Loss: 13829.4795\n",
            "Epoch [110/150], Loss: 14142.9246\n",
            "Epoch [120/150], Loss: 12954.5049\n",
            "Epoch [130/150], Loss: 12741.0007\n",
            "Epoch [140/150], Loss: 9351.4290\n",
            "Epoch [150/150], Loss: 13782.9412\n",
            "Fold 4, RMSE: 36.47625732421875\n",
            "Epoch [10/150], Loss: 58872.1914\n",
            "Epoch [20/150], Loss: 24744.6309\n",
            "Epoch [30/150], Loss: 22369.6206\n",
            "Epoch [40/150], Loss: 13456.5432\n",
            "Epoch [50/150], Loss: 25398.6899\n",
            "Epoch [60/150], Loss: 11245.3662\n",
            "Epoch [70/150], Loss: 15074.4604\n",
            "Epoch [80/150], Loss: 13176.1812\n",
            "Epoch [90/150], Loss: 10239.0078\n",
            "Epoch [100/150], Loss: 7638.9827\n",
            "Epoch [110/150], Loss: 12965.6724\n",
            "Epoch [120/150], Loss: 15669.1755\n",
            "Epoch [130/150], Loss: 10601.5359\n",
            "Epoch [140/150], Loss: 8968.3855\n",
            "Epoch [150/150], Loss: 11550.3658\n",
            "Fold 5, RMSE: 45.594383239746094\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 57.42876281738281\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 51007.1807\n",
            "Epoch [20/100], Loss: 25906.8447\n",
            "Epoch [30/100], Loss: 13648.6213\n",
            "Epoch [40/100], Loss: 22979.7559\n",
            "Epoch [50/100], Loss: 18888.8960\n",
            "Epoch [60/100], Loss: 13971.1063\n",
            "Epoch [70/100], Loss: 11103.9653\n",
            "Epoch [80/100], Loss: 14127.4949\n",
            "Epoch [90/100], Loss: 19152.0635\n",
            "Epoch [100/100], Loss: 11394.4658\n",
            "Fold 1, RMSE: 43.655311584472656\n",
            "Epoch [10/100], Loss: 47995.7334\n",
            "Epoch [20/100], Loss: 15549.2905\n",
            "Epoch [30/100], Loss: 11729.6533\n",
            "Epoch [40/100], Loss: 8727.6282\n",
            "Epoch [50/100], Loss: 11212.1636\n",
            "Epoch [60/100], Loss: 11168.1714\n",
            "Epoch [70/100], Loss: 13337.0713\n",
            "Epoch [80/100], Loss: 10588.7612\n",
            "Epoch [90/100], Loss: 13377.5972\n",
            "Epoch [100/100], Loss: 9872.1416\n",
            "Fold 2, RMSE: 72.82740783691406\n",
            "Epoch [10/100], Loss: 29711.6504\n",
            "Epoch [20/100], Loss: 13531.0906\n",
            "Epoch [30/100], Loss: 9904.8662\n",
            "Epoch [40/100], Loss: 9440.1609\n",
            "Epoch [50/100], Loss: 8528.7841\n",
            "Epoch [60/100], Loss: 8859.1606\n",
            "Epoch [70/100], Loss: 7331.6431\n",
            "Epoch [80/100], Loss: 7201.5070\n",
            "Epoch [90/100], Loss: 7485.2509\n",
            "Epoch [100/100], Loss: 7336.3210\n",
            "Fold 3, RMSE: 92.88614654541016\n",
            "Epoch [10/100], Loss: 43297.8662\n",
            "Epoch [20/100], Loss: 14059.2109\n",
            "Epoch [30/100], Loss: 13962.6890\n",
            "Epoch [40/100], Loss: 15133.3139\n",
            "Epoch [50/100], Loss: 16967.9106\n",
            "Epoch [60/100], Loss: 10716.5831\n",
            "Epoch [70/100], Loss: 12053.8477\n",
            "Epoch [80/100], Loss: 13591.7397\n",
            "Epoch [90/100], Loss: 11099.8506\n",
            "Epoch [100/100], Loss: 12696.8346\n",
            "Fold 4, RMSE: 36.893646240234375\n",
            "Epoch [10/100], Loss: 52836.3599\n",
            "Epoch [20/100], Loss: 12278.8997\n",
            "Epoch [30/100], Loss: 13463.4397\n",
            "Epoch [40/100], Loss: 13889.4956\n",
            "Epoch [50/100], Loss: 12745.7494\n",
            "Epoch [60/100], Loss: 10341.0145\n",
            "Epoch [70/100], Loss: 13458.2793\n",
            "Epoch [80/100], Loss: 11050.8091\n",
            "Epoch [90/100], Loss: 15497.8909\n",
            "Epoch [100/100], Loss: 9873.3540\n",
            "Fold 5, RMSE: 48.919677734375\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 59.03643798828125\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 28258.5981\n",
            "Epoch [20/150], Loss: 12536.9822\n",
            "Epoch [30/150], Loss: 11708.5503\n",
            "Epoch [40/150], Loss: 11209.5732\n",
            "Epoch [50/150], Loss: 11705.1833\n",
            "Epoch [60/150], Loss: 9934.6250\n",
            "Epoch [70/150], Loss: 10047.6893\n",
            "Epoch [80/150], Loss: 10821.3535\n",
            "Epoch [90/150], Loss: 10821.3079\n",
            "Epoch [100/150], Loss: 11603.5515\n",
            "Epoch [110/150], Loss: 16294.9218\n",
            "Epoch [120/150], Loss: 10770.2041\n",
            "Epoch [130/150], Loss: 11829.0623\n",
            "Epoch [140/150], Loss: 9023.3090\n",
            "Epoch [150/150], Loss: 9444.2025\n",
            "Fold 1, RMSE: 45.99257278442383\n",
            "Epoch [10/150], Loss: 31858.6187\n",
            "Epoch [20/150], Loss: 14456.2815\n",
            "Epoch [30/150], Loss: 15616.9241\n",
            "Epoch [40/150], Loss: 12443.7417\n",
            "Epoch [50/150], Loss: 11437.9897\n",
            "Epoch [60/150], Loss: 11280.0553\n",
            "Epoch [70/150], Loss: 9977.4426\n",
            "Epoch [80/150], Loss: 9585.3899\n",
            "Epoch [90/150], Loss: 10397.7217\n",
            "Epoch [100/150], Loss: 11939.1445\n",
            "Epoch [110/150], Loss: 10972.7644\n",
            "Epoch [120/150], Loss: 14288.4398\n",
            "Epoch [130/150], Loss: 9403.1345\n",
            "Epoch [140/150], Loss: 11433.7776\n",
            "Epoch [150/150], Loss: 8115.6554\n",
            "Fold 2, RMSE: 68.1904067993164\n",
            "Epoch [10/150], Loss: 37659.5527\n",
            "Epoch [20/150], Loss: 11489.2581\n",
            "Epoch [30/150], Loss: 6386.4435\n",
            "Epoch [40/150], Loss: 7498.2100\n",
            "Epoch [50/150], Loss: 11242.7043\n",
            "Epoch [60/150], Loss: 7411.6000\n",
            "Epoch [70/150], Loss: 7362.0343\n",
            "Epoch [80/150], Loss: 8022.7351\n",
            "Epoch [90/150], Loss: 5490.3368\n",
            "Epoch [100/150], Loss: 8101.2767\n",
            "Epoch [110/150], Loss: 10023.4967\n",
            "Epoch [120/150], Loss: 6591.8287\n",
            "Epoch [130/150], Loss: 6375.7057\n",
            "Epoch [140/150], Loss: 7230.8123\n",
            "Epoch [150/150], Loss: 9819.3408\n",
            "Fold 3, RMSE: 93.2668228149414\n",
            "Epoch [10/150], Loss: 60830.8691\n",
            "Epoch [20/150], Loss: 20676.0234\n",
            "Epoch [30/150], Loss: 14461.8940\n",
            "Epoch [40/150], Loss: 12032.8899\n",
            "Epoch [50/150], Loss: 15785.4644\n",
            "Epoch [60/150], Loss: 12750.6846\n",
            "Epoch [70/150], Loss: 14143.0415\n",
            "Epoch [80/150], Loss: 12197.7726\n",
            "Epoch [90/150], Loss: 13681.3623\n",
            "Epoch [100/150], Loss: 12672.0493\n",
            "Epoch [110/150], Loss: 10132.9106\n",
            "Epoch [120/150], Loss: 12997.4348\n",
            "Epoch [130/150], Loss: 12970.8848\n",
            "Epoch [140/150], Loss: 10616.2144\n",
            "Epoch [150/150], Loss: 16121.8992\n",
            "Fold 4, RMSE: 37.60014343261719\n",
            "Epoch [10/150], Loss: 42462.6494\n",
            "Epoch [20/150], Loss: 18470.1807\n",
            "Epoch [30/150], Loss: 12540.2805\n",
            "Epoch [40/150], Loss: 10153.2328\n",
            "Epoch [50/150], Loss: 12590.9541\n",
            "Epoch [60/150], Loss: 14583.9573\n",
            "Epoch [70/150], Loss: 10359.0292\n",
            "Epoch [80/150], Loss: 11105.1716\n",
            "Epoch [90/150], Loss: 10211.6733\n",
            "Epoch [100/150], Loss: 15037.6467\n",
            "Epoch [110/150], Loss: 12081.0342\n",
            "Epoch [120/150], Loss: 9299.0734\n",
            "Epoch [130/150], Loss: 7970.7422\n",
            "Epoch [140/150], Loss: 9632.8719\n",
            "Epoch [150/150], Loss: 7152.2327\n",
            "Fold 5, RMSE: 44.79209899902344\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 57.96840896606445\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 259988.9531\n",
            "Epoch [20/100], Loss: 90680.5645\n",
            "Epoch [30/100], Loss: 35144.0391\n",
            "Epoch [40/100], Loss: 21524.9808\n",
            "Epoch [50/100], Loss: 19662.1809\n",
            "Epoch [60/100], Loss: 18539.7612\n",
            "Epoch [70/100], Loss: 16184.5007\n",
            "Epoch [80/100], Loss: 15040.8774\n",
            "Epoch [90/100], Loss: 14326.8069\n",
            "Epoch [100/100], Loss: 15916.0027\n",
            "Fold 1, RMSE: 46.11836242675781\n",
            "Epoch [10/100], Loss: 410637.3945\n",
            "Epoch [20/100], Loss: 51775.3662\n",
            "Epoch [30/100], Loss: 30586.3340\n",
            "Epoch [40/100], Loss: 26087.0962\n",
            "Epoch [50/100], Loss: 24221.4692\n",
            "Epoch [60/100], Loss: 16217.7095\n",
            "Epoch [70/100], Loss: 13046.6726\n",
            "Epoch [80/100], Loss: 14731.5505\n",
            "Epoch [90/100], Loss: 17169.1318\n",
            "Epoch [100/100], Loss: 11742.3137\n",
            "Fold 2, RMSE: 68.88917541503906\n",
            "Epoch [10/100], Loss: 425801.4531\n",
            "Epoch [20/100], Loss: 55136.5840\n",
            "Epoch [30/100], Loss: 47609.9678\n",
            "Epoch [40/100], Loss: 31370.2388\n",
            "Epoch [50/100], Loss: 10069.4419\n",
            "Epoch [60/100], Loss: 17651.1130\n",
            "Epoch [70/100], Loss: 16035.3521\n",
            "Epoch [80/100], Loss: 10410.2930\n",
            "Epoch [90/100], Loss: 16833.3457\n",
            "Epoch [100/100], Loss: 9812.8578\n",
            "Fold 3, RMSE: 92.30445861816406\n",
            "Epoch [10/100], Loss: 417763.2031\n",
            "Epoch [20/100], Loss: 59072.8879\n",
            "Epoch [30/100], Loss: 41063.8286\n",
            "Epoch [40/100], Loss: 41522.7729\n",
            "Epoch [50/100], Loss: 34607.3672\n",
            "Epoch [60/100], Loss: 27586.4302\n",
            "Epoch [70/100], Loss: 22334.4541\n",
            "Epoch [80/100], Loss: 21648.4697\n",
            "Epoch [90/100], Loss: 23293.7661\n",
            "Epoch [100/100], Loss: 15870.6270\n",
            "Fold 4, RMSE: 35.03194808959961\n",
            "Epoch [10/100], Loss: 150699.9043\n",
            "Epoch [20/100], Loss: 43784.3691\n",
            "Epoch [30/100], Loss: 22085.4937\n",
            "Epoch [40/100], Loss: 21267.7988\n",
            "Epoch [50/100], Loss: 14343.6641\n",
            "Epoch [60/100], Loss: 17676.0942\n",
            "Epoch [70/100], Loss: 16586.3896\n",
            "Epoch [80/100], Loss: 14008.6831\n",
            "Epoch [90/100], Loss: 10929.4011\n",
            "Epoch [100/100], Loss: 10397.7964\n",
            "Fold 5, RMSE: 46.81873321533203\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 57.83253555297851\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 455313.0391\n",
            "Epoch [20/150], Loss: 45272.8857\n",
            "Epoch [30/150], Loss: 32390.7119\n",
            "Epoch [40/150], Loss: 45234.5508\n",
            "Epoch [50/150], Loss: 34086.4780\n",
            "Epoch [60/150], Loss: 30129.5435\n",
            "Epoch [70/150], Loss: 16756.6362\n",
            "Epoch [80/150], Loss: 20243.1162\n",
            "Epoch [90/150], Loss: 13172.0073\n",
            "Epoch [100/150], Loss: 18746.1606\n",
            "Epoch [110/150], Loss: 14168.9590\n",
            "Epoch [120/150], Loss: 15440.4277\n",
            "Epoch [130/150], Loss: 10001.7911\n",
            "Epoch [140/150], Loss: 18262.1963\n",
            "Epoch [150/150], Loss: 10367.8134\n",
            "Fold 1, RMSE: 45.37303161621094\n",
            "Epoch [10/150], Loss: 222775.9648\n",
            "Epoch [20/150], Loss: 44761.3623\n",
            "Epoch [30/150], Loss: 26319.9307\n",
            "Epoch [40/150], Loss: 15269.5349\n",
            "Epoch [50/150], Loss: 10384.7377\n",
            "Epoch [60/150], Loss: 20951.3757\n",
            "Epoch [70/150], Loss: 13057.4236\n",
            "Epoch [80/150], Loss: 13950.4395\n",
            "Epoch [90/150], Loss: 9010.4343\n",
            "Epoch [100/150], Loss: 9755.3164\n",
            "Epoch [110/150], Loss: 11213.2747\n",
            "Epoch [120/150], Loss: 10428.6406\n",
            "Epoch [130/150], Loss: 7575.2886\n",
            "Epoch [140/150], Loss: 6451.3066\n",
            "Epoch [150/150], Loss: 5406.9084\n",
            "Fold 2, RMSE: 64.0339126586914\n",
            "Epoch [10/150], Loss: 307534.6406\n",
            "Epoch [20/150], Loss: 47176.3926\n",
            "Epoch [30/150], Loss: 37540.9912\n",
            "Epoch [40/150], Loss: 36190.1333\n",
            "Epoch [50/150], Loss: 14767.8672\n",
            "Epoch [60/150], Loss: 16314.6985\n",
            "Epoch [70/150], Loss: 9023.9512\n",
            "Epoch [80/150], Loss: 11037.0627\n",
            "Epoch [90/150], Loss: 8104.8535\n",
            "Epoch [100/150], Loss: 11383.2390\n",
            "Epoch [110/150], Loss: 9821.4380\n",
            "Epoch [120/150], Loss: 11771.9810\n",
            "Epoch [130/150], Loss: 12079.0635\n",
            "Epoch [140/150], Loss: 6212.0295\n",
            "Epoch [150/150], Loss: 8083.2948\n",
            "Fold 3, RMSE: 90.42057037353516\n",
            "Epoch [10/150], Loss: 377159.6562\n",
            "Epoch [20/150], Loss: 72676.2031\n",
            "Epoch [30/150], Loss: 42492.4180\n",
            "Epoch [40/150], Loss: 33040.7656\n",
            "Epoch [50/150], Loss: 23987.5166\n",
            "Epoch [60/150], Loss: 24528.8774\n",
            "Epoch [70/150], Loss: 10130.5632\n",
            "Epoch [80/150], Loss: 12161.8901\n",
            "Epoch [90/150], Loss: 17983.7859\n",
            "Epoch [100/150], Loss: 16429.8706\n",
            "Epoch [110/150], Loss: 14940.1138\n",
            "Epoch [120/150], Loss: 19071.4683\n",
            "Epoch [130/150], Loss: 17721.0337\n",
            "Epoch [140/150], Loss: 16953.3804\n",
            "Epoch [150/150], Loss: 13548.1389\n",
            "Fold 4, RMSE: 34.2816162109375\n",
            "Epoch [10/150], Loss: 305727.0273\n",
            "Epoch [20/150], Loss: 37770.5098\n",
            "Epoch [30/150], Loss: 29696.5850\n",
            "Epoch [40/150], Loss: 15232.6890\n",
            "Epoch [50/150], Loss: 15971.6528\n",
            "Epoch [60/150], Loss: 12567.5001\n",
            "Epoch [70/150], Loss: 16183.7623\n",
            "Epoch [80/150], Loss: 10743.5952\n",
            "Epoch [90/150], Loss: 9370.3768\n",
            "Epoch [100/150], Loss: 12332.9148\n",
            "Epoch [110/150], Loss: 10655.8289\n",
            "Epoch [120/150], Loss: 5061.0935\n",
            "Epoch [130/150], Loss: 5658.7540\n",
            "Epoch [140/150], Loss: 8145.2914\n",
            "Epoch [150/150], Loss: 3505.0603\n",
            "Fold 5, RMSE: 51.11210632324219\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 57.044247436523435\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 33222.8047\n",
            "Epoch [20/100], Loss: 13112.9321\n",
            "Epoch [30/100], Loss: 12840.4094\n",
            "Epoch [40/100], Loss: 19672.1836\n",
            "Epoch [50/100], Loss: 11970.7070\n",
            "Epoch [60/100], Loss: 10539.2266\n",
            "Epoch [70/100], Loss: 11116.4246\n",
            "Epoch [80/100], Loss: 13665.3606\n",
            "Epoch [90/100], Loss: 11495.5120\n",
            "Epoch [100/100], Loss: 10777.5840\n",
            "Fold 1, RMSE: 48.723567962646484\n",
            "Epoch [10/100], Loss: 49706.4697\n",
            "Epoch [20/100], Loss: 12054.1985\n",
            "Epoch [30/100], Loss: 19236.8284\n",
            "Epoch [40/100], Loss: 15036.9084\n",
            "Epoch [50/100], Loss: 11901.3230\n",
            "Epoch [60/100], Loss: 12413.2698\n",
            "Epoch [70/100], Loss: 9079.0040\n",
            "Epoch [80/100], Loss: 11078.3025\n",
            "Epoch [90/100], Loss: 18682.0814\n",
            "Epoch [100/100], Loss: 9091.2708\n",
            "Fold 2, RMSE: 69.9100112915039\n",
            "Epoch [10/100], Loss: 64487.9673\n",
            "Epoch [20/100], Loss: 16463.3359\n",
            "Epoch [30/100], Loss: 9417.2932\n",
            "Epoch [40/100], Loss: 11343.7864\n",
            "Epoch [50/100], Loss: 12225.4946\n",
            "Epoch [60/100], Loss: 12306.2168\n",
            "Epoch [70/100], Loss: 15311.2676\n",
            "Epoch [80/100], Loss: 15199.3223\n",
            "Epoch [90/100], Loss: 12175.5725\n",
            "Epoch [100/100], Loss: 10577.9757\n",
            "Fold 3, RMSE: 89.09478759765625\n",
            "Epoch [10/100], Loss: 42026.9834\n",
            "Epoch [20/100], Loss: 19657.3530\n",
            "Epoch [30/100], Loss: 11927.5579\n",
            "Epoch [40/100], Loss: 10963.3317\n",
            "Epoch [50/100], Loss: 21138.2715\n",
            "Epoch [60/100], Loss: 13697.6172\n",
            "Epoch [70/100], Loss: 11754.5455\n",
            "Epoch [80/100], Loss: 18142.8948\n",
            "Epoch [90/100], Loss: 12109.8625\n",
            "Epoch [100/100], Loss: 12769.8894\n",
            "Fold 4, RMSE: 35.6133918762207\n",
            "Epoch [10/100], Loss: 36353.0459\n",
            "Epoch [20/100], Loss: 16941.7087\n",
            "Epoch [30/100], Loss: 22514.1924\n",
            "Epoch [40/100], Loss: 16000.3760\n",
            "Epoch [50/100], Loss: 14681.5405\n",
            "Epoch [60/100], Loss: 13509.6675\n",
            "Epoch [70/100], Loss: 13154.8372\n",
            "Epoch [80/100], Loss: 14130.1267\n",
            "Epoch [90/100], Loss: 13121.5063\n",
            "Epoch [100/100], Loss: 15774.8511\n",
            "Fold 5, RMSE: 46.44076919555664\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 57.956505584716794\n",
            "Training with neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 85984.3789\n",
            "Epoch [20/150], Loss: 17076.6235\n",
            "Epoch [30/150], Loss: 12351.5515\n",
            "Epoch [40/150], Loss: 18883.3953\n",
            "Epoch [50/150], Loss: 14634.0796\n",
            "Epoch [60/150], Loss: 12286.6194\n",
            "Epoch [70/150], Loss: 13035.9409\n",
            "Epoch [80/150], Loss: 14401.3210\n",
            "Epoch [90/150], Loss: 13078.8975\n",
            "Epoch [100/150], Loss: 10477.9709\n",
            "Epoch [110/150], Loss: 13348.9521\n",
            "Epoch [120/150], Loss: 12778.1177\n",
            "Epoch [130/150], Loss: 12312.5247\n",
            "Epoch [140/150], Loss: 13905.2676\n",
            "Epoch [150/150], Loss: 10716.2999\n",
            "Fold 1, RMSE: 46.90119171142578\n",
            "Epoch [10/150], Loss: 42118.5225\n",
            "Epoch [20/150], Loss: 15840.4424\n",
            "Epoch [30/150], Loss: 10270.0762\n",
            "Epoch [40/150], Loss: 11424.8162\n",
            "Epoch [50/150], Loss: 12980.6096\n",
            "Epoch [60/150], Loss: 10493.3580\n",
            "Epoch [70/150], Loss: 11701.8850\n",
            "Epoch [80/150], Loss: 9958.2695\n",
            "Epoch [90/150], Loss: 11043.4053\n",
            "Epoch [100/150], Loss: 13993.0593\n",
            "Epoch [110/150], Loss: 14878.8223\n",
            "Epoch [120/150], Loss: 12400.7120\n",
            "Epoch [130/150], Loss: 9921.4351\n",
            "Epoch [140/150], Loss: 7655.5836\n",
            "Epoch [150/150], Loss: 10718.6343\n",
            "Fold 2, RMSE: 68.01778411865234\n",
            "Epoch [10/150], Loss: 86271.8730\n",
            "Epoch [20/150], Loss: 17792.3740\n",
            "Epoch [30/150], Loss: 12365.4409\n",
            "Epoch [40/150], Loss: 14210.8093\n",
            "Epoch [50/150], Loss: 9353.0181\n",
            "Epoch [60/150], Loss: 8608.8918\n",
            "Epoch [70/150], Loss: 7189.2898\n",
            "Epoch [80/150], Loss: 7748.5194\n",
            "Epoch [90/150], Loss: 9792.7273\n",
            "Epoch [100/150], Loss: 8092.6680\n",
            "Epoch [110/150], Loss: 7458.0186\n",
            "Epoch [120/150], Loss: 8546.5540\n",
            "Epoch [130/150], Loss: 9689.8669\n",
            "Epoch [140/150], Loss: 7256.8873\n",
            "Epoch [150/150], Loss: 9919.6180\n",
            "Fold 3, RMSE: 88.96290588378906\n",
            "Epoch [10/150], Loss: 70195.0000\n",
            "Epoch [20/150], Loss: 18286.5605\n",
            "Epoch [30/150], Loss: 15789.2922\n",
            "Epoch [40/150], Loss: 17380.8613\n",
            "Epoch [50/150], Loss: 11603.5247\n",
            "Epoch [60/150], Loss: 12148.3206\n",
            "Epoch [70/150], Loss: 11486.2273\n",
            "Epoch [80/150], Loss: 11489.6938\n",
            "Epoch [90/150], Loss: 13809.1619\n",
            "Epoch [100/150], Loss: 13241.1934\n",
            "Epoch [110/150], Loss: 11855.5596\n",
            "Epoch [120/150], Loss: 9842.2515\n",
            "Epoch [130/150], Loss: 9983.8687\n",
            "Epoch [140/150], Loss: 13249.1453\n",
            "Epoch [150/150], Loss: 11578.5583\n",
            "Fold 4, RMSE: 34.74962615966797\n",
            "Epoch [10/150], Loss: 62450.3086\n",
            "Epoch [20/150], Loss: 12725.5068\n",
            "Epoch [30/150], Loss: 16871.7214\n",
            "Epoch [40/150], Loss: 14718.3755\n",
            "Epoch [50/150], Loss: 12789.4951\n",
            "Epoch [60/150], Loss: 12008.2097\n",
            "Epoch [70/150], Loss: 14138.2642\n",
            "Epoch [80/150], Loss: 12155.8271\n",
            "Epoch [90/150], Loss: 10287.6318\n",
            "Epoch [100/150], Loss: 9786.6263\n",
            "Epoch [110/150], Loss: 9922.7366\n",
            "Epoch [120/150], Loss: 9772.9973\n",
            "Epoch [130/150], Loss: 10025.6284\n",
            "Epoch [140/150], Loss: 9088.5859\n",
            "Epoch [150/150], Loss: 8517.0835\n",
            "Fold 5, RMSE: 44.703041076660156\n",
            "Avg RMSE for neurons=96, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 56.666909790039064\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 12888.3904\n",
            "Epoch [20/100], Loss: 9152.8369\n",
            "Epoch [30/100], Loss: 8021.2780\n",
            "Epoch [40/100], Loss: 5932.6130\n",
            "Epoch [50/100], Loss: 4278.8265\n",
            "Epoch [60/100], Loss: 6315.0393\n",
            "Epoch [70/100], Loss: 4018.1302\n",
            "Epoch [80/100], Loss: 5948.9813\n",
            "Epoch [90/100], Loss: 8658.3680\n",
            "Epoch [100/100], Loss: 5655.0995\n",
            "Fold 1, RMSE: 57.797019958496094\n",
            "Epoch [10/100], Loss: 13101.0146\n",
            "Epoch [20/100], Loss: 11429.2189\n",
            "Epoch [30/100], Loss: 7900.4277\n",
            "Epoch [40/100], Loss: 5183.5715\n",
            "Epoch [50/100], Loss: 6870.2166\n",
            "Epoch [60/100], Loss: 8452.9649\n",
            "Epoch [70/100], Loss: 3803.1118\n",
            "Epoch [80/100], Loss: 5132.0894\n",
            "Epoch [90/100], Loss: 6594.2818\n",
            "Epoch [100/100], Loss: 2971.3712\n",
            "Fold 2, RMSE: 70.84465026855469\n",
            "Epoch [10/100], Loss: 12877.8594\n",
            "Epoch [20/100], Loss: 7760.7435\n",
            "Epoch [30/100], Loss: 9423.9160\n",
            "Epoch [40/100], Loss: 5834.5441\n",
            "Epoch [50/100], Loss: 5121.0909\n",
            "Epoch [60/100], Loss: 3225.6556\n",
            "Epoch [70/100], Loss: 4062.6026\n",
            "Epoch [80/100], Loss: 2783.5847\n",
            "Epoch [90/100], Loss: 1640.2427\n",
            "Epoch [100/100], Loss: 3396.8113\n",
            "Fold 3, RMSE: 86.65582275390625\n",
            "Epoch [10/100], Loss: 18538.6313\n",
            "Epoch [20/100], Loss: 12204.9209\n",
            "Epoch [30/100], Loss: 9788.8312\n",
            "Epoch [40/100], Loss: 6463.4067\n",
            "Epoch [50/100], Loss: 8194.5022\n",
            "Epoch [60/100], Loss: 4529.9482\n",
            "Epoch [70/100], Loss: 5339.2900\n",
            "Epoch [80/100], Loss: 9165.1949\n",
            "Epoch [90/100], Loss: 7238.8942\n",
            "Epoch [100/100], Loss: 5038.6278\n",
            "Fold 4, RMSE: 43.966190338134766\n",
            "Epoch [10/100], Loss: 21938.0303\n",
            "Epoch [20/100], Loss: 10008.5818\n",
            "Epoch [30/100], Loss: 11012.9800\n",
            "Epoch [40/100], Loss: 5536.1729\n",
            "Epoch [50/100], Loss: 2836.0071\n",
            "Epoch [60/100], Loss: 3366.5278\n",
            "Epoch [70/100], Loss: 2823.6833\n",
            "Epoch [80/100], Loss: 2784.5488\n",
            "Epoch [90/100], Loss: 6322.7728\n",
            "Epoch [100/100], Loss: 4669.4945\n",
            "Fold 5, RMSE: 45.33198547363281\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 60.91913375854492\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 18592.2119\n",
            "Epoch [20/150], Loss: 10283.5935\n",
            "Epoch [30/150], Loss: 20152.5474\n",
            "Epoch [40/150], Loss: 13450.4696\n",
            "Epoch [50/150], Loss: 3790.2653\n",
            "Epoch [60/150], Loss: 6006.9758\n",
            "Epoch [70/150], Loss: 3751.1190\n",
            "Epoch [80/150], Loss: 3860.2494\n",
            "Epoch [90/150], Loss: 3219.8103\n",
            "Epoch [100/150], Loss: 3295.5873\n",
            "Epoch [110/150], Loss: 1962.4671\n",
            "Epoch [120/150], Loss: 2306.6003\n",
            "Epoch [130/150], Loss: 3956.1089\n",
            "Epoch [140/150], Loss: 2770.9988\n",
            "Epoch [150/150], Loss: 2867.0569\n",
            "Fold 1, RMSE: 58.20494842529297\n",
            "Epoch [10/150], Loss: 12587.6819\n",
            "Epoch [20/150], Loss: 10448.8418\n",
            "Epoch [30/150], Loss: 8134.0764\n",
            "Epoch [40/150], Loss: 6807.3276\n",
            "Epoch [50/150], Loss: 3231.0858\n",
            "Epoch [60/150], Loss: 5034.1123\n",
            "Epoch [70/150], Loss: 3176.1990\n",
            "Epoch [80/150], Loss: 2875.5787\n",
            "Epoch [90/150], Loss: 3240.3958\n",
            "Epoch [100/150], Loss: 4286.6169\n",
            "Epoch [110/150], Loss: 2481.5995\n",
            "Epoch [120/150], Loss: 4536.4773\n",
            "Epoch [130/150], Loss: 3922.1783\n",
            "Epoch [140/150], Loss: 1600.2821\n",
            "Epoch [150/150], Loss: 2442.7636\n",
            "Fold 2, RMSE: 65.68877410888672\n",
            "Epoch [10/150], Loss: 9719.5581\n",
            "Epoch [20/150], Loss: 6890.6163\n",
            "Epoch [30/150], Loss: 6545.0806\n",
            "Epoch [40/150], Loss: 3836.7690\n",
            "Epoch [50/150], Loss: 4642.4486\n",
            "Epoch [60/150], Loss: 4178.4691\n",
            "Epoch [70/150], Loss: 4349.8383\n",
            "Epoch [80/150], Loss: 2291.9592\n",
            "Epoch [90/150], Loss: 2600.9606\n",
            "Epoch [100/150], Loss: 1924.1162\n",
            "Epoch [110/150], Loss: 1248.7287\n",
            "Epoch [120/150], Loss: 2715.9380\n",
            "Epoch [130/150], Loss: 2178.8195\n",
            "Epoch [140/150], Loss: 1272.4453\n",
            "Epoch [150/150], Loss: 1558.3259\n",
            "Fold 3, RMSE: 89.54887390136719\n",
            "Epoch [10/150], Loss: 13394.1790\n",
            "Epoch [20/150], Loss: 13258.2244\n",
            "Epoch [30/150], Loss: 13019.0498\n",
            "Epoch [40/150], Loss: 8276.6718\n",
            "Epoch [50/150], Loss: 4988.3815\n",
            "Epoch [60/150], Loss: 6691.6648\n",
            "Epoch [70/150], Loss: 4114.3396\n",
            "Epoch [80/150], Loss: 2394.9089\n",
            "Epoch [90/150], Loss: 7329.5392\n",
            "Epoch [100/150], Loss: 2588.5723\n",
            "Epoch [110/150], Loss: 3365.5002\n",
            "Epoch [120/150], Loss: 1695.4367\n",
            "Epoch [130/150], Loss: 5442.6358\n",
            "Epoch [140/150], Loss: 4096.0628\n",
            "Epoch [150/150], Loss: 1428.8506\n",
            "Fold 4, RMSE: 41.23481750488281\n",
            "Epoch [10/150], Loss: 18702.0571\n",
            "Epoch [20/150], Loss: 9065.3271\n",
            "Epoch [30/150], Loss: 8792.6033\n",
            "Epoch [40/150], Loss: 10734.5146\n",
            "Epoch [50/150], Loss: 5480.3652\n",
            "Epoch [60/150], Loss: 4680.5785\n",
            "Epoch [70/150], Loss: 4541.6646\n",
            "Epoch [80/150], Loss: 3745.1193\n",
            "Epoch [90/150], Loss: 3101.5746\n",
            "Epoch [100/150], Loss: 6923.1201\n",
            "Epoch [110/150], Loss: 3066.0020\n",
            "Epoch [120/150], Loss: 6352.0538\n",
            "Epoch [130/150], Loss: 3701.6647\n",
            "Epoch [140/150], Loss: 2659.0997\n",
            "Epoch [150/150], Loss: 2574.8349\n",
            "Fold 5, RMSE: 46.05314636230469\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 60.146112060546876\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 10680.9097\n",
            "Epoch [20/100], Loss: 14914.0260\n",
            "Epoch [30/100], Loss: 5244.5554\n",
            "Epoch [40/100], Loss: 3772.3434\n",
            "Epoch [50/100], Loss: 3797.3289\n",
            "Epoch [60/100], Loss: 2493.2253\n",
            "Epoch [70/100], Loss: 9236.0174\n",
            "Epoch [80/100], Loss: 2660.5054\n",
            "Epoch [90/100], Loss: 2987.2181\n",
            "Epoch [100/100], Loss: 1630.2948\n",
            "Fold 1, RMSE: 55.731693267822266\n",
            "Epoch [10/100], Loss: 9526.9048\n",
            "Epoch [20/100], Loss: 15092.8193\n",
            "Epoch [30/100], Loss: 6295.7010\n",
            "Epoch [40/100], Loss: 5954.8376\n",
            "Epoch [50/100], Loss: 4145.5914\n",
            "Epoch [60/100], Loss: 6990.5803\n",
            "Epoch [70/100], Loss: 8164.2905\n",
            "Epoch [80/100], Loss: 2161.4362\n",
            "Epoch [90/100], Loss: 1886.9374\n",
            "Epoch [100/100], Loss: 2053.2510\n",
            "Fold 2, RMSE: 62.97649002075195\n",
            "Epoch [10/100], Loss: 7552.2926\n",
            "Epoch [20/100], Loss: 5739.7159\n",
            "Epoch [30/100], Loss: 5062.6097\n",
            "Epoch [40/100], Loss: 4660.8807\n",
            "Epoch [50/100], Loss: 4996.7914\n",
            "Epoch [60/100], Loss: 2385.0070\n",
            "Epoch [70/100], Loss: 3760.6798\n",
            "Epoch [80/100], Loss: 4365.8472\n",
            "Epoch [90/100], Loss: 2123.2431\n",
            "Epoch [100/100], Loss: 3126.0228\n",
            "Fold 3, RMSE: 91.77164459228516\n",
            "Epoch [10/100], Loss: 10798.1357\n",
            "Epoch [20/100], Loss: 11948.9233\n",
            "Epoch [30/100], Loss: 10190.1865\n",
            "Epoch [40/100], Loss: 4950.7811\n",
            "Epoch [50/100], Loss: 4897.1359\n",
            "Epoch [60/100], Loss: 5475.4229\n",
            "Epoch [70/100], Loss: 4897.4401\n",
            "Epoch [80/100], Loss: 5411.2261\n",
            "Epoch [90/100], Loss: 3823.9000\n",
            "Epoch [100/100], Loss: 2943.1770\n",
            "Fold 4, RMSE: 34.62142562866211\n",
            "Epoch [10/100], Loss: 9625.3311\n",
            "Epoch [20/100], Loss: 13378.0183\n",
            "Epoch [30/100], Loss: 11267.5667\n",
            "Epoch [40/100], Loss: 4048.9964\n",
            "Epoch [50/100], Loss: 4255.6195\n",
            "Epoch [60/100], Loss: 4717.8858\n",
            "Epoch [70/100], Loss: 2786.4657\n",
            "Epoch [80/100], Loss: 1888.8479\n",
            "Epoch [90/100], Loss: 2441.7515\n",
            "Epoch [100/100], Loss: 3660.5711\n",
            "Fold 5, RMSE: 44.481998443603516\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 57.916650390625\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 12494.5101\n",
            "Epoch [20/150], Loss: 7254.5073\n",
            "Epoch [30/150], Loss: 5908.8929\n",
            "Epoch [40/150], Loss: 2990.4599\n",
            "Epoch [50/150], Loss: 3984.1746\n",
            "Epoch [60/150], Loss: 2302.1548\n",
            "Epoch [70/150], Loss: 3321.4924\n",
            "Epoch [80/150], Loss: 2956.4821\n",
            "Epoch [90/150], Loss: 3456.4399\n",
            "Epoch [100/150], Loss: 2150.4944\n",
            "Epoch [110/150], Loss: 3702.1393\n",
            "Epoch [120/150], Loss: 2751.7137\n",
            "Epoch [130/150], Loss: 9013.4257\n",
            "Epoch [140/150], Loss: 2487.4547\n",
            "Epoch [150/150], Loss: 1581.0904\n",
            "Fold 1, RMSE: 56.086509704589844\n",
            "Epoch [10/150], Loss: 15953.9011\n",
            "Epoch [20/150], Loss: 10852.2725\n",
            "Epoch [30/150], Loss: 8351.8373\n",
            "Epoch [40/150], Loss: 6943.2235\n",
            "Epoch [50/150], Loss: 1995.8822\n",
            "Epoch [60/150], Loss: 4440.5142\n",
            "Epoch [70/150], Loss: 7661.1501\n",
            "Epoch [80/150], Loss: 2996.8340\n",
            "Epoch [90/150], Loss: 2646.9039\n",
            "Epoch [100/150], Loss: 4503.7881\n",
            "Epoch [110/150], Loss: 6322.4401\n",
            "Epoch [120/150], Loss: 1366.5626\n",
            "Epoch [130/150], Loss: 1425.5782\n",
            "Epoch [140/150], Loss: 1450.3253\n",
            "Epoch [150/150], Loss: 3016.1191\n",
            "Fold 2, RMSE: 68.51504516601562\n",
            "Epoch [10/150], Loss: 8543.8105\n",
            "Epoch [20/150], Loss: 6144.0237\n",
            "Epoch [30/150], Loss: 6218.3344\n",
            "Epoch [40/150], Loss: 4033.8843\n",
            "Epoch [50/150], Loss: 5179.4653\n",
            "Epoch [60/150], Loss: 6378.8264\n",
            "Epoch [70/150], Loss: 2766.7910\n",
            "Epoch [80/150], Loss: 4609.4922\n",
            "Epoch [90/150], Loss: 880.6104\n",
            "Epoch [100/150], Loss: 2883.5745\n",
            "Epoch [110/150], Loss: 3330.9469\n",
            "Epoch [120/150], Loss: 3519.5592\n",
            "Epoch [130/150], Loss: 1174.3134\n",
            "Epoch [140/150], Loss: 2955.4263\n",
            "Epoch [150/150], Loss: 1397.8665\n",
            "Fold 3, RMSE: 93.29289245605469\n",
            "Epoch [10/150], Loss: 17637.1838\n",
            "Epoch [20/150], Loss: 11197.7428\n",
            "Epoch [30/150], Loss: 4602.5508\n",
            "Epoch [40/150], Loss: 6990.1105\n",
            "Epoch [50/150], Loss: 6265.1011\n",
            "Epoch [60/150], Loss: 4582.4532\n",
            "Epoch [70/150], Loss: 3876.9272\n",
            "Epoch [80/150], Loss: 4495.4708\n",
            "Epoch [90/150], Loss: 3380.9653\n",
            "Epoch [100/150], Loss: 4591.4255\n",
            "Epoch [110/150], Loss: 4101.4154\n",
            "Epoch [120/150], Loss: 2655.3077\n",
            "Epoch [130/150], Loss: 3724.6511\n",
            "Epoch [140/150], Loss: 2379.9165\n",
            "Epoch [150/150], Loss: 2718.6093\n",
            "Fold 4, RMSE: 45.06110382080078\n",
            "Epoch [10/150], Loss: 20621.2297\n",
            "Epoch [20/150], Loss: 10205.2377\n",
            "Epoch [30/150], Loss: 6137.1544\n",
            "Epoch [40/150], Loss: 4881.2363\n",
            "Epoch [50/150], Loss: 5514.8953\n",
            "Epoch [60/150], Loss: 7415.7753\n",
            "Epoch [70/150], Loss: 5187.5881\n",
            "Epoch [80/150], Loss: 3930.3995\n",
            "Epoch [90/150], Loss: 3883.6968\n",
            "Epoch [100/150], Loss: 4398.9856\n",
            "Epoch [110/150], Loss: 4275.4551\n",
            "Epoch [120/150], Loss: 2576.0580\n",
            "Epoch [130/150], Loss: 1065.8711\n",
            "Epoch [140/150], Loss: 2869.7164\n",
            "Epoch [150/150], Loss: 1504.6266\n",
            "Fold 5, RMSE: 42.46889877319336\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 61.08488998413086\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 9180.1166\n",
            "Epoch [20/100], Loss: 8174.2670\n",
            "Epoch [30/100], Loss: 5784.3025\n",
            "Epoch [40/100], Loss: 4095.1998\n",
            "Epoch [50/100], Loss: 6794.2663\n",
            "Epoch [60/100], Loss: 5520.7755\n",
            "Epoch [70/100], Loss: 7533.1877\n",
            "Epoch [80/100], Loss: 3979.9855\n",
            "Epoch [90/100], Loss: 2620.8085\n",
            "Epoch [100/100], Loss: 2715.2825\n",
            "Fold 1, RMSE: 60.7680549621582\n",
            "Epoch [10/100], Loss: 12613.6301\n",
            "Epoch [20/100], Loss: 13331.1133\n",
            "Epoch [30/100], Loss: 10590.3313\n",
            "Epoch [40/100], Loss: 8664.1310\n",
            "Epoch [50/100], Loss: 5416.4772\n",
            "Epoch [60/100], Loss: 2092.7319\n",
            "Epoch [70/100], Loss: 6846.4749\n",
            "Epoch [80/100], Loss: 2130.5375\n",
            "Epoch [90/100], Loss: 3704.2360\n",
            "Epoch [100/100], Loss: 1727.6433\n",
            "Fold 2, RMSE: 59.615997314453125\n",
            "Epoch [10/100], Loss: 11909.8599\n",
            "Epoch [20/100], Loss: 9889.4659\n",
            "Epoch [30/100], Loss: 5603.2863\n",
            "Epoch [40/100], Loss: 6418.0682\n",
            "Epoch [50/100], Loss: 6313.8896\n",
            "Epoch [60/100], Loss: 7901.5415\n",
            "Epoch [70/100], Loss: 7971.5484\n",
            "Epoch [80/100], Loss: 2362.9276\n",
            "Epoch [90/100], Loss: 9321.9830\n",
            "Epoch [100/100], Loss: 4675.8393\n",
            "Fold 3, RMSE: 100.31854248046875\n",
            "Epoch [10/100], Loss: 14476.1208\n",
            "Epoch [20/100], Loss: 12706.4829\n",
            "Epoch [30/100], Loss: 9191.4767\n",
            "Epoch [40/100], Loss: 4859.9575\n",
            "Epoch [50/100], Loss: 5304.6338\n",
            "Epoch [60/100], Loss: 10676.9768\n",
            "Epoch [70/100], Loss: 10809.9198\n",
            "Epoch [80/100], Loss: 8115.1865\n",
            "Epoch [90/100], Loss: 7121.6375\n",
            "Epoch [100/100], Loss: 5631.9931\n",
            "Fold 4, RMSE: 44.750389099121094\n",
            "Epoch [10/100], Loss: 22417.3970\n",
            "Epoch [20/100], Loss: 9976.8850\n",
            "Epoch [30/100], Loss: 7948.4020\n",
            "Epoch [40/100], Loss: 8584.7671\n",
            "Epoch [50/100], Loss: 3889.0432\n",
            "Epoch [60/100], Loss: 5375.8381\n",
            "Epoch [70/100], Loss: 3365.7768\n",
            "Epoch [80/100], Loss: 3771.5176\n",
            "Epoch [90/100], Loss: 2855.4247\n",
            "Epoch [100/100], Loss: 2288.4033\n",
            "Fold 5, RMSE: 47.61222457885742\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 62.613041687011716\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16124.6487\n",
            "Epoch [20/150], Loss: 7225.9907\n",
            "Epoch [30/150], Loss: 7764.1287\n",
            "Epoch [40/150], Loss: 7297.7715\n",
            "Epoch [50/150], Loss: 2903.7747\n",
            "Epoch [60/150], Loss: 6805.9937\n",
            "Epoch [70/150], Loss: 6385.2268\n",
            "Epoch [80/150], Loss: 6611.5197\n",
            "Epoch [90/150], Loss: 2178.8102\n",
            "Epoch [100/150], Loss: 4188.5343\n",
            "Epoch [110/150], Loss: 2015.3209\n",
            "Epoch [120/150], Loss: 2112.8553\n",
            "Epoch [130/150], Loss: 3154.7745\n",
            "Epoch [140/150], Loss: 2220.8160\n",
            "Epoch [150/150], Loss: 2606.4059\n",
            "Fold 1, RMSE: 59.7199821472168\n",
            "Epoch [10/150], Loss: 29436.1030\n",
            "Epoch [20/150], Loss: 12481.6171\n",
            "Epoch [30/150], Loss: 10071.8435\n",
            "Epoch [40/150], Loss: 8240.9513\n",
            "Epoch [50/150], Loss: 8434.1667\n",
            "Epoch [60/150], Loss: 8054.6062\n",
            "Epoch [70/150], Loss: 4275.5639\n",
            "Epoch [80/150], Loss: 3307.9319\n",
            "Epoch [90/150], Loss: 2835.6884\n",
            "Epoch [100/150], Loss: 2087.4568\n",
            "Epoch [110/150], Loss: 2854.6411\n",
            "Epoch [120/150], Loss: 4151.4677\n",
            "Epoch [130/150], Loss: 3271.1844\n",
            "Epoch [140/150], Loss: 1665.2953\n",
            "Epoch [150/150], Loss: 1647.2272\n",
            "Fold 2, RMSE: 63.00386047363281\n",
            "Epoch [10/150], Loss: 9694.6049\n",
            "Epoch [20/150], Loss: 5462.5269\n",
            "Epoch [30/150], Loss: 3960.4846\n",
            "Epoch [40/150], Loss: 5882.7935\n",
            "Epoch [50/150], Loss: 11444.3506\n",
            "Epoch [60/150], Loss: 3112.7637\n",
            "Epoch [70/150], Loss: 2912.0479\n",
            "Epoch [80/150], Loss: 2821.6405\n",
            "Epoch [90/150], Loss: 4006.9390\n",
            "Epoch [100/150], Loss: 3724.1287\n",
            "Epoch [110/150], Loss: 2956.4694\n",
            "Epoch [120/150], Loss: 2161.9417\n",
            "Epoch [130/150], Loss: 2298.1095\n",
            "Epoch [140/150], Loss: 2986.2664\n",
            "Epoch [150/150], Loss: 2435.4193\n",
            "Fold 3, RMSE: 91.76499938964844\n",
            "Epoch [10/150], Loss: 13428.3662\n",
            "Epoch [20/150], Loss: 7273.8827\n",
            "Epoch [30/150], Loss: 7361.7445\n",
            "Epoch [40/150], Loss: 5506.1573\n",
            "Epoch [50/150], Loss: 2371.6727\n",
            "Epoch [60/150], Loss: 5570.6261\n",
            "Epoch [70/150], Loss: 2942.9744\n",
            "Epoch [80/150], Loss: 5341.7415\n",
            "Epoch [90/150], Loss: 2773.6867\n",
            "Epoch [100/150], Loss: 2143.1134\n",
            "Epoch [110/150], Loss: 2361.2468\n",
            "Epoch [120/150], Loss: 6350.3695\n",
            "Epoch [130/150], Loss: 2372.8801\n",
            "Epoch [140/150], Loss: 2982.3841\n",
            "Epoch [150/150], Loss: 2900.1944\n",
            "Fold 4, RMSE: 40.32627868652344\n",
            "Epoch [10/150], Loss: 17613.3613\n",
            "Epoch [20/150], Loss: 12330.2545\n",
            "Epoch [30/150], Loss: 11655.1865\n",
            "Epoch [40/150], Loss: 8226.2834\n",
            "Epoch [50/150], Loss: 5026.5499\n",
            "Epoch [60/150], Loss: 5371.4136\n",
            "Epoch [70/150], Loss: 4411.9893\n",
            "Epoch [80/150], Loss: 2512.2759\n",
            "Epoch [90/150], Loss: 7386.0739\n",
            "Epoch [100/150], Loss: 1661.4390\n",
            "Epoch [110/150], Loss: 9370.1293\n",
            "Epoch [120/150], Loss: 5215.7075\n",
            "Epoch [130/150], Loss: 3291.7893\n",
            "Epoch [140/150], Loss: 2076.7983\n",
            "Epoch [150/150], Loss: 1845.8680\n",
            "Fold 5, RMSE: 45.57755661010742\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 60.07853546142578\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 13792.7400\n",
            "Epoch [20/100], Loss: 10029.1025\n",
            "Epoch [30/100], Loss: 4927.9762\n",
            "Epoch [40/100], Loss: 4040.1898\n",
            "Epoch [50/100], Loss: 3431.7988\n",
            "Epoch [60/100], Loss: 2007.1072\n",
            "Epoch [70/100], Loss: 5153.1326\n",
            "Epoch [80/100], Loss: 1771.4272\n",
            "Epoch [90/100], Loss: 2657.0863\n",
            "Epoch [100/100], Loss: 2771.4776\n",
            "Fold 1, RMSE: 56.51951599121094\n",
            "Epoch [10/100], Loss: 11924.8585\n",
            "Epoch [20/100], Loss: 6947.5632\n",
            "Epoch [30/100], Loss: 11116.2109\n",
            "Epoch [40/100], Loss: 4835.8729\n",
            "Epoch [50/100], Loss: 3645.6627\n",
            "Epoch [60/100], Loss: 2543.9518\n",
            "Epoch [70/100], Loss: 2474.7303\n",
            "Epoch [80/100], Loss: 2050.9539\n",
            "Epoch [90/100], Loss: 3366.9166\n",
            "Epoch [100/100], Loss: 2626.7525\n",
            "Fold 2, RMSE: 67.9700927734375\n",
            "Epoch [10/100], Loss: 6152.3242\n",
            "Epoch [20/100], Loss: 6404.9032\n",
            "Epoch [30/100], Loss: 3999.1919\n",
            "Epoch [40/100], Loss: 2596.1233\n",
            "Epoch [50/100], Loss: 3573.0709\n",
            "Epoch [60/100], Loss: 3315.3188\n",
            "Epoch [70/100], Loss: 1505.6022\n",
            "Epoch [80/100], Loss: 2026.3326\n",
            "Epoch [90/100], Loss: 4611.7706\n",
            "Epoch [100/100], Loss: 1262.5950\n",
            "Fold 3, RMSE: 93.15898895263672\n",
            "Epoch [10/100], Loss: 16307.3904\n",
            "Epoch [20/100], Loss: 8707.9297\n",
            "Epoch [30/100], Loss: 6439.6725\n",
            "Epoch [40/100], Loss: 5053.7189\n",
            "Epoch [50/100], Loss: 3156.6900\n",
            "Epoch [60/100], Loss: 4335.8730\n",
            "Epoch [70/100], Loss: 2769.4011\n",
            "Epoch [80/100], Loss: 2691.2806\n",
            "Epoch [90/100], Loss: 1929.0554\n",
            "Epoch [100/100], Loss: 2304.0841\n",
            "Fold 4, RMSE: 42.29617691040039\n",
            "Epoch [10/100], Loss: 17290.6016\n",
            "Epoch [20/100], Loss: 15603.4709\n",
            "Epoch [30/100], Loss: 6539.8394\n",
            "Epoch [40/100], Loss: 6716.1216\n",
            "Epoch [50/100], Loss: 6568.6818\n",
            "Epoch [60/100], Loss: 7394.4995\n",
            "Epoch [70/100], Loss: 2374.4995\n",
            "Epoch [80/100], Loss: 1607.9203\n",
            "Epoch [90/100], Loss: 3740.5423\n",
            "Epoch [100/100], Loss: 2608.1785\n",
            "Fold 5, RMSE: 44.01229476928711\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 60.79141387939453\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14882.3486\n",
            "Epoch [20/150], Loss: 6956.2229\n",
            "Epoch [30/150], Loss: 6731.4900\n",
            "Epoch [40/150], Loss: 5439.5062\n",
            "Epoch [50/150], Loss: 5285.7386\n",
            "Epoch [60/150], Loss: 6093.9470\n",
            "Epoch [70/150], Loss: 5050.7356\n",
            "Epoch [80/150], Loss: 2986.8104\n",
            "Epoch [90/150], Loss: 6907.2026\n",
            "Epoch [100/150], Loss: 4380.0154\n",
            "Epoch [110/150], Loss: 2516.7566\n",
            "Epoch [120/150], Loss: 6156.4552\n",
            "Epoch [130/150], Loss: 2143.5787\n",
            "Epoch [140/150], Loss: 2454.9680\n",
            "Epoch [150/150], Loss: 2206.0262\n",
            "Fold 1, RMSE: 59.931922912597656\n",
            "Epoch [10/150], Loss: 11638.7555\n",
            "Epoch [20/150], Loss: 8650.2733\n",
            "Epoch [30/150], Loss: 8325.2396\n",
            "Epoch [40/150], Loss: 4132.5934\n",
            "Epoch [50/150], Loss: 2453.4565\n",
            "Epoch [60/150], Loss: 2825.7380\n",
            "Epoch [70/150], Loss: 4277.3731\n",
            "Epoch [80/150], Loss: 1640.0149\n",
            "Epoch [90/150], Loss: 2256.4904\n",
            "Epoch [100/150], Loss: 3157.6073\n",
            "Epoch [110/150], Loss: 3928.4720\n",
            "Epoch [120/150], Loss: 3323.8907\n",
            "Epoch [130/150], Loss: 1345.3299\n",
            "Epoch [140/150], Loss: 3727.6523\n",
            "Epoch [150/150], Loss: 1649.7942\n",
            "Fold 2, RMSE: 67.21383666992188\n",
            "Epoch [10/150], Loss: 7196.0217\n",
            "Epoch [20/150], Loss: 5918.5519\n",
            "Epoch [30/150], Loss: 5544.6482\n",
            "Epoch [40/150], Loss: 4866.3234\n",
            "Epoch [50/150], Loss: 2558.1528\n",
            "Epoch [60/150], Loss: 1417.8568\n",
            "Epoch [70/150], Loss: 3104.0184\n",
            "Epoch [80/150], Loss: 2232.7438\n",
            "Epoch [90/150], Loss: 2296.1602\n",
            "Epoch [100/150], Loss: 2599.9707\n",
            "Epoch [110/150], Loss: 3334.4452\n",
            "Epoch [120/150], Loss: 2099.1472\n",
            "Epoch [130/150], Loss: 1096.2496\n",
            "Epoch [140/150], Loss: 1032.5705\n",
            "Epoch [150/150], Loss: 1770.8210\n",
            "Fold 3, RMSE: 93.5888442993164\n",
            "Epoch [10/150], Loss: 12903.0457\n",
            "Epoch [20/150], Loss: 15962.9636\n",
            "Epoch [30/150], Loss: 7051.6042\n",
            "Epoch [40/150], Loss: 3469.3937\n",
            "Epoch [50/150], Loss: 4648.0383\n",
            "Epoch [60/150], Loss: 2875.4392\n",
            "Epoch [70/150], Loss: 5500.2733\n",
            "Epoch [80/150], Loss: 2660.4782\n",
            "Epoch [90/150], Loss: 5601.9600\n",
            "Epoch [100/150], Loss: 4679.7291\n",
            "Epoch [110/150], Loss: 2090.7662\n",
            "Epoch [120/150], Loss: 1902.8466\n",
            "Epoch [130/150], Loss: 6555.5994\n",
            "Epoch [140/150], Loss: 1496.2413\n",
            "Epoch [150/150], Loss: 1794.9502\n",
            "Fold 4, RMSE: 44.233177185058594\n",
            "Epoch [10/150], Loss: 11534.8407\n",
            "Epoch [20/150], Loss: 7621.7170\n",
            "Epoch [30/150], Loss: 8567.9299\n",
            "Epoch [40/150], Loss: 5808.7184\n",
            "Epoch [50/150], Loss: 4130.9227\n",
            "Epoch [60/150], Loss: 3756.5057\n",
            "Epoch [70/150], Loss: 3815.7371\n",
            "Epoch [80/150], Loss: 2322.6233\n",
            "Epoch [90/150], Loss: 1222.7473\n",
            "Epoch [100/150], Loss: 2890.7764\n",
            "Epoch [110/150], Loss: 1635.3840\n",
            "Epoch [120/150], Loss: 4187.4645\n",
            "Epoch [130/150], Loss: 1366.9530\n",
            "Epoch [140/150], Loss: 2674.8677\n",
            "Epoch [150/150], Loss: 2778.1441\n",
            "Fold 5, RMSE: 45.305545806884766\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 62.05466537475586\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17221.4641\n",
            "Epoch [20/100], Loss: 9183.9313\n",
            "Epoch [30/100], Loss: 9454.3917\n",
            "Epoch [40/100], Loss: 9753.0051\n",
            "Epoch [50/100], Loss: 5269.5422\n",
            "Epoch [60/100], Loss: 4640.4598\n",
            "Epoch [70/100], Loss: 3531.2523\n",
            "Epoch [80/100], Loss: 2334.2634\n",
            "Epoch [90/100], Loss: 3247.0800\n",
            "Epoch [100/100], Loss: 2984.2116\n",
            "Fold 1, RMSE: 60.028106689453125\n",
            "Epoch [10/100], Loss: 12024.9403\n",
            "Epoch [20/100], Loss: 14492.4187\n",
            "Epoch [30/100], Loss: 10626.6038\n",
            "Epoch [40/100], Loss: 12681.4490\n",
            "Epoch [50/100], Loss: 4939.2985\n",
            "Epoch [60/100], Loss: 5243.1469\n",
            "Epoch [70/100], Loss: 3384.6144\n",
            "Epoch [80/100], Loss: 4395.4845\n",
            "Epoch [90/100], Loss: 4585.1574\n",
            "Epoch [100/100], Loss: 1813.0711\n",
            "Fold 2, RMSE: 72.61754608154297\n",
            "Epoch [10/100], Loss: 13548.1404\n",
            "Epoch [20/100], Loss: 17523.6270\n",
            "Epoch [30/100], Loss: 14149.9446\n",
            "Epoch [40/100], Loss: 14320.6025\n",
            "Epoch [50/100], Loss: 11665.8756\n",
            "Epoch [60/100], Loss: 14237.1963\n",
            "Epoch [70/100], Loss: 11975.7811\n",
            "Epoch [80/100], Loss: 8200.6396\n",
            "Epoch [90/100], Loss: 7823.6078\n",
            "Epoch [100/100], Loss: 5205.5245\n",
            "Fold 3, RMSE: 93.28084564208984\n",
            "Epoch [10/100], Loss: 14523.3765\n",
            "Epoch [20/100], Loss: 11114.2927\n",
            "Epoch [30/100], Loss: 8326.5909\n",
            "Epoch [40/100], Loss: 15041.3309\n",
            "Epoch [50/100], Loss: 4155.3206\n",
            "Epoch [60/100], Loss: 5270.5035\n",
            "Epoch [70/100], Loss: 4292.6274\n",
            "Epoch [80/100], Loss: 7407.6274\n",
            "Epoch [90/100], Loss: 11318.7158\n",
            "Epoch [100/100], Loss: 3258.6252\n",
            "Fold 4, RMSE: 48.028724670410156\n",
            "Epoch [10/100], Loss: 12012.7079\n",
            "Epoch [20/100], Loss: 6190.0582\n",
            "Epoch [30/100], Loss: 4725.2308\n",
            "Epoch [40/100], Loss: 4202.2755\n",
            "Epoch [50/100], Loss: 8361.5085\n",
            "Epoch [60/100], Loss: 3446.0862\n",
            "Epoch [70/100], Loss: 1973.5269\n",
            "Epoch [80/100], Loss: 2320.8057\n",
            "Epoch [90/100], Loss: 4872.4011\n",
            "Epoch [100/100], Loss: 3966.5223\n",
            "Fold 5, RMSE: 44.52896499633789\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 63.6968376159668\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 13155.4849\n",
            "Epoch [20/150], Loss: 10256.6444\n",
            "Epoch [30/150], Loss: 6746.5985\n",
            "Epoch [40/150], Loss: 5622.4279\n",
            "Epoch [50/150], Loss: 5536.5164\n",
            "Epoch [60/150], Loss: 6118.5131\n",
            "Epoch [70/150], Loss: 1800.2040\n",
            "Epoch [80/150], Loss: 1676.2006\n",
            "Epoch [90/150], Loss: 2719.2861\n",
            "Epoch [100/150], Loss: 1720.3382\n",
            "Epoch [110/150], Loss: 1996.4465\n",
            "Epoch [120/150], Loss: 7101.5520\n",
            "Epoch [130/150], Loss: 1709.0208\n",
            "Epoch [140/150], Loss: 1493.2036\n",
            "Epoch [150/150], Loss: 2799.8765\n",
            "Fold 1, RMSE: 58.07966995239258\n",
            "Epoch [10/150], Loss: 11507.7402\n",
            "Epoch [20/150], Loss: 6355.6704\n",
            "Epoch [30/150], Loss: 5059.8163\n",
            "Epoch [40/150], Loss: 8274.9075\n",
            "Epoch [50/150], Loss: 5146.6183\n",
            "Epoch [60/150], Loss: 4559.7650\n",
            "Epoch [70/150], Loss: 4134.9703\n",
            "Epoch [80/150], Loss: 5152.4293\n",
            "Epoch [90/150], Loss: 5443.4595\n",
            "Epoch [100/150], Loss: 3610.6367\n",
            "Epoch [110/150], Loss: 2156.7222\n",
            "Epoch [120/150], Loss: 3334.2996\n",
            "Epoch [130/150], Loss: 1927.7970\n",
            "Epoch [140/150], Loss: 3218.6251\n",
            "Epoch [150/150], Loss: 1575.0164\n",
            "Fold 2, RMSE: 67.67030334472656\n",
            "Epoch [10/150], Loss: 8408.3196\n",
            "Epoch [20/150], Loss: 7856.0994\n",
            "Epoch [30/150], Loss: 4654.4647\n",
            "Epoch [40/150], Loss: 4107.4193\n",
            "Epoch [50/150], Loss: 6001.0902\n",
            "Epoch [60/150], Loss: 6727.6297\n",
            "Epoch [70/150], Loss: 5958.5535\n",
            "Epoch [80/150], Loss: 4135.2290\n",
            "Epoch [90/150], Loss: 5735.4762\n",
            "Epoch [100/150], Loss: 4323.0177\n",
            "Epoch [110/150], Loss: 1499.4770\n",
            "Epoch [120/150], Loss: 1825.8322\n",
            "Epoch [130/150], Loss: 2040.4969\n",
            "Epoch [140/150], Loss: 2178.7869\n",
            "Epoch [150/150], Loss: 3216.1226\n",
            "Fold 3, RMSE: 92.6157455444336\n",
            "Epoch [10/150], Loss: 15148.0923\n",
            "Epoch [20/150], Loss: 11901.9475\n",
            "Epoch [30/150], Loss: 9192.6598\n",
            "Epoch [40/150], Loss: 5794.5648\n",
            "Epoch [50/150], Loss: 7012.4408\n",
            "Epoch [60/150], Loss: 3928.1259\n",
            "Epoch [70/150], Loss: 5992.0253\n",
            "Epoch [80/150], Loss: 7566.3528\n",
            "Epoch [90/150], Loss: 4086.6895\n",
            "Epoch [100/150], Loss: 3344.9869\n",
            "Epoch [110/150], Loss: 1830.0107\n",
            "Epoch [120/150], Loss: 1658.6283\n",
            "Epoch [130/150], Loss: 1115.9637\n",
            "Epoch [140/150], Loss: 1580.2828\n",
            "Epoch [150/150], Loss: 2013.7371\n",
            "Fold 4, RMSE: 41.56978988647461\n",
            "Epoch [10/150], Loss: 12067.9387\n",
            "Epoch [20/150], Loss: 9669.8311\n",
            "Epoch [30/150], Loss: 6629.5182\n",
            "Epoch [40/150], Loss: 4194.4764\n",
            "Epoch [50/150], Loss: 6084.0232\n",
            "Epoch [60/150], Loss: 4957.4321\n",
            "Epoch [70/150], Loss: 2972.1238\n",
            "Epoch [80/150], Loss: 3469.7480\n",
            "Epoch [90/150], Loss: 3570.8679\n",
            "Epoch [100/150], Loss: 1773.4757\n",
            "Epoch [110/150], Loss: 1967.9946\n",
            "Epoch [120/150], Loss: 2921.5861\n",
            "Epoch [130/150], Loss: 2828.5320\n",
            "Epoch [140/150], Loss: 2670.8696\n",
            "Epoch [150/150], Loss: 1563.2586\n",
            "Fold 5, RMSE: 45.435855865478516\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 61.07427291870117\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 15396.8738\n",
            "Epoch [20/100], Loss: 7713.4539\n",
            "Epoch [30/100], Loss: 8714.7589\n",
            "Epoch [40/100], Loss: 3157.1147\n",
            "Epoch [50/100], Loss: 4581.9371\n",
            "Epoch [60/100], Loss: 2640.9352\n",
            "Epoch [70/100], Loss: 6194.5564\n",
            "Epoch [80/100], Loss: 3039.8257\n",
            "Epoch [90/100], Loss: 1608.9180\n",
            "Epoch [100/100], Loss: 2032.2427\n",
            "Fold 1, RMSE: 58.28471755981445\n",
            "Epoch [10/100], Loss: 11511.5688\n",
            "Epoch [20/100], Loss: 6156.1592\n",
            "Epoch [30/100], Loss: 6519.4615\n",
            "Epoch [40/100], Loss: 7186.5045\n",
            "Epoch [50/100], Loss: 7653.3818\n",
            "Epoch [60/100], Loss: 3263.4359\n",
            "Epoch [70/100], Loss: 3261.3587\n",
            "Epoch [80/100], Loss: 4123.1928\n",
            "Epoch [90/100], Loss: 2397.0271\n",
            "Epoch [100/100], Loss: 5688.3063\n",
            "Fold 2, RMSE: 65.58015441894531\n",
            "Epoch [10/100], Loss: 9654.4808\n",
            "Epoch [20/100], Loss: 7419.0952\n",
            "Epoch [30/100], Loss: 5384.8816\n",
            "Epoch [40/100], Loss: 4555.0823\n",
            "Epoch [50/100], Loss: 5134.7813\n",
            "Epoch [60/100], Loss: 3233.8315\n",
            "Epoch [70/100], Loss: 1489.2960\n",
            "Epoch [80/100], Loss: 2330.2780\n",
            "Epoch [90/100], Loss: 1248.1402\n",
            "Epoch [100/100], Loss: 2842.9362\n",
            "Fold 3, RMSE: 97.89479064941406\n",
            "Epoch [10/100], Loss: 13360.6462\n",
            "Epoch [20/100], Loss: 16031.6079\n",
            "Epoch [30/100], Loss: 9411.1599\n",
            "Epoch [40/100], Loss: 4540.6172\n",
            "Epoch [50/100], Loss: 5391.3497\n",
            "Epoch [60/100], Loss: 3302.5568\n",
            "Epoch [70/100], Loss: 4543.2865\n",
            "Epoch [80/100], Loss: 3362.5043\n",
            "Epoch [90/100], Loss: 3906.7322\n",
            "Epoch [100/100], Loss: 3041.3725\n",
            "Fold 4, RMSE: 40.20107650756836\n",
            "Epoch [10/100], Loss: 11551.5420\n",
            "Epoch [20/100], Loss: 11097.8064\n",
            "Epoch [30/100], Loss: 7273.5209\n",
            "Epoch [40/100], Loss: 7415.1459\n",
            "Epoch [50/100], Loss: 2190.7140\n",
            "Epoch [60/100], Loss: 6389.3411\n",
            "Epoch [70/100], Loss: 3220.1107\n",
            "Epoch [80/100], Loss: 1973.7667\n",
            "Epoch [90/100], Loss: 3864.1284\n",
            "Epoch [100/100], Loss: 1921.0064\n",
            "Fold 5, RMSE: 44.61297607421875\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 61.31474304199219\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13123.0103\n",
            "Epoch [20/150], Loss: 7213.2416\n",
            "Epoch [30/150], Loss: 6808.0001\n",
            "Epoch [40/150], Loss: 5713.2004\n",
            "Epoch [50/150], Loss: 3567.1094\n",
            "Epoch [60/150], Loss: 3133.8116\n",
            "Epoch [70/150], Loss: 4270.2861\n",
            "Epoch [80/150], Loss: 3215.3457\n",
            "Epoch [90/150], Loss: 5223.2369\n",
            "Epoch [100/150], Loss: 1932.8836\n",
            "Epoch [110/150], Loss: 1706.5319\n",
            "Epoch [120/150], Loss: 3101.8150\n",
            "Epoch [130/150], Loss: 2843.4922\n",
            "Epoch [140/150], Loss: 2123.8358\n",
            "Epoch [150/150], Loss: 1720.7274\n",
            "Fold 1, RMSE: 57.89722442626953\n",
            "Epoch [10/150], Loss: 15085.8665\n",
            "Epoch [20/150], Loss: 8808.7880\n",
            "Epoch [30/150], Loss: 4894.2253\n",
            "Epoch [40/150], Loss: 5909.3124\n",
            "Epoch [50/150], Loss: 2345.6944\n",
            "Epoch [60/150], Loss: 3469.2140\n",
            "Epoch [70/150], Loss: 1830.8239\n",
            "Epoch [80/150], Loss: 15572.6250\n",
            "Epoch [90/150], Loss: 3596.1825\n",
            "Epoch [100/150], Loss: 6169.1400\n",
            "Epoch [110/150], Loss: 2950.5990\n",
            "Epoch [120/150], Loss: 3967.7185\n",
            "Epoch [130/150], Loss: 6532.6278\n",
            "Epoch [140/150], Loss: 3165.6383\n",
            "Epoch [150/150], Loss: 2295.6068\n",
            "Fold 2, RMSE: 64.40415954589844\n",
            "Epoch [10/150], Loss: 6749.5511\n",
            "Epoch [20/150], Loss: 4989.3177\n",
            "Epoch [30/150], Loss: 5878.8131\n",
            "Epoch [40/150], Loss: 5259.4056\n",
            "Epoch [50/150], Loss: 2330.0296\n",
            "Epoch [60/150], Loss: 4421.7710\n",
            "Epoch [70/150], Loss: 3788.2729\n",
            "Epoch [80/150], Loss: 5548.9358\n",
            "Epoch [90/150], Loss: 2977.1475\n",
            "Epoch [100/150], Loss: 4821.3839\n",
            "Epoch [110/150], Loss: 3009.7841\n",
            "Epoch [120/150], Loss: 2750.2828\n",
            "Epoch [130/150], Loss: 3751.8607\n",
            "Epoch [140/150], Loss: 2461.7483\n",
            "Epoch [150/150], Loss: 2610.7449\n",
            "Fold 3, RMSE: 93.09773254394531\n",
            "Epoch [10/150], Loss: 13539.7290\n",
            "Epoch [20/150], Loss: 8015.3015\n",
            "Epoch [30/150], Loss: 6915.9844\n",
            "Epoch [40/150], Loss: 5056.9744\n",
            "Epoch [50/150], Loss: 6010.2746\n",
            "Epoch [60/150], Loss: 2858.1548\n",
            "Epoch [70/150], Loss: 6906.7122\n",
            "Epoch [80/150], Loss: 2133.0845\n",
            "Epoch [90/150], Loss: 2827.0248\n",
            "Epoch [100/150], Loss: 3970.7036\n",
            "Epoch [110/150], Loss: 2778.8082\n",
            "Epoch [120/150], Loss: 3765.1765\n",
            "Epoch [130/150], Loss: 2368.7068\n",
            "Epoch [140/150], Loss: 3805.2360\n",
            "Epoch [150/150], Loss: 1535.7099\n",
            "Fold 4, RMSE: 38.76441955566406\n",
            "Epoch [10/150], Loss: 10628.1069\n",
            "Epoch [20/150], Loss: 9926.8230\n",
            "Epoch [30/150], Loss: 7769.7948\n",
            "Epoch [40/150], Loss: 11366.6423\n",
            "Epoch [50/150], Loss: 3175.7480\n",
            "Epoch [60/150], Loss: 5222.3416\n",
            "Epoch [70/150], Loss: 3615.5629\n",
            "Epoch [80/150], Loss: 2966.1351\n",
            "Epoch [90/150], Loss: 2574.9356\n",
            "Epoch [100/150], Loss: 5685.1719\n",
            "Epoch [110/150], Loss: 2248.8887\n",
            "Epoch [120/150], Loss: 1555.3259\n",
            "Epoch [130/150], Loss: 1108.1156\n",
            "Epoch [140/150], Loss: 2294.1872\n",
            "Epoch [150/150], Loss: 1510.7432\n",
            "Fold 5, RMSE: 44.93147277832031\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 59.81900177001953\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 13459.1812\n",
            "Epoch [20/100], Loss: 13698.6638\n",
            "Epoch [30/100], Loss: 7709.1479\n",
            "Epoch [40/100], Loss: 5305.0621\n",
            "Epoch [50/100], Loss: 13071.3350\n",
            "Epoch [60/100], Loss: 9441.6318\n",
            "Epoch [70/100], Loss: 6042.1927\n",
            "Epoch [80/100], Loss: 6992.6838\n",
            "Epoch [90/100], Loss: 7788.4970\n",
            "Epoch [100/100], Loss: 10815.1199\n",
            "Fold 1, RMSE: 58.272422790527344\n",
            "Epoch [10/100], Loss: 16670.4634\n",
            "Epoch [20/100], Loss: 9646.9955\n",
            "Epoch [30/100], Loss: 9684.3806\n",
            "Epoch [40/100], Loss: 10098.7183\n",
            "Epoch [50/100], Loss: 9660.1147\n",
            "Epoch [60/100], Loss: 5170.5302\n",
            "Epoch [70/100], Loss: 5237.4000\n",
            "Epoch [80/100], Loss: 3873.8479\n",
            "Epoch [90/100], Loss: 2803.4504\n",
            "Epoch [100/100], Loss: 3262.5439\n",
            "Fold 2, RMSE: 63.94915008544922\n",
            "Epoch [10/100], Loss: 34915.3291\n",
            "Epoch [20/100], Loss: 8948.9116\n",
            "Epoch [30/100], Loss: 8638.1508\n",
            "Epoch [40/100], Loss: 8898.7937\n",
            "Epoch [50/100], Loss: 4085.4728\n",
            "Epoch [60/100], Loss: 4560.8323\n",
            "Epoch [70/100], Loss: 6893.5730\n",
            "Epoch [80/100], Loss: 7963.9008\n",
            "Epoch [90/100], Loss: 3541.6772\n",
            "Epoch [100/100], Loss: 3549.2256\n",
            "Fold 3, RMSE: 95.76581573486328\n",
            "Epoch [10/100], Loss: 26975.9482\n",
            "Epoch [20/100], Loss: 12326.9072\n",
            "Epoch [30/100], Loss: 9106.9476\n",
            "Epoch [40/100], Loss: 10166.4104\n",
            "Epoch [50/100], Loss: 8820.4011\n",
            "Epoch [60/100], Loss: 6568.7954\n",
            "Epoch [70/100], Loss: 6232.3169\n",
            "Epoch [80/100], Loss: 3564.6907\n",
            "Epoch [90/100], Loss: 3830.9733\n",
            "Epoch [100/100], Loss: 3739.2162\n",
            "Fold 4, RMSE: 38.247459411621094\n",
            "Epoch [10/100], Loss: 16430.1345\n",
            "Epoch [20/100], Loss: 12362.6257\n",
            "Epoch [30/100], Loss: 9609.0376\n",
            "Epoch [40/100], Loss: 7474.9609\n",
            "Epoch [50/100], Loss: 8628.7075\n",
            "Epoch [60/100], Loss: 7299.5178\n",
            "Epoch [70/100], Loss: 6949.5614\n",
            "Epoch [80/100], Loss: 4356.9050\n",
            "Epoch [90/100], Loss: 10609.4146\n",
            "Epoch [100/100], Loss: 5074.4962\n",
            "Fold 5, RMSE: 45.507545471191406\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 60.34847869873047\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 22450.1235\n",
            "Epoch [20/150], Loss: 13550.4851\n",
            "Epoch [30/150], Loss: 9976.6147\n",
            "Epoch [40/150], Loss: 6271.6637\n",
            "Epoch [50/150], Loss: 4785.8535\n",
            "Epoch [60/150], Loss: 7491.1062\n",
            "Epoch [70/150], Loss: 3164.5082\n",
            "Epoch [80/150], Loss: 4182.0808\n",
            "Epoch [90/150], Loss: 6554.6616\n",
            "Epoch [100/150], Loss: 7609.6559\n",
            "Epoch [110/150], Loss: 2214.2924\n",
            "Epoch [120/150], Loss: 2160.6007\n",
            "Epoch [130/150], Loss: 3277.7842\n",
            "Epoch [140/150], Loss: 2946.2820\n",
            "Epoch [150/150], Loss: 4504.2936\n",
            "Fold 1, RMSE: 57.340187072753906\n",
            "Epoch [10/150], Loss: 17807.7959\n",
            "Epoch [20/150], Loss: 9025.5012\n",
            "Epoch [30/150], Loss: 9010.9514\n",
            "Epoch [40/150], Loss: 11387.6741\n",
            "Epoch [50/150], Loss: 8233.1697\n",
            "Epoch [60/150], Loss: 8998.3881\n",
            "Epoch [70/150], Loss: 5541.2731\n",
            "Epoch [80/150], Loss: 5470.8794\n",
            "Epoch [90/150], Loss: 2447.8126\n",
            "Epoch [100/150], Loss: 7355.7224\n",
            "Epoch [110/150], Loss: 4478.3605\n",
            "Epoch [120/150], Loss: 3702.3894\n",
            "Epoch [130/150], Loss: 14134.1353\n",
            "Epoch [140/150], Loss: 4158.3713\n",
            "Epoch [150/150], Loss: 3177.3044\n",
            "Fold 2, RMSE: 63.70207977294922\n",
            "Epoch [10/150], Loss: 8108.3320\n",
            "Epoch [20/150], Loss: 9349.7532\n",
            "Epoch [30/150], Loss: 5944.5352\n",
            "Epoch [40/150], Loss: 6460.1614\n",
            "Epoch [50/150], Loss: 10135.4663\n",
            "Epoch [60/150], Loss: 4245.1373\n",
            "Epoch [70/150], Loss: 4647.0143\n",
            "Epoch [80/150], Loss: 3999.3380\n",
            "Epoch [90/150], Loss: 4724.4183\n",
            "Epoch [100/150], Loss: 3994.8109\n",
            "Epoch [110/150], Loss: 6266.6975\n",
            "Epoch [120/150], Loss: 5903.3145\n",
            "Epoch [130/150], Loss: 5729.7623\n",
            "Epoch [140/150], Loss: 4688.1307\n",
            "Epoch [150/150], Loss: 6682.9014\n",
            "Fold 3, RMSE: 97.74835968017578\n",
            "Epoch [10/150], Loss: 30422.5244\n",
            "Epoch [20/150], Loss: 14681.6809\n",
            "Epoch [30/150], Loss: 16942.6033\n",
            "Epoch [40/150], Loss: 10527.4598\n",
            "Epoch [50/150], Loss: 6373.9009\n",
            "Epoch [60/150], Loss: 8293.9772\n",
            "Epoch [70/150], Loss: 4249.8339\n",
            "Epoch [80/150], Loss: 9473.2595\n",
            "Epoch [90/150], Loss: 7668.8685\n",
            "Epoch [100/150], Loss: 5531.9435\n",
            "Epoch [110/150], Loss: 3506.6381\n",
            "Epoch [120/150], Loss: 2795.6796\n",
            "Epoch [130/150], Loss: 11876.9517\n",
            "Epoch [140/150], Loss: 2745.6790\n",
            "Epoch [150/150], Loss: 7548.1574\n",
            "Fold 4, RMSE: 42.951499938964844\n",
            "Epoch [10/150], Loss: 14464.2832\n",
            "Epoch [20/150], Loss: 12962.0740\n",
            "Epoch [30/150], Loss: 7364.4564\n",
            "Epoch [40/150], Loss: 14447.5392\n",
            "Epoch [50/150], Loss: 4324.2401\n",
            "Epoch [60/150], Loss: 4119.5213\n",
            "Epoch [70/150], Loss: 2854.3880\n",
            "Epoch [80/150], Loss: 6243.7653\n",
            "Epoch [90/150], Loss: 3706.4424\n",
            "Epoch [100/150], Loss: 7069.0297\n",
            "Epoch [110/150], Loss: 6565.4810\n",
            "Epoch [120/150], Loss: 6735.0638\n",
            "Epoch [130/150], Loss: 3193.4885\n",
            "Epoch [140/150], Loss: 3008.4488\n",
            "Epoch [150/150], Loss: 2543.0011\n",
            "Fold 5, RMSE: 46.95130157470703\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 61.738685607910156\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 23026.4717\n",
            "Epoch [20/100], Loss: 11325.9415\n",
            "Epoch [30/100], Loss: 8609.1108\n",
            "Epoch [40/100], Loss: 4657.1766\n",
            "Epoch [50/100], Loss: 3352.4080\n",
            "Epoch [60/100], Loss: 4587.3771\n",
            "Epoch [70/100], Loss: 2813.7551\n",
            "Epoch [80/100], Loss: 2962.7082\n",
            "Epoch [90/100], Loss: 1651.9289\n",
            "Epoch [100/100], Loss: 3658.0255\n",
            "Fold 1, RMSE: 57.828670501708984\n",
            "Epoch [10/100], Loss: 10828.2361\n",
            "Epoch [20/100], Loss: 9531.2019\n",
            "Epoch [30/100], Loss: 7090.4201\n",
            "Epoch [40/100], Loss: 5691.3560\n",
            "Epoch [50/100], Loss: 6108.5105\n",
            "Epoch [60/100], Loss: 4245.9669\n",
            "Epoch [70/100], Loss: 5252.0743\n",
            "Epoch [80/100], Loss: 4064.2963\n",
            "Epoch [90/100], Loss: 4024.9627\n",
            "Epoch [100/100], Loss: 1860.9626\n",
            "Fold 2, RMSE: 73.47014617919922\n",
            "Epoch [10/100], Loss: 9026.4026\n",
            "Epoch [20/100], Loss: 9005.4044\n",
            "Epoch [30/100], Loss: 4430.7501\n",
            "Epoch [40/100], Loss: 5976.4534\n",
            "Epoch [50/100], Loss: 3626.1620\n",
            "Epoch [60/100], Loss: 2550.9276\n",
            "Epoch [70/100], Loss: 4981.0133\n",
            "Epoch [80/100], Loss: 9506.7512\n",
            "Epoch [90/100], Loss: 3673.8351\n",
            "Epoch [100/100], Loss: 3281.0961\n",
            "Fold 3, RMSE: 96.84077453613281\n",
            "Epoch [10/100], Loss: 12551.6655\n",
            "Epoch [20/100], Loss: 10621.6083\n",
            "Epoch [30/100], Loss: 9298.2202\n",
            "Epoch [40/100], Loss: 8133.4359\n",
            "Epoch [50/100], Loss: 3874.4011\n",
            "Epoch [60/100], Loss: 6206.5457\n",
            "Epoch [70/100], Loss: 5453.7992\n",
            "Epoch [80/100], Loss: 6186.6550\n",
            "Epoch [90/100], Loss: 6046.3260\n",
            "Epoch [100/100], Loss: 4089.2589\n",
            "Fold 4, RMSE: 48.80195236206055\n",
            "Epoch [10/100], Loss: 15960.0415\n",
            "Epoch [20/100], Loss: 16583.3931\n",
            "Epoch [30/100], Loss: 8607.6370\n",
            "Epoch [40/100], Loss: 7237.0266\n",
            "Epoch [50/100], Loss: 4966.6974\n",
            "Epoch [60/100], Loss: 4646.1326\n",
            "Epoch [70/100], Loss: 3106.4302\n",
            "Epoch [80/100], Loss: 1735.6308\n",
            "Epoch [90/100], Loss: 8072.7716\n",
            "Epoch [100/100], Loss: 2837.1514\n",
            "Fold 5, RMSE: 47.82294845581055\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 64.95289840698243\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14585.4900\n",
            "Epoch [20/150], Loss: 8233.1157\n",
            "Epoch [30/150], Loss: 5949.0459\n",
            "Epoch [40/150], Loss: 6635.4421\n",
            "Epoch [50/150], Loss: 5009.9197\n",
            "Epoch [60/150], Loss: 4138.2568\n",
            "Epoch [70/150], Loss: 2522.3818\n",
            "Epoch [80/150], Loss: 1995.2039\n",
            "Epoch [90/150], Loss: 3963.5047\n",
            "Epoch [100/150], Loss: 3313.2865\n",
            "Epoch [110/150], Loss: 7423.4008\n",
            "Epoch [120/150], Loss: 2455.2635\n",
            "Epoch [130/150], Loss: 3560.5422\n",
            "Epoch [140/150], Loss: 3369.3193\n",
            "Epoch [150/150], Loss: 2823.9430\n",
            "Fold 1, RMSE: 56.22037124633789\n",
            "Epoch [10/150], Loss: 13513.5854\n",
            "Epoch [20/150], Loss: 10827.7590\n",
            "Epoch [30/150], Loss: 5199.2463\n",
            "Epoch [40/150], Loss: 13486.9517\n",
            "Epoch [50/150], Loss: 4649.1221\n",
            "Epoch [60/150], Loss: 7696.4111\n",
            "Epoch [70/150], Loss: 5703.7695\n",
            "Epoch [80/150], Loss: 6474.4535\n",
            "Epoch [90/150], Loss: 2450.9724\n",
            "Epoch [100/150], Loss: 5100.9302\n",
            "Epoch [110/150], Loss: 3835.2597\n",
            "Epoch [120/150], Loss: 4112.2925\n",
            "Epoch [130/150], Loss: 3789.4056\n",
            "Epoch [140/150], Loss: 1620.7089\n",
            "Epoch [150/150], Loss: 3007.6132\n",
            "Fold 2, RMSE: 66.76689910888672\n",
            "Epoch [10/150], Loss: 7820.5946\n",
            "Epoch [20/150], Loss: 5063.3802\n",
            "Epoch [30/150], Loss: 6228.7550\n",
            "Epoch [40/150], Loss: 9395.9421\n",
            "Epoch [50/150], Loss: 6415.9297\n",
            "Epoch [60/150], Loss: 6261.7190\n",
            "Epoch [70/150], Loss: 7637.2971\n",
            "Epoch [80/150], Loss: 3522.2748\n",
            "Epoch [90/150], Loss: 6509.8874\n",
            "Epoch [100/150], Loss: 3378.1251\n",
            "Epoch [110/150], Loss: 3054.6790\n",
            "Epoch [120/150], Loss: 3956.6060\n",
            "Epoch [130/150], Loss: 7591.2850\n",
            "Epoch [140/150], Loss: 2241.6153\n",
            "Epoch [150/150], Loss: 2927.2587\n",
            "Fold 3, RMSE: 99.67294311523438\n",
            "Epoch [10/150], Loss: 18412.8862\n",
            "Epoch [20/150], Loss: 8999.0790\n",
            "Epoch [30/150], Loss: 10041.4468\n",
            "Epoch [40/150], Loss: 5858.2588\n",
            "Epoch [50/150], Loss: 5085.8652\n",
            "Epoch [60/150], Loss: 7021.3760\n",
            "Epoch [70/150], Loss: 4387.2596\n",
            "Epoch [80/150], Loss: 2773.6167\n",
            "Epoch [90/150], Loss: 4294.6564\n",
            "Epoch [100/150], Loss: 4915.8333\n",
            "Epoch [110/150], Loss: 4226.6865\n",
            "Epoch [120/150], Loss: 2637.9749\n",
            "Epoch [130/150], Loss: 2597.0424\n",
            "Epoch [140/150], Loss: 1332.9544\n",
            "Epoch [150/150], Loss: 1065.2668\n",
            "Fold 4, RMSE: 40.9329833984375\n",
            "Epoch [10/150], Loss: 20139.3545\n",
            "Epoch [20/150], Loss: 14211.8079\n",
            "Epoch [30/150], Loss: 10847.6760\n",
            "Epoch [40/150], Loss: 5022.6807\n",
            "Epoch [50/150], Loss: 9604.7786\n",
            "Epoch [60/150], Loss: 4416.1581\n",
            "Epoch [70/150], Loss: 5496.6201\n",
            "Epoch [80/150], Loss: 7642.8160\n",
            "Epoch [90/150], Loss: 2143.1093\n",
            "Epoch [100/150], Loss: 3176.8066\n",
            "Epoch [110/150], Loss: 1520.7769\n",
            "Epoch [120/150], Loss: 2726.0347\n",
            "Epoch [130/150], Loss: 3410.0015\n",
            "Epoch [140/150], Loss: 1047.8753\n",
            "Epoch [150/150], Loss: 3568.3395\n",
            "Fold 5, RMSE: 44.85732650756836\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 61.69010467529297\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17091.4414\n",
            "Epoch [20/100], Loss: 14149.6782\n",
            "Epoch [30/100], Loss: 7615.6166\n",
            "Epoch [40/100], Loss: 4798.7455\n",
            "Epoch [50/100], Loss: 6083.1218\n",
            "Epoch [60/100], Loss: 4050.4709\n",
            "Epoch [70/100], Loss: 3686.9902\n",
            "Epoch [80/100], Loss: 5013.6621\n",
            "Epoch [90/100], Loss: 4698.1210\n",
            "Epoch [100/100], Loss: 1959.9551\n",
            "Fold 1, RMSE: 60.5251350402832\n",
            "Epoch [10/100], Loss: 14032.5210\n",
            "Epoch [20/100], Loss: 14463.9163\n",
            "Epoch [30/100], Loss: 9338.2970\n",
            "Epoch [40/100], Loss: 13768.7615\n",
            "Epoch [50/100], Loss: 5853.2290\n",
            "Epoch [60/100], Loss: 5159.4188\n",
            "Epoch [70/100], Loss: 8664.8346\n",
            "Epoch [80/100], Loss: 4793.1313\n",
            "Epoch [90/100], Loss: 5519.9480\n",
            "Epoch [100/100], Loss: 5304.4310\n",
            "Fold 2, RMSE: 67.764404296875\n",
            "Epoch [10/100], Loss: 15868.3459\n",
            "Epoch [20/100], Loss: 8266.7183\n",
            "Epoch [30/100], Loss: 9640.9004\n",
            "Epoch [40/100], Loss: 9069.5630\n",
            "Epoch [50/100], Loss: 9049.4221\n",
            "Epoch [60/100], Loss: 3311.2361\n",
            "Epoch [70/100], Loss: 3567.8726\n",
            "Epoch [80/100], Loss: 6454.3414\n",
            "Epoch [90/100], Loss: 4751.4091\n",
            "Epoch [100/100], Loss: 4160.6574\n",
            "Fold 3, RMSE: 95.01109313964844\n",
            "Epoch [10/100], Loss: 18077.7817\n",
            "Epoch [20/100], Loss: 10586.5734\n",
            "Epoch [30/100], Loss: 10181.5952\n",
            "Epoch [40/100], Loss: 10956.4587\n",
            "Epoch [50/100], Loss: 3192.4037\n",
            "Epoch [60/100], Loss: 5605.1049\n",
            "Epoch [70/100], Loss: 7598.6523\n",
            "Epoch [80/100], Loss: 12001.0756\n",
            "Epoch [90/100], Loss: 8638.6526\n",
            "Epoch [100/100], Loss: 8009.4185\n",
            "Fold 4, RMSE: 40.21451950073242\n",
            "Epoch [10/100], Loss: 25435.9976\n",
            "Epoch [20/100], Loss: 15868.6646\n",
            "Epoch [30/100], Loss: 14409.6121\n",
            "Epoch [40/100], Loss: 13724.1172\n",
            "Epoch [50/100], Loss: 11369.5403\n",
            "Epoch [60/100], Loss: 13203.3425\n",
            "Epoch [70/100], Loss: 10425.9485\n",
            "Epoch [80/100], Loss: 11521.1116\n",
            "Epoch [90/100], Loss: 5860.1145\n",
            "Epoch [100/100], Loss: 6250.9862\n",
            "Fold 5, RMSE: 45.17353439331055\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 61.73773727416992\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 15246.8457\n",
            "Epoch [20/150], Loss: 13972.4548\n",
            "Epoch [30/150], Loss: 12818.7117\n",
            "Epoch [40/150], Loss: 10259.7128\n",
            "Epoch [50/150], Loss: 8371.6810\n",
            "Epoch [60/150], Loss: 6141.0371\n",
            "Epoch [70/150], Loss: 6029.1750\n",
            "Epoch [80/150], Loss: 8075.4601\n",
            "Epoch [90/150], Loss: 7397.6177\n",
            "Epoch [100/150], Loss: 5129.6189\n",
            "Epoch [110/150], Loss: 9140.8020\n",
            "Epoch [120/150], Loss: 3011.1704\n",
            "Epoch [130/150], Loss: 2221.4353\n",
            "Epoch [140/150], Loss: 3237.5134\n",
            "Epoch [150/150], Loss: 7210.7035\n",
            "Fold 1, RMSE: 58.71538162231445\n",
            "Epoch [10/150], Loss: 18131.6514\n",
            "Epoch [20/150], Loss: 17524.1150\n",
            "Epoch [30/150], Loss: 18689.1030\n",
            "Epoch [40/150], Loss: 23354.0454\n",
            "Epoch [50/150], Loss: 15473.3130\n",
            "Epoch [60/150], Loss: 14923.1279\n",
            "Epoch [70/150], Loss: 23863.9861\n",
            "Epoch [80/150], Loss: 18282.3474\n",
            "Epoch [90/150], Loss: 15746.0312\n",
            "Epoch [100/150], Loss: 17959.7900\n",
            "Epoch [110/150], Loss: 14024.4541\n",
            "Epoch [120/150], Loss: 15693.4314\n",
            "Epoch [130/150], Loss: 25124.2297\n",
            "Epoch [140/150], Loss: 20480.5745\n",
            "Epoch [150/150], Loss: 15065.1025\n",
            "Fold 2, RMSE: 87.11773681640625\n",
            "Epoch [10/150], Loss: 12993.3130\n",
            "Epoch [20/150], Loss: 11495.2627\n",
            "Epoch [30/150], Loss: 6999.0415\n",
            "Epoch [40/150], Loss: 5393.0416\n",
            "Epoch [50/150], Loss: 5801.2880\n",
            "Epoch [60/150], Loss: 3631.2070\n",
            "Epoch [70/150], Loss: 3528.3708\n",
            "Epoch [80/150], Loss: 1852.3912\n",
            "Epoch [90/150], Loss: 5238.8582\n",
            "Epoch [100/150], Loss: 7302.4753\n",
            "Epoch [110/150], Loss: 3817.1268\n",
            "Epoch [120/150], Loss: 5600.8121\n",
            "Epoch [130/150], Loss: 6731.5219\n",
            "Epoch [140/150], Loss: 4709.2195\n",
            "Epoch [150/150], Loss: 2897.5297\n",
            "Fold 3, RMSE: 100.08240509033203\n",
            "Epoch [10/150], Loss: 19730.6179\n",
            "Epoch [20/150], Loss: 17613.8855\n",
            "Epoch [30/150], Loss: 17151.7144\n",
            "Epoch [40/150], Loss: 19544.8462\n",
            "Epoch [50/150], Loss: 9071.2825\n",
            "Epoch [60/150], Loss: 7766.7014\n",
            "Epoch [70/150], Loss: 14365.7137\n",
            "Epoch [80/150], Loss: 5462.3873\n",
            "Epoch [90/150], Loss: 16848.0044\n",
            "Epoch [100/150], Loss: 6660.5270\n",
            "Epoch [110/150], Loss: 9003.3142\n",
            "Epoch [120/150], Loss: 5139.2455\n",
            "Epoch [130/150], Loss: 5041.7801\n",
            "Epoch [140/150], Loss: 8275.8545\n",
            "Epoch [150/150], Loss: 8084.2822\n",
            "Fold 4, RMSE: 44.49663543701172\n",
            "Epoch [10/150], Loss: 16714.9719\n",
            "Epoch [20/150], Loss: 10426.0016\n",
            "Epoch [30/150], Loss: 11836.5093\n",
            "Epoch [40/150], Loss: 11323.9659\n",
            "Epoch [50/150], Loss: 9817.1069\n",
            "Epoch [60/150], Loss: 9316.4248\n",
            "Epoch [70/150], Loss: 8733.9062\n",
            "Epoch [80/150], Loss: 2864.1216\n",
            "Epoch [90/150], Loss: 8708.2058\n",
            "Epoch [100/150], Loss: 6239.4579\n",
            "Epoch [110/150], Loss: 8421.7563\n",
            "Epoch [120/150], Loss: 10730.1558\n",
            "Epoch [130/150], Loss: 2766.9167\n",
            "Epoch [140/150], Loss: 3505.1460\n",
            "Epoch [150/150], Loss: 6836.5625\n",
            "Fold 5, RMSE: 52.18739318847656\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 68.5199104309082\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 13677.6797\n",
            "Epoch [20/100], Loss: 6585.4335\n",
            "Epoch [30/100], Loss: 7296.9110\n",
            "Epoch [40/100], Loss: 3161.3010\n",
            "Epoch [50/100], Loss: 4546.1060\n",
            "Epoch [60/100], Loss: 4709.8168\n",
            "Epoch [70/100], Loss: 3203.9418\n",
            "Epoch [80/100], Loss: 1956.7674\n",
            "Epoch [90/100], Loss: 4208.6955\n",
            "Epoch [100/100], Loss: 3774.6463\n",
            "Fold 1, RMSE: 55.544071197509766\n",
            "Epoch [10/100], Loss: 14484.5591\n",
            "Epoch [20/100], Loss: 9825.4061\n",
            "Epoch [30/100], Loss: 7438.6517\n",
            "Epoch [40/100], Loss: 6061.1310\n",
            "Epoch [50/100], Loss: 6468.8070\n",
            "Epoch [60/100], Loss: 3391.8251\n",
            "Epoch [70/100], Loss: 2558.5358\n",
            "Epoch [80/100], Loss: 5003.0309\n",
            "Epoch [90/100], Loss: 6999.7108\n",
            "Epoch [100/100], Loss: 4460.5436\n",
            "Fold 2, RMSE: 65.19552612304688\n",
            "Epoch [10/100], Loss: 9539.8257\n",
            "Epoch [20/100], Loss: 5874.6906\n",
            "Epoch [30/100], Loss: 10941.6975\n",
            "Epoch [40/100], Loss: 3125.9296\n",
            "Epoch [50/100], Loss: 8026.9872\n",
            "Epoch [60/100], Loss: 2005.5132\n",
            "Epoch [70/100], Loss: 7620.9731\n",
            "Epoch [80/100], Loss: 2912.2141\n",
            "Epoch [90/100], Loss: 2100.3390\n",
            "Epoch [100/100], Loss: 2309.0606\n",
            "Fold 3, RMSE: 92.56741333007812\n",
            "Epoch [10/100], Loss: 13800.9600\n",
            "Epoch [20/100], Loss: 8849.6823\n",
            "Epoch [30/100], Loss: 5699.6296\n",
            "Epoch [40/100], Loss: 3178.7015\n",
            "Epoch [50/100], Loss: 6004.6057\n",
            "Epoch [60/100], Loss: 5719.9602\n",
            "Epoch [70/100], Loss: 2039.2240\n",
            "Epoch [80/100], Loss: 1986.6999\n",
            "Epoch [90/100], Loss: 1442.3227\n",
            "Epoch [100/100], Loss: 2887.7701\n",
            "Fold 4, RMSE: 43.45195770263672\n",
            "Epoch [10/100], Loss: 13600.1074\n",
            "Epoch [20/100], Loss: 8403.4985\n",
            "Epoch [30/100], Loss: 5421.2411\n",
            "Epoch [40/100], Loss: 3780.1727\n",
            "Epoch [50/100], Loss: 2415.0018\n",
            "Epoch [60/100], Loss: 3922.7668\n",
            "Epoch [70/100], Loss: 2904.2945\n",
            "Epoch [80/100], Loss: 2954.2512\n",
            "Epoch [90/100], Loss: 1069.4176\n",
            "Epoch [100/100], Loss: 2063.3112\n",
            "Fold 5, RMSE: 45.50645065307617\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 60.45308380126953\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 16327.6318\n",
            "Epoch [20/150], Loss: 10744.2434\n",
            "Epoch [30/150], Loss: 4020.9766\n",
            "Epoch [40/150], Loss: 2394.6644\n",
            "Epoch [50/150], Loss: 2713.2558\n",
            "Epoch [60/150], Loss: 2406.8962\n",
            "Epoch [70/150], Loss: 1902.3596\n",
            "Epoch [80/150], Loss: 3059.2286\n",
            "Epoch [90/150], Loss: 7062.3018\n",
            "Epoch [100/150], Loss: 2531.6398\n",
            "Epoch [110/150], Loss: 1487.9433\n",
            "Epoch [120/150], Loss: 1259.1840\n",
            "Epoch [130/150], Loss: 1572.8109\n",
            "Epoch [140/150], Loss: 1051.0212\n",
            "Epoch [150/150], Loss: 2307.3491\n",
            "Fold 1, RMSE: 58.58414840698242\n",
            "Epoch [10/150], Loss: 10847.3383\n",
            "Epoch [20/150], Loss: 11366.8779\n",
            "Epoch [30/150], Loss: 12344.3506\n",
            "Epoch [40/150], Loss: 6725.8955\n",
            "Epoch [50/150], Loss: 3643.4069\n",
            "Epoch [60/150], Loss: 4946.4407\n",
            "Epoch [70/150], Loss: 2546.2819\n",
            "Epoch [80/150], Loss: 2723.6260\n",
            "Epoch [90/150], Loss: 1639.0056\n",
            "Epoch [100/150], Loss: 3807.3697\n",
            "Epoch [110/150], Loss: 2638.9857\n",
            "Epoch [120/150], Loss: 2908.3312\n",
            "Epoch [130/150], Loss: 4577.0739\n",
            "Epoch [140/150], Loss: 4853.6528\n",
            "Epoch [150/150], Loss: 1794.8757\n",
            "Fold 2, RMSE: 70.82002258300781\n",
            "Epoch [10/150], Loss: 12406.5164\n",
            "Epoch [20/150], Loss: 11980.4304\n",
            "Epoch [30/150], Loss: 4617.1529\n",
            "Epoch [40/150], Loss: 4140.7081\n",
            "Epoch [50/150], Loss: 4857.1300\n",
            "Epoch [60/150], Loss: 4752.2660\n",
            "Epoch [70/150], Loss: 2847.5345\n",
            "Epoch [80/150], Loss: 3860.4777\n",
            "Epoch [90/150], Loss: 2276.7845\n",
            "Epoch [100/150], Loss: 3121.9693\n",
            "Epoch [110/150], Loss: 1699.8301\n",
            "Epoch [120/150], Loss: 2952.9167\n",
            "Epoch [130/150], Loss: 1131.3952\n",
            "Epoch [140/150], Loss: 2437.9885\n",
            "Epoch [150/150], Loss: 2643.7559\n",
            "Fold 3, RMSE: 92.3305892944336\n",
            "Epoch [10/150], Loss: 13483.1886\n",
            "Epoch [20/150], Loss: 10264.1598\n",
            "Epoch [30/150], Loss: 9364.1741\n",
            "Epoch [40/150], Loss: 5983.0116\n",
            "Epoch [50/150], Loss: 5985.2211\n",
            "Epoch [60/150], Loss: 2988.5486\n",
            "Epoch [70/150], Loss: 4655.3733\n",
            "Epoch [80/150], Loss: 3207.4565\n",
            "Epoch [90/150], Loss: 4927.9310\n",
            "Epoch [100/150], Loss: 3101.9757\n",
            "Epoch [110/150], Loss: 3083.6572\n",
            "Epoch [120/150], Loss: 8173.9714\n",
            "Epoch [130/150], Loss: 2726.6782\n",
            "Epoch [140/150], Loss: 8099.6084\n",
            "Epoch [150/150], Loss: 3106.5723\n",
            "Fold 4, RMSE: 41.88903045654297\n",
            "Epoch [10/150], Loss: 12907.4417\n",
            "Epoch [20/150], Loss: 9433.5637\n",
            "Epoch [30/150], Loss: 11286.7734\n",
            "Epoch [40/150], Loss: 9925.2477\n",
            "Epoch [50/150], Loss: 7099.1447\n",
            "Epoch [60/150], Loss: 2558.3607\n",
            "Epoch [70/150], Loss: 3397.8566\n",
            "Epoch [80/150], Loss: 6909.4543\n",
            "Epoch [90/150], Loss: 3263.5408\n",
            "Epoch [100/150], Loss: 6975.0419\n",
            "Epoch [110/150], Loss: 1789.8151\n",
            "Epoch [120/150], Loss: 1838.5783\n",
            "Epoch [130/150], Loss: 2327.2780\n",
            "Epoch [140/150], Loss: 2059.4996\n",
            "Epoch [150/150], Loss: 1849.6379\n",
            "Fold 5, RMSE: 44.2369270324707\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 61.5721435546875\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 13819.0962\n",
            "Epoch [20/100], Loss: 10402.8673\n",
            "Epoch [30/100], Loss: 10038.5814\n",
            "Epoch [40/100], Loss: 8922.1394\n",
            "Epoch [50/100], Loss: 4547.1227\n",
            "Epoch [60/100], Loss: 4682.7341\n",
            "Epoch [70/100], Loss: 4160.3368\n",
            "Epoch [80/100], Loss: 6937.2755\n",
            "Epoch [90/100], Loss: 6248.0094\n",
            "Epoch [100/100], Loss: 6806.9255\n",
            "Fold 1, RMSE: 59.96512222290039\n",
            "Epoch [10/100], Loss: 20233.4441\n",
            "Epoch [20/100], Loss: 10090.7023\n",
            "Epoch [30/100], Loss: 9862.8214\n",
            "Epoch [40/100], Loss: 7517.9302\n",
            "Epoch [50/100], Loss: 6115.2573\n",
            "Epoch [60/100], Loss: 6449.1770\n",
            "Epoch [70/100], Loss: 13116.8130\n",
            "Epoch [80/100], Loss: 3210.6711\n",
            "Epoch [90/100], Loss: 16495.7081\n",
            "Epoch [100/100], Loss: 6440.2141\n",
            "Fold 2, RMSE: 68.57389831542969\n",
            "Epoch [10/100], Loss: 11083.0698\n",
            "Epoch [20/100], Loss: 9302.2737\n",
            "Epoch [30/100], Loss: 9088.4740\n",
            "Epoch [40/100], Loss: 6966.7822\n",
            "Epoch [50/100], Loss: 6321.2213\n",
            "Epoch [60/100], Loss: 4907.7850\n",
            "Epoch [70/100], Loss: 4400.7320\n",
            "Epoch [80/100], Loss: 5885.8158\n",
            "Epoch [90/100], Loss: 4909.1584\n",
            "Epoch [100/100], Loss: 8722.1237\n",
            "Fold 3, RMSE: 95.7840576171875\n",
            "Epoch [10/100], Loss: 30646.0845\n",
            "Epoch [20/100], Loss: 14395.6694\n",
            "Epoch [30/100], Loss: 12616.8240\n",
            "Epoch [40/100], Loss: 11171.2661\n",
            "Epoch [50/100], Loss: 6928.8746\n",
            "Epoch [60/100], Loss: 5878.5776\n",
            "Epoch [70/100], Loss: 8566.5308\n",
            "Epoch [80/100], Loss: 6837.6823\n",
            "Epoch [90/100], Loss: 5650.7279\n",
            "Epoch [100/100], Loss: 6517.0636\n",
            "Fold 4, RMSE: 47.95745849609375\n",
            "Epoch [10/100], Loss: 24639.8970\n",
            "Epoch [20/100], Loss: 16391.6436\n",
            "Epoch [30/100], Loss: 11943.8673\n",
            "Epoch [40/100], Loss: 5871.2764\n",
            "Epoch [50/100], Loss: 6006.9612\n",
            "Epoch [60/100], Loss: 9259.2928\n",
            "Epoch [70/100], Loss: 5061.0316\n",
            "Epoch [80/100], Loss: 7090.4182\n",
            "Epoch [90/100], Loss: 4648.9211\n",
            "Epoch [100/100], Loss: 5365.7654\n",
            "Fold 5, RMSE: 44.131832122802734\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 63.282473754882815\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 20619.4951\n",
            "Epoch [20/150], Loss: 13768.5457\n",
            "Epoch [30/150], Loss: 8570.4270\n",
            "Epoch [40/150], Loss: 11360.7920\n",
            "Epoch [50/150], Loss: 6376.9318\n",
            "Epoch [60/150], Loss: 3917.2368\n",
            "Epoch [70/150], Loss: 8099.1957\n",
            "Epoch [80/150], Loss: 3805.4240\n",
            "Epoch [90/150], Loss: 10908.7002\n",
            "Epoch [100/150], Loss: 4363.4280\n",
            "Epoch [110/150], Loss: 3289.0222\n",
            "Epoch [120/150], Loss: 5269.2826\n",
            "Epoch [130/150], Loss: 3154.6609\n",
            "Epoch [140/150], Loss: 2014.9724\n",
            "Epoch [150/150], Loss: 4937.5428\n",
            "Fold 1, RMSE: 58.24088668823242\n",
            "Epoch [10/150], Loss: 20026.0054\n",
            "Epoch [20/150], Loss: 19361.8979\n",
            "Epoch [30/150], Loss: 8693.0989\n",
            "Epoch [40/150], Loss: 11750.4026\n",
            "Epoch [50/150], Loss: 7025.4701\n",
            "Epoch [60/150], Loss: 4963.1405\n",
            "Epoch [70/150], Loss: 5165.7859\n",
            "Epoch [80/150], Loss: 4938.1857\n",
            "Epoch [90/150], Loss: 3088.1168\n",
            "Epoch [100/150], Loss: 7606.5751\n",
            "Epoch [110/150], Loss: 4700.0288\n",
            "Epoch [120/150], Loss: 7268.4377\n",
            "Epoch [130/150], Loss: 1812.3273\n",
            "Epoch [140/150], Loss: 5670.5145\n",
            "Epoch [150/150], Loss: 6873.6540\n",
            "Fold 2, RMSE: 69.276123046875\n",
            "Epoch [10/150], Loss: 13499.5183\n",
            "Epoch [20/150], Loss: 9782.4241\n",
            "Epoch [30/150], Loss: 7272.7056\n",
            "Epoch [40/150], Loss: 8277.2712\n",
            "Epoch [50/150], Loss: 7595.6428\n",
            "Epoch [60/150], Loss: 4619.7335\n",
            "Epoch [70/150], Loss: 5239.5326\n",
            "Epoch [80/150], Loss: 4187.8372\n",
            "Epoch [90/150], Loss: 2735.2451\n",
            "Epoch [100/150], Loss: 4021.0132\n",
            "Epoch [110/150], Loss: 4681.0573\n",
            "Epoch [120/150], Loss: 2137.8355\n",
            "Epoch [130/150], Loss: 1736.3991\n",
            "Epoch [140/150], Loss: 3785.7554\n",
            "Epoch [150/150], Loss: 6222.2365\n",
            "Fold 3, RMSE: 106.47239685058594\n",
            "Epoch [10/150], Loss: 20910.9844\n",
            "Epoch [20/150], Loss: 20066.6274\n",
            "Epoch [30/150], Loss: 9289.8644\n",
            "Epoch [40/150], Loss: 14173.4622\n",
            "Epoch [50/150], Loss: 14699.0297\n",
            "Epoch [60/150], Loss: 8566.0408\n",
            "Epoch [70/150], Loss: 6751.0896\n",
            "Epoch [80/150], Loss: 9616.8582\n",
            "Epoch [90/150], Loss: 2066.2397\n",
            "Epoch [100/150], Loss: 3641.8449\n",
            "Epoch [110/150], Loss: 8839.9447\n",
            "Epoch [120/150], Loss: 8933.8634\n",
            "Epoch [130/150], Loss: 4795.9470\n",
            "Epoch [140/150], Loss: 2986.3976\n",
            "Epoch [150/150], Loss: 6415.8706\n",
            "Fold 4, RMSE: 50.11090087890625\n",
            "Epoch [10/150], Loss: 16790.2935\n",
            "Epoch [20/150], Loss: 10998.2695\n",
            "Epoch [30/150], Loss: 5686.9781\n",
            "Epoch [40/150], Loss: 6960.1340\n",
            "Epoch [50/150], Loss: 4191.8738\n",
            "Epoch [60/150], Loss: 4276.5935\n",
            "Epoch [70/150], Loss: 3185.9919\n",
            "Epoch [80/150], Loss: 4762.3741\n",
            "Epoch [90/150], Loss: 4098.5884\n",
            "Epoch [100/150], Loss: 2218.2192\n",
            "Epoch [110/150], Loss: 4628.1146\n",
            "Epoch [120/150], Loss: 857.7654\n",
            "Epoch [130/150], Loss: 4868.0788\n",
            "Epoch [140/150], Loss: 7402.6925\n",
            "Epoch [150/150], Loss: 4184.7516\n",
            "Fold 5, RMSE: 54.75975036621094\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 67.7720115661621\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 12152.9924\n",
            "Epoch [20/100], Loss: 12792.9822\n",
            "Epoch [30/100], Loss: 7807.8018\n",
            "Epoch [40/100], Loss: 2772.8507\n",
            "Epoch [50/100], Loss: 6180.6268\n",
            "Epoch [60/100], Loss: 3673.8552\n",
            "Epoch [70/100], Loss: 7292.5909\n",
            "Epoch [80/100], Loss: 5783.5201\n",
            "Epoch [90/100], Loss: 8483.4938\n",
            "Epoch [100/100], Loss: 4852.5575\n",
            "Fold 1, RMSE: 52.68397903442383\n",
            "Epoch [10/100], Loss: 12829.2175\n",
            "Epoch [20/100], Loss: 9073.5295\n",
            "Epoch [30/100], Loss: 3163.5783\n",
            "Epoch [40/100], Loss: 8001.8829\n",
            "Epoch [50/100], Loss: 6847.6925\n",
            "Epoch [60/100], Loss: 14779.7487\n",
            "Epoch [70/100], Loss: 3402.4060\n",
            "Epoch [80/100], Loss: 2908.7140\n",
            "Epoch [90/100], Loss: 1488.5638\n",
            "Epoch [100/100], Loss: 2912.3819\n",
            "Fold 2, RMSE: 72.99537658691406\n",
            "Epoch [10/100], Loss: 7701.3738\n",
            "Epoch [20/100], Loss: 8503.7919\n",
            "Epoch [30/100], Loss: 4828.7180\n",
            "Epoch [40/100], Loss: 4357.8947\n",
            "Epoch [50/100], Loss: 4332.2909\n",
            "Epoch [60/100], Loss: 3309.5132\n",
            "Epoch [70/100], Loss: 3710.5190\n",
            "Epoch [80/100], Loss: 6567.2245\n",
            "Epoch [90/100], Loss: 7164.0275\n",
            "Epoch [100/100], Loss: 3386.1869\n",
            "Fold 3, RMSE: 96.367431640625\n",
            "Epoch [10/100], Loss: 13202.8591\n",
            "Epoch [20/100], Loss: 8182.1436\n",
            "Epoch [30/100], Loss: 5379.6625\n",
            "Epoch [40/100], Loss: 4892.7712\n",
            "Epoch [50/100], Loss: 6074.9373\n",
            "Epoch [60/100], Loss: 2956.7126\n",
            "Epoch [70/100], Loss: 2748.7732\n",
            "Epoch [80/100], Loss: 5014.8077\n",
            "Epoch [90/100], Loss: 4076.5886\n",
            "Epoch [100/100], Loss: 2550.7101\n",
            "Fold 4, RMSE: 42.36539840698242\n",
            "Epoch [10/100], Loss: 19331.3164\n",
            "Epoch [20/100], Loss: 9243.6625\n",
            "Epoch [30/100], Loss: 10112.9624\n",
            "Epoch [40/100], Loss: 6829.7183\n",
            "Epoch [50/100], Loss: 10283.2400\n",
            "Epoch [60/100], Loss: 6354.5454\n",
            "Epoch [70/100], Loss: 3430.3154\n",
            "Epoch [80/100], Loss: 3722.6182\n",
            "Epoch [90/100], Loss: 2190.8195\n",
            "Epoch [100/100], Loss: 2276.4776\n",
            "Fold 5, RMSE: 44.96989822387695\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 61.876416778564455\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 9842.1200\n",
            "Epoch [20/150], Loss: 9050.7134\n",
            "Epoch [30/150], Loss: 4069.0641\n",
            "Epoch [40/150], Loss: 4193.0618\n",
            "Epoch [50/150], Loss: 4684.0468\n",
            "Epoch [60/150], Loss: 1853.2179\n",
            "Epoch [70/150], Loss: 3034.3823\n",
            "Epoch [80/150], Loss: 2144.8394\n",
            "Epoch [90/150], Loss: 2178.9528\n",
            "Epoch [100/150], Loss: 2547.0572\n",
            "Epoch [110/150], Loss: 2003.7144\n",
            "Epoch [120/150], Loss: 4644.1772\n",
            "Epoch [130/150], Loss: 2600.1982\n",
            "Epoch [140/150], Loss: 2128.7291\n",
            "Epoch [150/150], Loss: 2647.1507\n",
            "Fold 1, RMSE: 54.41068649291992\n",
            "Epoch [10/150], Loss: 25733.2769\n",
            "Epoch [20/150], Loss: 10389.9963\n",
            "Epoch [30/150], Loss: 8016.9194\n",
            "Epoch [40/150], Loss: 7799.9417\n",
            "Epoch [50/150], Loss: 5759.8890\n",
            "Epoch [60/150], Loss: 4312.2244\n",
            "Epoch [70/150], Loss: 6769.0079\n",
            "Epoch [80/150], Loss: 10038.7366\n",
            "Epoch [90/150], Loss: 6154.9465\n",
            "Epoch [100/150], Loss: 1821.5513\n",
            "Epoch [110/150], Loss: 5472.3899\n",
            "Epoch [120/150], Loss: 4389.9649\n",
            "Epoch [130/150], Loss: 4454.8021\n",
            "Epoch [140/150], Loss: 1293.1742\n",
            "Epoch [150/150], Loss: 2929.7465\n",
            "Fold 2, RMSE: 62.72315979003906\n",
            "Epoch [10/150], Loss: 8955.8877\n",
            "Epoch [20/150], Loss: 7943.2280\n",
            "Epoch [30/150], Loss: 11915.3926\n",
            "Epoch [40/150], Loss: 7144.6884\n",
            "Epoch [50/150], Loss: 6872.5535\n",
            "Epoch [60/150], Loss: 6806.2859\n",
            "Epoch [70/150], Loss: 3821.9680\n",
            "Epoch [80/150], Loss: 4467.0378\n",
            "Epoch [90/150], Loss: 3870.5641\n",
            "Epoch [100/150], Loss: 3960.9590\n",
            "Epoch [110/150], Loss: 4636.2958\n",
            "Epoch [120/150], Loss: 5615.9973\n",
            "Epoch [130/150], Loss: 4262.4576\n",
            "Epoch [140/150], Loss: 5549.8416\n",
            "Epoch [150/150], Loss: 5569.9763\n",
            "Fold 3, RMSE: 93.626953125\n",
            "Epoch [10/150], Loss: 16958.9023\n",
            "Epoch [20/150], Loss: 9779.6543\n",
            "Epoch [30/150], Loss: 8500.5111\n",
            "Epoch [40/150], Loss: 9996.4448\n",
            "Epoch [50/150], Loss: 5450.9829\n",
            "Epoch [60/150], Loss: 4492.8899\n",
            "Epoch [70/150], Loss: 3390.5818\n",
            "Epoch [80/150], Loss: 5382.6537\n",
            "Epoch [90/150], Loss: 2435.6357\n",
            "Epoch [100/150], Loss: 4578.6242\n",
            "Epoch [110/150], Loss: 6757.5233\n",
            "Epoch [120/150], Loss: 2260.2157\n",
            "Epoch [130/150], Loss: 1430.2388\n",
            "Epoch [140/150], Loss: 6159.8646\n",
            "Epoch [150/150], Loss: 5215.4570\n",
            "Fold 4, RMSE: 46.33464050292969\n",
            "Epoch [10/150], Loss: 13166.2239\n",
            "Epoch [20/150], Loss: 16082.5186\n",
            "Epoch [30/150], Loss: 5538.4257\n",
            "Epoch [40/150], Loss: 5652.0219\n",
            "Epoch [50/150], Loss: 16225.8683\n",
            "Epoch [60/150], Loss: 3788.9398\n",
            "Epoch [70/150], Loss: 2681.1978\n",
            "Epoch [80/150], Loss: 2524.7604\n",
            "Epoch [90/150], Loss: 2785.6965\n",
            "Epoch [100/150], Loss: 2405.9392\n",
            "Epoch [110/150], Loss: 2630.0447\n",
            "Epoch [120/150], Loss: 1933.2039\n",
            "Epoch [130/150], Loss: 1572.9448\n",
            "Epoch [140/150], Loss: 3723.5633\n",
            "Epoch [150/150], Loss: 1702.4447\n",
            "Fold 5, RMSE: 44.29389190673828\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 60.27786636352539\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 98026.0664\n",
            "Epoch [20/100], Loss: 22560.9697\n",
            "Epoch [30/100], Loss: 11937.7234\n",
            "Epoch [40/100], Loss: 14461.0691\n",
            "Epoch [50/100], Loss: 9005.2612\n",
            "Epoch [60/100], Loss: 7454.8542\n",
            "Epoch [70/100], Loss: 8192.1774\n",
            "Epoch [80/100], Loss: 4587.2233\n",
            "Epoch [90/100], Loss: 11944.7366\n",
            "Epoch [100/100], Loss: 5065.8951\n",
            "Fold 1, RMSE: 58.92133331298828\n",
            "Epoch [10/100], Loss: 334386.9453\n",
            "Epoch [20/100], Loss: 47787.1328\n",
            "Epoch [30/100], Loss: 21953.9819\n",
            "Epoch [40/100], Loss: 18686.2778\n",
            "Epoch [50/100], Loss: 15768.7271\n",
            "Epoch [60/100], Loss: 9886.0563\n",
            "Epoch [70/100], Loss: 15253.7703\n",
            "Epoch [80/100], Loss: 11983.1582\n",
            "Epoch [90/100], Loss: 17488.8230\n",
            "Epoch [100/100], Loss: 7959.3304\n",
            "Fold 2, RMSE: 67.32936096191406\n",
            "Epoch [10/100], Loss: 192506.2109\n",
            "Epoch [20/100], Loss: 15713.2637\n",
            "Epoch [30/100], Loss: 15264.6350\n",
            "Epoch [40/100], Loss: 8521.9388\n",
            "Epoch [50/100], Loss: 9044.2839\n",
            "Epoch [60/100], Loss: 11113.6565\n",
            "Epoch [70/100], Loss: 8981.1685\n",
            "Epoch [80/100], Loss: 8932.3601\n",
            "Epoch [90/100], Loss: 8654.2682\n",
            "Epoch [100/100], Loss: 5799.9309\n",
            "Fold 3, RMSE: 90.6859359741211\n",
            "Epoch [10/100], Loss: 22845.0977\n",
            "Epoch [20/100], Loss: 22742.0679\n",
            "Epoch [30/100], Loss: 20150.3318\n",
            "Epoch [40/100], Loss: 17650.7812\n",
            "Epoch [50/100], Loss: 11031.2134\n",
            "Epoch [60/100], Loss: 14742.6145\n",
            "Epoch [70/100], Loss: 10724.6506\n",
            "Epoch [80/100], Loss: 20431.6145\n",
            "Epoch [90/100], Loss: 8017.2156\n",
            "Epoch [100/100], Loss: 9148.7456\n",
            "Fold 4, RMSE: 48.89524459838867\n",
            "Epoch [10/100], Loss: 23987.2402\n",
            "Epoch [20/100], Loss: 26986.2964\n",
            "Epoch [30/100], Loss: 17971.5374\n",
            "Epoch [40/100], Loss: 24020.8022\n",
            "Epoch [50/100], Loss: 16933.3223\n",
            "Epoch [60/100], Loss: 25470.4878\n",
            "Epoch [70/100], Loss: 19318.4209\n",
            "Epoch [80/100], Loss: 19306.1318\n",
            "Epoch [90/100], Loss: 20789.5767\n",
            "Epoch [100/100], Loss: 20235.0605\n",
            "Fold 5, RMSE: 57.9354248046875\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 64.75345993041992\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 579442.8906\n",
            "Epoch [20/150], Loss: 111677.1914\n",
            "Epoch [30/150], Loss: 62294.0176\n",
            "Epoch [40/150], Loss: 29703.1187\n",
            "Epoch [50/150], Loss: 30172.1143\n",
            "Epoch [60/150], Loss: 26872.7397\n",
            "Epoch [70/150], Loss: 18932.4678\n",
            "Epoch [80/150], Loss: 22116.7871\n",
            "Epoch [90/150], Loss: 21749.0601\n",
            "Epoch [100/150], Loss: 13105.3691\n",
            "Epoch [110/150], Loss: 19574.9290\n",
            "Epoch [120/150], Loss: 14163.7849\n",
            "Epoch [130/150], Loss: 17357.5698\n",
            "Epoch [140/150], Loss: 14014.6201\n",
            "Epoch [150/150], Loss: 14467.0366\n",
            "Fold 1, RMSE: 47.297752380371094\n",
            "Epoch [10/150], Loss: 349098.6875\n",
            "Epoch [20/150], Loss: 50344.8164\n",
            "Epoch [30/150], Loss: 34314.5938\n",
            "Epoch [40/150], Loss: 27176.4927\n",
            "Epoch [50/150], Loss: 18604.4951\n",
            "Epoch [60/150], Loss: 20111.0278\n",
            "Epoch [70/150], Loss: 17577.0415\n",
            "Epoch [80/150], Loss: 12786.1995\n",
            "Epoch [90/150], Loss: 22650.2473\n",
            "Epoch [100/150], Loss: 16827.0803\n",
            "Epoch [110/150], Loss: 12934.3213\n",
            "Epoch [120/150], Loss: 13295.6453\n",
            "Epoch [130/150], Loss: 17045.8760\n",
            "Epoch [140/150], Loss: 19908.2891\n",
            "Epoch [150/150], Loss: 13416.9463\n",
            "Fold 2, RMSE: 72.43231201171875\n",
            "Epoch [10/150], Loss: 266053.3203\n",
            "Epoch [20/150], Loss: 38253.2363\n",
            "Epoch [30/150], Loss: 27019.6780\n",
            "Epoch [40/150], Loss: 20100.6387\n",
            "Epoch [50/150], Loss: 14391.2646\n",
            "Epoch [60/150], Loss: 13199.5486\n",
            "Epoch [70/150], Loss: 11241.9785\n",
            "Epoch [80/150], Loss: 10556.4031\n",
            "Epoch [90/150], Loss: 9320.1875\n",
            "Epoch [100/150], Loss: 12748.3455\n",
            "Epoch [110/150], Loss: 9985.6183\n",
            "Epoch [120/150], Loss: 7509.9558\n",
            "Epoch [130/150], Loss: 8700.4475\n",
            "Epoch [140/150], Loss: 6666.1530\n",
            "Epoch [150/150], Loss: 7999.1041\n",
            "Fold 3, RMSE: 92.25885009765625\n",
            "Epoch [10/150], Loss: 135843.0312\n",
            "Epoch [20/150], Loss: 19840.7104\n",
            "Epoch [30/150], Loss: 18553.8916\n",
            "Epoch [40/150], Loss: 18344.9399\n",
            "Epoch [50/150], Loss: 13943.8853\n",
            "Epoch [60/150], Loss: 16470.7568\n",
            "Epoch [70/150], Loss: 9745.8136\n",
            "Epoch [80/150], Loss: 10765.5643\n",
            "Epoch [90/150], Loss: 15315.0154\n",
            "Epoch [100/150], Loss: 8007.3119\n",
            "Epoch [110/150], Loss: 16565.1471\n",
            "Epoch [120/150], Loss: 8186.8086\n",
            "Epoch [130/150], Loss: 11379.4490\n",
            "Epoch [140/150], Loss: 9908.2761\n",
            "Epoch [150/150], Loss: 16402.6165\n",
            "Fold 4, RMSE: 42.73234176635742\n",
            "Epoch [10/150], Loss: 46059.4414\n",
            "Epoch [20/150], Loss: 19200.4961\n",
            "Epoch [30/150], Loss: 12906.3577\n",
            "Epoch [40/150], Loss: 7664.1718\n",
            "Epoch [50/150], Loss: 10808.9878\n",
            "Epoch [60/150], Loss: 7267.5703\n",
            "Epoch [70/150], Loss: 8975.5912\n",
            "Epoch [80/150], Loss: 6624.3453\n",
            "Epoch [90/150], Loss: 3993.1277\n",
            "Epoch [100/150], Loss: 13078.0281\n",
            "Epoch [110/150], Loss: 8542.7618\n",
            "Epoch [120/150], Loss: 2540.5091\n",
            "Epoch [130/150], Loss: 6110.3558\n",
            "Epoch [140/150], Loss: 4604.1816\n",
            "Epoch [150/150], Loss: 4136.3265\n",
            "Fold 5, RMSE: 44.09971237182617\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 59.76419372558594\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 38813.0190\n",
            "Epoch [20/100], Loss: 17965.0928\n",
            "Epoch [30/100], Loss: 11691.4612\n",
            "Epoch [40/100], Loss: 11028.6318\n",
            "Epoch [50/100], Loss: 10205.2896\n",
            "Epoch [60/100], Loss: 14388.2144\n",
            "Epoch [70/100], Loss: 10090.7018\n",
            "Epoch [80/100], Loss: 10226.9922\n",
            "Epoch [90/100], Loss: 12410.8118\n",
            "Epoch [100/100], Loss: 13077.1494\n",
            "Fold 1, RMSE: 52.01445007324219\n",
            "Epoch [10/100], Loss: 15895.4893\n",
            "Epoch [20/100], Loss: 13705.6194\n",
            "Epoch [30/100], Loss: 10493.7200\n",
            "Epoch [40/100], Loss: 12634.3815\n",
            "Epoch [50/100], Loss: 8947.9176\n",
            "Epoch [60/100], Loss: 9583.7931\n",
            "Epoch [70/100], Loss: 14464.7454\n",
            "Epoch [80/100], Loss: 7664.9196\n",
            "Epoch [90/100], Loss: 3643.9503\n",
            "Epoch [100/100], Loss: 6463.4956\n",
            "Fold 2, RMSE: 65.88638305664062\n",
            "Epoch [10/100], Loss: 24300.3555\n",
            "Epoch [20/100], Loss: 14555.1287\n",
            "Epoch [30/100], Loss: 11294.8557\n",
            "Epoch [40/100], Loss: 11772.9084\n",
            "Epoch [50/100], Loss: 9480.5861\n",
            "Epoch [60/100], Loss: 9270.3511\n",
            "Epoch [70/100], Loss: 7823.6489\n",
            "Epoch [80/100], Loss: 11012.2664\n",
            "Epoch [90/100], Loss: 7094.3115\n",
            "Epoch [100/100], Loss: 6689.4258\n",
            "Fold 3, RMSE: 94.56645202636719\n",
            "Epoch [10/100], Loss: 18921.6187\n",
            "Epoch [20/100], Loss: 19234.7795\n",
            "Epoch [30/100], Loss: 15541.2683\n",
            "Epoch [40/100], Loss: 11592.7346\n",
            "Epoch [50/100], Loss: 7896.5549\n",
            "Epoch [60/100], Loss: 6193.9355\n",
            "Epoch [70/100], Loss: 7002.6902\n",
            "Epoch [80/100], Loss: 3469.7988\n",
            "Epoch [90/100], Loss: 10317.6248\n",
            "Epoch [100/100], Loss: 9319.5875\n",
            "Fold 4, RMSE: 44.82270050048828\n",
            "Epoch [10/100], Loss: 29124.2944\n",
            "Epoch [20/100], Loss: 19968.5518\n",
            "Epoch [30/100], Loss: 13213.7021\n",
            "Epoch [40/100], Loss: 17962.3506\n",
            "Epoch [50/100], Loss: 14185.8918\n",
            "Epoch [60/100], Loss: 11710.4277\n",
            "Epoch [70/100], Loss: 16763.0844\n",
            "Epoch [80/100], Loss: 8565.2181\n",
            "Epoch [90/100], Loss: 11189.4760\n",
            "Epoch [100/100], Loss: 6549.2693\n",
            "Fold 5, RMSE: 48.34071350097656\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 61.12613983154297\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 101516.9531\n",
            "Epoch [20/150], Loss: 29783.6348\n",
            "Epoch [30/150], Loss: 17593.3149\n",
            "Epoch [40/150], Loss: 16473.8511\n",
            "Epoch [50/150], Loss: 17654.7830\n",
            "Epoch [60/150], Loss: 20719.5620\n",
            "Epoch [70/150], Loss: 23414.8900\n",
            "Epoch [80/150], Loss: 11468.6428\n",
            "Epoch [90/150], Loss: 11762.4724\n",
            "Epoch [100/150], Loss: 15766.7378\n",
            "Epoch [110/150], Loss: 13222.2444\n",
            "Epoch [120/150], Loss: 11874.0327\n",
            "Epoch [130/150], Loss: 13899.2085\n",
            "Epoch [140/150], Loss: 11319.4559\n",
            "Epoch [150/150], Loss: 11388.9932\n",
            "Fold 1, RMSE: 48.91864013671875\n",
            "Epoch [10/150], Loss: 14530.5034\n",
            "Epoch [20/150], Loss: 13931.8810\n",
            "Epoch [30/150], Loss: 22902.9832\n",
            "Epoch [40/150], Loss: 17548.0679\n",
            "Epoch [50/150], Loss: 15221.1079\n",
            "Epoch [60/150], Loss: 18505.8618\n",
            "Epoch [70/150], Loss: 14018.9453\n",
            "Epoch [80/150], Loss: 13417.0691\n",
            "Epoch [90/150], Loss: 12775.8126\n",
            "Epoch [100/150], Loss: 12439.9797\n",
            "Epoch [110/150], Loss: 11370.2789\n",
            "Epoch [120/150], Loss: 10553.1454\n",
            "Epoch [130/150], Loss: 10573.8262\n",
            "Epoch [140/150], Loss: 12340.5172\n",
            "Epoch [150/150], Loss: 9316.0237\n",
            "Fold 2, RMSE: 61.95948028564453\n",
            "Epoch [10/150], Loss: 43349.2412\n",
            "Epoch [20/150], Loss: 13971.1165\n",
            "Epoch [30/150], Loss: 9099.1167\n",
            "Epoch [40/150], Loss: 8877.0496\n",
            "Epoch [50/150], Loss: 7169.0264\n",
            "Epoch [60/150], Loss: 8052.1063\n",
            "Epoch [70/150], Loss: 11396.8269\n",
            "Epoch [80/150], Loss: 8674.8635\n",
            "Epoch [90/150], Loss: 6203.5829\n",
            "Epoch [100/150], Loss: 4070.9895\n",
            "Epoch [110/150], Loss: 5465.7802\n",
            "Epoch [120/150], Loss: 3569.6423\n",
            "Epoch [130/150], Loss: 3621.6495\n",
            "Epoch [140/150], Loss: 7300.4382\n",
            "Epoch [150/150], Loss: 7507.6089\n",
            "Fold 3, RMSE: 96.64118194580078\n",
            "Epoch [10/150], Loss: 73154.1484\n",
            "Epoch [20/150], Loss: 27909.3945\n",
            "Epoch [30/150], Loss: 15514.4048\n",
            "Epoch [40/150], Loss: 17475.1611\n",
            "Epoch [50/150], Loss: 21610.2256\n",
            "Epoch [60/150], Loss: 13936.0557\n",
            "Epoch [70/150], Loss: 10847.6202\n",
            "Epoch [80/150], Loss: 13975.0271\n",
            "Epoch [90/150], Loss: 18268.4207\n",
            "Epoch [100/150], Loss: 17339.4126\n",
            "Epoch [110/150], Loss: 12628.1358\n",
            "Epoch [120/150], Loss: 21564.7190\n",
            "Epoch [130/150], Loss: 15161.3259\n",
            "Epoch [140/150], Loss: 12892.1177\n",
            "Epoch [150/150], Loss: 16405.0388\n",
            "Fold 4, RMSE: 38.32612991333008\n",
            "Epoch [10/150], Loss: 51429.2090\n",
            "Epoch [20/150], Loss: 16317.3689\n",
            "Epoch [30/150], Loss: 15810.7993\n",
            "Epoch [40/150], Loss: 15732.9641\n",
            "Epoch [50/150], Loss: 13800.4009\n",
            "Epoch [60/150], Loss: 12699.4309\n",
            "Epoch [70/150], Loss: 15861.1091\n",
            "Epoch [80/150], Loss: 11256.5493\n",
            "Epoch [90/150], Loss: 10142.5735\n",
            "Epoch [100/150], Loss: 5880.4047\n",
            "Epoch [110/150], Loss: 8849.5291\n",
            "Epoch [120/150], Loss: 10305.6145\n",
            "Epoch [130/150], Loss: 7840.3513\n",
            "Epoch [140/150], Loss: 8034.2837\n",
            "Epoch [150/150], Loss: 6827.7185\n",
            "Fold 5, RMSE: 45.269126892089844\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 58.2229118347168\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 795707.9219\n",
            "Epoch [20/100], Loss: 37467.4683\n",
            "Epoch [30/100], Loss: 33065.9634\n",
            "Epoch [40/100], Loss: 19417.5649\n",
            "Epoch [50/100], Loss: 23162.6558\n",
            "Epoch [60/100], Loss: 22144.0171\n",
            "Epoch [70/100], Loss: 15340.2922\n",
            "Epoch [80/100], Loss: 11868.6499\n",
            "Epoch [90/100], Loss: 14505.2842\n",
            "Epoch [100/100], Loss: 13624.1687\n",
            "Fold 1, RMSE: 50.974403381347656\n",
            "Epoch [10/100], Loss: 185338.6797\n",
            "Epoch [20/100], Loss: 34992.5186\n",
            "Epoch [30/100], Loss: 33445.9180\n",
            "Epoch [40/100], Loss: 24162.3276\n",
            "Epoch [50/100], Loss: 19345.0679\n",
            "Epoch [60/100], Loss: 14914.6118\n",
            "Epoch [70/100], Loss: 9416.7117\n",
            "Epoch [80/100], Loss: 10999.3430\n",
            "Epoch [90/100], Loss: 14383.1995\n",
            "Epoch [100/100], Loss: 13856.5557\n",
            "Fold 2, RMSE: 72.72567749023438\n",
            "Epoch [10/100], Loss: 278592.0938\n",
            "Epoch [20/100], Loss: 56899.2383\n",
            "Epoch [30/100], Loss: 29688.5869\n",
            "Epoch [40/100], Loss: 26988.4219\n",
            "Epoch [50/100], Loss: 29855.8628\n",
            "Epoch [60/100], Loss: 21026.1558\n",
            "Epoch [70/100], Loss: 15135.9558\n",
            "Epoch [80/100], Loss: 14994.3357\n",
            "Epoch [90/100], Loss: 12087.6941\n",
            "Epoch [100/100], Loss: 12942.8594\n",
            "Fold 3, RMSE: 93.85006713867188\n",
            "Epoch [10/100], Loss: 190875.1055\n",
            "Epoch [20/100], Loss: 38136.1182\n",
            "Epoch [30/100], Loss: 17349.7495\n",
            "Epoch [40/100], Loss: 25218.0361\n",
            "Epoch [50/100], Loss: 19377.0542\n",
            "Epoch [60/100], Loss: 13372.4348\n",
            "Epoch [70/100], Loss: 20759.0991\n",
            "Epoch [80/100], Loss: 19677.0791\n",
            "Epoch [90/100], Loss: 17426.5249\n",
            "Epoch [100/100], Loss: 16168.5840\n",
            "Fold 4, RMSE: 37.76530838012695\n",
            "Epoch [10/100], Loss: 233528.9727\n",
            "Epoch [20/100], Loss: 46809.3760\n",
            "Epoch [30/100], Loss: 23050.0552\n",
            "Epoch [40/100], Loss: 17721.1360\n",
            "Epoch [50/100], Loss: 22060.0715\n",
            "Epoch [60/100], Loss: 11660.0337\n",
            "Epoch [70/100], Loss: 11485.5698\n",
            "Epoch [80/100], Loss: 10829.8391\n",
            "Epoch [90/100], Loss: 15093.6816\n",
            "Epoch [100/100], Loss: 11980.3832\n",
            "Fold 5, RMSE: 45.70198059082031\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 60.203487396240234\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 379087.4766\n",
            "Epoch [20/150], Loss: 81404.3008\n",
            "Epoch [30/150], Loss: 52193.8516\n",
            "Epoch [40/150], Loss: 35616.3320\n",
            "Epoch [50/150], Loss: 26112.2656\n",
            "Epoch [60/150], Loss: 24367.8687\n",
            "Epoch [70/150], Loss: 25970.4927\n",
            "Epoch [80/150], Loss: 21753.8984\n",
            "Epoch [90/150], Loss: 20850.3872\n",
            "Epoch [100/150], Loss: 23807.4136\n",
            "Epoch [110/150], Loss: 16758.4492\n",
            "Epoch [120/150], Loss: 14019.6008\n",
            "Epoch [130/150], Loss: 19634.0718\n",
            "Epoch [140/150], Loss: 11459.5688\n",
            "Epoch [150/150], Loss: 14164.6672\n",
            "Fold 1, RMSE: 45.06850051879883\n",
            "Epoch [10/150], Loss: 297899.4766\n",
            "Epoch [20/150], Loss: 76910.4502\n",
            "Epoch [30/150], Loss: 26192.8586\n",
            "Epoch [40/150], Loss: 15119.1475\n",
            "Epoch [50/150], Loss: 14439.8818\n",
            "Epoch [60/150], Loss: 12985.2227\n",
            "Epoch [70/150], Loss: 7677.2987\n",
            "Epoch [80/150], Loss: 12527.8115\n",
            "Epoch [90/150], Loss: 9767.2612\n",
            "Epoch [100/150], Loss: 12325.9670\n",
            "Epoch [110/150], Loss: 9836.1475\n",
            "Epoch [120/150], Loss: 12306.0254\n",
            "Epoch [130/150], Loss: 17418.8787\n",
            "Epoch [140/150], Loss: 10072.7394\n",
            "Epoch [150/150], Loss: 15997.3159\n",
            "Fold 2, RMSE: 70.71736907958984\n",
            "Epoch [10/150], Loss: 451522.6094\n",
            "Epoch [20/150], Loss: 53324.1006\n",
            "Epoch [30/150], Loss: 32079.7305\n",
            "Epoch [40/150], Loss: 24454.5229\n",
            "Epoch [50/150], Loss: 15829.6489\n",
            "Epoch [60/150], Loss: 20249.4517\n",
            "Epoch [70/150], Loss: 13594.6882\n",
            "Epoch [80/150], Loss: 13785.5352\n",
            "Epoch [90/150], Loss: 12952.3157\n",
            "Epoch [100/150], Loss: 12112.9309\n",
            "Epoch [110/150], Loss: 13501.5142\n",
            "Epoch [120/150], Loss: 15635.4277\n",
            "Epoch [130/150], Loss: 10646.4875\n",
            "Epoch [140/150], Loss: 12147.3604\n",
            "Epoch [150/150], Loss: 10795.0989\n",
            "Fold 3, RMSE: 93.4357681274414\n",
            "Epoch [10/150], Loss: 362665.0703\n",
            "Epoch [20/150], Loss: 78860.7168\n",
            "Epoch [30/150], Loss: 57242.3730\n",
            "Epoch [40/150], Loss: 27310.3381\n",
            "Epoch [50/150], Loss: 34885.3389\n",
            "Epoch [60/150], Loss: 21718.6050\n",
            "Epoch [70/150], Loss: 12633.8882\n",
            "Epoch [80/150], Loss: 21159.7166\n",
            "Epoch [90/150], Loss: 15585.2666\n",
            "Epoch [100/150], Loss: 31346.5933\n",
            "Epoch [110/150], Loss: 13892.4792\n",
            "Epoch [120/150], Loss: 16528.2905\n",
            "Epoch [130/150], Loss: 20365.6357\n",
            "Epoch [140/150], Loss: 18353.4272\n",
            "Epoch [150/150], Loss: 14078.3093\n",
            "Fold 4, RMSE: 37.57459259033203\n",
            "Epoch [10/150], Loss: 306807.8008\n",
            "Epoch [20/150], Loss: 48782.0322\n",
            "Epoch [30/150], Loss: 28128.9458\n",
            "Epoch [40/150], Loss: 17904.0532\n",
            "Epoch [50/150], Loss: 23479.1636\n",
            "Epoch [60/150], Loss: 24133.0620\n",
            "Epoch [70/150], Loss: 15451.4250\n",
            "Epoch [80/150], Loss: 22482.8660\n",
            "Epoch [90/150], Loss: 17555.1167\n",
            "Epoch [100/150], Loss: 16328.1714\n",
            "Epoch [110/150], Loss: 11208.7903\n",
            "Epoch [120/150], Loss: 16663.9954\n",
            "Epoch [130/150], Loss: 12803.6509\n",
            "Epoch [140/150], Loss: 11947.1997\n",
            "Epoch [150/150], Loss: 13157.7834\n",
            "Fold 5, RMSE: 49.6031608581543\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 59.27987823486328\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 26410.0137\n",
            "Epoch [20/100], Loss: 15044.6580\n",
            "Epoch [30/100], Loss: 14581.2192\n",
            "Epoch [40/100], Loss: 17492.4761\n",
            "Epoch [50/100], Loss: 21946.3818\n",
            "Epoch [60/100], Loss: 12936.5742\n",
            "Epoch [70/100], Loss: 10590.7411\n",
            "Epoch [80/100], Loss: 9759.2665\n",
            "Epoch [90/100], Loss: 12504.1261\n",
            "Epoch [100/100], Loss: 9368.6765\n",
            "Fold 1, RMSE: 50.48270797729492\n",
            "Epoch [10/100], Loss: 42939.6172\n",
            "Epoch [20/100], Loss: 20469.3003\n",
            "Epoch [30/100], Loss: 10663.8459\n",
            "Epoch [40/100], Loss: 11873.3997\n",
            "Epoch [50/100], Loss: 12286.4209\n",
            "Epoch [60/100], Loss: 20407.8262\n",
            "Epoch [70/100], Loss: 17039.6660\n",
            "Epoch [80/100], Loss: 12009.5522\n",
            "Epoch [90/100], Loss: 10260.6387\n",
            "Epoch [100/100], Loss: 9446.9272\n",
            "Fold 2, RMSE: 74.83204650878906\n",
            "Epoch [10/100], Loss: 93931.5469\n",
            "Epoch [20/100], Loss: 24899.5645\n",
            "Epoch [30/100], Loss: 11216.1897\n",
            "Epoch [40/100], Loss: 9773.3801\n",
            "Epoch [50/100], Loss: 9960.1750\n",
            "Epoch [60/100], Loss: 7960.5968\n",
            "Epoch [70/100], Loss: 9836.4529\n",
            "Epoch [80/100], Loss: 13109.3237\n",
            "Epoch [90/100], Loss: 7010.6139\n",
            "Epoch [100/100], Loss: 8255.2170\n",
            "Fold 3, RMSE: 92.48866271972656\n",
            "Epoch [10/100], Loss: 40742.5542\n",
            "Epoch [20/100], Loss: 22956.9419\n",
            "Epoch [30/100], Loss: 13282.4702\n",
            "Epoch [40/100], Loss: 14827.9209\n",
            "Epoch [50/100], Loss: 11724.1278\n",
            "Epoch [60/100], Loss: 18351.9497\n",
            "Epoch [70/100], Loss: 12252.6460\n",
            "Epoch [80/100], Loss: 10622.0619\n",
            "Epoch [90/100], Loss: 15325.2690\n",
            "Epoch [100/100], Loss: 13362.6250\n",
            "Fold 4, RMSE: 36.784873962402344\n",
            "Epoch [10/100], Loss: 47289.4844\n",
            "Epoch [20/100], Loss: 21751.4492\n",
            "Epoch [30/100], Loss: 13002.7312\n",
            "Epoch [40/100], Loss: 11297.2533\n",
            "Epoch [50/100], Loss: 16962.0559\n",
            "Epoch [60/100], Loss: 12316.1024\n",
            "Epoch [70/100], Loss: 14446.6145\n",
            "Epoch [80/100], Loss: 9530.8447\n",
            "Epoch [90/100], Loss: 11582.6758\n",
            "Epoch [100/100], Loss: 12869.3115\n",
            "Fold 5, RMSE: 46.735660552978516\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 60.26479034423828\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 102526.7422\n",
            "Epoch [20/150], Loss: 18726.6045\n",
            "Epoch [30/150], Loss: 12942.0950\n",
            "Epoch [40/150], Loss: 13308.7805\n",
            "Epoch [50/150], Loss: 11528.8336\n",
            "Epoch [60/150], Loss: 12886.6663\n",
            "Epoch [70/150], Loss: 11101.3939\n",
            "Epoch [80/150], Loss: 11328.4413\n",
            "Epoch [90/150], Loss: 13542.2200\n",
            "Epoch [100/150], Loss: 11715.8635\n",
            "Epoch [110/150], Loss: 17988.6982\n",
            "Epoch [120/150], Loss: 14814.0469\n",
            "Epoch [130/150], Loss: 9507.8164\n",
            "Epoch [140/150], Loss: 7476.3154\n",
            "Epoch [150/150], Loss: 7323.5836\n",
            "Fold 1, RMSE: 51.204833984375\n",
            "Epoch [10/150], Loss: 63058.9082\n",
            "Epoch [20/150], Loss: 27321.8096\n",
            "Epoch [30/150], Loss: 12863.9158\n",
            "Epoch [40/150], Loss: 16366.2444\n",
            "Epoch [50/150], Loss: 12464.2500\n",
            "Epoch [60/150], Loss: 14650.1030\n",
            "Epoch [70/150], Loss: 9374.0945\n",
            "Epoch [80/150], Loss: 13064.5452\n",
            "Epoch [90/150], Loss: 9775.4773\n",
            "Epoch [100/150], Loss: 16603.2189\n",
            "Epoch [110/150], Loss: 14573.9431\n",
            "Epoch [120/150], Loss: 9582.1772\n",
            "Epoch [130/150], Loss: 10299.8048\n",
            "Epoch [140/150], Loss: 9139.0293\n",
            "Epoch [150/150], Loss: 10235.9636\n",
            "Fold 2, RMSE: 69.91720581054688\n",
            "Epoch [10/150], Loss: 66612.3379\n",
            "Epoch [20/150], Loss: 17730.5933\n",
            "Epoch [30/150], Loss: 12299.8882\n",
            "Epoch [40/150], Loss: 12454.7395\n",
            "Epoch [50/150], Loss: 11013.6685\n",
            "Epoch [60/150], Loss: 11367.2551\n",
            "Epoch [70/150], Loss: 12786.8904\n",
            "Epoch [80/150], Loss: 10529.6619\n",
            "Epoch [90/150], Loss: 6391.6819\n",
            "Epoch [100/150], Loss: 7999.5938\n",
            "Epoch [110/150], Loss: 7302.0105\n",
            "Epoch [120/150], Loss: 9277.6428\n",
            "Epoch [130/150], Loss: 10936.2859\n",
            "Epoch [140/150], Loss: 9641.4453\n",
            "Epoch [150/150], Loss: 7701.9038\n",
            "Fold 3, RMSE: 96.4958724975586\n",
            "Epoch [10/150], Loss: 65070.1074\n",
            "Epoch [20/150], Loss: 19049.5916\n",
            "Epoch [30/150], Loss: 16575.2262\n",
            "Epoch [40/150], Loss: 14048.3430\n",
            "Epoch [50/150], Loss: 11112.2058\n",
            "Epoch [60/150], Loss: 13966.0010\n",
            "Epoch [70/150], Loss: 16355.7034\n",
            "Epoch [80/150], Loss: 12840.8127\n",
            "Epoch [90/150], Loss: 15535.6621\n",
            "Epoch [100/150], Loss: 11998.6045\n",
            "Epoch [110/150], Loss: 15965.4697\n",
            "Epoch [120/150], Loss: 12301.0923\n",
            "Epoch [130/150], Loss: 13897.1807\n",
            "Epoch [140/150], Loss: 13813.6470\n",
            "Epoch [150/150], Loss: 10363.1494\n",
            "Fold 4, RMSE: 35.6483268737793\n",
            "Epoch [10/150], Loss: 46712.0781\n",
            "Epoch [20/150], Loss: 19653.5713\n",
            "Epoch [30/150], Loss: 17223.3816\n",
            "Epoch [40/150], Loss: 11091.9326\n",
            "Epoch [50/150], Loss: 10277.9940\n",
            "Epoch [60/150], Loss: 12392.1362\n",
            "Epoch [70/150], Loss: 12260.4167\n",
            "Epoch [80/150], Loss: 10594.9502\n",
            "Epoch [90/150], Loss: 12492.8123\n",
            "Epoch [100/150], Loss: 10353.3798\n",
            "Epoch [110/150], Loss: 11675.1630\n",
            "Epoch [120/150], Loss: 10127.9189\n",
            "Epoch [130/150], Loss: 10812.5942\n",
            "Epoch [140/150], Loss: 9330.7495\n",
            "Epoch [150/150], Loss: 11632.2493\n",
            "Fold 5, RMSE: 47.217281341552734\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 60.0967041015625\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 333937.5820\n",
            "Epoch [20/100], Loss: 30320.9111\n",
            "Epoch [30/100], Loss: 22425.7637\n",
            "Epoch [40/100], Loss: 21144.0986\n",
            "Epoch [50/100], Loss: 12338.4111\n",
            "Epoch [60/100], Loss: 15276.7939\n",
            "Epoch [70/100], Loss: 12984.2561\n",
            "Epoch [80/100], Loss: 9741.5706\n",
            "Epoch [90/100], Loss: 10121.7404\n",
            "Epoch [100/100], Loss: 10009.4689\n",
            "Fold 1, RMSE: 49.6851806640625\n",
            "Epoch [10/100], Loss: 205053.0234\n",
            "Epoch [20/100], Loss: 46009.1709\n",
            "Epoch [30/100], Loss: 30418.9082\n",
            "Epoch [40/100], Loss: 23136.1523\n",
            "Epoch [50/100], Loss: 16138.8408\n",
            "Epoch [60/100], Loss: 18094.1614\n",
            "Epoch [70/100], Loss: 23996.1392\n",
            "Epoch [80/100], Loss: 13801.2925\n",
            "Epoch [90/100], Loss: 12695.6035\n",
            "Epoch [100/100], Loss: 11036.5742\n",
            "Fold 2, RMSE: 69.87001037597656\n",
            "Epoch [10/100], Loss: 569348.5625\n",
            "Epoch [20/100], Loss: 107885.0000\n",
            "Epoch [30/100], Loss: 30150.9585\n",
            "Epoch [40/100], Loss: 28865.4243\n",
            "Epoch [50/100], Loss: 19734.9407\n",
            "Epoch [60/100], Loss: 19355.5708\n",
            "Epoch [70/100], Loss: 12256.3972\n",
            "Epoch [80/100], Loss: 41677.7122\n",
            "Epoch [90/100], Loss: 17658.6111\n",
            "Epoch [100/100], Loss: 14200.2100\n",
            "Fold 3, RMSE: 96.25296020507812\n",
            "Epoch [10/100], Loss: 136361.4570\n",
            "Epoch [20/100], Loss: 26223.2886\n",
            "Epoch [30/100], Loss: 18579.0396\n",
            "Epoch [40/100], Loss: 16828.2161\n",
            "Epoch [50/100], Loss: 14910.9858\n",
            "Epoch [60/100], Loss: 14895.3506\n",
            "Epoch [70/100], Loss: 23951.2197\n",
            "Epoch [80/100], Loss: 16646.1150\n",
            "Epoch [90/100], Loss: 11245.5264\n",
            "Epoch [100/100], Loss: 12359.9004\n",
            "Fold 4, RMSE: 33.13699722290039\n",
            "Epoch [10/100], Loss: 355530.3672\n",
            "Epoch [20/100], Loss: 46439.1025\n",
            "Epoch [30/100], Loss: 30002.4429\n",
            "Epoch [40/100], Loss: 32567.3516\n",
            "Epoch [50/100], Loss: 16397.5972\n",
            "Epoch [60/100], Loss: 23184.3955\n",
            "Epoch [70/100], Loss: 12275.6040\n",
            "Epoch [80/100], Loss: 14875.3682\n",
            "Epoch [90/100], Loss: 34009.1768\n",
            "Epoch [100/100], Loss: 12522.6350\n",
            "Fold 5, RMSE: 47.83027648925781\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 59.35508499145508\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 211028.8086\n",
            "Epoch [20/150], Loss: 45107.3037\n",
            "Epoch [30/150], Loss: 23350.0662\n",
            "Epoch [40/150], Loss: 28169.4170\n",
            "Epoch [50/150], Loss: 16472.8044\n",
            "Epoch [60/150], Loss: 19443.7427\n",
            "Epoch [70/150], Loss: 23544.7461\n",
            "Epoch [80/150], Loss: 18708.6675\n",
            "Epoch [90/150], Loss: 20953.3157\n",
            "Epoch [100/150], Loss: 16629.0037\n",
            "Epoch [110/150], Loss: 13459.9031\n",
            "Epoch [120/150], Loss: 16489.7911\n",
            "Epoch [130/150], Loss: 13703.4089\n",
            "Epoch [140/150], Loss: 13619.5803\n",
            "Epoch [150/150], Loss: 18423.8589\n",
            "Fold 1, RMSE: 44.70133590698242\n",
            "Epoch [10/150], Loss: 636816.2109\n",
            "Epoch [20/150], Loss: 80936.6348\n",
            "Epoch [30/150], Loss: 46843.8828\n",
            "Epoch [40/150], Loss: 19927.7256\n",
            "Epoch [50/150], Loss: 23832.8164\n",
            "Epoch [60/150], Loss: 14116.4175\n",
            "Epoch [70/150], Loss: 11569.1083\n",
            "Epoch [80/150], Loss: 20218.5913\n",
            "Epoch [90/150], Loss: 11112.9641\n",
            "Epoch [100/150], Loss: 11968.3340\n",
            "Epoch [110/150], Loss: 17554.2566\n",
            "Epoch [120/150], Loss: 9550.5520\n",
            "Epoch [130/150], Loss: 14151.3218\n",
            "Epoch [140/150], Loss: 12759.0712\n",
            "Epoch [150/150], Loss: 9173.8286\n",
            "Fold 2, RMSE: 62.242916107177734\n",
            "Epoch [10/150], Loss: 167511.2129\n",
            "Epoch [20/150], Loss: 29472.8252\n",
            "Epoch [30/150], Loss: 15763.3633\n",
            "Epoch [40/150], Loss: 20300.9753\n",
            "Epoch [50/150], Loss: 23618.4204\n",
            "Epoch [60/150], Loss: 11436.1819\n",
            "Epoch [70/150], Loss: 12882.3132\n",
            "Epoch [80/150], Loss: 13429.2371\n",
            "Epoch [90/150], Loss: 10726.6890\n",
            "Epoch [100/150], Loss: 9392.2310\n",
            "Epoch [110/150], Loss: 9132.2367\n",
            "Epoch [120/150], Loss: 7893.0403\n",
            "Epoch [130/150], Loss: 10288.3467\n",
            "Epoch [140/150], Loss: 7905.1589\n",
            "Epoch [150/150], Loss: 7815.9480\n",
            "Fold 3, RMSE: 90.5054931640625\n",
            "Epoch [10/150], Loss: 452247.8359\n",
            "Epoch [20/150], Loss: 65234.7051\n",
            "Epoch [30/150], Loss: 40210.6250\n",
            "Epoch [40/150], Loss: 54713.1802\n",
            "Epoch [50/150], Loss: 23327.6509\n",
            "Epoch [60/150], Loss: 22656.3262\n",
            "Epoch [70/150], Loss: 20105.8757\n",
            "Epoch [80/150], Loss: 17294.2046\n",
            "Epoch [90/150], Loss: 15872.1313\n",
            "Epoch [100/150], Loss: 15538.9585\n",
            "Epoch [110/150], Loss: 16948.6799\n",
            "Epoch [120/150], Loss: 19661.6575\n",
            "Epoch [130/150], Loss: 15878.8833\n",
            "Epoch [140/150], Loss: 33225.1946\n",
            "Epoch [150/150], Loss: 12971.7959\n",
            "Fold 4, RMSE: 35.88058853149414\n",
            "Epoch [10/150], Loss: 269668.7461\n",
            "Epoch [20/150], Loss: 27196.0039\n",
            "Epoch [30/150], Loss: 20702.0757\n",
            "Epoch [40/150], Loss: 10001.1652\n",
            "Epoch [50/150], Loss: 13320.7383\n",
            "Epoch [60/150], Loss: 13086.9604\n",
            "Epoch [70/150], Loss: 13388.1035\n",
            "Epoch [80/150], Loss: 11729.2468\n",
            "Epoch [90/150], Loss: 10527.9487\n",
            "Epoch [100/150], Loss: 10292.4011\n",
            "Epoch [110/150], Loss: 6249.2012\n",
            "Epoch [120/150], Loss: 6852.2906\n",
            "Epoch [130/150], Loss: 8327.6370\n",
            "Epoch [140/150], Loss: 7452.8630\n",
            "Epoch [150/150], Loss: 5158.9779\n",
            "Fold 5, RMSE: 42.37419891357422\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 55.140906524658206\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 56980.4268\n",
            "Epoch [20/100], Loss: 15678.7869\n",
            "Epoch [30/100], Loss: 13988.6887\n",
            "Epoch [40/100], Loss: 15276.4758\n",
            "Epoch [50/100], Loss: 10774.9155\n",
            "Epoch [60/100], Loss: 9939.4532\n",
            "Epoch [70/100], Loss: 9223.5513\n",
            "Epoch [80/100], Loss: 8883.7786\n",
            "Epoch [90/100], Loss: 7027.5798\n",
            "Epoch [100/100], Loss: 3969.5712\n",
            "Fold 1, RMSE: 52.15900802612305\n",
            "Epoch [10/100], Loss: 84581.1484\n",
            "Epoch [20/100], Loss: 18466.7461\n",
            "Epoch [30/100], Loss: 16573.7861\n",
            "Epoch [40/100], Loss: 16338.6211\n",
            "Epoch [50/100], Loss: 12205.1174\n",
            "Epoch [60/100], Loss: 10415.3054\n",
            "Epoch [70/100], Loss: 13905.0273\n",
            "Epoch [80/100], Loss: 16264.2820\n",
            "Epoch [90/100], Loss: 13624.1915\n",
            "Epoch [100/100], Loss: 11046.5693\n",
            "Fold 2, RMSE: 73.77962493896484\n",
            "Epoch [10/100], Loss: 56380.4277\n",
            "Epoch [20/100], Loss: 27925.2410\n",
            "Epoch [30/100], Loss: 15832.6943\n",
            "Epoch [40/100], Loss: 18683.7068\n",
            "Epoch [50/100], Loss: 14742.2058\n",
            "Epoch [60/100], Loss: 16860.4482\n",
            "Epoch [70/100], Loss: 10471.9138\n",
            "Epoch [80/100], Loss: 11531.3545\n",
            "Epoch [90/100], Loss: 9591.5251\n",
            "Epoch [100/100], Loss: 11007.5249\n",
            "Fold 3, RMSE: 94.8892822265625\n",
            "Epoch [10/100], Loss: 54006.7539\n",
            "Epoch [20/100], Loss: 19343.5576\n",
            "Epoch [30/100], Loss: 13435.5894\n",
            "Epoch [40/100], Loss: 15778.5295\n",
            "Epoch [50/100], Loss: 12268.5009\n",
            "Epoch [60/100], Loss: 17412.4507\n",
            "Epoch [70/100], Loss: 17214.7729\n",
            "Epoch [80/100], Loss: 12302.1848\n",
            "Epoch [90/100], Loss: 11955.0015\n",
            "Epoch [100/100], Loss: 12866.6287\n",
            "Fold 4, RMSE: 37.426719665527344\n",
            "Epoch [10/100], Loss: 20014.7842\n",
            "Epoch [20/100], Loss: 10884.5886\n",
            "Epoch [30/100], Loss: 11159.0474\n",
            "Epoch [40/100], Loss: 10383.5079\n",
            "Epoch [50/100], Loss: 11811.3076\n",
            "Epoch [60/100], Loss: 7916.1738\n",
            "Epoch [70/100], Loss: 6744.2857\n",
            "Epoch [80/100], Loss: 7513.1310\n",
            "Epoch [90/100], Loss: 8224.3214\n",
            "Epoch [100/100], Loss: 3432.3594\n",
            "Fold 5, RMSE: 49.38074493408203\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 61.527075958251956\n",
            "Training with neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 28456.3892\n",
            "Epoch [20/150], Loss: 14871.4380\n",
            "Epoch [30/150], Loss: 12270.5510\n",
            "Epoch [40/150], Loss: 14341.2377\n",
            "Epoch [50/150], Loss: 17671.9287\n",
            "Epoch [60/150], Loss: 13024.3584\n",
            "Epoch [70/150], Loss: 10583.5699\n",
            "Epoch [80/150], Loss: 17691.2034\n",
            "Epoch [90/150], Loss: 15974.6606\n",
            "Epoch [100/150], Loss: 11127.6752\n",
            "Epoch [110/150], Loss: 11221.0234\n",
            "Epoch [120/150], Loss: 16743.4558\n",
            "Epoch [130/150], Loss: 11621.6151\n",
            "Epoch [140/150], Loss: 10118.5755\n",
            "Epoch [150/150], Loss: 11712.0474\n",
            "Fold 1, RMSE: 48.1324577331543\n",
            "Epoch [10/150], Loss: 67171.7471\n",
            "Epoch [20/150], Loss: 13010.2520\n",
            "Epoch [30/150], Loss: 19902.0210\n",
            "Epoch [40/150], Loss: 10345.3835\n",
            "Epoch [50/150], Loss: 16585.3188\n",
            "Epoch [60/150], Loss: 11755.2815\n",
            "Epoch [70/150], Loss: 11766.8191\n",
            "Epoch [80/150], Loss: 18952.3936\n",
            "Epoch [90/150], Loss: 10492.5294\n",
            "Epoch [100/150], Loss: 9464.7534\n",
            "Epoch [110/150], Loss: 10700.1543\n",
            "Epoch [120/150], Loss: 10205.0488\n",
            "Epoch [130/150], Loss: 9441.6108\n",
            "Epoch [140/150], Loss: 16217.5020\n",
            "Epoch [150/150], Loss: 12055.4319\n",
            "Fold 2, RMSE: 72.6191635131836\n",
            "Epoch [10/150], Loss: 31603.3135\n",
            "Epoch [20/150], Loss: 12307.1471\n",
            "Epoch [30/150], Loss: 6565.8962\n",
            "Epoch [40/150], Loss: 9366.6960\n",
            "Epoch [50/150], Loss: 7883.3396\n",
            "Epoch [60/150], Loss: 6063.0125\n",
            "Epoch [70/150], Loss: 4151.8456\n",
            "Epoch [80/150], Loss: 9620.0878\n",
            "Epoch [90/150], Loss: 7519.7583\n",
            "Epoch [100/150], Loss: 5894.4147\n",
            "Epoch [110/150], Loss: 3549.2404\n",
            "Epoch [120/150], Loss: 2725.9561\n",
            "Epoch [130/150], Loss: 5313.7191\n",
            "Epoch [140/150], Loss: 3322.6534\n",
            "Epoch [150/150], Loss: 3314.0990\n",
            "Fold 3, RMSE: 91.35457611083984\n",
            "Epoch [10/150], Loss: 71161.8750\n",
            "Epoch [20/150], Loss: 18936.7063\n",
            "Epoch [30/150], Loss: 16560.3445\n",
            "Epoch [40/150], Loss: 12725.1094\n",
            "Epoch [50/150], Loss: 14024.2419\n",
            "Epoch [60/150], Loss: 11940.7095\n",
            "Epoch [70/150], Loss: 12868.1309\n",
            "Epoch [80/150], Loss: 14162.0205\n",
            "Epoch [90/150], Loss: 11571.9183\n",
            "Epoch [100/150], Loss: 16688.4475\n",
            "Epoch [110/150], Loss: 13789.8047\n",
            "Epoch [120/150], Loss: 11558.2004\n",
            "Epoch [130/150], Loss: 11290.2159\n",
            "Epoch [140/150], Loss: 12390.2583\n",
            "Epoch [150/150], Loss: 11329.8115\n",
            "Fold 4, RMSE: 38.57424545288086\n",
            "Epoch [10/150], Loss: 30634.1279\n",
            "Epoch [20/150], Loss: 21408.8860\n",
            "Epoch [30/150], Loss: 11386.7458\n",
            "Epoch [40/150], Loss: 11617.9436\n",
            "Epoch [50/150], Loss: 9544.5184\n",
            "Epoch [60/150], Loss: 10594.9536\n",
            "Epoch [70/150], Loss: 11079.5786\n",
            "Epoch [80/150], Loss: 10668.2803\n",
            "Epoch [90/150], Loss: 10324.5890\n",
            "Epoch [100/150], Loss: 17374.5691\n",
            "Epoch [110/150], Loss: 10213.9614\n",
            "Epoch [120/150], Loss: 8789.2542\n",
            "Epoch [130/150], Loss: 11188.8160\n",
            "Epoch [140/150], Loss: 5836.2103\n",
            "Epoch [150/150], Loss: 4820.4260\n",
            "Fold 5, RMSE: 45.501487731933594\n",
            "Avg RMSE for neurons=96, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 59.236386108398435\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 20982.8574\n",
            "Epoch [20/100], Loss: 19913.5037\n",
            "Epoch [30/100], Loss: 19190.4885\n",
            "Epoch [40/100], Loss: 7869.9941\n",
            "Epoch [50/100], Loss: 12969.9954\n",
            "Epoch [60/100], Loss: 16015.6584\n",
            "Epoch [70/100], Loss: 11120.1749\n",
            "Epoch [80/100], Loss: 11512.8854\n",
            "Epoch [90/100], Loss: 9502.2659\n",
            "Epoch [100/100], Loss: 5070.8802\n",
            "Fold 1, RMSE: 56.562652587890625\n",
            "Epoch [10/100], Loss: 13324.5621\n",
            "Epoch [20/100], Loss: 13576.8630\n",
            "Epoch [30/100], Loss: 9884.8246\n",
            "Epoch [40/100], Loss: 7678.8740\n",
            "Epoch [50/100], Loss: 7076.7231\n",
            "Epoch [60/100], Loss: 12023.5459\n",
            "Epoch [70/100], Loss: 14567.9437\n",
            "Epoch [80/100], Loss: 14020.8635\n",
            "Epoch [90/100], Loss: 9020.8384\n",
            "Epoch [100/100], Loss: 8850.6661\n",
            "Fold 2, RMSE: 76.15518951416016\n",
            "Epoch [10/100], Loss: 11492.7661\n",
            "Epoch [20/100], Loss: 13431.0283\n",
            "Epoch [30/100], Loss: 11098.3229\n",
            "Epoch [40/100], Loss: 16086.0618\n",
            "Epoch [50/100], Loss: 12142.2429\n",
            "Epoch [60/100], Loss: 19144.3020\n",
            "Epoch [70/100], Loss: 15039.2180\n",
            "Epoch [80/100], Loss: 13322.4856\n",
            "Epoch [90/100], Loss: 13568.3325\n",
            "Epoch [100/100], Loss: 13766.6538\n",
            "Fold 3, RMSE: 109.69763946533203\n",
            "Epoch [10/100], Loss: 15057.8425\n",
            "Epoch [20/100], Loss: 12506.3499\n",
            "Epoch [30/100], Loss: 19323.6914\n",
            "Epoch [40/100], Loss: 15316.2061\n",
            "Epoch [50/100], Loss: 14971.5459\n",
            "Epoch [60/100], Loss: 9037.3826\n",
            "Epoch [70/100], Loss: 8737.9836\n",
            "Epoch [80/100], Loss: 17576.5503\n",
            "Epoch [90/100], Loss: 10954.1411\n",
            "Epoch [100/100], Loss: 14787.8311\n",
            "Fold 4, RMSE: 51.44401931762695\n",
            "Epoch [10/100], Loss: 17667.8999\n",
            "Epoch [20/100], Loss: 11610.4971\n",
            "Epoch [30/100], Loss: 9925.0774\n",
            "Epoch [40/100], Loss: 12225.1528\n",
            "Epoch [50/100], Loss: 9831.8684\n",
            "Epoch [60/100], Loss: 11524.3398\n",
            "Epoch [70/100], Loss: 7236.2188\n",
            "Epoch [80/100], Loss: 14027.6323\n",
            "Epoch [90/100], Loss: 7214.6943\n",
            "Epoch [100/100], Loss: 13557.0879\n",
            "Fold 5, RMSE: 45.01224899291992\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 67.77434997558593\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 21913.9542\n",
            "Epoch [20/150], Loss: 21728.2341\n",
            "Epoch [30/150], Loss: 13680.1855\n",
            "Epoch [40/150], Loss: 10383.5276\n",
            "Epoch [50/150], Loss: 13214.5095\n",
            "Epoch [60/150], Loss: 17677.4004\n",
            "Epoch [70/150], Loss: 9782.3624\n",
            "Epoch [80/150], Loss: 7022.0977\n",
            "Epoch [90/150], Loss: 11833.0125\n",
            "Epoch [100/150], Loss: 12945.0120\n",
            "Epoch [110/150], Loss: 12144.5551\n",
            "Epoch [120/150], Loss: 18873.8550\n",
            "Epoch [130/150], Loss: 11359.4675\n",
            "Epoch [140/150], Loss: 10225.5867\n",
            "Epoch [150/150], Loss: 20947.8652\n",
            "Fold 1, RMSE: 55.81027603149414\n",
            "Epoch [10/150], Loss: 14669.8037\n",
            "Epoch [20/150], Loss: 25486.1176\n",
            "Epoch [30/150], Loss: 11696.1143\n",
            "Epoch [40/150], Loss: 8541.6499\n",
            "Epoch [50/150], Loss: 10352.4509\n",
            "Epoch [60/150], Loss: 6559.2080\n",
            "Epoch [70/150], Loss: 13199.8928\n",
            "Epoch [80/150], Loss: 9266.3116\n",
            "Epoch [90/150], Loss: 8779.9001\n",
            "Epoch [100/150], Loss: 9654.6497\n",
            "Epoch [110/150], Loss: 11605.9336\n",
            "Epoch [120/150], Loss: 13761.6384\n",
            "Epoch [130/150], Loss: 9089.7068\n",
            "Epoch [140/150], Loss: 10354.4429\n",
            "Epoch [150/150], Loss: 10221.1545\n",
            "Fold 2, RMSE: 65.9716567993164\n",
            "Epoch [10/150], Loss: 17153.0503\n",
            "Epoch [20/150], Loss: 10848.3652\n",
            "Epoch [30/150], Loss: 14585.7080\n",
            "Epoch [40/150], Loss: 9615.9434\n",
            "Epoch [50/150], Loss: 11111.4929\n",
            "Epoch [60/150], Loss: 6815.9117\n",
            "Epoch [70/150], Loss: 13919.7793\n",
            "Epoch [80/150], Loss: 6267.6989\n",
            "Epoch [90/150], Loss: 8401.2212\n",
            "Epoch [100/150], Loss: 7672.9056\n",
            "Epoch [110/150], Loss: 9057.8059\n",
            "Epoch [120/150], Loss: 6395.4739\n",
            "Epoch [130/150], Loss: 6860.1777\n",
            "Epoch [140/150], Loss: 9792.0774\n",
            "Epoch [150/150], Loss: 6902.7435\n",
            "Fold 3, RMSE: 108.2843246459961\n",
            "Epoch [10/150], Loss: 22921.4004\n",
            "Epoch [20/150], Loss: 18467.0369\n",
            "Epoch [30/150], Loss: 12684.9760\n",
            "Epoch [40/150], Loss: 13014.9414\n",
            "Epoch [50/150], Loss: 11322.8032\n",
            "Epoch [60/150], Loss: 11344.7854\n",
            "Epoch [70/150], Loss: 5555.6324\n",
            "Epoch [80/150], Loss: 13402.1648\n",
            "Epoch [90/150], Loss: 11611.8326\n",
            "Epoch [100/150], Loss: 6930.9380\n",
            "Epoch [110/150], Loss: 10450.4108\n",
            "Epoch [120/150], Loss: 7721.8306\n",
            "Epoch [130/150], Loss: 11704.8601\n",
            "Epoch [140/150], Loss: 14316.2814\n",
            "Epoch [150/150], Loss: 10457.0394\n",
            "Fold 4, RMSE: 49.52665328979492\n",
            "Epoch [10/150], Loss: 17330.6365\n",
            "Epoch [20/150], Loss: 23806.8645\n",
            "Epoch [30/150], Loss: 10209.5099\n",
            "Epoch [40/150], Loss: 16957.3611\n",
            "Epoch [50/150], Loss: 12509.4817\n",
            "Epoch [60/150], Loss: 13514.0103\n",
            "Epoch [70/150], Loss: 10686.0681\n",
            "Epoch [80/150], Loss: 18510.8181\n",
            "Epoch [90/150], Loss: 10137.7825\n",
            "Epoch [100/150], Loss: 19810.1295\n",
            "Epoch [110/150], Loss: 7260.6426\n",
            "Epoch [120/150], Loss: 10151.2597\n",
            "Epoch [130/150], Loss: 9377.1721\n",
            "Epoch [140/150], Loss: 9524.2834\n",
            "Epoch [150/150], Loss: 8693.3541\n",
            "Fold 5, RMSE: 45.80281066894531\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 65.07914428710937\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16272.5942\n",
            "Epoch [20/100], Loss: 16542.3711\n",
            "Epoch [30/100], Loss: 8055.0255\n",
            "Epoch [40/100], Loss: 17706.6321\n",
            "Epoch [50/100], Loss: 7516.8568\n",
            "Epoch [60/100], Loss: 9551.1238\n",
            "Epoch [70/100], Loss: 9774.7590\n",
            "Epoch [80/100], Loss: 11657.5349\n",
            "Epoch [90/100], Loss: 12492.1650\n",
            "Epoch [100/100], Loss: 8073.4375\n",
            "Fold 1, RMSE: 59.80904006958008\n",
            "Epoch [10/100], Loss: 22709.9753\n",
            "Epoch [20/100], Loss: 14551.5010\n",
            "Epoch [30/100], Loss: 12798.0811\n",
            "Epoch [40/100], Loss: 11162.3137\n",
            "Epoch [50/100], Loss: 8651.6583\n",
            "Epoch [60/100], Loss: 8522.8986\n",
            "Epoch [70/100], Loss: 8133.8829\n",
            "Epoch [80/100], Loss: 8955.6042\n",
            "Epoch [90/100], Loss: 4889.1188\n",
            "Epoch [100/100], Loss: 11731.2488\n",
            "Fold 2, RMSE: 74.0711669921875\n",
            "Epoch [10/100], Loss: 13606.6519\n",
            "Epoch [20/100], Loss: 15467.7871\n",
            "Epoch [30/100], Loss: 10305.5623\n",
            "Epoch [40/100], Loss: 9328.3409\n",
            "Epoch [50/100], Loss: 6351.6353\n",
            "Epoch [60/100], Loss: 5845.0266\n",
            "Epoch [70/100], Loss: 6730.5912\n",
            "Epoch [80/100], Loss: 10204.8528\n",
            "Epoch [90/100], Loss: 2067.2707\n",
            "Epoch [100/100], Loss: 8204.2313\n",
            "Fold 3, RMSE: 94.9451904296875\n",
            "Epoch [10/100], Loss: 20738.3245\n",
            "Epoch [20/100], Loss: 15373.9780\n",
            "Epoch [30/100], Loss: 12832.3917\n",
            "Epoch [40/100], Loss: 8525.0443\n",
            "Epoch [50/100], Loss: 7086.2133\n",
            "Epoch [60/100], Loss: 17492.6721\n",
            "Epoch [70/100], Loss: 12342.8596\n",
            "Epoch [80/100], Loss: 5932.7284\n",
            "Epoch [90/100], Loss: 10353.3264\n",
            "Epoch [100/100], Loss: 11438.2902\n",
            "Fold 4, RMSE: 50.70561599731445\n",
            "Epoch [10/100], Loss: 15567.1479\n",
            "Epoch [20/100], Loss: 15645.7485\n",
            "Epoch [30/100], Loss: 9692.7117\n",
            "Epoch [40/100], Loss: 10412.9229\n",
            "Epoch [50/100], Loss: 11453.0889\n",
            "Epoch [60/100], Loss: 6052.7616\n",
            "Epoch [70/100], Loss: 7198.7686\n",
            "Epoch [80/100], Loss: 6879.2361\n",
            "Epoch [90/100], Loss: 8015.6299\n",
            "Epoch [100/100], Loss: 8617.0633\n",
            "Fold 5, RMSE: 45.26636505126953\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 64.95947570800782\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15529.8374\n",
            "Epoch [20/150], Loss: 13949.0205\n",
            "Epoch [30/150], Loss: 10776.7827\n",
            "Epoch [40/150], Loss: 9105.2882\n",
            "Epoch [50/150], Loss: 9565.9594\n",
            "Epoch [60/150], Loss: 9555.2197\n",
            "Epoch [70/150], Loss: 3913.5586\n",
            "Epoch [80/150], Loss: 11049.3696\n",
            "Epoch [90/150], Loss: 6680.3182\n",
            "Epoch [100/150], Loss: 9899.6748\n",
            "Epoch [110/150], Loss: 4564.0297\n",
            "Epoch [120/150], Loss: 9713.1157\n",
            "Epoch [130/150], Loss: 5354.9080\n",
            "Epoch [140/150], Loss: 8199.5739\n",
            "Epoch [150/150], Loss: 6999.6532\n",
            "Fold 1, RMSE: 63.84186553955078\n",
            "Epoch [10/150], Loss: 11577.2178\n",
            "Epoch [20/150], Loss: 14284.2231\n",
            "Epoch [30/150], Loss: 13688.5503\n",
            "Epoch [40/150], Loss: 11000.3232\n",
            "Epoch [50/150], Loss: 9862.5667\n",
            "Epoch [60/150], Loss: 11941.9165\n",
            "Epoch [70/150], Loss: 8001.2350\n",
            "Epoch [80/150], Loss: 13779.8540\n",
            "Epoch [90/150], Loss: 10268.7451\n",
            "Epoch [100/150], Loss: 12143.4355\n",
            "Epoch [110/150], Loss: 7000.8840\n",
            "Epoch [120/150], Loss: 5174.5767\n",
            "Epoch [130/150], Loss: 6042.3825\n",
            "Epoch [140/150], Loss: 7002.4813\n",
            "Epoch [150/150], Loss: 4010.4464\n",
            "Fold 2, RMSE: 72.8922119140625\n",
            "Epoch [10/150], Loss: 9103.2783\n",
            "Epoch [20/150], Loss: 11863.6826\n",
            "Epoch [30/150], Loss: 7896.3677\n",
            "Epoch [40/150], Loss: 6595.9878\n",
            "Epoch [50/150], Loss: 7665.4098\n",
            "Epoch [60/150], Loss: 11557.5623\n",
            "Epoch [70/150], Loss: 7219.0847\n",
            "Epoch [80/150], Loss: 6871.5507\n",
            "Epoch [90/150], Loss: 6148.4749\n",
            "Epoch [100/150], Loss: 6536.6562\n",
            "Epoch [110/150], Loss: 10488.7305\n",
            "Epoch [120/150], Loss: 7620.5583\n",
            "Epoch [130/150], Loss: 7427.6642\n",
            "Epoch [140/150], Loss: 4805.1827\n",
            "Epoch [150/150], Loss: 6265.5481\n",
            "Fold 3, RMSE: 106.68598175048828\n",
            "Epoch [10/150], Loss: 23487.5762\n",
            "Epoch [20/150], Loss: 17456.4204\n",
            "Epoch [30/150], Loss: 13268.7471\n",
            "Epoch [40/150], Loss: 15535.6750\n",
            "Epoch [50/150], Loss: 10864.1055\n",
            "Epoch [60/150], Loss: 9407.6233\n",
            "Epoch [70/150], Loss: 20076.9614\n",
            "Epoch [80/150], Loss: 16247.5955\n",
            "Epoch [90/150], Loss: 16697.9126\n",
            "Epoch [100/150], Loss: 19940.1426\n",
            "Epoch [110/150], Loss: 12715.4780\n",
            "Epoch [120/150], Loss: 15782.4210\n",
            "Epoch [130/150], Loss: 12007.9072\n",
            "Epoch [140/150], Loss: 8694.9291\n",
            "Epoch [150/150], Loss: 18252.0813\n",
            "Fold 4, RMSE: 47.51495361328125\n",
            "Epoch [10/150], Loss: 17351.6570\n",
            "Epoch [20/150], Loss: 9582.4043\n",
            "Epoch [30/150], Loss: 11598.0632\n",
            "Epoch [40/150], Loss: 11749.4675\n",
            "Epoch [50/150], Loss: 13196.5139\n",
            "Epoch [60/150], Loss: 6916.7954\n",
            "Epoch [70/150], Loss: 8495.3782\n",
            "Epoch [80/150], Loss: 7820.7007\n",
            "Epoch [90/150], Loss: 6866.0565\n",
            "Epoch [100/150], Loss: 8056.9042\n",
            "Epoch [110/150], Loss: 7770.4724\n",
            "Epoch [120/150], Loss: 5181.0878\n",
            "Epoch [130/150], Loss: 10054.5325\n",
            "Epoch [140/150], Loss: 5886.9778\n",
            "Epoch [150/150], Loss: 9999.8243\n",
            "Fold 5, RMSE: 43.16193771362305\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 66.81939010620117\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 20632.4878\n",
            "Epoch [20/100], Loss: 12990.5950\n",
            "Epoch [30/100], Loss: 10067.2123\n",
            "Epoch [40/100], Loss: 11081.1399\n",
            "Epoch [50/100], Loss: 6317.5240\n",
            "Epoch [60/100], Loss: 7100.3099\n",
            "Epoch [70/100], Loss: 5863.4502\n",
            "Epoch [80/100], Loss: 3328.1498\n",
            "Epoch [90/100], Loss: 16460.9265\n",
            "Epoch [100/100], Loss: 6785.5638\n",
            "Fold 1, RMSE: 60.97172546386719\n",
            "Epoch [10/100], Loss: 20025.2476\n",
            "Epoch [20/100], Loss: 16298.0935\n",
            "Epoch [30/100], Loss: 20353.8762\n",
            "Epoch [40/100], Loss: 15095.4688\n",
            "Epoch [50/100], Loss: 10142.5382\n",
            "Epoch [60/100], Loss: 13459.1580\n",
            "Epoch [70/100], Loss: 11631.3469\n",
            "Epoch [80/100], Loss: 6796.0293\n",
            "Epoch [90/100], Loss: 14117.0709\n",
            "Epoch [100/100], Loss: 15521.4307\n",
            "Fold 2, RMSE: 87.35758209228516\n",
            "Epoch [10/100], Loss: 13637.1743\n",
            "Epoch [20/100], Loss: 13855.0640\n",
            "Epoch [30/100], Loss: 14047.1802\n",
            "Epoch [40/100], Loss: 12075.2413\n",
            "Epoch [50/100], Loss: 11833.8135\n",
            "Epoch [60/100], Loss: 12502.0491\n",
            "Epoch [70/100], Loss: 12432.0437\n",
            "Epoch [80/100], Loss: 14593.2146\n",
            "Epoch [90/100], Loss: 13074.0381\n",
            "Epoch [100/100], Loss: 14583.8000\n",
            "Fold 3, RMSE: 109.7467269897461\n",
            "Epoch [10/100], Loss: 18395.1567\n",
            "Epoch [20/100], Loss: 13992.8145\n",
            "Epoch [30/100], Loss: 13599.5964\n",
            "Epoch [40/100], Loss: 13319.6289\n",
            "Epoch [50/100], Loss: 15143.9011\n",
            "Epoch [60/100], Loss: 9529.3071\n",
            "Epoch [70/100], Loss: 7389.2445\n",
            "Epoch [80/100], Loss: 6208.7428\n",
            "Epoch [90/100], Loss: 8557.2552\n",
            "Epoch [100/100], Loss: 7470.9147\n",
            "Fold 4, RMSE: 43.74462127685547\n",
            "Epoch [10/100], Loss: 18170.7957\n",
            "Epoch [20/100], Loss: 27038.4219\n",
            "Epoch [30/100], Loss: 17606.1855\n",
            "Epoch [40/100], Loss: 10681.2522\n",
            "Epoch [50/100], Loss: 11249.5940\n",
            "Epoch [60/100], Loss: 7537.8870\n",
            "Epoch [70/100], Loss: 15054.5747\n",
            "Epoch [80/100], Loss: 12709.2737\n",
            "Epoch [90/100], Loss: 7040.1385\n",
            "Epoch [100/100], Loss: 9581.5278\n",
            "Fold 5, RMSE: 46.66474914550781\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 69.69708099365235\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 17117.9304\n",
            "Epoch [20/150], Loss: 25606.2100\n",
            "Epoch [30/150], Loss: 18041.6470\n",
            "Epoch [40/150], Loss: 13940.4917\n",
            "Epoch [50/150], Loss: 15244.7754\n",
            "Epoch [60/150], Loss: 13099.6986\n",
            "Epoch [70/150], Loss: 11581.7544\n",
            "Epoch [80/150], Loss: 15265.6084\n",
            "Epoch [90/150], Loss: 12442.3506\n",
            "Epoch [100/150], Loss: 14197.4521\n",
            "Epoch [110/150], Loss: 19701.4399\n",
            "Epoch [120/150], Loss: 16453.2612\n",
            "Epoch [130/150], Loss: 11943.5513\n",
            "Epoch [140/150], Loss: 17138.7444\n",
            "Epoch [150/150], Loss: 10621.1216\n",
            "Fold 1, RMSE: 60.82185363769531\n",
            "Epoch [10/150], Loss: 15871.2278\n",
            "Epoch [20/150], Loss: 16609.6494\n",
            "Epoch [30/150], Loss: 16156.7341\n",
            "Epoch [40/150], Loss: 10273.0669\n",
            "Epoch [50/150], Loss: 9279.6453\n",
            "Epoch [60/150], Loss: 11574.4395\n",
            "Epoch [70/150], Loss: 13701.8779\n",
            "Epoch [80/150], Loss: 14728.4258\n",
            "Epoch [90/150], Loss: 14200.5125\n",
            "Epoch [100/150], Loss: 11439.4219\n",
            "Epoch [110/150], Loss: 12679.3806\n",
            "Epoch [120/150], Loss: 12126.6569\n",
            "Epoch [130/150], Loss: 11333.5010\n",
            "Epoch [140/150], Loss: 7569.0757\n",
            "Epoch [150/150], Loss: 13097.9133\n",
            "Fold 2, RMSE: 64.97508239746094\n",
            "Epoch [10/150], Loss: 13100.4253\n",
            "Epoch [20/150], Loss: 14067.8008\n",
            "Epoch [30/150], Loss: 14662.7327\n",
            "Epoch [40/150], Loss: 19534.1292\n",
            "Epoch [50/150], Loss: 15941.3853\n",
            "Epoch [60/150], Loss: 12749.1404\n",
            "Epoch [70/150], Loss: 16707.7305\n",
            "Epoch [80/150], Loss: 12335.0745\n",
            "Epoch [90/150], Loss: 14474.7773\n",
            "Epoch [100/150], Loss: 13802.7988\n",
            "Epoch [110/150], Loss: 15573.7610\n",
            "Epoch [120/150], Loss: 12032.4061\n",
            "Epoch [130/150], Loss: 12078.2299\n",
            "Epoch [140/150], Loss: 13205.0457\n",
            "Epoch [150/150], Loss: 13517.8811\n",
            "Fold 3, RMSE: 109.55241394042969\n",
            "Epoch [10/150], Loss: 15667.9861\n",
            "Epoch [20/150], Loss: 17275.9341\n",
            "Epoch [30/150], Loss: 15410.4492\n",
            "Epoch [40/150], Loss: 16475.3054\n",
            "Epoch [50/150], Loss: 8546.3437\n",
            "Epoch [60/150], Loss: 17257.9224\n",
            "Epoch [70/150], Loss: 14701.9263\n",
            "Epoch [80/150], Loss: 8888.5548\n",
            "Epoch [90/150], Loss: 12753.7852\n",
            "Epoch [100/150], Loss: 10258.3853\n",
            "Epoch [110/150], Loss: 11601.2039\n",
            "Epoch [120/150], Loss: 8754.8939\n",
            "Epoch [130/150], Loss: 11896.8779\n",
            "Epoch [140/150], Loss: 10590.2400\n",
            "Epoch [150/150], Loss: 9627.8199\n",
            "Fold 4, RMSE: 48.49070739746094\n",
            "Epoch [10/150], Loss: 18477.3477\n",
            "Epoch [20/150], Loss: 19676.0977\n",
            "Epoch [30/150], Loss: 22387.9927\n",
            "Epoch [40/150], Loss: 13913.9893\n",
            "Epoch [50/150], Loss: 16914.7393\n",
            "Epoch [60/150], Loss: 13674.8191\n",
            "Epoch [70/150], Loss: 9410.0815\n",
            "Epoch [80/150], Loss: 7612.6224\n",
            "Epoch [90/150], Loss: 11968.7598\n",
            "Epoch [100/150], Loss: 20318.4185\n",
            "Epoch [110/150], Loss: 14275.1675\n",
            "Epoch [120/150], Loss: 14038.2120\n",
            "Epoch [130/150], Loss: 12784.3604\n",
            "Epoch [140/150], Loss: 16175.3787\n",
            "Epoch [150/150], Loss: 7805.9716\n",
            "Fold 5, RMSE: 45.30709457397461\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 65.8294303894043\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 13976.3965\n",
            "Epoch [20/100], Loss: 11441.8655\n",
            "Epoch [30/100], Loss: 10455.8440\n",
            "Epoch [40/100], Loss: 5277.6636\n",
            "Epoch [50/100], Loss: 10315.3998\n",
            "Epoch [60/100], Loss: 7019.0967\n",
            "Epoch [70/100], Loss: 6041.9043\n",
            "Epoch [80/100], Loss: 6777.9073\n",
            "Epoch [90/100], Loss: 7385.4763\n",
            "Epoch [100/100], Loss: 8596.0901\n",
            "Fold 1, RMSE: 56.18443298339844\n",
            "Epoch [10/100], Loss: 20688.6211\n",
            "Epoch [20/100], Loss: 11363.3752\n",
            "Epoch [30/100], Loss: 10659.2480\n",
            "Epoch [40/100], Loss: 13130.7830\n",
            "Epoch [50/100], Loss: 6266.1913\n",
            "Epoch [60/100], Loss: 10890.8046\n",
            "Epoch [70/100], Loss: 9498.3461\n",
            "Epoch [80/100], Loss: 7850.0044\n",
            "Epoch [90/100], Loss: 7011.3055\n",
            "Epoch [100/100], Loss: 3885.0945\n",
            "Fold 2, RMSE: 75.69412231445312\n",
            "Epoch [10/100], Loss: 10511.0134\n",
            "Epoch [20/100], Loss: 10118.8872\n",
            "Epoch [30/100], Loss: 13485.3538\n",
            "Epoch [40/100], Loss: 10862.6299\n",
            "Epoch [50/100], Loss: 10168.9456\n",
            "Epoch [60/100], Loss: 10590.5214\n",
            "Epoch [70/100], Loss: 7637.9894\n",
            "Epoch [80/100], Loss: 11969.3711\n",
            "Epoch [90/100], Loss: 13086.8965\n",
            "Epoch [100/100], Loss: 10241.1113\n",
            "Fold 3, RMSE: 102.74734497070312\n",
            "Epoch [10/100], Loss: 20279.3418\n",
            "Epoch [20/100], Loss: 10532.0348\n",
            "Epoch [30/100], Loss: 11588.2698\n",
            "Epoch [40/100], Loss: 7897.7004\n",
            "Epoch [50/100], Loss: 7185.2697\n",
            "Epoch [60/100], Loss: 12041.7914\n",
            "Epoch [70/100], Loss: 8571.8120\n",
            "Epoch [80/100], Loss: 6669.9084\n",
            "Epoch [90/100], Loss: 13588.6571\n",
            "Epoch [100/100], Loss: 7729.3680\n",
            "Fold 4, RMSE: 44.342796325683594\n",
            "Epoch [10/100], Loss: 15966.8188\n",
            "Epoch [20/100], Loss: 13325.5359\n",
            "Epoch [30/100], Loss: 10585.3555\n",
            "Epoch [40/100], Loss: 16866.0376\n",
            "Epoch [50/100], Loss: 7419.3949\n",
            "Epoch [60/100], Loss: 11738.9353\n",
            "Epoch [70/100], Loss: 8645.5699\n",
            "Epoch [80/100], Loss: 9626.3729\n",
            "Epoch [90/100], Loss: 13772.7122\n",
            "Epoch [100/100], Loss: 15573.7026\n",
            "Fold 5, RMSE: 47.05496597290039\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 65.20473251342773\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14724.4883\n",
            "Epoch [20/150], Loss: 18632.5024\n",
            "Epoch [30/150], Loss: 16239.6533\n",
            "Epoch [40/150], Loss: 12550.8416\n",
            "Epoch [50/150], Loss: 9369.6993\n",
            "Epoch [60/150], Loss: 5699.1896\n",
            "Epoch [70/150], Loss: 4913.1440\n",
            "Epoch [80/150], Loss: 5685.1217\n",
            "Epoch [90/150], Loss: 8133.7996\n",
            "Epoch [100/150], Loss: 8375.1387\n",
            "Epoch [110/150], Loss: 10476.4023\n",
            "Epoch [120/150], Loss: 3670.1858\n",
            "Epoch [130/150], Loss: 10275.2273\n",
            "Epoch [140/150], Loss: 8241.3762\n",
            "Epoch [150/150], Loss: 8864.8572\n",
            "Fold 1, RMSE: 57.793739318847656\n",
            "Epoch [10/150], Loss: 14894.5950\n",
            "Epoch [20/150], Loss: 11061.2866\n",
            "Epoch [30/150], Loss: 9175.1948\n",
            "Epoch [40/150], Loss: 11628.5518\n",
            "Epoch [50/150], Loss: 9847.2136\n",
            "Epoch [60/150], Loss: 10350.9636\n",
            "Epoch [70/150], Loss: 15816.3405\n",
            "Epoch [80/150], Loss: 17491.6788\n",
            "Epoch [90/150], Loss: 7441.6407\n",
            "Epoch [100/150], Loss: 4240.2364\n",
            "Epoch [110/150], Loss: 6512.0362\n",
            "Epoch [120/150], Loss: 5051.4049\n",
            "Epoch [130/150], Loss: 6499.6502\n",
            "Epoch [140/150], Loss: 6012.4420\n",
            "Epoch [150/150], Loss: 11225.9556\n",
            "Fold 2, RMSE: 76.41541290283203\n",
            "Epoch [10/150], Loss: 12313.7117\n",
            "Epoch [20/150], Loss: 12399.1321\n",
            "Epoch [30/150], Loss: 7814.5065\n",
            "Epoch [40/150], Loss: 11791.2179\n",
            "Epoch [50/150], Loss: 11312.8428\n",
            "Epoch [60/150], Loss: 7158.2012\n",
            "Epoch [70/150], Loss: 6497.3882\n",
            "Epoch [80/150], Loss: 9112.3573\n",
            "Epoch [90/150], Loss: 12947.2400\n",
            "Epoch [100/150], Loss: 14770.2856\n",
            "Epoch [110/150], Loss: 5891.5276\n",
            "Epoch [120/150], Loss: 8066.6453\n",
            "Epoch [130/150], Loss: 9284.8633\n",
            "Epoch [140/150], Loss: 8872.4126\n",
            "Epoch [150/150], Loss: 5246.7184\n",
            "Fold 3, RMSE: 99.91277313232422\n",
            "Epoch [10/150], Loss: 20236.5652\n",
            "Epoch [20/150], Loss: 19433.1943\n",
            "Epoch [30/150], Loss: 21042.3315\n",
            "Epoch [40/150], Loss: 19370.9802\n",
            "Epoch [50/150], Loss: 13025.6392\n",
            "Epoch [60/150], Loss: 11088.8494\n",
            "Epoch [70/150], Loss: 13813.3967\n",
            "Epoch [80/150], Loss: 10759.7767\n",
            "Epoch [90/150], Loss: 16248.8354\n",
            "Epoch [100/150], Loss: 11367.3292\n",
            "Epoch [110/150], Loss: 21425.5930\n",
            "Epoch [120/150], Loss: 12067.5596\n",
            "Epoch [130/150], Loss: 7294.6195\n",
            "Epoch [140/150], Loss: 10666.4363\n",
            "Epoch [150/150], Loss: 5779.8231\n",
            "Fold 4, RMSE: 41.72315216064453\n",
            "Epoch [10/150], Loss: 17230.7603\n",
            "Epoch [20/150], Loss: 11437.6533\n",
            "Epoch [30/150], Loss: 11274.5160\n",
            "Epoch [40/150], Loss: 13024.0027\n",
            "Epoch [50/150], Loss: 12927.9590\n",
            "Epoch [60/150], Loss: 13565.1169\n",
            "Epoch [70/150], Loss: 13435.7620\n",
            "Epoch [80/150], Loss: 8460.2552\n",
            "Epoch [90/150], Loss: 6382.7448\n",
            "Epoch [100/150], Loss: 9566.0359\n",
            "Epoch [110/150], Loss: 2990.3607\n",
            "Epoch [120/150], Loss: 8517.2881\n",
            "Epoch [130/150], Loss: 5523.7669\n",
            "Epoch [140/150], Loss: 4150.9773\n",
            "Epoch [150/150], Loss: 10485.3066\n",
            "Fold 5, RMSE: 44.840049743652344\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 64.13702545166015\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 21027.7249\n",
            "Epoch [20/100], Loss: 20478.9231\n",
            "Epoch [30/100], Loss: 11391.0911\n",
            "Epoch [40/100], Loss: 18446.4654\n",
            "Epoch [50/100], Loss: 11749.8328\n",
            "Epoch [60/100], Loss: 8976.3829\n",
            "Epoch [70/100], Loss: 6532.8734\n",
            "Epoch [80/100], Loss: 7725.6078\n",
            "Epoch [90/100], Loss: 5057.4586\n",
            "Epoch [100/100], Loss: 10692.5244\n",
            "Fold 1, RMSE: 58.77446746826172\n",
            "Epoch [10/100], Loss: 14039.2358\n",
            "Epoch [20/100], Loss: 14701.7437\n",
            "Epoch [30/100], Loss: 11989.2611\n",
            "Epoch [40/100], Loss: 12983.6855\n",
            "Epoch [50/100], Loss: 8355.0953\n",
            "Epoch [60/100], Loss: 15136.5613\n",
            "Epoch [70/100], Loss: 11353.9221\n",
            "Epoch [80/100], Loss: 10848.4624\n",
            "Epoch [90/100], Loss: 8414.5898\n",
            "Epoch [100/100], Loss: 5116.3775\n",
            "Fold 2, RMSE: 68.84866333007812\n",
            "Epoch [10/100], Loss: 10028.3474\n",
            "Epoch [20/100], Loss: 8303.1917\n",
            "Epoch [30/100], Loss: 9784.7874\n",
            "Epoch [40/100], Loss: 11250.0295\n",
            "Epoch [50/100], Loss: 8087.5762\n",
            "Epoch [60/100], Loss: 7023.4202\n",
            "Epoch [70/100], Loss: 5866.7567\n",
            "Epoch [80/100], Loss: 7012.1862\n",
            "Epoch [90/100], Loss: 13809.3772\n",
            "Epoch [100/100], Loss: 13210.1130\n",
            "Fold 3, RMSE: 109.79956817626953\n",
            "Epoch [10/100], Loss: 17365.1987\n",
            "Epoch [20/100], Loss: 19785.3799\n",
            "Epoch [30/100], Loss: 18605.4453\n",
            "Epoch [40/100], Loss: 20947.7993\n",
            "Epoch [50/100], Loss: 18966.8247\n",
            "Epoch [60/100], Loss: 18926.8872\n",
            "Epoch [70/100], Loss: 18211.2419\n",
            "Epoch [80/100], Loss: 19143.9241\n",
            "Epoch [90/100], Loss: 18265.3274\n",
            "Epoch [100/100], Loss: 17769.9373\n",
            "Fold 4, RMSE: 54.407405853271484\n",
            "Epoch [10/100], Loss: 14564.2571\n",
            "Epoch [20/100], Loss: 14679.7607\n",
            "Epoch [30/100], Loss: 13516.2705\n",
            "Epoch [40/100], Loss: 8538.3651\n",
            "Epoch [50/100], Loss: 12871.9530\n",
            "Epoch [60/100], Loss: 10342.9246\n",
            "Epoch [70/100], Loss: 14718.1052\n",
            "Epoch [80/100], Loss: 9255.8142\n",
            "Epoch [90/100], Loss: 6707.4070\n",
            "Epoch [100/100], Loss: 10812.7168\n",
            "Fold 5, RMSE: 46.109344482421875\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 67.58788986206055\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19944.3247\n",
            "Epoch [20/150], Loss: 16251.3711\n",
            "Epoch [30/150], Loss: 12626.0679\n",
            "Epoch [40/150], Loss: 14138.7117\n",
            "Epoch [50/150], Loss: 11170.9604\n",
            "Epoch [60/150], Loss: 9285.9797\n",
            "Epoch [70/150], Loss: 6487.3499\n",
            "Epoch [80/150], Loss: 9664.3003\n",
            "Epoch [90/150], Loss: 8474.7454\n",
            "Epoch [100/150], Loss: 13428.0249\n",
            "Epoch [110/150], Loss: 14041.2920\n",
            "Epoch [120/150], Loss: 9259.6921\n",
            "Epoch [130/150], Loss: 13704.7338\n",
            "Epoch [140/150], Loss: 15151.5073\n",
            "Epoch [150/150], Loss: 11719.4124\n",
            "Fold 1, RMSE: 57.196475982666016\n",
            "Epoch [10/150], Loss: 17092.3618\n",
            "Epoch [20/150], Loss: 13357.2783\n",
            "Epoch [30/150], Loss: 12796.0117\n",
            "Epoch [40/150], Loss: 23967.4050\n",
            "Epoch [50/150], Loss: 6089.9441\n",
            "Epoch [60/150], Loss: 13103.5541\n",
            "Epoch [70/150], Loss: 7704.5542\n",
            "Epoch [80/150], Loss: 7478.9816\n",
            "Epoch [90/150], Loss: 8116.7725\n",
            "Epoch [100/150], Loss: 11083.6703\n",
            "Epoch [110/150], Loss: 5661.2752\n",
            "Epoch [120/150], Loss: 12818.0017\n",
            "Epoch [130/150], Loss: 7211.8652\n",
            "Epoch [140/150], Loss: 11044.1382\n",
            "Epoch [150/150], Loss: 4423.9362\n",
            "Fold 2, RMSE: 76.22532653808594\n",
            "Epoch [10/150], Loss: 16391.3452\n",
            "Epoch [20/150], Loss: 12546.9500\n",
            "Epoch [30/150], Loss: 9067.0192\n",
            "Epoch [40/150], Loss: 14597.3252\n",
            "Epoch [50/150], Loss: 5605.3921\n",
            "Epoch [60/150], Loss: 9438.4116\n",
            "Epoch [70/150], Loss: 12330.3484\n",
            "Epoch [80/150], Loss: 11683.5159\n",
            "Epoch [90/150], Loss: 12261.4541\n",
            "Epoch [100/150], Loss: 13573.1304\n",
            "Epoch [110/150], Loss: 11026.6466\n",
            "Epoch [120/150], Loss: 11178.1536\n",
            "Epoch [130/150], Loss: 11837.8511\n",
            "Epoch [140/150], Loss: 11423.6599\n",
            "Epoch [150/150], Loss: 16393.2861\n",
            "Fold 3, RMSE: 109.642822265625\n",
            "Epoch [10/150], Loss: 16872.8569\n",
            "Epoch [20/150], Loss: 13094.9329\n",
            "Epoch [30/150], Loss: 10496.9690\n",
            "Epoch [40/150], Loss: 14595.4365\n",
            "Epoch [50/150], Loss: 17472.9575\n",
            "Epoch [60/150], Loss: 16216.7874\n",
            "Epoch [70/150], Loss: 10028.2806\n",
            "Epoch [80/150], Loss: 9061.3885\n",
            "Epoch [90/150], Loss: 8329.5586\n",
            "Epoch [100/150], Loss: 12500.0968\n",
            "Epoch [110/150], Loss: 12796.5001\n",
            "Epoch [120/150], Loss: 11890.4741\n",
            "Epoch [130/150], Loss: 11805.3574\n",
            "Epoch [140/150], Loss: 14474.2253\n",
            "Epoch [150/150], Loss: 9423.3140\n",
            "Fold 4, RMSE: 44.26883316040039\n",
            "Epoch [10/150], Loss: 20162.6812\n",
            "Epoch [20/150], Loss: 13674.8405\n",
            "Epoch [30/150], Loss: 13524.8970\n",
            "Epoch [40/150], Loss: 9579.6558\n",
            "Epoch [50/150], Loss: 6981.8170\n",
            "Epoch [60/150], Loss: 12527.1243\n",
            "Epoch [70/150], Loss: 7678.0785\n",
            "Epoch [80/150], Loss: 5684.8521\n",
            "Epoch [90/150], Loss: 9702.3070\n",
            "Epoch [100/150], Loss: 6599.3624\n",
            "Epoch [110/150], Loss: 6285.9902\n",
            "Epoch [120/150], Loss: 11593.1189\n",
            "Epoch [130/150], Loss: 7687.9575\n",
            "Epoch [140/150], Loss: 5713.1301\n",
            "Epoch [150/150], Loss: 13264.1614\n",
            "Fold 5, RMSE: 45.86260986328125\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 66.63921356201172\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14116.4565\n",
            "Epoch [20/100], Loss: 11621.0498\n",
            "Epoch [30/100], Loss: 18011.8940\n",
            "Epoch [40/100], Loss: 8696.7424\n",
            "Epoch [50/100], Loss: 9492.3521\n",
            "Epoch [60/100], Loss: 7458.6915\n",
            "Epoch [70/100], Loss: 16349.9753\n",
            "Epoch [80/100], Loss: 8801.0176\n",
            "Epoch [90/100], Loss: 10698.3887\n",
            "Epoch [100/100], Loss: 6278.2675\n",
            "Fold 1, RMSE: 56.58606719970703\n",
            "Epoch [10/100], Loss: 14853.0457\n",
            "Epoch [20/100], Loss: 12286.3610\n",
            "Epoch [30/100], Loss: 11338.1544\n",
            "Epoch [40/100], Loss: 8050.6813\n",
            "Epoch [50/100], Loss: 8121.1235\n",
            "Epoch [60/100], Loss: 14462.1641\n",
            "Epoch [70/100], Loss: 11806.7795\n",
            "Epoch [80/100], Loss: 9231.3380\n",
            "Epoch [90/100], Loss: 6861.3186\n",
            "Epoch [100/100], Loss: 4600.3696\n",
            "Fold 2, RMSE: 73.12853240966797\n",
            "Epoch [10/100], Loss: 15673.9894\n",
            "Epoch [20/100], Loss: 8779.6503\n",
            "Epoch [30/100], Loss: 8957.5237\n",
            "Epoch [40/100], Loss: 9699.9509\n",
            "Epoch [50/100], Loss: 14168.9570\n",
            "Epoch [60/100], Loss: 7066.2217\n",
            "Epoch [70/100], Loss: 4266.5301\n",
            "Epoch [80/100], Loss: 3635.5854\n",
            "Epoch [90/100], Loss: 5482.3885\n",
            "Epoch [100/100], Loss: 8909.4829\n",
            "Fold 3, RMSE: 99.0440673828125\n",
            "Epoch [10/100], Loss: 23974.0688\n",
            "Epoch [20/100], Loss: 21936.8076\n",
            "Epoch [30/100], Loss: 11618.0427\n",
            "Epoch [40/100], Loss: 15389.0994\n",
            "Epoch [50/100], Loss: 8675.4570\n",
            "Epoch [60/100], Loss: 9014.4440\n",
            "Epoch [70/100], Loss: 8874.5209\n",
            "Epoch [80/100], Loss: 14647.7603\n",
            "Epoch [90/100], Loss: 7449.3864\n",
            "Epoch [100/100], Loss: 16872.8628\n",
            "Fold 4, RMSE: 46.798702239990234\n",
            "Epoch [10/100], Loss: 18668.9790\n",
            "Epoch [20/100], Loss: 10606.4167\n",
            "Epoch [30/100], Loss: 12314.3855\n",
            "Epoch [40/100], Loss: 11769.9562\n",
            "Epoch [50/100], Loss: 12327.4614\n",
            "Epoch [60/100], Loss: 7303.9268\n",
            "Epoch [70/100], Loss: 8538.1608\n",
            "Epoch [80/100], Loss: 8951.5504\n",
            "Epoch [90/100], Loss: 5987.2391\n",
            "Epoch [100/100], Loss: 7351.1644\n",
            "Fold 5, RMSE: 44.787601470947266\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 64.068994140625\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 11886.4506\n",
            "Epoch [20/150], Loss: 13591.3025\n",
            "Epoch [30/150], Loss: 12522.0432\n",
            "Epoch [40/150], Loss: 14806.3717\n",
            "Epoch [50/150], Loss: 4511.7003\n",
            "Epoch [60/150], Loss: 11620.5739\n",
            "Epoch [70/150], Loss: 9445.2412\n",
            "Epoch [80/150], Loss: 5047.8678\n",
            "Epoch [90/150], Loss: 5729.3195\n",
            "Epoch [100/150], Loss: 5049.6255\n",
            "Epoch [110/150], Loss: 9216.3298\n",
            "Epoch [120/150], Loss: 9435.4025\n",
            "Epoch [130/150], Loss: 3080.2397\n",
            "Epoch [140/150], Loss: 10179.3447\n",
            "Epoch [150/150], Loss: 5212.6247\n",
            "Fold 1, RMSE: 57.49170684814453\n",
            "Epoch [10/150], Loss: 12853.4181\n",
            "Epoch [20/150], Loss: 11114.4896\n",
            "Epoch [30/150], Loss: 7786.1780\n",
            "Epoch [40/150], Loss: 8233.4698\n",
            "Epoch [50/150], Loss: 7230.9591\n",
            "Epoch [60/150], Loss: 7453.4104\n",
            "Epoch [70/150], Loss: 10553.9922\n",
            "Epoch [80/150], Loss: 9809.0608\n",
            "Epoch [90/150], Loss: 9578.7192\n",
            "Epoch [100/150], Loss: 6901.3095\n",
            "Epoch [110/150], Loss: 4032.8198\n",
            "Epoch [120/150], Loss: 10110.0680\n",
            "Epoch [130/150], Loss: 4514.9138\n",
            "Epoch [140/150], Loss: 5426.3008\n",
            "Epoch [150/150], Loss: 6639.3712\n",
            "Fold 2, RMSE: 75.25796508789062\n",
            "Epoch [10/150], Loss: 11803.0403\n",
            "Epoch [20/150], Loss: 10967.9824\n",
            "Epoch [30/150], Loss: 8919.1919\n",
            "Epoch [40/150], Loss: 9018.1926\n",
            "Epoch [50/150], Loss: 7405.0564\n",
            "Epoch [60/150], Loss: 9396.7032\n",
            "Epoch [70/150], Loss: 9204.1431\n",
            "Epoch [80/150], Loss: 7871.6719\n",
            "Epoch [90/150], Loss: 4578.8861\n",
            "Epoch [100/150], Loss: 7243.5465\n",
            "Epoch [110/150], Loss: 9856.8467\n",
            "Epoch [120/150], Loss: 14963.9434\n",
            "Epoch [130/150], Loss: 15476.7856\n",
            "Epoch [140/150], Loss: 15348.8511\n",
            "Epoch [150/150], Loss: 15707.4247\n",
            "Fold 3, RMSE: 109.29765319824219\n",
            "Epoch [10/150], Loss: 17604.7932\n",
            "Epoch [20/150], Loss: 16776.6682\n",
            "Epoch [30/150], Loss: 9219.2019\n",
            "Epoch [40/150], Loss: 13936.2849\n",
            "Epoch [50/150], Loss: 14680.6235\n",
            "Epoch [60/150], Loss: 19039.1960\n",
            "Epoch [70/150], Loss: 10790.5219\n",
            "Epoch [80/150], Loss: 10858.5648\n",
            "Epoch [90/150], Loss: 14999.4197\n",
            "Epoch [100/150], Loss: 11975.1616\n",
            "Epoch [110/150], Loss: 6288.3386\n",
            "Epoch [120/150], Loss: 11716.8870\n",
            "Epoch [130/150], Loss: 12411.5857\n",
            "Epoch [140/150], Loss: 11381.2393\n",
            "Epoch [150/150], Loss: 12595.3575\n",
            "Fold 4, RMSE: 44.4706916809082\n",
            "Epoch [10/150], Loss: 12314.2290\n",
            "Epoch [20/150], Loss: 10599.5850\n",
            "Epoch [30/150], Loss: 14393.0212\n",
            "Epoch [40/150], Loss: 7496.4110\n",
            "Epoch [50/150], Loss: 7881.7776\n",
            "Epoch [60/150], Loss: 9194.8798\n",
            "Epoch [70/150], Loss: 11129.2671\n",
            "Epoch [80/150], Loss: 5530.6737\n",
            "Epoch [90/150], Loss: 6559.7098\n",
            "Epoch [100/150], Loss: 10686.2601\n",
            "Epoch [110/150], Loss: 8669.5880\n",
            "Epoch [120/150], Loss: 8195.0243\n",
            "Epoch [130/150], Loss: 6416.0826\n",
            "Epoch [140/150], Loss: 9249.2300\n",
            "Epoch [150/150], Loss: 5081.5695\n",
            "Fold 5, RMSE: 44.62451171875\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 66.2285057067871\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 25179.4463\n",
            "Epoch [20/100], Loss: 18254.5381\n",
            "Epoch [30/100], Loss: 29646.9104\n",
            "Epoch [40/100], Loss: 14480.8354\n",
            "Epoch [50/100], Loss: 29112.0164\n",
            "Epoch [60/100], Loss: 16206.5178\n",
            "Epoch [70/100], Loss: 27728.6455\n",
            "Epoch [80/100], Loss: 18332.5024\n",
            "Epoch [90/100], Loss: 18980.6641\n",
            "Epoch [100/100], Loss: 20920.6547\n",
            "Fold 1, RMSE: 67.45854949951172\n",
            "Epoch [10/100], Loss: 15749.8989\n",
            "Epoch [20/100], Loss: 18414.8696\n",
            "Epoch [30/100], Loss: 15421.1536\n",
            "Epoch [40/100], Loss: 14462.6765\n",
            "Epoch [50/100], Loss: 11367.1680\n",
            "Epoch [60/100], Loss: 10910.7065\n",
            "Epoch [70/100], Loss: 5951.4973\n",
            "Epoch [80/100], Loss: 9679.6111\n",
            "Epoch [90/100], Loss: 9730.4453\n",
            "Epoch [100/100], Loss: 6665.4393\n",
            "Fold 2, RMSE: 73.47241973876953\n",
            "Epoch [10/100], Loss: 14667.7227\n",
            "Epoch [20/100], Loss: 14913.4390\n",
            "Epoch [30/100], Loss: 13377.4009\n",
            "Epoch [40/100], Loss: 15283.8713\n",
            "Epoch [50/100], Loss: 14075.6523\n",
            "Epoch [60/100], Loss: 11183.6885\n",
            "Epoch [70/100], Loss: 11945.6677\n",
            "Epoch [80/100], Loss: 13183.8765\n",
            "Epoch [90/100], Loss: 10706.0936\n",
            "Epoch [100/100], Loss: 14515.4856\n",
            "Fold 3, RMSE: 109.66057586669922\n",
            "Epoch [10/100], Loss: 17534.9413\n",
            "Epoch [20/100], Loss: 18538.8362\n",
            "Epoch [30/100], Loss: 20819.7500\n",
            "Epoch [40/100], Loss: 16889.5840\n",
            "Epoch [50/100], Loss: 20548.6946\n",
            "Epoch [60/100], Loss: 18904.0471\n",
            "Epoch [70/100], Loss: 17558.8826\n",
            "Epoch [80/100], Loss: 16845.0597\n",
            "Epoch [90/100], Loss: 21441.7573\n",
            "Epoch [100/100], Loss: 20221.2783\n",
            "Fold 4, RMSE: 54.42667007446289\n",
            "Epoch [10/100], Loss: 23289.1631\n",
            "Epoch [20/100], Loss: 17065.9460\n",
            "Epoch [30/100], Loss: 16494.8735\n",
            "Epoch [40/100], Loss: 14566.3938\n",
            "Epoch [50/100], Loss: 18403.4192\n",
            "Epoch [60/100], Loss: 10041.7097\n",
            "Epoch [70/100], Loss: 8159.2537\n",
            "Epoch [80/100], Loss: 12636.9067\n",
            "Epoch [90/100], Loss: 10615.6361\n",
            "Epoch [100/100], Loss: 19591.1123\n",
            "Fold 5, RMSE: 47.58012771606445\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 70.51966857910156\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 29529.7324\n",
            "Epoch [20/150], Loss: 21416.6450\n",
            "Epoch [30/150], Loss: 21734.3511\n",
            "Epoch [40/150], Loss: 22037.9609\n",
            "Epoch [50/150], Loss: 18752.0107\n",
            "Epoch [60/150], Loss: 16737.5359\n",
            "Epoch [70/150], Loss: 18978.8047\n",
            "Epoch [80/150], Loss: 17149.8779\n",
            "Epoch [90/150], Loss: 17217.3760\n",
            "Epoch [100/150], Loss: 29497.8777\n",
            "Epoch [110/150], Loss: 20078.9116\n",
            "Epoch [120/150], Loss: 15748.3825\n",
            "Epoch [130/150], Loss: 16299.6567\n",
            "Epoch [140/150], Loss: 26378.0303\n",
            "Epoch [150/150], Loss: 24496.8687\n",
            "Fold 1, RMSE: 67.34535217285156\n",
            "Epoch [10/150], Loss: 14684.6111\n",
            "Epoch [20/150], Loss: 12114.6377\n",
            "Epoch [30/150], Loss: 7911.8739\n",
            "Epoch [40/150], Loss: 7779.5240\n",
            "Epoch [50/150], Loss: 11519.4488\n",
            "Epoch [60/150], Loss: 12252.3044\n",
            "Epoch [70/150], Loss: 13147.6797\n",
            "Epoch [80/150], Loss: 9477.3916\n",
            "Epoch [90/150], Loss: 15656.2867\n",
            "Epoch [100/150], Loss: 8498.2432\n",
            "Epoch [110/150], Loss: 10796.1860\n",
            "Epoch [120/150], Loss: 10080.2301\n",
            "Epoch [130/150], Loss: 10293.6140\n",
            "Epoch [140/150], Loss: 12070.2793\n",
            "Epoch [150/150], Loss: 13455.8820\n",
            "Fold 2, RMSE: 80.33331298828125\n",
            "Epoch [10/150], Loss: 11558.0247\n",
            "Epoch [20/150], Loss: 13551.0454\n",
            "Epoch [30/150], Loss: 12333.2363\n",
            "Epoch [40/150], Loss: 16749.4060\n",
            "Epoch [50/150], Loss: 10752.3751\n",
            "Epoch [60/150], Loss: 17423.3711\n",
            "Epoch [70/150], Loss: 13436.7737\n",
            "Epoch [80/150], Loss: 11908.9481\n",
            "Epoch [90/150], Loss: 10414.5978\n",
            "Epoch [100/150], Loss: 10956.4580\n",
            "Epoch [110/150], Loss: 9125.4800\n",
            "Epoch [120/150], Loss: 9345.1293\n",
            "Epoch [130/150], Loss: 12821.5242\n",
            "Epoch [140/150], Loss: 18539.5911\n",
            "Epoch [150/150], Loss: 12344.2656\n",
            "Fold 3, RMSE: 109.4608154296875\n",
            "Epoch [10/150], Loss: 27366.2915\n",
            "Epoch [20/150], Loss: 21435.8926\n",
            "Epoch [30/150], Loss: 16858.9834\n",
            "Epoch [40/150], Loss: 20128.3647\n",
            "Epoch [50/150], Loss: 22816.8481\n",
            "Epoch [60/150], Loss: 22353.1099\n",
            "Epoch [70/150], Loss: 19727.0449\n",
            "Epoch [80/150], Loss: 21381.3508\n",
            "Epoch [90/150], Loss: 17804.9639\n",
            "Epoch [100/150], Loss: 21690.4482\n",
            "Epoch [110/150], Loss: 18025.6399\n",
            "Epoch [120/150], Loss: 17828.2825\n",
            "Epoch [130/150], Loss: 20874.4307\n",
            "Epoch [140/150], Loss: 18212.0308\n",
            "Epoch [150/150], Loss: 19738.2158\n",
            "Fold 4, RMSE: 54.23948287963867\n",
            "Epoch [10/150], Loss: 15584.6497\n",
            "Epoch [20/150], Loss: 17488.3037\n",
            "Epoch [30/150], Loss: 10920.2164\n",
            "Epoch [40/150], Loss: 18004.2310\n",
            "Epoch [50/150], Loss: 16488.4541\n",
            "Epoch [60/150], Loss: 12021.0413\n",
            "Epoch [70/150], Loss: 16991.4801\n",
            "Epoch [80/150], Loss: 19140.2349\n",
            "Epoch [90/150], Loss: 18437.8369\n",
            "Epoch [100/150], Loss: 17668.2581\n",
            "Epoch [110/150], Loss: 16314.3948\n",
            "Epoch [120/150], Loss: 24072.8638\n",
            "Epoch [130/150], Loss: 23585.9766\n",
            "Epoch [140/150], Loss: 20359.9438\n",
            "Epoch [150/150], Loss: 17185.4072\n",
            "Fold 5, RMSE: 57.62971878051758\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 73.8017364501953\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 23074.0537\n",
            "Epoch [20/100], Loss: 27511.1321\n",
            "Epoch [30/100], Loss: 19986.6191\n",
            "Epoch [40/100], Loss: 18416.3364\n",
            "Epoch [50/100], Loss: 19341.4355\n",
            "Epoch [60/100], Loss: 16553.8958\n",
            "Epoch [70/100], Loss: 23746.3330\n",
            "Epoch [80/100], Loss: 20946.3242\n",
            "Epoch [90/100], Loss: 20054.6235\n",
            "Epoch [100/100], Loss: 21080.3494\n",
            "Fold 1, RMSE: 67.04122924804688\n",
            "Epoch [10/100], Loss: 14825.3855\n",
            "Epoch [20/100], Loss: 19365.5645\n",
            "Epoch [30/100], Loss: 22485.0964\n",
            "Epoch [40/100], Loss: 18027.7839\n",
            "Epoch [50/100], Loss: 15489.0007\n",
            "Epoch [60/100], Loss: 15809.4136\n",
            "Epoch [70/100], Loss: 19234.8816\n",
            "Epoch [80/100], Loss: 18046.3271\n",
            "Epoch [90/100], Loss: 16279.7954\n",
            "Epoch [100/100], Loss: 25281.3820\n",
            "Fold 2, RMSE: 85.8426742553711\n",
            "Epoch [10/100], Loss: 11373.6616\n",
            "Epoch [20/100], Loss: 10268.8929\n",
            "Epoch [30/100], Loss: 5842.9838\n",
            "Epoch [40/100], Loss: 7532.0199\n",
            "Epoch [50/100], Loss: 10024.7936\n",
            "Epoch [60/100], Loss: 7231.1595\n",
            "Epoch [70/100], Loss: 7446.4308\n",
            "Epoch [80/100], Loss: 10406.8018\n",
            "Epoch [90/100], Loss: 6828.9376\n",
            "Epoch [100/100], Loss: 10382.6260\n",
            "Fold 3, RMSE: 104.90425872802734\n",
            "Epoch [10/100], Loss: 20386.2637\n",
            "Epoch [20/100], Loss: 17159.4048\n",
            "Epoch [30/100], Loss: 8831.3472\n",
            "Epoch [40/100], Loss: 14239.2388\n",
            "Epoch [50/100], Loss: 12010.7053\n",
            "Epoch [60/100], Loss: 17861.3716\n",
            "Epoch [70/100], Loss: 18564.3638\n",
            "Epoch [80/100], Loss: 13751.5403\n",
            "Epoch [90/100], Loss: 19867.3247\n",
            "Epoch [100/100], Loss: 14760.0557\n",
            "Fold 4, RMSE: 42.43377685546875\n",
            "Epoch [10/100], Loss: 16667.5537\n",
            "Epoch [20/100], Loss: 13990.8832\n",
            "Epoch [30/100], Loss: 16430.1431\n",
            "Epoch [40/100], Loss: 13214.6029\n",
            "Epoch [50/100], Loss: 14019.4402\n",
            "Epoch [60/100], Loss: 10925.1211\n",
            "Epoch [70/100], Loss: 11988.3517\n",
            "Epoch [80/100], Loss: 8655.8044\n",
            "Epoch [90/100], Loss: 13364.1941\n",
            "Epoch [100/100], Loss: 12483.6649\n",
            "Fold 5, RMSE: 50.228546142578125\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 70.09009704589843\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 19045.3167\n",
            "Epoch [20/150], Loss: 17653.6619\n",
            "Epoch [30/150], Loss: 12094.5371\n",
            "Epoch [40/150], Loss: 10140.7059\n",
            "Epoch [50/150], Loss: 7168.1273\n",
            "Epoch [60/150], Loss: 14153.5381\n",
            "Epoch [70/150], Loss: 15398.0867\n",
            "Epoch [80/150], Loss: 12113.9990\n",
            "Epoch [90/150], Loss: 9955.6193\n",
            "Epoch [100/150], Loss: 12752.7777\n",
            "Epoch [110/150], Loss: 10215.1770\n",
            "Epoch [120/150], Loss: 13872.0715\n",
            "Epoch [130/150], Loss: 13099.8811\n",
            "Epoch [140/150], Loss: 13812.2959\n",
            "Epoch [150/150], Loss: 9063.2952\n",
            "Fold 1, RMSE: 54.0623664855957\n",
            "Epoch [10/150], Loss: 13996.4868\n",
            "Epoch [20/150], Loss: 12418.0580\n",
            "Epoch [30/150], Loss: 12965.8059\n",
            "Epoch [40/150], Loss: 21832.5100\n",
            "Epoch [50/150], Loss: 7797.2439\n",
            "Epoch [60/150], Loss: 8578.0644\n",
            "Epoch [70/150], Loss: 8840.9805\n",
            "Epoch [80/150], Loss: 11248.0640\n",
            "Epoch [90/150], Loss: 10357.0938\n",
            "Epoch [100/150], Loss: 11392.8379\n",
            "Epoch [110/150], Loss: 14410.2228\n",
            "Epoch [120/150], Loss: 15256.4009\n",
            "Epoch [130/150], Loss: 15231.8105\n",
            "Epoch [140/150], Loss: 14864.7026\n",
            "Epoch [150/150], Loss: 9006.2690\n",
            "Fold 2, RMSE: 75.81468200683594\n",
            "Epoch [10/150], Loss: 12958.0081\n",
            "Epoch [20/150], Loss: 13262.3071\n",
            "Epoch [30/150], Loss: 9875.6230\n",
            "Epoch [40/150], Loss: 11228.2502\n",
            "Epoch [50/150], Loss: 9377.9268\n",
            "Epoch [60/150], Loss: 9548.2225\n",
            "Epoch [70/150], Loss: 15737.4324\n",
            "Epoch [80/150], Loss: 11308.4392\n",
            "Epoch [90/150], Loss: 11793.7339\n",
            "Epoch [100/150], Loss: 12942.8521\n",
            "Epoch [110/150], Loss: 11787.9939\n",
            "Epoch [120/150], Loss: 16162.0553\n",
            "Epoch [130/150], Loss: 15643.9111\n",
            "Epoch [140/150], Loss: 12280.2837\n",
            "Epoch [150/150], Loss: 11734.6013\n",
            "Fold 3, RMSE: 108.86311340332031\n",
            "Epoch [10/150], Loss: 13392.4619\n",
            "Epoch [20/150], Loss: 8887.4726\n",
            "Epoch [30/150], Loss: 13131.0479\n",
            "Epoch [40/150], Loss: 24815.7856\n",
            "Epoch [50/150], Loss: 24397.6060\n",
            "Epoch [60/150], Loss: 20768.1553\n",
            "Epoch [70/150], Loss: 29009.0552\n",
            "Epoch [80/150], Loss: 17567.6868\n",
            "Epoch [90/150], Loss: 16589.3899\n",
            "Epoch [100/150], Loss: 18452.6406\n",
            "Epoch [110/150], Loss: 27513.8530\n",
            "Epoch [120/150], Loss: 22437.5127\n",
            "Epoch [130/150], Loss: 26322.2056\n",
            "Epoch [140/150], Loss: 16211.1635\n",
            "Epoch [150/150], Loss: 19964.7236\n",
            "Fold 4, RMSE: 52.73291015625\n",
            "Epoch [10/150], Loss: 14192.7991\n",
            "Epoch [20/150], Loss: 22993.0068\n",
            "Epoch [30/150], Loss: 24453.9268\n",
            "Epoch [40/150], Loss: 24389.7395\n",
            "Epoch [50/150], Loss: 11625.7157\n",
            "Epoch [60/150], Loss: 15566.0742\n",
            "Epoch [70/150], Loss: 11325.5607\n",
            "Epoch [80/150], Loss: 12676.3683\n",
            "Epoch [90/150], Loss: 9738.5415\n",
            "Epoch [100/150], Loss: 12394.5840\n",
            "Epoch [110/150], Loss: 6329.9540\n",
            "Epoch [120/150], Loss: 17531.9805\n",
            "Epoch [130/150], Loss: 9484.5968\n",
            "Epoch [140/150], Loss: 11119.0066\n",
            "Epoch [150/150], Loss: 12010.0391\n",
            "Fold 5, RMSE: 44.10861587524414\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 67.11633758544922\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 20007.3696\n",
            "Epoch [20/100], Loss: 17926.0332\n",
            "Epoch [30/100], Loss: 14687.1284\n",
            "Epoch [40/100], Loss: 9179.6317\n",
            "Epoch [50/100], Loss: 7087.4103\n",
            "Epoch [60/100], Loss: 8199.6528\n",
            "Epoch [70/100], Loss: 21781.7939\n",
            "Epoch [80/100], Loss: 11984.3682\n",
            "Epoch [90/100], Loss: 11116.6035\n",
            "Epoch [100/100], Loss: 12609.7576\n",
            "Fold 1, RMSE: 67.57344055175781\n",
            "Epoch [10/100], Loss: 15510.7107\n",
            "Epoch [20/100], Loss: 14997.2659\n",
            "Epoch [30/100], Loss: 16870.5361\n",
            "Epoch [40/100], Loss: 17615.7368\n",
            "Epoch [50/100], Loss: 19621.1489\n",
            "Epoch [60/100], Loss: 18364.8232\n",
            "Epoch [70/100], Loss: 14953.6562\n",
            "Epoch [80/100], Loss: 18555.9849\n",
            "Epoch [90/100], Loss: 14998.8889\n",
            "Epoch [100/100], Loss: 15183.7988\n",
            "Fold 2, RMSE: 87.37482452392578\n",
            "Epoch [10/100], Loss: 10425.3342\n",
            "Epoch [20/100], Loss: 11124.3191\n",
            "Epoch [30/100], Loss: 12925.8875\n",
            "Epoch [40/100], Loss: 12975.0308\n",
            "Epoch [50/100], Loss: 9781.8547\n",
            "Epoch [60/100], Loss: 3366.3730\n",
            "Epoch [70/100], Loss: 8033.1685\n",
            "Epoch [80/100], Loss: 12485.2844\n",
            "Epoch [90/100], Loss: 5361.6315\n",
            "Epoch [100/100], Loss: 8920.9069\n",
            "Fold 3, RMSE: 99.5385971069336\n",
            "Epoch [10/100], Loss: 17949.9978\n",
            "Epoch [20/100], Loss: 24789.2397\n",
            "Epoch [30/100], Loss: 19930.1675\n",
            "Epoch [40/100], Loss: 21632.9685\n",
            "Epoch [50/100], Loss: 17101.0986\n",
            "Epoch [60/100], Loss: 23813.4509\n",
            "Epoch [70/100], Loss: 26830.8462\n",
            "Epoch [80/100], Loss: 18768.9204\n",
            "Epoch [90/100], Loss: 21062.1248\n",
            "Epoch [100/100], Loss: 19533.3125\n",
            "Fold 4, RMSE: 54.42015838623047\n",
            "Epoch [10/100], Loss: 30623.2295\n",
            "Epoch [20/100], Loss: 22517.0637\n",
            "Epoch [30/100], Loss: 17738.1479\n",
            "Epoch [40/100], Loss: 18900.7710\n",
            "Epoch [50/100], Loss: 17238.3507\n",
            "Epoch [60/100], Loss: 19466.3716\n",
            "Epoch [70/100], Loss: 20541.5908\n",
            "Epoch [80/100], Loss: 19772.3486\n",
            "Epoch [90/100], Loss: 17415.9965\n",
            "Epoch [100/100], Loss: 20736.9604\n",
            "Fold 5, RMSE: 57.937435150146484\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 73.36889114379883\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19860.7617\n",
            "Epoch [20/150], Loss: 18433.9204\n",
            "Epoch [30/150], Loss: 16872.8262\n",
            "Epoch [40/150], Loss: 19220.3779\n",
            "Epoch [50/150], Loss: 18044.7402\n",
            "Epoch [60/150], Loss: 21299.1021\n",
            "Epoch [70/150], Loss: 17481.5024\n",
            "Epoch [80/150], Loss: 19116.8589\n",
            "Epoch [90/150], Loss: 18902.5032\n",
            "Epoch [100/150], Loss: 16165.6624\n",
            "Epoch [110/150], Loss: 17417.0344\n",
            "Epoch [120/150], Loss: 16603.8245\n",
            "Epoch [130/150], Loss: 17921.0269\n",
            "Epoch [140/150], Loss: 17744.6025\n",
            "Epoch [150/150], Loss: 16984.1672\n",
            "Fold 1, RMSE: 67.39019775390625\n",
            "Epoch [10/150], Loss: 19427.4795\n",
            "Epoch [20/150], Loss: 16370.4263\n",
            "Epoch [30/150], Loss: 18825.4746\n",
            "Epoch [40/150], Loss: 16779.9443\n",
            "Epoch [50/150], Loss: 18934.1018\n",
            "Epoch [60/150], Loss: 17043.9739\n",
            "Epoch [70/150], Loss: 15392.1968\n",
            "Epoch [80/150], Loss: 18219.9543\n",
            "Epoch [90/150], Loss: 15084.5483\n",
            "Epoch [100/150], Loss: 16108.5027\n",
            "Epoch [110/150], Loss: 15698.3181\n",
            "Epoch [120/150], Loss: 15035.2749\n",
            "Epoch [130/150], Loss: 24085.0457\n",
            "Epoch [140/150], Loss: 20595.3303\n",
            "Epoch [150/150], Loss: 16090.4417\n",
            "Fold 2, RMSE: 86.68241882324219\n",
            "Epoch [10/150], Loss: 20076.9556\n",
            "Epoch [20/150], Loss: 21394.7996\n",
            "Epoch [30/150], Loss: 14848.8662\n",
            "Epoch [40/150], Loss: 14963.1753\n",
            "Epoch [50/150], Loss: 12851.7266\n",
            "Epoch [60/150], Loss: 12377.9788\n",
            "Epoch [70/150], Loss: 12649.2954\n",
            "Epoch [80/150], Loss: 15149.2866\n",
            "Epoch [90/150], Loss: 13262.1399\n",
            "Epoch [100/150], Loss: 10852.3202\n",
            "Epoch [110/150], Loss: 12109.1826\n",
            "Epoch [120/150], Loss: 11146.0020\n",
            "Epoch [130/150], Loss: 14536.4575\n",
            "Epoch [140/150], Loss: 11664.4275\n",
            "Epoch [150/150], Loss: 8638.7347\n",
            "Fold 3, RMSE: 109.21134185791016\n",
            "Epoch [10/150], Loss: 17135.9061\n",
            "Epoch [20/150], Loss: 17219.3705\n",
            "Epoch [30/150], Loss: 19573.2466\n",
            "Epoch [40/150], Loss: 24090.1416\n",
            "Epoch [50/150], Loss: 20998.1589\n",
            "Epoch [60/150], Loss: 19057.9338\n",
            "Epoch [70/150], Loss: 16633.4649\n",
            "Epoch [80/150], Loss: 19334.2334\n",
            "Epoch [90/150], Loss: 19810.1670\n",
            "Epoch [100/150], Loss: 18575.6917\n",
            "Epoch [110/150], Loss: 30706.1108\n",
            "Epoch [120/150], Loss: 20061.2329\n",
            "Epoch [130/150], Loss: 21561.3994\n",
            "Epoch [140/150], Loss: 18424.2212\n",
            "Epoch [150/150], Loss: 18679.5825\n",
            "Fold 4, RMSE: 54.186729431152344\n",
            "Epoch [10/150], Loss: 19151.9478\n",
            "Epoch [20/150], Loss: 18607.5530\n",
            "Epoch [30/150], Loss: 17181.7356\n",
            "Epoch [40/150], Loss: 21495.1562\n",
            "Epoch [50/150], Loss: 23390.5771\n",
            "Epoch [60/150], Loss: 17058.7465\n",
            "Epoch [70/150], Loss: 20573.6172\n",
            "Epoch [80/150], Loss: 19412.5889\n",
            "Epoch [90/150], Loss: 19309.1699\n",
            "Epoch [100/150], Loss: 19403.8765\n",
            "Epoch [110/150], Loss: 18569.3447\n",
            "Epoch [120/150], Loss: 19119.1948\n",
            "Epoch [130/150], Loss: 22015.3203\n",
            "Epoch [140/150], Loss: 19185.1343\n",
            "Epoch [150/150], Loss: 22346.9258\n",
            "Fold 5, RMSE: 57.69153594970703\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 75.03244476318359\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18408.0234\n",
            "Epoch [20/100], Loss: 12926.1584\n",
            "Epoch [30/100], Loss: 10610.8391\n",
            "Epoch [40/100], Loss: 20526.7544\n",
            "Epoch [50/100], Loss: 14806.4609\n",
            "Epoch [60/100], Loss: 12073.8341\n",
            "Epoch [70/100], Loss: 15118.6350\n",
            "Epoch [80/100], Loss: 25608.9056\n",
            "Epoch [90/100], Loss: 13078.3848\n",
            "Epoch [100/100], Loss: 13136.4550\n",
            "Fold 1, RMSE: 61.82175827026367\n",
            "Epoch [10/100], Loss: 13542.0217\n",
            "Epoch [20/100], Loss: 29607.5198\n",
            "Epoch [30/100], Loss: 16578.7314\n",
            "Epoch [40/100], Loss: 15798.2788\n",
            "Epoch [50/100], Loss: 20010.2910\n",
            "Epoch [60/100], Loss: 7399.2298\n",
            "Epoch [70/100], Loss: 12336.6506\n",
            "Epoch [80/100], Loss: 6538.4850\n",
            "Epoch [90/100], Loss: 9921.1017\n",
            "Epoch [100/100], Loss: 13642.9351\n",
            "Fold 2, RMSE: 85.30059051513672\n",
            "Epoch [10/100], Loss: 12922.8398\n",
            "Epoch [20/100], Loss: 14958.6460\n",
            "Epoch [30/100], Loss: 10688.6306\n",
            "Epoch [40/100], Loss: 9582.4446\n",
            "Epoch [50/100], Loss: 5096.5728\n",
            "Epoch [60/100], Loss: 8141.9644\n",
            "Epoch [70/100], Loss: 9490.0667\n",
            "Epoch [80/100], Loss: 9725.7607\n",
            "Epoch [90/100], Loss: 11360.9007\n",
            "Epoch [100/100], Loss: 6224.8944\n",
            "Fold 3, RMSE: 96.9241943359375\n",
            "Epoch [10/100], Loss: 21403.2173\n",
            "Epoch [20/100], Loss: 23889.0916\n",
            "Epoch [30/100], Loss: 11060.3222\n",
            "Epoch [40/100], Loss: 21409.3184\n",
            "Epoch [50/100], Loss: 15315.6147\n",
            "Epoch [60/100], Loss: 14036.1626\n",
            "Epoch [70/100], Loss: 8244.5481\n",
            "Epoch [80/100], Loss: 14338.1113\n",
            "Epoch [90/100], Loss: 12720.5081\n",
            "Epoch [100/100], Loss: 7540.3982\n",
            "Fold 4, RMSE: 44.71931457519531\n",
            "Epoch [10/100], Loss: 23123.9622\n",
            "Epoch [20/100], Loss: 19837.6294\n",
            "Epoch [30/100], Loss: 13995.4680\n",
            "Epoch [40/100], Loss: 12419.2942\n",
            "Epoch [50/100], Loss: 17785.0122\n",
            "Epoch [60/100], Loss: 20928.2539\n",
            "Epoch [70/100], Loss: 18386.1025\n",
            "Epoch [80/100], Loss: 21326.5063\n",
            "Epoch [90/100], Loss: 26539.8884\n",
            "Epoch [100/100], Loss: 24374.4189\n",
            "Fold 5, RMSE: 57.41627883911133\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 69.2364273071289\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 13598.1475\n",
            "Epoch [20/150], Loss: 16457.2480\n",
            "Epoch [30/150], Loss: 12780.3369\n",
            "Epoch [40/150], Loss: 17666.1782\n",
            "Epoch [50/150], Loss: 19811.6709\n",
            "Epoch [60/150], Loss: 8833.7959\n",
            "Epoch [70/150], Loss: 11001.1138\n",
            "Epoch [80/150], Loss: 13015.8693\n",
            "Epoch [90/150], Loss: 13161.0425\n",
            "Epoch [100/150], Loss: 9916.6638\n",
            "Epoch [110/150], Loss: 20886.5664\n",
            "Epoch [120/150], Loss: 10510.9106\n",
            "Epoch [130/150], Loss: 14037.1768\n",
            "Epoch [140/150], Loss: 14183.4409\n",
            "Epoch [150/150], Loss: 14137.5931\n",
            "Fold 1, RMSE: 65.17818450927734\n",
            "Epoch [10/150], Loss: 15156.3992\n",
            "Epoch [20/150], Loss: 15858.7988\n",
            "Epoch [30/150], Loss: 12206.4767\n",
            "Epoch [40/150], Loss: 13917.5894\n",
            "Epoch [50/150], Loss: 9057.3261\n",
            "Epoch [60/150], Loss: 9643.6169\n",
            "Epoch [70/150], Loss: 12042.0901\n",
            "Epoch [80/150], Loss: 15973.9084\n",
            "Epoch [90/150], Loss: 17249.0554\n",
            "Epoch [100/150], Loss: 8642.7803\n",
            "Epoch [110/150], Loss: 7943.3101\n",
            "Epoch [120/150], Loss: 4522.0203\n",
            "Epoch [130/150], Loss: 10891.0865\n",
            "Epoch [140/150], Loss: 7111.0154\n",
            "Epoch [150/150], Loss: 8199.0659\n",
            "Fold 2, RMSE: 73.28915405273438\n",
            "Epoch [10/150], Loss: 12921.1005\n",
            "Epoch [20/150], Loss: 8919.7280\n",
            "Epoch [30/150], Loss: 8785.7148\n",
            "Epoch [40/150], Loss: 11510.8315\n",
            "Epoch [50/150], Loss: 7159.9991\n",
            "Epoch [60/150], Loss: 9099.6664\n",
            "Epoch [70/150], Loss: 8414.9553\n",
            "Epoch [80/150], Loss: 11342.6228\n",
            "Epoch [90/150], Loss: 16788.3210\n",
            "Epoch [100/150], Loss: 8854.1930\n",
            "Epoch [110/150], Loss: 11587.9512\n",
            "Epoch [120/150], Loss: 5157.2979\n",
            "Epoch [130/150], Loss: 10631.5469\n",
            "Epoch [140/150], Loss: 9718.8660\n",
            "Epoch [150/150], Loss: 5038.8225\n",
            "Fold 3, RMSE: 104.09186553955078\n",
            "Epoch [10/150], Loss: 20097.8525\n",
            "Epoch [20/150], Loss: 15722.1978\n",
            "Epoch [30/150], Loss: 12793.6653\n",
            "Epoch [40/150], Loss: 9908.9207\n",
            "Epoch [50/150], Loss: 14349.2867\n",
            "Epoch [60/150], Loss: 12955.9924\n",
            "Epoch [70/150], Loss: 5190.0845\n",
            "Epoch [80/150], Loss: 11184.4662\n",
            "Epoch [90/150], Loss: 10380.5286\n",
            "Epoch [100/150], Loss: 10390.3303\n",
            "Epoch [110/150], Loss: 15390.0315\n",
            "Epoch [120/150], Loss: 20671.3459\n",
            "Epoch [130/150], Loss: 17096.5752\n",
            "Epoch [140/150], Loss: 11311.8386\n",
            "Epoch [150/150], Loss: 13170.8718\n",
            "Fold 4, RMSE: 41.810028076171875\n",
            "Epoch [10/150], Loss: 15011.2534\n",
            "Epoch [20/150], Loss: 13857.6066\n",
            "Epoch [30/150], Loss: 12960.5652\n",
            "Epoch [40/150], Loss: 12452.1533\n",
            "Epoch [50/150], Loss: 14961.4648\n",
            "Epoch [60/150], Loss: 14930.8042\n",
            "Epoch [70/150], Loss: 24725.9668\n",
            "Epoch [80/150], Loss: 20773.9033\n",
            "Epoch [90/150], Loss: 7597.9642\n",
            "Epoch [100/150], Loss: 6500.5314\n",
            "Epoch [110/150], Loss: 10453.9346\n",
            "Epoch [120/150], Loss: 12514.6294\n",
            "Epoch [130/150], Loss: 8772.2571\n",
            "Epoch [140/150], Loss: 10713.9989\n",
            "Epoch [150/150], Loss: 10916.6057\n",
            "Fold 5, RMSE: 52.6455078125\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 67.40294799804687\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 17955.7456\n",
            "Epoch [20/100], Loss: 15988.9941\n",
            "Epoch [30/100], Loss: 17237.4282\n",
            "Epoch [40/100], Loss: 22108.2905\n",
            "Epoch [50/100], Loss: 22275.3816\n",
            "Epoch [60/100], Loss: 17543.2136\n",
            "Epoch [70/100], Loss: 28063.2913\n",
            "Epoch [80/100], Loss: 25129.3167\n",
            "Epoch [90/100], Loss: 17020.8354\n",
            "Epoch [100/100], Loss: 16429.9475\n",
            "Fold 1, RMSE: 67.47913360595703\n",
            "Epoch [10/100], Loss: 13783.5669\n",
            "Epoch [20/100], Loss: 13925.5217\n",
            "Epoch [30/100], Loss: 16681.3179\n",
            "Epoch [40/100], Loss: 14109.9585\n",
            "Epoch [50/100], Loss: 16540.7302\n",
            "Epoch [60/100], Loss: 9686.9403\n",
            "Epoch [70/100], Loss: 6393.5754\n",
            "Epoch [80/100], Loss: 6910.6165\n",
            "Epoch [90/100], Loss: 11789.9558\n",
            "Epoch [100/100], Loss: 10094.4497\n",
            "Fold 2, RMSE: 75.30152893066406\n",
            "Epoch [10/100], Loss: 14577.2310\n",
            "Epoch [20/100], Loss: 17016.0420\n",
            "Epoch [30/100], Loss: 12309.7844\n",
            "Epoch [40/100], Loss: 17037.8394\n",
            "Epoch [50/100], Loss: 13880.8525\n",
            "Epoch [60/100], Loss: 15434.9155\n",
            "Epoch [70/100], Loss: 12423.7439\n",
            "Epoch [80/100], Loss: 15192.0103\n",
            "Epoch [90/100], Loss: 17832.0310\n",
            "Epoch [100/100], Loss: 12223.8210\n",
            "Fold 3, RMSE: 109.72561645507812\n",
            "Epoch [10/100], Loss: 20716.3291\n",
            "Epoch [20/100], Loss: 25727.4297\n",
            "Epoch [30/100], Loss: 27296.4082\n",
            "Epoch [40/100], Loss: 17886.4497\n",
            "Epoch [50/100], Loss: 15418.7017\n",
            "Epoch [60/100], Loss: 17968.0884\n",
            "Epoch [70/100], Loss: 12425.3372\n",
            "Epoch [80/100], Loss: 14849.4019\n",
            "Epoch [90/100], Loss: 9105.3203\n",
            "Epoch [100/100], Loss: 13991.7820\n",
            "Fold 4, RMSE: 47.7505989074707\n",
            "Epoch [10/100], Loss: 22882.7612\n",
            "Epoch [20/100], Loss: 20253.4744\n",
            "Epoch [30/100], Loss: 19189.0269\n",
            "Epoch [40/100], Loss: 12144.9223\n",
            "Epoch [50/100], Loss: 13496.7788\n",
            "Epoch [60/100], Loss: 8971.0889\n",
            "Epoch [70/100], Loss: 14427.7935\n",
            "Epoch [80/100], Loss: 13910.8833\n",
            "Epoch [90/100], Loss: 11814.6860\n",
            "Epoch [100/100], Loss: 10357.8359\n",
            "Fold 5, RMSE: 48.47056198120117\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 69.74548797607422\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 23564.6306\n",
            "Epoch [20/150], Loss: 15295.1768\n",
            "Epoch [30/150], Loss: 19878.1631\n",
            "Epoch [40/150], Loss: 24947.4336\n",
            "Epoch [50/150], Loss: 17242.5610\n",
            "Epoch [60/150], Loss: 16901.2314\n",
            "Epoch [70/150], Loss: 14914.3762\n",
            "Epoch [80/150], Loss: 14170.0662\n",
            "Epoch [90/150], Loss: 14772.3920\n",
            "Epoch [100/150], Loss: 21847.6858\n",
            "Epoch [110/150], Loss: 12268.9425\n",
            "Epoch [120/150], Loss: 11833.8317\n",
            "Epoch [130/150], Loss: 14487.4646\n",
            "Epoch [140/150], Loss: 18915.6165\n",
            "Epoch [150/150], Loss: 11653.4409\n",
            "Fold 1, RMSE: 58.2924919128418\n",
            "Epoch [10/150], Loss: 15331.6187\n",
            "Epoch [20/150], Loss: 15494.1875\n",
            "Epoch [30/150], Loss: 14504.5319\n",
            "Epoch [40/150], Loss: 22452.3760\n",
            "Epoch [50/150], Loss: 14258.6726\n",
            "Epoch [60/150], Loss: 14228.2322\n",
            "Epoch [70/150], Loss: 16115.1716\n",
            "Epoch [80/150], Loss: 18847.8091\n",
            "Epoch [90/150], Loss: 15309.5220\n",
            "Epoch [100/150], Loss: 16903.2678\n",
            "Epoch [110/150], Loss: 16643.7915\n",
            "Epoch [120/150], Loss: 25285.5955\n",
            "Epoch [130/150], Loss: 16506.2261\n",
            "Epoch [140/150], Loss: 28337.7407\n",
            "Epoch [150/150], Loss: 15808.6475\n",
            "Fold 2, RMSE: 87.16436767578125\n",
            "Epoch [10/150], Loss: 12734.6511\n",
            "Epoch [20/150], Loss: 13672.8022\n",
            "Epoch [30/150], Loss: 15376.9136\n",
            "Epoch [40/150], Loss: 11633.8259\n",
            "Epoch [50/150], Loss: 13466.5891\n",
            "Epoch [60/150], Loss: 12792.7795\n",
            "Epoch [70/150], Loss: 14936.0723\n",
            "Epoch [80/150], Loss: 13039.7979\n",
            "Epoch [90/150], Loss: 12397.1912\n",
            "Epoch [100/150], Loss: 15541.0547\n",
            "Epoch [110/150], Loss: 13780.8145\n",
            "Epoch [120/150], Loss: 15601.1992\n",
            "Epoch [130/150], Loss: 17423.1914\n",
            "Epoch [140/150], Loss: 11035.0375\n",
            "Epoch [150/150], Loss: 14176.0288\n",
            "Fold 3, RMSE: 109.4146499633789\n",
            "Epoch [10/150], Loss: 18219.5479\n",
            "Epoch [20/150], Loss: 17965.5190\n",
            "Epoch [30/150], Loss: 15558.7676\n",
            "Epoch [40/150], Loss: 8506.0817\n",
            "Epoch [50/150], Loss: 20535.3333\n",
            "Epoch [60/150], Loss: 15917.3655\n",
            "Epoch [70/150], Loss: 18500.9976\n",
            "Epoch [80/150], Loss: 19045.6924\n",
            "Epoch [90/150], Loss: 21664.0322\n",
            "Epoch [100/150], Loss: 17835.3496\n",
            "Epoch [110/150], Loss: 16510.6611\n",
            "Epoch [120/150], Loss: 16231.8123\n",
            "Epoch [130/150], Loss: 19251.8677\n",
            "Epoch [140/150], Loss: 17648.6965\n",
            "Epoch [150/150], Loss: 13777.9031\n",
            "Fold 4, RMSE: 50.631534576416016\n",
            "Epoch [10/150], Loss: 19017.8417\n",
            "Epoch [20/150], Loss: 17764.6898\n",
            "Epoch [30/150], Loss: 16867.8345\n",
            "Epoch [40/150], Loss: 14336.6021\n",
            "Epoch [50/150], Loss: 16450.6243\n",
            "Epoch [60/150], Loss: 12408.7205\n",
            "Epoch [70/150], Loss: 11939.4595\n",
            "Epoch [80/150], Loss: 13669.7268\n",
            "Epoch [90/150], Loss: 11883.9500\n",
            "Epoch [100/150], Loss: 14746.7598\n",
            "Epoch [110/150], Loss: 9634.8427\n",
            "Epoch [120/150], Loss: 7189.3181\n",
            "Epoch [130/150], Loss: 20488.7292\n",
            "Epoch [140/150], Loss: 14524.0522\n",
            "Epoch [150/150], Loss: 14332.5649\n",
            "Fold 5, RMSE: 48.5019645690918\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 70.80100173950196\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 19754.0063\n",
            "Epoch [20/100], Loss: 18587.2456\n",
            "Epoch [30/100], Loss: 18005.6689\n",
            "Epoch [40/100], Loss: 16385.4077\n",
            "Epoch [50/100], Loss: 13040.7498\n",
            "Epoch [60/100], Loss: 17579.9536\n",
            "Epoch [70/100], Loss: 12319.2830\n",
            "Epoch [80/100], Loss: 13176.0979\n",
            "Epoch [90/100], Loss: 9142.5464\n",
            "Epoch [100/100], Loss: 10662.6643\n",
            "Fold 1, RMSE: 56.073368072509766\n",
            "Epoch [10/100], Loss: 16782.1494\n",
            "Epoch [20/100], Loss: 16740.9048\n",
            "Epoch [30/100], Loss: 14690.3660\n",
            "Epoch [40/100], Loss: 15360.4834\n",
            "Epoch [50/100], Loss: 18490.9885\n",
            "Epoch [60/100], Loss: 14740.4067\n",
            "Epoch [70/100], Loss: 17073.2461\n",
            "Epoch [80/100], Loss: 15484.9053\n",
            "Epoch [90/100], Loss: 14354.8733\n",
            "Epoch [100/100], Loss: 16120.6614\n",
            "Fold 2, RMSE: 86.4397964477539\n",
            "Epoch [10/100], Loss: 10870.4336\n",
            "Epoch [20/100], Loss: 10820.1384\n",
            "Epoch [30/100], Loss: 9402.5372\n",
            "Epoch [40/100], Loss: 10966.5000\n",
            "Epoch [50/100], Loss: 9200.4697\n",
            "Epoch [60/100], Loss: 11780.4360\n",
            "Epoch [70/100], Loss: 8292.0620\n",
            "Epoch [80/100], Loss: 4929.7812\n",
            "Epoch [90/100], Loss: 3460.7466\n",
            "Epoch [100/100], Loss: 6179.7018\n",
            "Fold 3, RMSE: 95.30856323242188\n",
            "Epoch [10/100], Loss: 17001.0911\n",
            "Epoch [20/100], Loss: 18448.2600\n",
            "Epoch [30/100], Loss: 18235.1870\n",
            "Epoch [40/100], Loss: 22094.4509\n",
            "Epoch [50/100], Loss: 12175.9070\n",
            "Epoch [60/100], Loss: 16318.1187\n",
            "Epoch [70/100], Loss: 11604.2195\n",
            "Epoch [80/100], Loss: 11622.0179\n",
            "Epoch [90/100], Loss: 15818.7919\n",
            "Epoch [100/100], Loss: 24528.4500\n",
            "Fold 4, RMSE: 54.00284194946289\n",
            "Epoch [10/100], Loss: 17560.6768\n",
            "Epoch [20/100], Loss: 25116.6697\n",
            "Epoch [30/100], Loss: 11198.1580\n",
            "Epoch [40/100], Loss: 13738.8076\n",
            "Epoch [50/100], Loss: 18375.3481\n",
            "Epoch [60/100], Loss: 21859.6265\n",
            "Epoch [70/100], Loss: 18766.0210\n",
            "Epoch [80/100], Loss: 23683.6035\n",
            "Epoch [90/100], Loss: 18173.6719\n",
            "Epoch [100/100], Loss: 19950.5654\n",
            "Fold 5, RMSE: 57.444332122802734\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 69.85378036499023\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14775.0337\n",
            "Epoch [20/150], Loss: 15788.5085\n",
            "Epoch [30/150], Loss: 16499.0974\n",
            "Epoch [40/150], Loss: 17515.1113\n",
            "Epoch [50/150], Loss: 14772.0433\n",
            "Epoch [60/150], Loss: 21013.0659\n",
            "Epoch [70/150], Loss: 14687.3538\n",
            "Epoch [80/150], Loss: 15867.2258\n",
            "Epoch [90/150], Loss: 17585.4531\n",
            "Epoch [100/150], Loss: 16448.8271\n",
            "Epoch [110/150], Loss: 17755.0522\n",
            "Epoch [120/150], Loss: 17137.5146\n",
            "Epoch [130/150], Loss: 17416.5796\n",
            "Epoch [140/150], Loss: 25994.0439\n",
            "Epoch [150/150], Loss: 21983.8335\n",
            "Fold 1, RMSE: 66.73274993896484\n",
            "Epoch [10/150], Loss: 14606.8163\n",
            "Epoch [20/150], Loss: 21898.9460\n",
            "Epoch [30/150], Loss: 22734.0798\n",
            "Epoch [40/150], Loss: 15116.7576\n",
            "Epoch [50/150], Loss: 17262.1221\n",
            "Epoch [60/150], Loss: 10433.1936\n",
            "Epoch [70/150], Loss: 11387.3953\n",
            "Epoch [80/150], Loss: 19547.1624\n",
            "Epoch [90/150], Loss: 14082.0417\n",
            "Epoch [100/150], Loss: 18091.3577\n",
            "Epoch [110/150], Loss: 23158.6953\n",
            "Epoch [120/150], Loss: 16377.4722\n",
            "Epoch [130/150], Loss: 14527.1550\n",
            "Epoch [140/150], Loss: 21520.5488\n",
            "Epoch [150/150], Loss: 14046.1770\n",
            "Fold 2, RMSE: 85.87158966064453\n",
            "Epoch [10/150], Loss: 12818.2434\n",
            "Epoch [20/150], Loss: 15210.4685\n",
            "Epoch [30/150], Loss: 8051.0684\n",
            "Epoch [40/150], Loss: 9100.1504\n",
            "Epoch [50/150], Loss: 9277.1660\n",
            "Epoch [60/150], Loss: 8924.9614\n",
            "Epoch [70/150], Loss: 6884.1701\n",
            "Epoch [80/150], Loss: 7796.3256\n",
            "Epoch [90/150], Loss: 6922.6433\n",
            "Epoch [100/150], Loss: 10803.4886\n",
            "Epoch [110/150], Loss: 7885.2946\n",
            "Epoch [120/150], Loss: 6552.3293\n",
            "Epoch [130/150], Loss: 10352.7469\n",
            "Epoch [140/150], Loss: 7381.1512\n",
            "Epoch [150/150], Loss: 6594.3470\n",
            "Fold 3, RMSE: 102.02095794677734\n",
            "Epoch [10/150], Loss: 20197.9006\n",
            "Epoch [20/150], Loss: 15474.0103\n",
            "Epoch [30/150], Loss: 14895.5620\n",
            "Epoch [40/150], Loss: 6948.5648\n",
            "Epoch [50/150], Loss: 8954.2830\n",
            "Epoch [60/150], Loss: 9264.9646\n",
            "Epoch [70/150], Loss: 12034.9009\n",
            "Epoch [80/150], Loss: 9330.1862\n",
            "Epoch [90/150], Loss: 10745.0410\n",
            "Epoch [100/150], Loss: 12516.9236\n",
            "Epoch [110/150], Loss: 11347.6466\n",
            "Epoch [120/150], Loss: 5392.8258\n",
            "Epoch [130/150], Loss: 10771.6628\n",
            "Epoch [140/150], Loss: 5605.0551\n",
            "Epoch [150/150], Loss: 9891.8660\n",
            "Fold 4, RMSE: 45.39933395385742\n",
            "Epoch [10/150], Loss: 18344.4326\n",
            "Epoch [20/150], Loss: 20981.6633\n",
            "Epoch [30/150], Loss: 10552.3773\n",
            "Epoch [40/150], Loss: 7287.8904\n",
            "Epoch [50/150], Loss: 6754.8320\n",
            "Epoch [60/150], Loss: 6513.8525\n",
            "Epoch [70/150], Loss: 12251.2498\n",
            "Epoch [80/150], Loss: 15475.6479\n",
            "Epoch [90/150], Loss: 13013.9373\n",
            "Epoch [100/150], Loss: 13433.5383\n",
            "Epoch [110/150], Loss: 12920.9891\n",
            "Epoch [120/150], Loss: 17202.3707\n",
            "Epoch [130/150], Loss: 15746.4197\n",
            "Epoch [140/150], Loss: 13734.3281\n",
            "Epoch [150/150], Loss: 10445.1140\n",
            "Fold 5, RMSE: 49.35003662109375\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 69.87493362426758\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 237000.8242\n",
            "Epoch [20/100], Loss: 30184.5952\n",
            "Epoch [30/100], Loss: 24335.4185\n",
            "Epoch [40/100], Loss: 21131.1082\n",
            "Epoch [50/100], Loss: 15665.2615\n",
            "Epoch [60/100], Loss: 19172.7886\n",
            "Epoch [70/100], Loss: 19509.1736\n",
            "Epoch [80/100], Loss: 16707.8025\n",
            "Epoch [90/100], Loss: 16836.5801\n",
            "Epoch [100/100], Loss: 21053.2720\n",
            "Fold 1, RMSE: 55.09857940673828\n",
            "Epoch [10/100], Loss: 42895.1973\n",
            "Epoch [20/100], Loss: 16913.0107\n",
            "Epoch [30/100], Loss: 24044.1736\n",
            "Epoch [40/100], Loss: 18535.9595\n",
            "Epoch [50/100], Loss: 19589.5935\n",
            "Epoch [60/100], Loss: 25119.7124\n",
            "Epoch [70/100], Loss: 18099.2256\n",
            "Epoch [80/100], Loss: 17229.3787\n",
            "Epoch [90/100], Loss: 14172.7059\n",
            "Epoch [100/100], Loss: 23930.3608\n",
            "Fold 2, RMSE: 87.35231018066406\n",
            "Epoch [10/100], Loss: 12394.1575\n",
            "Epoch [20/100], Loss: 12143.2091\n",
            "Epoch [30/100], Loss: 11636.9351\n",
            "Epoch [40/100], Loss: 12895.8101\n",
            "Epoch [50/100], Loss: 12143.8159\n",
            "Epoch [60/100], Loss: 14006.2124\n",
            "Epoch [70/100], Loss: 12545.7300\n",
            "Epoch [80/100], Loss: 14415.5024\n",
            "Epoch [90/100], Loss: 13759.2065\n",
            "Epoch [100/100], Loss: 14888.0962\n",
            "Fold 3, RMSE: 109.9374008178711\n",
            "Epoch [10/100], Loss: 88559.0205\n",
            "Epoch [20/100], Loss: 25704.1099\n",
            "Epoch [30/100], Loss: 20327.0415\n",
            "Epoch [40/100], Loss: 20255.1152\n",
            "Epoch [50/100], Loss: 18732.1770\n",
            "Epoch [60/100], Loss: 18310.0215\n",
            "Epoch [70/100], Loss: 20345.2100\n",
            "Epoch [80/100], Loss: 19652.4250\n",
            "Epoch [90/100], Loss: 17577.9053\n",
            "Epoch [100/100], Loss: 23782.6138\n",
            "Fold 4, RMSE: 54.489078521728516\n",
            "Epoch [10/100], Loss: 20824.1636\n",
            "Epoch [20/100], Loss: 20508.9851\n",
            "Epoch [30/100], Loss: 17046.4401\n",
            "Epoch [40/100], Loss: 20041.2881\n",
            "Epoch [50/100], Loss: 19858.4907\n",
            "Epoch [60/100], Loss: 18288.4875\n",
            "Epoch [70/100], Loss: 21494.6504\n",
            "Epoch [80/100], Loss: 21002.5967\n",
            "Epoch [90/100], Loss: 17952.0251\n",
            "Epoch [100/100], Loss: 21044.4980\n",
            "Fold 5, RMSE: 57.967464447021484\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 72.96896667480469\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19315.1270\n",
            "Epoch [20/150], Loss: 18287.7300\n",
            "Epoch [30/150], Loss: 16986.7812\n",
            "Epoch [40/150], Loss: 20243.6494\n",
            "Epoch [50/150], Loss: 21296.6943\n",
            "Epoch [60/150], Loss: 19374.6001\n",
            "Epoch [70/150], Loss: 19746.4219\n",
            "Epoch [80/150], Loss: 16521.0569\n",
            "Epoch [90/150], Loss: 17280.6602\n",
            "Epoch [100/150], Loss: 15436.2183\n",
            "Epoch [110/150], Loss: 17790.4167\n",
            "Epoch [120/150], Loss: 26567.5566\n",
            "Epoch [130/150], Loss: 23179.5684\n",
            "Epoch [140/150], Loss: 18600.2700\n",
            "Epoch [150/150], Loss: 17781.5420\n",
            "Fold 1, RMSE: 67.18897247314453\n",
            "Epoch [10/150], Loss: 28976.6226\n",
            "Epoch [20/150], Loss: 15064.1570\n",
            "Epoch [30/150], Loss: 22365.4561\n",
            "Epoch [40/150], Loss: 15200.3545\n",
            "Epoch [50/150], Loss: 15100.3594\n",
            "Epoch [60/150], Loss: 16712.6873\n",
            "Epoch [70/150], Loss: 14011.6024\n",
            "Epoch [80/150], Loss: 14850.8344\n",
            "Epoch [90/150], Loss: 14156.5758\n",
            "Epoch [100/150], Loss: 14786.5958\n",
            "Epoch [110/150], Loss: 16962.4712\n",
            "Epoch [120/150], Loss: 15738.6362\n",
            "Epoch [130/150], Loss: 19570.0820\n",
            "Epoch [140/150], Loss: 16375.6758\n",
            "Epoch [150/150], Loss: 14679.6660\n",
            "Fold 2, RMSE: 88.18960571289062\n",
            "Epoch [10/150], Loss: 42759.6699\n",
            "Epoch [20/150], Loss: 15534.0217\n",
            "Epoch [30/150], Loss: 12904.2480\n",
            "Epoch [40/150], Loss: 16131.6426\n",
            "Epoch [50/150], Loss: 12418.4927\n",
            "Epoch [60/150], Loss: 15639.6501\n",
            "Epoch [70/150], Loss: 13270.7686\n",
            "Epoch [80/150], Loss: 13394.0347\n",
            "Epoch [90/150], Loss: 14406.9709\n",
            "Epoch [100/150], Loss: 12528.8145\n",
            "Epoch [110/150], Loss: 12848.4331\n",
            "Epoch [120/150], Loss: 14818.9453\n",
            "Epoch [130/150], Loss: 12271.5127\n",
            "Epoch [140/150], Loss: 9892.7582\n",
            "Epoch [150/150], Loss: 10444.8625\n",
            "Fold 3, RMSE: 103.22344970703125\n",
            "Epoch [10/150], Loss: 19733.5190\n",
            "Epoch [20/150], Loss: 29987.9194\n",
            "Epoch [30/150], Loss: 19315.8735\n",
            "Epoch [40/150], Loss: 22735.3079\n",
            "Epoch [50/150], Loss: 18476.5032\n",
            "Epoch [60/150], Loss: 18425.2744\n",
            "Epoch [70/150], Loss: 17279.8649\n",
            "Epoch [80/150], Loss: 19257.1753\n",
            "Epoch [90/150], Loss: 19382.3838\n",
            "Epoch [100/150], Loss: 27383.1050\n",
            "Epoch [110/150], Loss: 17167.9042\n",
            "Epoch [120/150], Loss: 24028.0454\n",
            "Epoch [130/150], Loss: 17830.1804\n",
            "Epoch [140/150], Loss: 17094.8469\n",
            "Epoch [150/150], Loss: 21400.8403\n",
            "Fold 4, RMSE: 54.43740463256836\n",
            "Epoch [10/150], Loss: 18994.2844\n",
            "Epoch [20/150], Loss: 29034.6892\n",
            "Epoch [30/150], Loss: 18742.2673\n",
            "Epoch [40/150], Loss: 17355.7379\n",
            "Epoch [50/150], Loss: 25189.0466\n",
            "Epoch [60/150], Loss: 19685.5073\n",
            "Epoch [70/150], Loss: 16840.4521\n",
            "Epoch [80/150], Loss: 18547.5471\n",
            "Epoch [90/150], Loss: 19326.3672\n",
            "Epoch [100/150], Loss: 18667.5352\n",
            "Epoch [110/150], Loss: 26518.1758\n",
            "Epoch [120/150], Loss: 17990.7522\n",
            "Epoch [130/150], Loss: 22149.9619\n",
            "Epoch [140/150], Loss: 19804.3794\n",
            "Epoch [150/150], Loss: 21206.9043\n",
            "Fold 5, RMSE: 57.79991912841797\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 74.16787033081054\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 159500.0762\n",
            "Epoch [20/100], Loss: 27756.1309\n",
            "Epoch [30/100], Loss: 27154.0762\n",
            "Epoch [40/100], Loss: 27826.3652\n",
            "Epoch [50/100], Loss: 14241.2307\n",
            "Epoch [60/100], Loss: 27932.3896\n",
            "Epoch [70/100], Loss: 12514.1409\n",
            "Epoch [80/100], Loss: 18330.2080\n",
            "Epoch [90/100], Loss: 13405.9963\n",
            "Epoch [100/100], Loss: 23966.8857\n",
            "Fold 1, RMSE: 59.55591583251953\n",
            "Epoch [10/100], Loss: 31378.0923\n",
            "Epoch [20/100], Loss: 19108.4314\n",
            "Epoch [30/100], Loss: 20729.2207\n",
            "Epoch [40/100], Loss: 15052.8413\n",
            "Epoch [50/100], Loss: 16973.4309\n",
            "Epoch [60/100], Loss: 15245.8157\n",
            "Epoch [70/100], Loss: 17467.8477\n",
            "Epoch [80/100], Loss: 17332.4250\n",
            "Epoch [90/100], Loss: 15741.6218\n",
            "Epoch [100/100], Loss: 15744.8193\n",
            "Fold 2, RMSE: 86.14894104003906\n",
            "Epoch [10/100], Loss: 24936.2388\n",
            "Epoch [20/100], Loss: 10998.8518\n",
            "Epoch [30/100], Loss: 14103.9683\n",
            "Epoch [40/100], Loss: 14588.7852\n",
            "Epoch [50/100], Loss: 14126.4736\n",
            "Epoch [60/100], Loss: 12165.7502\n",
            "Epoch [70/100], Loss: 8371.3260\n",
            "Epoch [80/100], Loss: 10068.0598\n",
            "Epoch [90/100], Loss: 12288.6895\n",
            "Epoch [100/100], Loss: 9056.2080\n",
            "Fold 3, RMSE: 101.27748107910156\n",
            "Epoch [10/100], Loss: 306229.7109\n",
            "Epoch [20/100], Loss: 27911.5942\n",
            "Epoch [30/100], Loss: 30935.0312\n",
            "Epoch [40/100], Loss: 21276.1606\n",
            "Epoch [50/100], Loss: 21226.6025\n",
            "Epoch [60/100], Loss: 13963.8379\n",
            "Epoch [70/100], Loss: 17608.6174\n",
            "Epoch [80/100], Loss: 14547.7279\n",
            "Epoch [90/100], Loss: 15774.9019\n",
            "Epoch [100/100], Loss: 20099.1753\n",
            "Fold 4, RMSE: 46.785491943359375\n",
            "Epoch [10/100], Loss: 17775.9246\n",
            "Epoch [20/100], Loss: 19727.4927\n",
            "Epoch [30/100], Loss: 18802.0708\n",
            "Epoch [40/100], Loss: 26158.3174\n",
            "Epoch [50/100], Loss: 19378.0117\n",
            "Epoch [60/100], Loss: 17728.4800\n",
            "Epoch [70/100], Loss: 19487.0669\n",
            "Epoch [80/100], Loss: 17870.7278\n",
            "Epoch [90/100], Loss: 17911.9395\n",
            "Epoch [100/100], Loss: 17475.4573\n",
            "Fold 5, RMSE: 57.201900482177734\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 70.19394607543946\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 18634.9766\n",
            "Epoch [20/150], Loss: 20255.5354\n",
            "Epoch [30/150], Loss: 20748.2578\n",
            "Epoch [40/150], Loss: 16480.8954\n",
            "Epoch [50/150], Loss: 20853.9805\n",
            "Epoch [60/150], Loss: 15633.8706\n",
            "Epoch [70/150], Loss: 21055.5601\n",
            "Epoch [80/150], Loss: 19118.6006\n",
            "Epoch [90/150], Loss: 19067.4702\n",
            "Epoch [100/150], Loss: 20839.8340\n",
            "Epoch [110/150], Loss: 16853.6177\n",
            "Epoch [120/150], Loss: 17915.0620\n",
            "Epoch [130/150], Loss: 26371.7344\n",
            "Epoch [140/150], Loss: 17330.7546\n",
            "Epoch [150/150], Loss: 20083.2542\n",
            "Fold 1, RMSE: 65.80563354492188\n",
            "Epoch [10/150], Loss: 21531.2168\n",
            "Epoch [20/150], Loss: 15854.9028\n",
            "Epoch [30/150], Loss: 16314.7290\n",
            "Epoch [40/150], Loss: 15338.3984\n",
            "Epoch [50/150], Loss: 15000.5312\n",
            "Epoch [60/150], Loss: 14799.6465\n",
            "Epoch [70/150], Loss: 15013.2798\n",
            "Epoch [80/150], Loss: 15785.0991\n",
            "Epoch [90/150], Loss: 16038.4331\n",
            "Epoch [100/150], Loss: 16108.3491\n",
            "Epoch [110/150], Loss: 16610.1321\n",
            "Epoch [120/150], Loss: 17219.3555\n",
            "Epoch [130/150], Loss: 19181.6211\n",
            "Epoch [140/150], Loss: 13627.5964\n",
            "Epoch [150/150], Loss: 14559.5378\n",
            "Fold 2, RMSE: 83.13157653808594\n",
            "Epoch [10/150], Loss: 10996.7771\n",
            "Epoch [20/150], Loss: 13837.4292\n",
            "Epoch [30/150], Loss: 12702.3044\n",
            "Epoch [40/150], Loss: 11161.8549\n",
            "Epoch [50/150], Loss: 11353.6155\n",
            "Epoch [60/150], Loss: 12614.7896\n",
            "Epoch [70/150], Loss: 15349.8674\n",
            "Epoch [80/150], Loss: 12334.1702\n",
            "Epoch [90/150], Loss: 12598.9302\n",
            "Epoch [100/150], Loss: 8558.0813\n",
            "Epoch [110/150], Loss: 10096.6958\n",
            "Epoch [120/150], Loss: 13456.5103\n",
            "Epoch [130/150], Loss: 11535.3557\n",
            "Epoch [140/150], Loss: 16467.9875\n",
            "Epoch [150/150], Loss: 11020.7859\n",
            "Fold 3, RMSE: 107.03619384765625\n",
            "Epoch [10/150], Loss: 27008.1079\n",
            "Epoch [20/150], Loss: 20334.2261\n",
            "Epoch [30/150], Loss: 26359.8318\n",
            "Epoch [40/150], Loss: 16711.8546\n",
            "Epoch [50/150], Loss: 17701.6609\n",
            "Epoch [60/150], Loss: 16497.8064\n",
            "Epoch [70/150], Loss: 20823.6030\n",
            "Epoch [80/150], Loss: 22465.6011\n",
            "Epoch [90/150], Loss: 22470.8618\n",
            "Epoch [100/150], Loss: 19465.3438\n",
            "Epoch [110/150], Loss: 24846.5142\n",
            "Epoch [120/150], Loss: 19642.2949\n",
            "Epoch [130/150], Loss: 16508.3020\n",
            "Epoch [140/150], Loss: 25682.9805\n",
            "Epoch [150/150], Loss: 17261.5918\n",
            "Fold 4, RMSE: 46.39134216308594\n",
            "Epoch [10/150], Loss: 32800.7192\n",
            "Epoch [20/150], Loss: 16027.3696\n",
            "Epoch [30/150], Loss: 18071.4409\n",
            "Epoch [40/150], Loss: 15118.9834\n",
            "Epoch [50/150], Loss: 10667.9592\n",
            "Epoch [60/150], Loss: 15730.8062\n",
            "Epoch [70/150], Loss: 12616.6467\n",
            "Epoch [80/150], Loss: 13702.0010\n",
            "Epoch [90/150], Loss: 17543.9790\n",
            "Epoch [100/150], Loss: 18293.7688\n",
            "Epoch [110/150], Loss: 9767.1560\n",
            "Epoch [120/150], Loss: 12592.2439\n",
            "Epoch [130/150], Loss: 12396.4852\n",
            "Epoch [140/150], Loss: 18015.8602\n",
            "Epoch [150/150], Loss: 9539.3829\n",
            "Fold 5, RMSE: 43.94619369506836\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 69.26218795776367\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 22546.1716\n",
            "Epoch [20/100], Loss: 23860.8516\n",
            "Epoch [30/100], Loss: 24502.1206\n",
            "Epoch [40/100], Loss: 19996.2256\n",
            "Epoch [50/100], Loss: 18960.6997\n",
            "Epoch [60/100], Loss: 20455.3821\n",
            "Epoch [70/100], Loss: 17747.6270\n",
            "Epoch [80/100], Loss: 21092.1401\n",
            "Epoch [90/100], Loss: 17972.1294\n",
            "Epoch [100/100], Loss: 15066.2507\n",
            "Fold 1, RMSE: 67.49794006347656\n",
            "Epoch [10/100], Loss: 17539.1566\n",
            "Epoch [20/100], Loss: 16547.6333\n",
            "Epoch [30/100], Loss: 17133.4771\n",
            "Epoch [40/100], Loss: 25092.1973\n",
            "Epoch [50/100], Loss: 17317.5107\n",
            "Epoch [60/100], Loss: 13440.1743\n",
            "Epoch [70/100], Loss: 14290.3545\n",
            "Epoch [80/100], Loss: 12090.1135\n",
            "Epoch [90/100], Loss: 13405.4834\n",
            "Epoch [100/100], Loss: 13637.0569\n",
            "Fold 2, RMSE: 72.48880004882812\n",
            "Epoch [10/100], Loss: 14783.2012\n",
            "Epoch [20/100], Loss: 17483.2841\n",
            "Epoch [30/100], Loss: 19393.9414\n",
            "Epoch [40/100], Loss: 11816.5038\n",
            "Epoch [50/100], Loss: 13823.0210\n",
            "Epoch [60/100], Loss: 12980.3982\n",
            "Epoch [70/100], Loss: 16514.8374\n",
            "Epoch [80/100], Loss: 16658.8311\n",
            "Epoch [90/100], Loss: 13794.8955\n",
            "Epoch [100/100], Loss: 13861.3286\n",
            "Fold 3, RMSE: 109.73970031738281\n",
            "Epoch [10/100], Loss: 323191.0000\n",
            "Epoch [20/100], Loss: 29887.2290\n",
            "Epoch [30/100], Loss: 14493.3138\n",
            "Epoch [40/100], Loss: 16743.4543\n",
            "Epoch [50/100], Loss: 17363.8413\n",
            "Epoch [60/100], Loss: 15600.4775\n",
            "Epoch [70/100], Loss: 17686.2415\n",
            "Epoch [80/100], Loss: 13000.6484\n",
            "Epoch [90/100], Loss: 13468.6099\n",
            "Epoch [100/100], Loss: 13219.8423\n",
            "Fold 4, RMSE: 41.973472595214844\n",
            "Epoch [10/100], Loss: 25982.7339\n",
            "Epoch [20/100], Loss: 18963.4785\n",
            "Epoch [30/100], Loss: 21839.5354\n",
            "Epoch [40/100], Loss: 24050.4458\n",
            "Epoch [50/100], Loss: 18800.2534\n",
            "Epoch [60/100], Loss: 28740.9082\n",
            "Epoch [70/100], Loss: 29580.4551\n",
            "Epoch [80/100], Loss: 17264.5144\n",
            "Epoch [90/100], Loss: 17199.5110\n",
            "Epoch [100/100], Loss: 18427.1873\n",
            "Fold 5, RMSE: 57.93058395385742\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 69.92609939575195\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 115799.8223\n",
            "Epoch [20/150], Loss: 17367.7510\n",
            "Epoch [30/150], Loss: 17633.8096\n",
            "Epoch [40/150], Loss: 28593.9426\n",
            "Epoch [50/150], Loss: 24777.2407\n",
            "Epoch [60/150], Loss: 14668.5178\n",
            "Epoch [70/150], Loss: 14215.3933\n",
            "Epoch [80/150], Loss: 13767.0706\n",
            "Epoch [90/150], Loss: 11626.0104\n",
            "Epoch [100/150], Loss: 18677.4211\n",
            "Epoch [110/150], Loss: 12952.1870\n",
            "Epoch [120/150], Loss: 10842.7961\n",
            "Epoch [130/150], Loss: 18772.2124\n",
            "Epoch [140/150], Loss: 21751.1016\n",
            "Epoch [150/150], Loss: 13296.5747\n",
            "Fold 1, RMSE: 59.3835563659668\n",
            "Epoch [10/150], Loss: 178003.7578\n",
            "Epoch [20/150], Loss: 33328.8354\n",
            "Epoch [30/150], Loss: 15351.3491\n",
            "Epoch [40/150], Loss: 20709.4683\n",
            "Epoch [50/150], Loss: 17863.8496\n",
            "Epoch [60/150], Loss: 17544.2050\n",
            "Epoch [70/150], Loss: 12871.7235\n",
            "Epoch [80/150], Loss: 15113.9204\n",
            "Epoch [90/150], Loss: 13315.9785\n",
            "Epoch [100/150], Loss: 11342.7098\n",
            "Epoch [110/150], Loss: 9763.2220\n",
            "Epoch [120/150], Loss: 9938.1588\n",
            "Epoch [130/150], Loss: 10856.6191\n",
            "Epoch [140/150], Loss: 9000.4935\n",
            "Epoch [150/150], Loss: 11860.7587\n",
            "Fold 2, RMSE: 72.33052825927734\n",
            "Epoch [10/150], Loss: 390015.7500\n",
            "Epoch [20/150], Loss: 56670.5176\n",
            "Epoch [30/150], Loss: 19550.9204\n",
            "Epoch [40/150], Loss: 16310.1846\n",
            "Epoch [50/150], Loss: 15872.7856\n",
            "Epoch [60/150], Loss: 18011.5645\n",
            "Epoch [70/150], Loss: 12521.5515\n",
            "Epoch [80/150], Loss: 11413.9502\n",
            "Epoch [90/150], Loss: 11059.7629\n",
            "Epoch [100/150], Loss: 13494.0667\n",
            "Epoch [110/150], Loss: 13291.7644\n",
            "Epoch [120/150], Loss: 11310.9312\n",
            "Epoch [130/150], Loss: 7004.9066\n",
            "Epoch [140/150], Loss: 10851.0618\n",
            "Epoch [150/150], Loss: 7079.7350\n",
            "Fold 3, RMSE: 97.33151245117188\n",
            "Epoch [10/150], Loss: 207867.4961\n",
            "Epoch [20/150], Loss: 40733.9604\n",
            "Epoch [30/150], Loss: 24121.8655\n",
            "Epoch [40/150], Loss: 14942.4558\n",
            "Epoch [50/150], Loss: 17331.7603\n",
            "Epoch [60/150], Loss: 11624.9768\n",
            "Epoch [70/150], Loss: 17735.6216\n",
            "Epoch [80/150], Loss: 18054.1631\n",
            "Epoch [90/150], Loss: 14618.9053\n",
            "Epoch [100/150], Loss: 14038.3921\n",
            "Epoch [110/150], Loss: 32028.5095\n",
            "Epoch [120/150], Loss: 22059.0811\n",
            "Epoch [130/150], Loss: 15461.9253\n",
            "Epoch [140/150], Loss: 13889.2896\n",
            "Epoch [150/150], Loss: 10601.2249\n",
            "Fold 4, RMSE: 40.82428741455078\n",
            "Epoch [10/150], Loss: 30473.4951\n",
            "Epoch [20/150], Loss: 16630.2382\n",
            "Epoch [30/150], Loss: 15671.0928\n",
            "Epoch [40/150], Loss: 20813.0464\n",
            "Epoch [50/150], Loss: 15228.3206\n",
            "Epoch [60/150], Loss: 25396.7820\n",
            "Epoch [70/150], Loss: 16093.8960\n",
            "Epoch [80/150], Loss: 16954.9832\n",
            "Epoch [90/150], Loss: 17000.8965\n",
            "Epoch [100/150], Loss: 10554.5173\n",
            "Epoch [110/150], Loss: 12057.2092\n",
            "Epoch [120/150], Loss: 15151.4580\n",
            "Epoch [130/150], Loss: 10741.9501\n",
            "Epoch [140/150], Loss: 12426.2621\n",
            "Epoch [150/150], Loss: 12448.9424\n",
            "Fold 5, RMSE: 44.44802474975586\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 62.863581848144534\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 38093.9014\n",
            "Epoch [20/100], Loss: 18692.3970\n",
            "Epoch [30/100], Loss: 22499.0286\n",
            "Epoch [40/100], Loss: 25957.6145\n",
            "Epoch [50/100], Loss: 24515.9285\n",
            "Epoch [60/100], Loss: 22812.9404\n",
            "Epoch [70/100], Loss: 12635.1630\n",
            "Epoch [80/100], Loss: 12454.6450\n",
            "Epoch [90/100], Loss: 15299.0508\n",
            "Epoch [100/100], Loss: 13340.2024\n",
            "Fold 1, RMSE: 60.33443069458008\n",
            "Epoch [10/100], Loss: 34251.9141\n",
            "Epoch [20/100], Loss: 17555.6807\n",
            "Epoch [30/100], Loss: 14465.1631\n",
            "Epoch [40/100], Loss: 10687.4980\n",
            "Epoch [50/100], Loss: 10282.0513\n",
            "Epoch [60/100], Loss: 14119.7119\n",
            "Epoch [70/100], Loss: 12167.7688\n",
            "Epoch [80/100], Loss: 12446.5498\n",
            "Epoch [90/100], Loss: 11107.2468\n",
            "Epoch [100/100], Loss: 10475.8220\n",
            "Fold 2, RMSE: 76.76941680908203\n",
            "Epoch [10/100], Loss: 58637.3555\n",
            "Epoch [20/100], Loss: 21044.3096\n",
            "Epoch [30/100], Loss: 9567.2917\n",
            "Epoch [40/100], Loss: 14208.4741\n",
            "Epoch [50/100], Loss: 8962.6230\n",
            "Epoch [60/100], Loss: 8732.0100\n",
            "Epoch [70/100], Loss: 11008.5862\n",
            "Epoch [80/100], Loss: 6927.7366\n",
            "Epoch [90/100], Loss: 8751.0458\n",
            "Epoch [100/100], Loss: 12837.1759\n",
            "Fold 3, RMSE: 100.92731475830078\n",
            "Epoch [10/100], Loss: 35152.1074\n",
            "Epoch [20/100], Loss: 17331.8259\n",
            "Epoch [30/100], Loss: 18025.3872\n",
            "Epoch [40/100], Loss: 19884.5508\n",
            "Epoch [50/100], Loss: 17496.3748\n",
            "Epoch [60/100], Loss: 22179.4924\n",
            "Epoch [70/100], Loss: 19130.8179\n",
            "Epoch [80/100], Loss: 21500.5322\n",
            "Epoch [90/100], Loss: 18213.8032\n",
            "Epoch [100/100], Loss: 27250.8494\n",
            "Fold 4, RMSE: 53.23625946044922\n",
            "Epoch [10/100], Loss: 52531.9229\n",
            "Epoch [20/100], Loss: 23373.0044\n",
            "Epoch [30/100], Loss: 19028.4436\n",
            "Epoch [40/100], Loss: 14575.6384\n",
            "Epoch [50/100], Loss: 13237.9651\n",
            "Epoch [60/100], Loss: 14372.0818\n",
            "Epoch [70/100], Loss: 14014.5537\n",
            "Epoch [80/100], Loss: 14877.3882\n",
            "Epoch [90/100], Loss: 24293.8196\n",
            "Epoch [100/100], Loss: 16735.8824\n",
            "Fold 5, RMSE: 50.20836639404297\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 68.29515762329102\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 45511.0303\n",
            "Epoch [20/150], Loss: 19415.6934\n",
            "Epoch [30/150], Loss: 17478.3481\n",
            "Epoch [40/150], Loss: 12496.7085\n",
            "Epoch [50/150], Loss: 15720.7588\n",
            "Epoch [60/150], Loss: 16286.2625\n",
            "Epoch [70/150], Loss: 15041.2666\n",
            "Epoch [80/150], Loss: 16716.9902\n",
            "Epoch [90/150], Loss: 10396.8782\n",
            "Epoch [100/150], Loss: 11478.7375\n",
            "Epoch [110/150], Loss: 11180.0137\n",
            "Epoch [120/150], Loss: 11332.5118\n",
            "Epoch [130/150], Loss: 20246.6697\n",
            "Epoch [140/150], Loss: 11975.9000\n",
            "Epoch [150/150], Loss: 8997.1549\n",
            "Fold 1, RMSE: 55.54450607299805\n",
            "Epoch [10/150], Loss: 64330.4668\n",
            "Epoch [20/150], Loss: 21346.6758\n",
            "Epoch [30/150], Loss: 17014.4866\n",
            "Epoch [40/150], Loss: 16960.3064\n",
            "Epoch [50/150], Loss: 14225.3613\n",
            "Epoch [60/150], Loss: 12673.3905\n",
            "Epoch [70/150], Loss: 24483.6182\n",
            "Epoch [80/150], Loss: 12335.0151\n",
            "Epoch [90/150], Loss: 10010.8413\n",
            "Epoch [100/150], Loss: 12630.1968\n",
            "Epoch [110/150], Loss: 11835.6162\n",
            "Epoch [120/150], Loss: 11996.9424\n",
            "Epoch [130/150], Loss: 7836.0604\n",
            "Epoch [140/150], Loss: 5951.2301\n",
            "Epoch [150/150], Loss: 9533.1626\n",
            "Fold 2, RMSE: 77.10503387451172\n",
            "Epoch [10/150], Loss: 17311.4866\n",
            "Epoch [20/150], Loss: 13982.9526\n",
            "Epoch [30/150], Loss: 11775.1692\n",
            "Epoch [40/150], Loss: 14303.8428\n",
            "Epoch [50/150], Loss: 8705.2161\n",
            "Epoch [60/150], Loss: 14102.8047\n",
            "Epoch [70/150], Loss: 12233.8860\n",
            "Epoch [80/150], Loss: 8908.9723\n",
            "Epoch [90/150], Loss: 9467.0161\n",
            "Epoch [100/150], Loss: 7565.8312\n",
            "Epoch [110/150], Loss: 7742.5261\n",
            "Epoch [120/150], Loss: 14569.8849\n",
            "Epoch [130/150], Loss: 11429.3831\n",
            "Epoch [140/150], Loss: 8077.6438\n",
            "Epoch [150/150], Loss: 7239.0841\n",
            "Fold 3, RMSE: 97.98906707763672\n",
            "Epoch [10/150], Loss: 30302.4067\n",
            "Epoch [20/150], Loss: 27157.0044\n",
            "Epoch [30/150], Loss: 18625.8848\n",
            "Epoch [40/150], Loss: 22980.0508\n",
            "Epoch [50/150], Loss: 25261.4468\n",
            "Epoch [60/150], Loss: 27991.5610\n",
            "Epoch [70/150], Loss: 17228.7251\n",
            "Epoch [80/150], Loss: 16563.9177\n",
            "Epoch [90/150], Loss: 18025.4934\n",
            "Epoch [100/150], Loss: 15593.7536\n",
            "Epoch [110/150], Loss: 18900.5205\n",
            "Epoch [120/150], Loss: 17829.0693\n",
            "Epoch [130/150], Loss: 18001.5254\n",
            "Epoch [140/150], Loss: 23876.6426\n",
            "Epoch [150/150], Loss: 16882.6357\n",
            "Fold 4, RMSE: 44.04526901245117\n",
            "Epoch [10/150], Loss: 41262.0562\n",
            "Epoch [20/150], Loss: 18656.8032\n",
            "Epoch [30/150], Loss: 18898.5996\n",
            "Epoch [40/150], Loss: 18587.2441\n",
            "Epoch [50/150], Loss: 20493.6809\n",
            "Epoch [60/150], Loss: 15470.9856\n",
            "Epoch [70/150], Loss: 13825.2039\n",
            "Epoch [80/150], Loss: 14840.9700\n",
            "Epoch [90/150], Loss: 14249.4340\n",
            "Epoch [100/150], Loss: 12135.4229\n",
            "Epoch [110/150], Loss: 11404.6636\n",
            "Epoch [120/150], Loss: 11118.0809\n",
            "Epoch [130/150], Loss: 13186.0813\n",
            "Epoch [140/150], Loss: 12824.2285\n",
            "Epoch [150/150], Loss: 12141.2427\n",
            "Fold 5, RMSE: 49.748165130615234\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 64.88640823364258\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 96721.2129\n",
            "Epoch [20/100], Loss: 19651.0845\n",
            "Epoch [30/100], Loss: 21250.9639\n",
            "Epoch [40/100], Loss: 18536.4783\n",
            "Epoch [50/100], Loss: 16857.7166\n",
            "Epoch [60/100], Loss: 22958.0962\n",
            "Epoch [70/100], Loss: 16987.6936\n",
            "Epoch [80/100], Loss: 17005.0437\n",
            "Epoch [90/100], Loss: 21571.6697\n",
            "Epoch [100/100], Loss: 17368.2207\n",
            "Fold 1, RMSE: 67.58997344970703\n",
            "Epoch [10/100], Loss: 311811.6953\n",
            "Epoch [20/100], Loss: 21216.4956\n",
            "Epoch [30/100], Loss: 28947.8242\n",
            "Epoch [40/100], Loss: 15186.4658\n",
            "Epoch [50/100], Loss: 23109.7607\n",
            "Epoch [60/100], Loss: 11585.2852\n",
            "Epoch [70/100], Loss: 19686.6931\n",
            "Epoch [80/100], Loss: 9551.1003\n",
            "Epoch [90/100], Loss: 11576.2043\n",
            "Epoch [100/100], Loss: 10232.1151\n",
            "Fold 2, RMSE: 72.85941314697266\n",
            "Epoch [10/100], Loss: 48918.1411\n",
            "Epoch [20/100], Loss: 11784.5753\n",
            "Epoch [30/100], Loss: 16127.8064\n",
            "Epoch [40/100], Loss: 12616.0457\n",
            "Epoch [50/100], Loss: 15368.1729\n",
            "Epoch [60/100], Loss: 8787.5007\n",
            "Epoch [70/100], Loss: 11412.6306\n",
            "Epoch [80/100], Loss: 10988.5393\n",
            "Epoch [90/100], Loss: 12443.5820\n",
            "Epoch [100/100], Loss: 10501.8293\n",
            "Fold 3, RMSE: 94.76824951171875\n",
            "Epoch [10/100], Loss: 91982.6279\n",
            "Epoch [20/100], Loss: 28907.5527\n",
            "Epoch [30/100], Loss: 16879.0444\n",
            "Epoch [40/100], Loss: 23422.5942\n",
            "Epoch [50/100], Loss: 22956.1196\n",
            "Epoch [60/100], Loss: 17510.2390\n",
            "Epoch [70/100], Loss: 24108.0740\n",
            "Epoch [80/100], Loss: 15605.2664\n",
            "Epoch [90/100], Loss: 22132.4712\n",
            "Epoch [100/100], Loss: 13320.6609\n",
            "Fold 4, RMSE: 40.5982666015625\n",
            "Epoch [10/100], Loss: 23319.0438\n",
            "Epoch [20/100], Loss: 22735.1565\n",
            "Epoch [30/100], Loss: 28366.9146\n",
            "Epoch [40/100], Loss: 17908.2864\n",
            "Epoch [50/100], Loss: 20837.6013\n",
            "Epoch [60/100], Loss: 19677.0703\n",
            "Epoch [70/100], Loss: 18886.1851\n",
            "Epoch [80/100], Loss: 16706.5905\n",
            "Epoch [90/100], Loss: 18056.7278\n",
            "Epoch [100/100], Loss: 19404.0454\n",
            "Fold 5, RMSE: 57.89734649658203\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 66.74264984130859\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 125985.0430\n",
            "Epoch [20/150], Loss: 20690.7905\n",
            "Epoch [30/150], Loss: 14880.9794\n",
            "Epoch [40/150], Loss: 18180.4790\n",
            "Epoch [50/150], Loss: 18646.2236\n",
            "Epoch [60/150], Loss: 20222.7551\n",
            "Epoch [70/150], Loss: 13949.0203\n",
            "Epoch [80/150], Loss: 14773.4397\n",
            "Epoch [90/150], Loss: 21870.5486\n",
            "Epoch [100/150], Loss: 12735.0798\n",
            "Epoch [110/150], Loss: 11385.3718\n",
            "Epoch [120/150], Loss: 16864.5879\n",
            "Epoch [130/150], Loss: 15588.9858\n",
            "Epoch [140/150], Loss: 21708.6008\n",
            "Epoch [150/150], Loss: 10953.8963\n",
            "Fold 1, RMSE: 57.643924713134766\n",
            "Epoch [10/150], Loss: 54262.0938\n",
            "Epoch [20/150], Loss: 13760.5286\n",
            "Epoch [30/150], Loss: 15129.3879\n",
            "Epoch [40/150], Loss: 16666.7734\n",
            "Epoch [50/150], Loss: 16978.3049\n",
            "Epoch [60/150], Loss: 11753.3427\n",
            "Epoch [70/150], Loss: 18747.2456\n",
            "Epoch [80/150], Loss: 13235.2125\n",
            "Epoch [90/150], Loss: 12251.7166\n",
            "Epoch [100/150], Loss: 8956.4642\n",
            "Epoch [110/150], Loss: 8308.1809\n",
            "Epoch [120/150], Loss: 9770.6199\n",
            "Epoch [130/150], Loss: 12434.5427\n",
            "Epoch [140/150], Loss: 6193.5574\n",
            "Epoch [150/150], Loss: 9423.8483\n",
            "Fold 2, RMSE: 80.54962921142578\n",
            "Epoch [10/150], Loss: 394198.1172\n",
            "Epoch [20/150], Loss: 44093.3733\n",
            "Epoch [30/150], Loss: 21961.1914\n",
            "Epoch [40/150], Loss: 19949.3384\n",
            "Epoch [50/150], Loss: 16369.1987\n",
            "Epoch [60/150], Loss: 12848.3440\n",
            "Epoch [70/150], Loss: 12595.8140\n",
            "Epoch [80/150], Loss: 13180.3970\n",
            "Epoch [90/150], Loss: 9088.4996\n",
            "Epoch [100/150], Loss: 10648.9971\n",
            "Epoch [110/150], Loss: 10326.1240\n",
            "Epoch [120/150], Loss: 8324.2529\n",
            "Epoch [130/150], Loss: 10676.2544\n",
            "Epoch [140/150], Loss: 6966.5634\n",
            "Epoch [150/150], Loss: 10573.1145\n",
            "Fold 3, RMSE: 101.00157928466797\n",
            "Epoch [10/150], Loss: 158560.3652\n",
            "Epoch [20/150], Loss: 23662.1309\n",
            "Epoch [30/150], Loss: 22099.4922\n",
            "Epoch [40/150], Loss: 17549.0988\n",
            "Epoch [50/150], Loss: 17088.6655\n",
            "Epoch [60/150], Loss: 14698.5488\n",
            "Epoch [70/150], Loss: 14214.1252\n",
            "Epoch [80/150], Loss: 15464.8992\n",
            "Epoch [90/150], Loss: 16529.4678\n",
            "Epoch [100/150], Loss: 16203.6440\n",
            "Epoch [110/150], Loss: 11650.2206\n",
            "Epoch [120/150], Loss: 18333.7371\n",
            "Epoch [130/150], Loss: 11607.9226\n",
            "Epoch [140/150], Loss: 15975.5037\n",
            "Epoch [150/150], Loss: 13399.2869\n",
            "Fold 4, RMSE: 42.61670684814453\n",
            "Epoch [10/150], Loss: 41732.1768\n",
            "Epoch [20/150], Loss: 28262.6855\n",
            "Epoch [30/150], Loss: 25929.5996\n",
            "Epoch [40/150], Loss: 18691.6409\n",
            "Epoch [50/150], Loss: 18975.0967\n",
            "Epoch [60/150], Loss: 26979.2061\n",
            "Epoch [70/150], Loss: 22804.5210\n",
            "Epoch [80/150], Loss: 17445.2292\n",
            "Epoch [90/150], Loss: 19877.8630\n",
            "Epoch [100/150], Loss: 16877.6924\n",
            "Epoch [110/150], Loss: 14996.3936\n",
            "Epoch [120/150], Loss: 14663.6115\n",
            "Epoch [130/150], Loss: 19462.6499\n",
            "Epoch [140/150], Loss: 13876.4338\n",
            "Epoch [150/150], Loss: 14055.2166\n",
            "Fold 5, RMSE: 44.08765411376953\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 65.17989883422851\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 29528.4619\n",
            "Epoch [20/100], Loss: 17823.2629\n",
            "Epoch [30/100], Loss: 20177.9170\n",
            "Epoch [40/100], Loss: 18653.1169\n",
            "Epoch [50/100], Loss: 15019.2734\n",
            "Epoch [60/100], Loss: 14021.1565\n",
            "Epoch [70/100], Loss: 12221.0527\n",
            "Epoch [80/100], Loss: 14976.5747\n",
            "Epoch [90/100], Loss: 18296.3000\n",
            "Epoch [100/100], Loss: 10011.4021\n",
            "Fold 1, RMSE: 57.49258804321289\n",
            "Epoch [10/100], Loss: 152082.1797\n",
            "Epoch [20/100], Loss: 17978.1111\n",
            "Epoch [30/100], Loss: 19804.0205\n",
            "Epoch [40/100], Loss: 24718.7292\n",
            "Epoch [50/100], Loss: 20802.9565\n",
            "Epoch [60/100], Loss: 19882.7432\n",
            "Epoch [70/100], Loss: 13160.4360\n",
            "Epoch [80/100], Loss: 13313.1160\n",
            "Epoch [90/100], Loss: 11945.2554\n",
            "Epoch [100/100], Loss: 13134.2744\n",
            "Fold 2, RMSE: 81.10040283203125\n",
            "Epoch [10/100], Loss: 67941.5566\n",
            "Epoch [20/100], Loss: 10713.2009\n",
            "Epoch [30/100], Loss: 9165.9657\n",
            "Epoch [40/100], Loss: 12061.0369\n",
            "Epoch [50/100], Loss: 10862.3953\n",
            "Epoch [60/100], Loss: 7924.5195\n",
            "Epoch [70/100], Loss: 12471.7358\n",
            "Epoch [80/100], Loss: 10968.0303\n",
            "Epoch [90/100], Loss: 8343.9294\n",
            "Epoch [100/100], Loss: 12600.2145\n",
            "Fold 3, RMSE: 105.76919555664062\n",
            "Epoch [10/100], Loss: 29158.6987\n",
            "Epoch [20/100], Loss: 30013.0557\n",
            "Epoch [30/100], Loss: 19984.6289\n",
            "Epoch [40/100], Loss: 24106.9253\n",
            "Epoch [50/100], Loss: 18999.4375\n",
            "Epoch [60/100], Loss: 21457.4009\n",
            "Epoch [70/100], Loss: 19773.8892\n",
            "Epoch [80/100], Loss: 17485.6851\n",
            "Epoch [90/100], Loss: 26048.8132\n",
            "Epoch [100/100], Loss: 17693.5676\n",
            "Fold 4, RMSE: 50.97848129272461\n",
            "Epoch [10/100], Loss: 103385.7363\n",
            "Epoch [20/100], Loss: 16465.9343\n",
            "Epoch [30/100], Loss: 16078.0396\n",
            "Epoch [40/100], Loss: 17624.0715\n",
            "Epoch [50/100], Loss: 19053.8372\n",
            "Epoch [60/100], Loss: 12569.1798\n",
            "Epoch [70/100], Loss: 10653.9839\n",
            "Epoch [80/100], Loss: 17895.3132\n",
            "Epoch [90/100], Loss: 15481.5737\n",
            "Epoch [100/100], Loss: 15171.3000\n",
            "Fold 5, RMSE: 50.33426284790039\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 69.13498611450196\n",
            "Training with neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 170053.5664\n",
            "Epoch [20/150], Loss: 40189.7207\n",
            "Epoch [30/150], Loss: 24995.5239\n",
            "Epoch [40/150], Loss: 32002.6699\n",
            "Epoch [50/150], Loss: 24562.8232\n",
            "Epoch [60/150], Loss: 14350.8230\n",
            "Epoch [70/150], Loss: 18305.8794\n",
            "Epoch [80/150], Loss: 18604.3916\n",
            "Epoch [90/150], Loss: 20513.8501\n",
            "Epoch [100/150], Loss: 17278.5625\n",
            "Epoch [110/150], Loss: 12754.9639\n",
            "Epoch [120/150], Loss: 12268.2358\n",
            "Epoch [130/150], Loss: 17014.4541\n",
            "Epoch [140/150], Loss: 15817.1055\n",
            "Epoch [150/150], Loss: 11404.6415\n",
            "Fold 1, RMSE: 56.68489074707031\n",
            "Epoch [10/150], Loss: 42353.5762\n",
            "Epoch [20/150], Loss: 14011.0916\n",
            "Epoch [30/150], Loss: 17098.9033\n",
            "Epoch [40/150], Loss: 13813.2542\n",
            "Epoch [50/150], Loss: 13925.5979\n",
            "Epoch [60/150], Loss: 15675.5037\n",
            "Epoch [70/150], Loss: 13456.9971\n",
            "Epoch [80/150], Loss: 12047.7178\n",
            "Epoch [90/150], Loss: 16734.0498\n",
            "Epoch [100/150], Loss: 12918.6389\n",
            "Epoch [110/150], Loss: 13040.0554\n",
            "Epoch [120/150], Loss: 13071.0317\n",
            "Epoch [130/150], Loss: 13978.1538\n",
            "Epoch [140/150], Loss: 14815.5442\n",
            "Epoch [150/150], Loss: 13046.7018\n",
            "Fold 2, RMSE: 79.86463165283203\n",
            "Epoch [10/150], Loss: 111732.2207\n",
            "Epoch [20/150], Loss: 35670.4849\n",
            "Epoch [30/150], Loss: 21046.3254\n",
            "Epoch [40/150], Loss: 13096.7454\n",
            "Epoch [50/150], Loss: 9562.3098\n",
            "Epoch [60/150], Loss: 9677.2343\n",
            "Epoch [70/150], Loss: 12863.3086\n",
            "Epoch [80/150], Loss: 11488.8745\n",
            "Epoch [90/150], Loss: 15289.0676\n",
            "Epoch [100/150], Loss: 12534.6475\n",
            "Epoch [110/150], Loss: 9903.6189\n",
            "Epoch [120/150], Loss: 9757.4597\n",
            "Epoch [130/150], Loss: 15870.1174\n",
            "Epoch [140/150], Loss: 10619.0007\n",
            "Epoch [150/150], Loss: 13633.3051\n",
            "Fold 3, RMSE: 100.8686294555664\n",
            "Epoch [10/150], Loss: 20975.3054\n",
            "Epoch [20/150], Loss: 17348.4841\n",
            "Epoch [30/150], Loss: 22680.1479\n",
            "Epoch [40/150], Loss: 17180.3525\n",
            "Epoch [50/150], Loss: 19417.6699\n",
            "Epoch [60/150], Loss: 21174.5767\n",
            "Epoch [70/150], Loss: 13773.2354\n",
            "Epoch [80/150], Loss: 12881.1954\n",
            "Epoch [90/150], Loss: 14076.2432\n",
            "Epoch [100/150], Loss: 11850.8110\n",
            "Epoch [110/150], Loss: 15900.5104\n",
            "Epoch [120/150], Loss: 13209.4448\n",
            "Epoch [130/150], Loss: 14441.1074\n",
            "Epoch [140/150], Loss: 11730.1460\n",
            "Epoch [150/150], Loss: 18608.3784\n",
            "Fold 4, RMSE: 51.03717041015625\n",
            "Epoch [10/150], Loss: 21104.9878\n",
            "Epoch [20/150], Loss: 17069.6161\n",
            "Epoch [30/150], Loss: 21665.3413\n",
            "Epoch [40/150], Loss: 18148.2200\n",
            "Epoch [50/150], Loss: 25369.7876\n",
            "Epoch [60/150], Loss: 18578.7979\n",
            "Epoch [70/150], Loss: 25107.6357\n",
            "Epoch [80/150], Loss: 24200.5854\n",
            "Epoch [90/150], Loss: 17579.3831\n",
            "Epoch [100/150], Loss: 17645.6206\n",
            "Epoch [110/150], Loss: 16585.8245\n",
            "Epoch [120/150], Loss: 19736.0947\n",
            "Epoch [130/150], Loss: 20228.8481\n",
            "Epoch [140/150], Loss: 20507.5840\n",
            "Epoch [150/150], Loss: 19357.5786\n",
            "Fold 5, RMSE: 55.16497039794922\n",
            "Avg RMSE for neurons=96, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 68.72405853271485\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 18156.9639\n",
            "Epoch [20/100], Loss: 8233.6880\n",
            "Epoch [30/100], Loss: 10278.0552\n",
            "Epoch [40/100], Loss: 3810.6285\n",
            "Epoch [50/100], Loss: 3183.1287\n",
            "Epoch [60/100], Loss: 3589.0006\n",
            "Epoch [70/100], Loss: 2244.8494\n",
            "Epoch [80/100], Loss: 2519.5189\n",
            "Epoch [90/100], Loss: 1734.9612\n",
            "Epoch [100/100], Loss: 2248.5461\n",
            "Fold 1, RMSE: 62.537315368652344\n",
            "Epoch [10/100], Loss: 10014.1997\n",
            "Epoch [20/100], Loss: 6871.0305\n",
            "Epoch [30/100], Loss: 5751.3757\n",
            "Epoch [40/100], Loss: 3760.5193\n",
            "Epoch [50/100], Loss: 2044.3860\n",
            "Epoch [60/100], Loss: 1873.9308\n",
            "Epoch [70/100], Loss: 2310.8022\n",
            "Epoch [80/100], Loss: 2010.7697\n",
            "Epoch [90/100], Loss: 1321.7312\n",
            "Epoch [100/100], Loss: 1729.1566\n",
            "Fold 2, RMSE: 62.4747200012207\n",
            "Epoch [10/100], Loss: 13614.8101\n",
            "Epoch [20/100], Loss: 11567.4611\n",
            "Epoch [30/100], Loss: 4484.7546\n",
            "Epoch [40/100], Loss: 3680.0638\n",
            "Epoch [50/100], Loss: 3345.4935\n",
            "Epoch [60/100], Loss: 2207.0633\n",
            "Epoch [70/100], Loss: 631.1490\n",
            "Epoch [80/100], Loss: 500.6143\n",
            "Epoch [90/100], Loss: 2071.7675\n",
            "Epoch [100/100], Loss: 1674.9904\n",
            "Fold 3, RMSE: 90.98133087158203\n",
            "Epoch [10/100], Loss: 23829.5913\n",
            "Epoch [20/100], Loss: 17145.3486\n",
            "Epoch [30/100], Loss: 7430.1089\n",
            "Epoch [40/100], Loss: 9934.9409\n",
            "Epoch [50/100], Loss: 4008.4005\n",
            "Epoch [60/100], Loss: 2359.8858\n",
            "Epoch [70/100], Loss: 3925.8289\n",
            "Epoch [80/100], Loss: 1806.7120\n",
            "Epoch [90/100], Loss: 2024.5821\n",
            "Epoch [100/100], Loss: 944.8706\n",
            "Fold 4, RMSE: 43.78411865234375\n",
            "Epoch [10/100], Loss: 19454.1953\n",
            "Epoch [20/100], Loss: 10437.6908\n",
            "Epoch [30/100], Loss: 7872.6112\n",
            "Epoch [40/100], Loss: 5141.1267\n",
            "Epoch [50/100], Loss: 2767.1228\n",
            "Epoch [60/100], Loss: 1325.5551\n",
            "Epoch [70/100], Loss: 1400.7550\n",
            "Epoch [80/100], Loss: 1771.3707\n",
            "Epoch [90/100], Loss: 3227.4466\n",
            "Epoch [100/100], Loss: 2862.3894\n",
            "Fold 5, RMSE: 46.39291763305664\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 61.2340805053711\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 21709.7207\n",
            "Epoch [20/150], Loss: 12888.3906\n",
            "Epoch [30/150], Loss: 4215.8967\n",
            "Epoch [40/150], Loss: 8992.0189\n",
            "Epoch [50/150], Loss: 3937.9119\n",
            "Epoch [60/150], Loss: 2668.6427\n",
            "Epoch [70/150], Loss: 3008.9828\n",
            "Epoch [80/150], Loss: 1732.3985\n",
            "Epoch [90/150], Loss: 2137.7048\n",
            "Epoch [100/150], Loss: 1891.8610\n",
            "Epoch [110/150], Loss: 2182.5530\n",
            "Epoch [120/150], Loss: 1286.7877\n",
            "Epoch [130/150], Loss: 3009.7716\n",
            "Epoch [140/150], Loss: 1399.1786\n",
            "Epoch [150/150], Loss: 1613.0937\n",
            "Fold 1, RMSE: 56.79619598388672\n",
            "Epoch [10/150], Loss: 16006.8716\n",
            "Epoch [20/150], Loss: 10784.7385\n",
            "Epoch [30/150], Loss: 6887.6023\n",
            "Epoch [40/150], Loss: 2903.5747\n",
            "Epoch [50/150], Loss: 3550.7245\n",
            "Epoch [60/150], Loss: 1468.8579\n",
            "Epoch [70/150], Loss: 1810.7530\n",
            "Epoch [80/150], Loss: 2232.6033\n",
            "Epoch [90/150], Loss: 725.3492\n",
            "Epoch [100/150], Loss: 3296.4723\n",
            "Epoch [110/150], Loss: 1643.6733\n",
            "Epoch [120/150], Loss: 805.4127\n",
            "Epoch [130/150], Loss: 2562.3325\n",
            "Epoch [140/150], Loss: 2334.6437\n",
            "Epoch [150/150], Loss: 997.7564\n",
            "Fold 2, RMSE: 59.60914611816406\n",
            "Epoch [10/150], Loss: 24977.3569\n",
            "Epoch [20/150], Loss: 7204.1802\n",
            "Epoch [30/150], Loss: 5592.4108\n",
            "Epoch [40/150], Loss: 5350.5560\n",
            "Epoch [50/150], Loss: 5093.0456\n",
            "Epoch [60/150], Loss: 2454.5150\n",
            "Epoch [70/150], Loss: 1742.4188\n",
            "Epoch [80/150], Loss: 2249.4475\n",
            "Epoch [90/150], Loss: 2875.5810\n",
            "Epoch [100/150], Loss: 2783.3025\n",
            "Epoch [110/150], Loss: 887.7843\n",
            "Epoch [120/150], Loss: 2412.6129\n",
            "Epoch [130/150], Loss: 1900.6509\n",
            "Epoch [140/150], Loss: 1562.2454\n",
            "Epoch [150/150], Loss: 1337.2463\n",
            "Fold 3, RMSE: 89.86304473876953\n",
            "Epoch [10/150], Loss: 16576.3296\n",
            "Epoch [20/150], Loss: 15314.6658\n",
            "Epoch [30/150], Loss: 10122.9958\n",
            "Epoch [40/150], Loss: 7138.4124\n",
            "Epoch [50/150], Loss: 3613.6934\n",
            "Epoch [60/150], Loss: 2782.0115\n",
            "Epoch [70/150], Loss: 2872.2101\n",
            "Epoch [80/150], Loss: 4615.6707\n",
            "Epoch [90/150], Loss: 2330.8181\n",
            "Epoch [100/150], Loss: 2047.4080\n",
            "Epoch [110/150], Loss: 1287.8216\n",
            "Epoch [120/150], Loss: 1620.8693\n",
            "Epoch [130/150], Loss: 2625.9952\n",
            "Epoch [140/150], Loss: 1284.3694\n",
            "Epoch [150/150], Loss: 1430.4420\n",
            "Fold 4, RMSE: 41.048336029052734\n",
            "Epoch [10/150], Loss: 20832.5559\n",
            "Epoch [20/150], Loss: 15231.3254\n",
            "Epoch [30/150], Loss: 8133.8975\n",
            "Epoch [40/150], Loss: 5324.7565\n",
            "Epoch [50/150], Loss: 5323.1917\n",
            "Epoch [60/150], Loss: 5304.2171\n",
            "Epoch [70/150], Loss: 904.1335\n",
            "Epoch [80/150], Loss: 1431.8038\n",
            "Epoch [90/150], Loss: 2563.6797\n",
            "Epoch [100/150], Loss: 3710.3183\n",
            "Epoch [110/150], Loss: 1960.4997\n",
            "Epoch [120/150], Loss: 1970.4342\n",
            "Epoch [130/150], Loss: 1417.9384\n",
            "Epoch [140/150], Loss: 1142.1194\n",
            "Epoch [150/150], Loss: 1436.6056\n",
            "Fold 5, RMSE: 51.21187210083008\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 59.705718994140625\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 17820.6890\n",
            "Epoch [20/100], Loss: 6428.2457\n",
            "Epoch [30/100], Loss: 5228.6165\n",
            "Epoch [40/100], Loss: 2431.6830\n",
            "Epoch [50/100], Loss: 2154.3951\n",
            "Epoch [60/100], Loss: 1765.2732\n",
            "Epoch [70/100], Loss: 1263.2311\n",
            "Epoch [80/100], Loss: 2202.1010\n",
            "Epoch [90/100], Loss: 1611.5901\n",
            "Epoch [100/100], Loss: 1605.8816\n",
            "Fold 1, RMSE: 56.63801193237305\n",
            "Epoch [10/100], Loss: 17130.8625\n",
            "Epoch [20/100], Loss: 8866.8499\n",
            "Epoch [30/100], Loss: 6596.6153\n",
            "Epoch [40/100], Loss: 4129.0375\n",
            "Epoch [50/100], Loss: 3897.1239\n",
            "Epoch [60/100], Loss: 1540.2257\n",
            "Epoch [70/100], Loss: 1938.4307\n",
            "Epoch [80/100], Loss: 3441.5792\n",
            "Epoch [90/100], Loss: 2588.3728\n",
            "Epoch [100/100], Loss: 891.9518\n",
            "Fold 2, RMSE: 61.12196350097656\n",
            "Epoch [10/100], Loss: 12460.0308\n",
            "Epoch [20/100], Loss: 6111.4795\n",
            "Epoch [30/100], Loss: 3259.6616\n",
            "Epoch [40/100], Loss: 1401.7786\n",
            "Epoch [50/100], Loss: 2762.4945\n",
            "Epoch [60/100], Loss: 2575.5598\n",
            "Epoch [70/100], Loss: 1122.5337\n",
            "Epoch [80/100], Loss: 2376.0366\n",
            "Epoch [90/100], Loss: 3077.2678\n",
            "Epoch [100/100], Loss: 2415.6702\n",
            "Fold 3, RMSE: 88.8033218383789\n",
            "Epoch [10/100], Loss: 15759.4805\n",
            "Epoch [20/100], Loss: 6815.4283\n",
            "Epoch [30/100], Loss: 10303.5688\n",
            "Epoch [40/100], Loss: 2534.6843\n",
            "Epoch [50/100], Loss: 2106.9938\n",
            "Epoch [60/100], Loss: 1941.6389\n",
            "Epoch [70/100], Loss: 2284.6335\n",
            "Epoch [80/100], Loss: 3843.2341\n",
            "Epoch [90/100], Loss: 2211.2686\n",
            "Epoch [100/100], Loss: 1798.4458\n",
            "Fold 4, RMSE: 42.11171340942383\n",
            "Epoch [10/100], Loss: 11697.5575\n",
            "Epoch [20/100], Loss: 10162.7092\n",
            "Epoch [30/100], Loss: 7188.2726\n",
            "Epoch [40/100], Loss: 3706.5586\n",
            "Epoch [50/100], Loss: 1770.9790\n",
            "Epoch [60/100], Loss: 817.2961\n",
            "Epoch [70/100], Loss: 1880.0570\n",
            "Epoch [80/100], Loss: 1023.6057\n",
            "Epoch [90/100], Loss: 1357.5422\n",
            "Epoch [100/100], Loss: 1127.1138\n",
            "Fold 5, RMSE: 43.84496307373047\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 58.50399475097656\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 9666.6091\n",
            "Epoch [20/150], Loss: 5692.1808\n",
            "Epoch [30/150], Loss: 5026.3578\n",
            "Epoch [40/150], Loss: 5519.6479\n",
            "Epoch [50/150], Loss: 4528.6443\n",
            "Epoch [60/150], Loss: 2897.8420\n",
            "Epoch [70/150], Loss: 1447.3293\n",
            "Epoch [80/150], Loss: 2088.7852\n",
            "Epoch [90/150], Loss: 990.1024\n",
            "Epoch [100/150], Loss: 1296.0803\n",
            "Epoch [110/150], Loss: 1298.4910\n",
            "Epoch [120/150], Loss: 1928.3148\n",
            "Epoch [130/150], Loss: 1087.4469\n",
            "Epoch [140/150], Loss: 1238.0427\n",
            "Epoch [150/150], Loss: 709.2605\n",
            "Fold 1, RMSE: 59.121055603027344\n",
            "Epoch [10/150], Loss: 9503.1759\n",
            "Epoch [20/150], Loss: 9848.8851\n",
            "Epoch [30/150], Loss: 4061.1390\n",
            "Epoch [40/150], Loss: 2185.9158\n",
            "Epoch [50/150], Loss: 3436.9329\n",
            "Epoch [60/150], Loss: 2557.4782\n",
            "Epoch [70/150], Loss: 3398.1987\n",
            "Epoch [80/150], Loss: 1918.7482\n",
            "Epoch [90/150], Loss: 2502.3897\n",
            "Epoch [100/150], Loss: 1606.6229\n",
            "Epoch [110/150], Loss: 948.0406\n",
            "Epoch [120/150], Loss: 607.7378\n",
            "Epoch [130/150], Loss: 6055.6779\n",
            "Epoch [140/150], Loss: 1995.8031\n",
            "Epoch [150/150], Loss: 1654.8578\n",
            "Fold 2, RMSE: 61.882869720458984\n",
            "Epoch [10/150], Loss: 7697.2928\n",
            "Epoch [20/150], Loss: 6507.3468\n",
            "Epoch [30/150], Loss: 4128.4945\n",
            "Epoch [40/150], Loss: 2158.6228\n",
            "Epoch [50/150], Loss: 1421.5272\n",
            "Epoch [60/150], Loss: 2087.1292\n",
            "Epoch [70/150], Loss: 4655.2847\n",
            "Epoch [80/150], Loss: 3148.6565\n",
            "Epoch [90/150], Loss: 1359.1994\n",
            "Epoch [100/150], Loss: 957.4754\n",
            "Epoch [110/150], Loss: 1289.1523\n",
            "Epoch [120/150], Loss: 462.7525\n",
            "Epoch [130/150], Loss: 987.5494\n",
            "Epoch [140/150], Loss: 601.8884\n",
            "Epoch [150/150], Loss: 554.1483\n",
            "Fold 3, RMSE: 94.35084533691406\n",
            "Epoch [10/150], Loss: 18359.8125\n",
            "Epoch [20/150], Loss: 10894.1069\n",
            "Epoch [30/150], Loss: 5139.6755\n",
            "Epoch [40/150], Loss: 5611.1206\n",
            "Epoch [50/150], Loss: 3907.0012\n",
            "Epoch [60/150], Loss: 1800.5536\n",
            "Epoch [70/150], Loss: 1093.3386\n",
            "Epoch [80/150], Loss: 1428.1747\n",
            "Epoch [90/150], Loss: 1717.2565\n",
            "Epoch [100/150], Loss: 3090.8621\n",
            "Epoch [110/150], Loss: 2725.2823\n",
            "Epoch [120/150], Loss: 1879.7179\n",
            "Epoch [130/150], Loss: 1417.6667\n",
            "Epoch [140/150], Loss: 1145.0922\n",
            "Epoch [150/150], Loss: 3803.7570\n",
            "Fold 4, RMSE: 41.29988098144531\n",
            "Epoch [10/150], Loss: 12463.6368\n",
            "Epoch [20/150], Loss: 9641.1753\n",
            "Epoch [30/150], Loss: 6494.3525\n",
            "Epoch [40/150], Loss: 4632.8501\n",
            "Epoch [50/150], Loss: 1889.1387\n",
            "Epoch [60/150], Loss: 5701.3858\n",
            "Epoch [70/150], Loss: 1959.0478\n",
            "Epoch [80/150], Loss: 1327.9176\n",
            "Epoch [90/150], Loss: 1580.0154\n",
            "Epoch [100/150], Loss: 1071.6913\n",
            "Epoch [110/150], Loss: 1493.7010\n",
            "Epoch [120/150], Loss: 1749.9739\n",
            "Epoch [130/150], Loss: 2041.6177\n",
            "Epoch [140/150], Loss: 1403.0651\n",
            "Epoch [150/150], Loss: 867.4805\n",
            "Fold 5, RMSE: 47.93141174316406\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 60.917212677001956\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14437.4292\n",
            "Epoch [20/100], Loss: 8942.7680\n",
            "Epoch [30/100], Loss: 8092.2859\n",
            "Epoch [40/100], Loss: 3447.8832\n",
            "Epoch [50/100], Loss: 3468.1218\n",
            "Epoch [60/100], Loss: 1943.4552\n",
            "Epoch [70/100], Loss: 1829.1682\n",
            "Epoch [80/100], Loss: 1791.9278\n",
            "Epoch [90/100], Loss: 1959.7599\n",
            "Epoch [100/100], Loss: 1571.1445\n",
            "Fold 1, RMSE: 63.040958404541016\n",
            "Epoch [10/100], Loss: 12014.0099\n",
            "Epoch [20/100], Loss: 9801.0229\n",
            "Epoch [30/100], Loss: 16362.5941\n",
            "Epoch [40/100], Loss: 3617.8176\n",
            "Epoch [50/100], Loss: 3379.2662\n",
            "Epoch [60/100], Loss: 2443.5415\n",
            "Epoch [70/100], Loss: 2959.0749\n",
            "Epoch [80/100], Loss: 1670.4920\n",
            "Epoch [90/100], Loss: 1552.7272\n",
            "Epoch [100/100], Loss: 2499.5645\n",
            "Fold 2, RMSE: 59.677734375\n",
            "Epoch [10/100], Loss: 34710.8066\n",
            "Epoch [20/100], Loss: 13257.4127\n",
            "Epoch [30/100], Loss: 6862.3539\n",
            "Epoch [40/100], Loss: 4505.4374\n",
            "Epoch [50/100], Loss: 4884.6632\n",
            "Epoch [60/100], Loss: 3424.8128\n",
            "Epoch [70/100], Loss: 2130.6632\n",
            "Epoch [80/100], Loss: 2530.3911\n",
            "Epoch [90/100], Loss: 1299.8662\n",
            "Epoch [100/100], Loss: 1333.8642\n",
            "Fold 3, RMSE: 92.5811996459961\n",
            "Epoch [10/100], Loss: 13069.8428\n",
            "Epoch [20/100], Loss: 11198.6665\n",
            "Epoch [30/100], Loss: 7002.7872\n",
            "Epoch [40/100], Loss: 4400.8251\n",
            "Epoch [50/100], Loss: 2627.8658\n",
            "Epoch [60/100], Loss: 5242.1805\n",
            "Epoch [70/100], Loss: 1239.2749\n",
            "Epoch [80/100], Loss: 1868.0576\n",
            "Epoch [90/100], Loss: 1604.2418\n",
            "Epoch [100/100], Loss: 1908.8406\n",
            "Fold 4, RMSE: 39.73286056518555\n",
            "Epoch [10/100], Loss: 26727.1416\n",
            "Epoch [20/100], Loss: 11603.4248\n",
            "Epoch [30/100], Loss: 15811.8584\n",
            "Epoch [40/100], Loss: 7860.0936\n",
            "Epoch [50/100], Loss: 7810.7938\n",
            "Epoch [60/100], Loss: 6100.2023\n",
            "Epoch [70/100], Loss: 5066.3938\n",
            "Epoch [80/100], Loss: 1554.0172\n",
            "Epoch [90/100], Loss: 1761.9739\n",
            "Epoch [100/100], Loss: 1253.6244\n",
            "Fold 5, RMSE: 46.2026481628418\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 60.24708023071289\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 31251.5488\n",
            "Epoch [20/150], Loss: 10529.4392\n",
            "Epoch [30/150], Loss: 7007.2544\n",
            "Epoch [40/150], Loss: 6401.8972\n",
            "Epoch [50/150], Loss: 3174.0889\n",
            "Epoch [60/150], Loss: 2764.0659\n",
            "Epoch [70/150], Loss: 2813.7773\n",
            "Epoch [80/150], Loss: 3434.4808\n",
            "Epoch [90/150], Loss: 1222.0415\n",
            "Epoch [100/150], Loss: 1705.4007\n",
            "Epoch [110/150], Loss: 1228.5447\n",
            "Epoch [120/150], Loss: 811.0430\n",
            "Epoch [130/150], Loss: 1873.3076\n",
            "Epoch [140/150], Loss: 1429.6210\n",
            "Epoch [150/150], Loss: 1150.7956\n",
            "Fold 1, RMSE: 59.697696685791016\n",
            "Epoch [10/150], Loss: 17765.2095\n",
            "Epoch [20/150], Loss: 12876.9736\n",
            "Epoch [30/150], Loss: 8202.4056\n",
            "Epoch [40/150], Loss: 12970.1953\n",
            "Epoch [50/150], Loss: 3777.0817\n",
            "Epoch [60/150], Loss: 1939.9226\n",
            "Epoch [70/150], Loss: 3151.4287\n",
            "Epoch [80/150], Loss: 3650.3006\n",
            "Epoch [90/150], Loss: 1972.7744\n",
            "Epoch [100/150], Loss: 1262.7999\n",
            "Epoch [110/150], Loss: 1375.9645\n",
            "Epoch [120/150], Loss: 1515.7280\n",
            "Epoch [130/150], Loss: 1320.4747\n",
            "Epoch [140/150], Loss: 907.2077\n",
            "Epoch [150/150], Loss: 2203.6481\n",
            "Fold 2, RMSE: 60.03504943847656\n",
            "Epoch [10/150], Loss: 21367.4985\n",
            "Epoch [20/150], Loss: 5134.3533\n",
            "Epoch [30/150], Loss: 5375.4270\n",
            "Epoch [40/150], Loss: 4596.4875\n",
            "Epoch [50/150], Loss: 2306.3633\n",
            "Epoch [60/150], Loss: 1741.4570\n",
            "Epoch [70/150], Loss: 2087.9809\n",
            "Epoch [80/150], Loss: 3313.1394\n",
            "Epoch [90/150], Loss: 1698.8940\n",
            "Epoch [100/150], Loss: 2700.4869\n",
            "Epoch [110/150], Loss: 1515.7630\n",
            "Epoch [120/150], Loss: 1775.5529\n",
            "Epoch [130/150], Loss: 1939.1298\n",
            "Epoch [140/150], Loss: 965.7738\n",
            "Epoch [150/150], Loss: 682.6067\n",
            "Fold 3, RMSE: 92.84371948242188\n",
            "Epoch [10/150], Loss: 23810.8159\n",
            "Epoch [20/150], Loss: 10526.8662\n",
            "Epoch [30/150], Loss: 6681.0896\n",
            "Epoch [40/150], Loss: 3811.9566\n",
            "Epoch [50/150], Loss: 6281.8721\n",
            "Epoch [60/150], Loss: 2807.2639\n",
            "Epoch [70/150], Loss: 1671.6342\n",
            "Epoch [80/150], Loss: 2998.6212\n",
            "Epoch [90/150], Loss: 1866.0717\n",
            "Epoch [100/150], Loss: 6710.5402\n",
            "Epoch [110/150], Loss: 4153.6787\n",
            "Epoch [120/150], Loss: 954.7516\n",
            "Epoch [130/150], Loss: 2043.7776\n",
            "Epoch [140/150], Loss: 781.1047\n",
            "Epoch [150/150], Loss: 1337.5131\n",
            "Fold 4, RMSE: 40.172176361083984\n",
            "Epoch [10/150], Loss: 40109.5249\n",
            "Epoch [20/150], Loss: 13552.8462\n",
            "Epoch [30/150], Loss: 8169.7651\n",
            "Epoch [40/150], Loss: 5361.6957\n",
            "Epoch [50/150], Loss: 4388.0292\n",
            "Epoch [60/150], Loss: 3396.5454\n",
            "Epoch [70/150], Loss: 4946.0725\n",
            "Epoch [80/150], Loss: 4362.8488\n",
            "Epoch [90/150], Loss: 1796.8649\n",
            "Epoch [100/150], Loss: 3667.6797\n",
            "Epoch [110/150], Loss: 1566.4799\n",
            "Epoch [120/150], Loss: 2990.8978\n",
            "Epoch [130/150], Loss: 1226.1810\n",
            "Epoch [140/150], Loss: 3227.4528\n",
            "Epoch [150/150], Loss: 2094.6518\n",
            "Fold 5, RMSE: 47.57286834716797\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 60.06430206298828\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 10230.1603\n",
            "Epoch [20/100], Loss: 11099.0190\n",
            "Epoch [30/100], Loss: 3517.3467\n",
            "Epoch [40/100], Loss: 1433.0575\n",
            "Epoch [50/100], Loss: 1115.3439\n",
            "Epoch [60/100], Loss: 2492.8078\n",
            "Epoch [70/100], Loss: 2218.6987\n",
            "Epoch [80/100], Loss: 1269.4808\n",
            "Epoch [90/100], Loss: 1224.0539\n",
            "Epoch [100/100], Loss: 1129.3093\n",
            "Fold 1, RMSE: 54.96604919433594\n",
            "Epoch [10/100], Loss: 9928.4280\n",
            "Epoch [20/100], Loss: 8033.2537\n",
            "Epoch [30/100], Loss: 4960.5421\n",
            "Epoch [40/100], Loss: 3506.0937\n",
            "Epoch [50/100], Loss: 2116.8739\n",
            "Epoch [60/100], Loss: 1589.4994\n",
            "Epoch [70/100], Loss: 3719.9488\n",
            "Epoch [80/100], Loss: 1202.4637\n",
            "Epoch [90/100], Loss: 1225.5680\n",
            "Epoch [100/100], Loss: 807.4034\n",
            "Fold 2, RMSE: 62.0297737121582\n",
            "Epoch [10/100], Loss: 8014.4333\n",
            "Epoch [20/100], Loss: 5702.8368\n",
            "Epoch [30/100], Loss: 5280.3199\n",
            "Epoch [40/100], Loss: 3104.4541\n",
            "Epoch [50/100], Loss: 1354.7924\n",
            "Epoch [60/100], Loss: 1758.9973\n",
            "Epoch [70/100], Loss: 1380.7236\n",
            "Epoch [80/100], Loss: 2366.0778\n",
            "Epoch [90/100], Loss: 2127.1945\n",
            "Epoch [100/100], Loss: 1072.0445\n",
            "Fold 3, RMSE: 93.4063720703125\n",
            "Epoch [10/100], Loss: 10551.1909\n",
            "Epoch [20/100], Loss: 6443.2990\n",
            "Epoch [30/100], Loss: 3541.7518\n",
            "Epoch [40/100], Loss: 3642.3023\n",
            "Epoch [50/100], Loss: 3235.7954\n",
            "Epoch [60/100], Loss: 3568.5134\n",
            "Epoch [70/100], Loss: 2267.4408\n",
            "Epoch [80/100], Loss: 1494.1761\n",
            "Epoch [90/100], Loss: 2044.9846\n",
            "Epoch [100/100], Loss: 2097.4268\n",
            "Fold 4, RMSE: 40.76282501220703\n",
            "Epoch [10/100], Loss: 10701.2421\n",
            "Epoch [20/100], Loss: 11273.9619\n",
            "Epoch [30/100], Loss: 4915.7242\n",
            "Epoch [40/100], Loss: 2175.8557\n",
            "Epoch [50/100], Loss: 1606.7436\n",
            "Epoch [60/100], Loss: 983.8427\n",
            "Epoch [70/100], Loss: 1961.9439\n",
            "Epoch [80/100], Loss: 1070.5038\n",
            "Epoch [90/100], Loss: 3729.1107\n",
            "Epoch [100/100], Loss: 1902.1096\n",
            "Fold 5, RMSE: 44.67509078979492\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 59.16802215576172\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 22627.2554\n",
            "Epoch [20/150], Loss: 11276.7231\n",
            "Epoch [30/150], Loss: 4283.4420\n",
            "Epoch [40/150], Loss: 2525.9647\n",
            "Epoch [50/150], Loss: 1753.1541\n",
            "Epoch [60/150], Loss: 1598.9203\n",
            "Epoch [70/150], Loss: 2272.6969\n",
            "Epoch [80/150], Loss: 1527.0103\n",
            "Epoch [90/150], Loss: 1059.8738\n",
            "Epoch [100/150], Loss: 1621.8668\n",
            "Epoch [110/150], Loss: 1225.9584\n",
            "Epoch [120/150], Loss: 996.4488\n",
            "Epoch [130/150], Loss: 1573.4417\n",
            "Epoch [140/150], Loss: 2018.7219\n",
            "Epoch [150/150], Loss: 581.3180\n",
            "Fold 1, RMSE: 54.67689895629883\n",
            "Epoch [10/150], Loss: 13393.7595\n",
            "Epoch [20/150], Loss: 5789.9158\n",
            "Epoch [30/150], Loss: 3720.5349\n",
            "Epoch [40/150], Loss: 3352.5267\n",
            "Epoch [50/150], Loss: 1023.0706\n",
            "Epoch [60/150], Loss: 2355.6625\n",
            "Epoch [70/150], Loss: 3761.5259\n",
            "Epoch [80/150], Loss: 2677.3867\n",
            "Epoch [90/150], Loss: 2570.4125\n",
            "Epoch [100/150], Loss: 2850.5436\n",
            "Epoch [110/150], Loss: 1781.3040\n",
            "Epoch [120/150], Loss: 1776.1099\n",
            "Epoch [130/150], Loss: 3009.8260\n",
            "Epoch [140/150], Loss: 1526.8750\n",
            "Epoch [150/150], Loss: 1159.4169\n",
            "Fold 2, RMSE: 63.13289260864258\n",
            "Epoch [10/150], Loss: 11079.9238\n",
            "Epoch [20/150], Loss: 5535.4771\n",
            "Epoch [30/150], Loss: 3920.6077\n",
            "Epoch [40/150], Loss: 2765.5764\n",
            "Epoch [50/150], Loss: 2297.7902\n",
            "Epoch [60/150], Loss: 2197.7008\n",
            "Epoch [70/150], Loss: 1212.1519\n",
            "Epoch [80/150], Loss: 2783.6277\n",
            "Epoch [90/150], Loss: 1718.2469\n",
            "Epoch [100/150], Loss: 1584.5239\n",
            "Epoch [110/150], Loss: 1128.5637\n",
            "Epoch [120/150], Loss: 1969.7566\n",
            "Epoch [130/150], Loss: 1550.4679\n",
            "Epoch [140/150], Loss: 1854.4842\n",
            "Epoch [150/150], Loss: 916.9778\n",
            "Fold 3, RMSE: 92.6141586303711\n",
            "Epoch [10/150], Loss: 11416.7865\n",
            "Epoch [20/150], Loss: 13648.5693\n",
            "Epoch [30/150], Loss: 4763.1939\n",
            "Epoch [40/150], Loss: 3385.3812\n",
            "Epoch [50/150], Loss: 2212.0590\n",
            "Epoch [60/150], Loss: 1769.4007\n",
            "Epoch [70/150], Loss: 2904.4225\n",
            "Epoch [80/150], Loss: 2910.6204\n",
            "Epoch [90/150], Loss: 2905.6541\n",
            "Epoch [100/150], Loss: 2370.3405\n",
            "Epoch [110/150], Loss: 1951.2936\n",
            "Epoch [120/150], Loss: 1118.9491\n",
            "Epoch [130/150], Loss: 961.1255\n",
            "Epoch [140/150], Loss: 1197.8222\n",
            "Epoch [150/150], Loss: 897.3453\n",
            "Fold 4, RMSE: 39.822425842285156\n",
            "Epoch [10/150], Loss: 20021.9331\n",
            "Epoch [20/150], Loss: 9676.7214\n",
            "Epoch [30/150], Loss: 11587.9906\n",
            "Epoch [40/150], Loss: 3420.7772\n",
            "Epoch [50/150], Loss: 1596.5755\n",
            "Epoch [60/150], Loss: 2260.9087\n",
            "Epoch [70/150], Loss: 1033.1015\n",
            "Epoch [80/150], Loss: 660.0839\n",
            "Epoch [90/150], Loss: 4993.2358\n",
            "Epoch [100/150], Loss: 1016.5521\n",
            "Epoch [110/150], Loss: 1843.1048\n",
            "Epoch [120/150], Loss: 3932.2021\n",
            "Epoch [130/150], Loss: 1093.2364\n",
            "Epoch [140/150], Loss: 3256.7342\n",
            "Epoch [150/150], Loss: 1643.7105\n",
            "Fold 5, RMSE: 45.89646911621094\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 59.22856903076172\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 22136.4917\n",
            "Epoch [20/100], Loss: 9914.5674\n",
            "Epoch [30/100], Loss: 9649.1528\n",
            "Epoch [40/100], Loss: 7707.0233\n",
            "Epoch [50/100], Loss: 3824.5378\n",
            "Epoch [60/100], Loss: 1264.1970\n",
            "Epoch [70/100], Loss: 2845.4368\n",
            "Epoch [80/100], Loss: 2671.8529\n",
            "Epoch [90/100], Loss: 1532.3312\n",
            "Epoch [100/100], Loss: 5324.7909\n",
            "Fold 1, RMSE: 65.83135223388672\n",
            "Epoch [10/100], Loss: 19924.6218\n",
            "Epoch [20/100], Loss: 8052.8933\n",
            "Epoch [30/100], Loss: 7892.5813\n",
            "Epoch [40/100], Loss: 3983.1248\n",
            "Epoch [50/100], Loss: 1968.5131\n",
            "Epoch [60/100], Loss: 2004.8210\n",
            "Epoch [70/100], Loss: 1570.7220\n",
            "Epoch [80/100], Loss: 3212.6888\n",
            "Epoch [90/100], Loss: 2029.7230\n",
            "Epoch [100/100], Loss: 2054.4421\n",
            "Fold 2, RMSE: 61.50863265991211\n",
            "Epoch [10/100], Loss: 22365.2783\n",
            "Epoch [20/100], Loss: 8950.6941\n",
            "Epoch [30/100], Loss: 8799.2068\n",
            "Epoch [40/100], Loss: 7236.0052\n",
            "Epoch [50/100], Loss: 5606.6212\n",
            "Epoch [60/100], Loss: 3531.7098\n",
            "Epoch [70/100], Loss: 6869.6143\n",
            "Epoch [80/100], Loss: 1696.9706\n",
            "Epoch [90/100], Loss: 1605.1046\n",
            "Epoch [100/100], Loss: 1482.5305\n",
            "Fold 3, RMSE: 93.54811096191406\n",
            "Epoch [10/100], Loss: 15410.3442\n",
            "Epoch [20/100], Loss: 11425.0825\n",
            "Epoch [30/100], Loss: 6855.7402\n",
            "Epoch [40/100], Loss: 4500.4950\n",
            "Epoch [50/100], Loss: 2738.3801\n",
            "Epoch [60/100], Loss: 2486.5298\n",
            "Epoch [70/100], Loss: 1992.5282\n",
            "Epoch [80/100], Loss: 5210.3876\n",
            "Epoch [90/100], Loss: 2697.0847\n",
            "Epoch [100/100], Loss: 2916.8488\n",
            "Fold 4, RMSE: 41.826393127441406\n",
            "Epoch [10/100], Loss: 18899.4114\n",
            "Epoch [20/100], Loss: 10232.0006\n",
            "Epoch [30/100], Loss: 6722.2018\n",
            "Epoch [40/100], Loss: 5923.4659\n",
            "Epoch [50/100], Loss: 3821.2014\n",
            "Epoch [60/100], Loss: 3183.9825\n",
            "Epoch [70/100], Loss: 3023.7544\n",
            "Epoch [80/100], Loss: 4648.3420\n",
            "Epoch [90/100], Loss: 1324.1554\n",
            "Epoch [100/100], Loss: 2440.6629\n",
            "Fold 5, RMSE: 48.315025329589844\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 62.205902862548825\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 42286.0859\n",
            "Epoch [20/150], Loss: 10376.4587\n",
            "Epoch [30/150], Loss: 6816.1786\n",
            "Epoch [40/150], Loss: 6401.7612\n",
            "Epoch [50/150], Loss: 4142.9119\n",
            "Epoch [60/150], Loss: 2761.5685\n",
            "Epoch [70/150], Loss: 2514.9985\n",
            "Epoch [80/150], Loss: 3217.2625\n",
            "Epoch [90/150], Loss: 3579.9213\n",
            "Epoch [100/150], Loss: 1539.8984\n",
            "Epoch [110/150], Loss: 2834.1177\n",
            "Epoch [120/150], Loss: 1986.2717\n",
            "Epoch [130/150], Loss: 1701.2233\n",
            "Epoch [140/150], Loss: 2332.9312\n",
            "Epoch [150/150], Loss: 2583.2373\n",
            "Fold 1, RMSE: 63.66983413696289\n",
            "Epoch [10/150], Loss: 17400.3965\n",
            "Epoch [20/150], Loss: 9790.5288\n",
            "Epoch [30/150], Loss: 5284.3585\n",
            "Epoch [40/150], Loss: 5873.5341\n",
            "Epoch [50/150], Loss: 3559.0744\n",
            "Epoch [60/150], Loss: 2835.6095\n",
            "Epoch [70/150], Loss: 1776.7267\n",
            "Epoch [80/150], Loss: 1107.1055\n",
            "Epoch [90/150], Loss: 1110.9096\n",
            "Epoch [100/150], Loss: 1665.3033\n",
            "Epoch [110/150], Loss: 1317.7849\n",
            "Epoch [120/150], Loss: 1806.1858\n",
            "Epoch [130/150], Loss: 2713.3434\n",
            "Epoch [140/150], Loss: 3403.6778\n",
            "Epoch [150/150], Loss: 1297.9286\n",
            "Fold 2, RMSE: 62.4842414855957\n",
            "Epoch [10/150], Loss: 12261.8657\n",
            "Epoch [20/150], Loss: 6800.5973\n",
            "Epoch [30/150], Loss: 4316.4062\n",
            "Epoch [40/150], Loss: 2986.2369\n",
            "Epoch [50/150], Loss: 2048.5198\n",
            "Epoch [60/150], Loss: 1696.7971\n",
            "Epoch [70/150], Loss: 1165.0666\n",
            "Epoch [80/150], Loss: 1576.8387\n",
            "Epoch [90/150], Loss: 2417.5019\n",
            "Epoch [100/150], Loss: 3514.7808\n",
            "Epoch [110/150], Loss: 3331.2448\n",
            "Epoch [120/150], Loss: 2035.2871\n",
            "Epoch [130/150], Loss: 1124.2565\n",
            "Epoch [140/150], Loss: 1118.2402\n",
            "Epoch [150/150], Loss: 1070.9037\n",
            "Fold 3, RMSE: 94.17032623291016\n",
            "Epoch [10/150], Loss: 17808.7026\n",
            "Epoch [20/150], Loss: 11589.3950\n",
            "Epoch [30/150], Loss: 5308.0752\n",
            "Epoch [40/150], Loss: 6069.1432\n",
            "Epoch [50/150], Loss: 3556.4532\n",
            "Epoch [60/150], Loss: 5822.3855\n",
            "Epoch [70/150], Loss: 3693.1771\n",
            "Epoch [80/150], Loss: 2105.5406\n",
            "Epoch [90/150], Loss: 4642.9496\n",
            "Epoch [100/150], Loss: 2608.4984\n",
            "Epoch [110/150], Loss: 862.3903\n",
            "Epoch [120/150], Loss: 1818.8567\n",
            "Epoch [130/150], Loss: 1211.3319\n",
            "Epoch [140/150], Loss: 3752.6174\n",
            "Epoch [150/150], Loss: 3190.0182\n",
            "Fold 4, RMSE: 41.74468231201172\n",
            "Epoch [10/150], Loss: 12328.2717\n",
            "Epoch [20/150], Loss: 9835.6626\n",
            "Epoch [30/150], Loss: 7260.3253\n",
            "Epoch [40/150], Loss: 4218.9145\n",
            "Epoch [50/150], Loss: 4733.9924\n",
            "Epoch [60/150], Loss: 2005.9850\n",
            "Epoch [70/150], Loss: 2249.4916\n",
            "Epoch [80/150], Loss: 5176.5625\n",
            "Epoch [90/150], Loss: 1326.8413\n",
            "Epoch [100/150], Loss: 1802.2527\n",
            "Epoch [110/150], Loss: 4230.2739\n",
            "Epoch [120/150], Loss: 1751.2342\n",
            "Epoch [130/150], Loss: 5145.7471\n",
            "Epoch [140/150], Loss: 2382.4057\n",
            "Epoch [150/150], Loss: 1375.3042\n",
            "Fold 5, RMSE: 46.438289642333984\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 61.70147476196289\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16200.1912\n",
            "Epoch [20/100], Loss: 6720.1271\n",
            "Epoch [30/100], Loss: 7430.0464\n",
            "Epoch [40/100], Loss: 3524.9388\n",
            "Epoch [50/100], Loss: 2128.6476\n",
            "Epoch [60/100], Loss: 1484.9832\n",
            "Epoch [70/100], Loss: 947.3246\n",
            "Epoch [80/100], Loss: 1523.3314\n",
            "Epoch [90/100], Loss: 1550.4331\n",
            "Epoch [100/100], Loss: 770.0268\n",
            "Fold 1, RMSE: 58.14494323730469\n",
            "Epoch [10/100], Loss: 16384.2483\n",
            "Epoch [20/100], Loss: 12518.2561\n",
            "Epoch [30/100], Loss: 7593.2740\n",
            "Epoch [40/100], Loss: 3008.0084\n",
            "Epoch [50/100], Loss: 2740.0278\n",
            "Epoch [60/100], Loss: 3164.8879\n",
            "Epoch [70/100], Loss: 2130.7718\n",
            "Epoch [80/100], Loss: 1336.5273\n",
            "Epoch [90/100], Loss: 1184.1822\n",
            "Epoch [100/100], Loss: 2503.6128\n",
            "Fold 2, RMSE: 59.84595489501953\n",
            "Epoch [10/100], Loss: 8685.4454\n",
            "Epoch [20/100], Loss: 6912.9630\n",
            "Epoch [30/100], Loss: 3564.0574\n",
            "Epoch [40/100], Loss: 3449.3772\n",
            "Epoch [50/100], Loss: 4331.5225\n",
            "Epoch [60/100], Loss: 2335.1424\n",
            "Epoch [70/100], Loss: 1443.6817\n",
            "Epoch [80/100], Loss: 1089.0272\n",
            "Epoch [90/100], Loss: 1781.4220\n",
            "Epoch [100/100], Loss: 788.0493\n",
            "Fold 3, RMSE: 96.26700592041016\n",
            "Epoch [10/100], Loss: 22211.2266\n",
            "Epoch [20/100], Loss: 10210.1467\n",
            "Epoch [30/100], Loss: 9053.5778\n",
            "Epoch [40/100], Loss: 3401.1074\n",
            "Epoch [50/100], Loss: 3254.2245\n",
            "Epoch [60/100], Loss: 1660.2955\n",
            "Epoch [70/100], Loss: 1820.8153\n",
            "Epoch [80/100], Loss: 1294.7158\n",
            "Epoch [90/100], Loss: 1218.2791\n",
            "Epoch [100/100], Loss: 2349.0963\n",
            "Fold 4, RMSE: 41.108341217041016\n",
            "Epoch [10/100], Loss: 12064.5161\n",
            "Epoch [20/100], Loss: 5569.7937\n",
            "Epoch [30/100], Loss: 4034.3898\n",
            "Epoch [40/100], Loss: 3430.2592\n",
            "Epoch [50/100], Loss: 2592.7720\n",
            "Epoch [60/100], Loss: 1657.8453\n",
            "Epoch [70/100], Loss: 1773.1226\n",
            "Epoch [80/100], Loss: 1861.7563\n",
            "Epoch [90/100], Loss: 1588.7561\n",
            "Epoch [100/100], Loss: 1630.9838\n",
            "Fold 5, RMSE: 51.53965759277344\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 61.38118057250976\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 21721.6562\n",
            "Epoch [20/150], Loss: 8219.9763\n",
            "Epoch [30/150], Loss: 4512.0580\n",
            "Epoch [40/150], Loss: 4180.3359\n",
            "Epoch [50/150], Loss: 2045.3583\n",
            "Epoch [60/150], Loss: 2412.8538\n",
            "Epoch [70/150], Loss: 2808.8856\n",
            "Epoch [80/150], Loss: 2093.3080\n",
            "Epoch [90/150], Loss: 2468.4098\n",
            "Epoch [100/150], Loss: 2597.6747\n",
            "Epoch [110/150], Loss: 1694.2545\n",
            "Epoch [120/150], Loss: 3474.6853\n",
            "Epoch [130/150], Loss: 1686.9164\n",
            "Epoch [140/150], Loss: 641.9352\n",
            "Epoch [150/150], Loss: 2394.1467\n",
            "Fold 1, RMSE: 56.65190887451172\n",
            "Epoch [10/150], Loss: 10224.7478\n",
            "Epoch [20/150], Loss: 7620.9432\n",
            "Epoch [30/150], Loss: 6613.0933\n",
            "Epoch [40/150], Loss: 3687.4655\n",
            "Epoch [50/150], Loss: 1801.6160\n",
            "Epoch [60/150], Loss: 4149.4606\n",
            "Epoch [70/150], Loss: 1351.4104\n",
            "Epoch [80/150], Loss: 1777.3432\n",
            "Epoch [90/150], Loss: 2539.4998\n",
            "Epoch [100/150], Loss: 1033.6723\n",
            "Epoch [110/150], Loss: 1254.3909\n",
            "Epoch [120/150], Loss: 608.9811\n",
            "Epoch [130/150], Loss: 5543.3563\n",
            "Epoch [140/150], Loss: 1397.1460\n",
            "Epoch [150/150], Loss: 1048.5778\n",
            "Fold 2, RMSE: 63.34724044799805\n",
            "Epoch [10/150], Loss: 12051.0254\n",
            "Epoch [20/150], Loss: 6930.1293\n",
            "Epoch [30/150], Loss: 5038.5087\n",
            "Epoch [40/150], Loss: 2624.8217\n",
            "Epoch [50/150], Loss: 3316.2198\n",
            "Epoch [60/150], Loss: 2833.4106\n",
            "Epoch [70/150], Loss: 2132.4050\n",
            "Epoch [80/150], Loss: 529.3748\n",
            "Epoch [90/150], Loss: 1665.8162\n",
            "Epoch [100/150], Loss: 976.7052\n",
            "Epoch [110/150], Loss: 518.4239\n",
            "Epoch [120/150], Loss: 1840.4666\n",
            "Epoch [130/150], Loss: 723.0200\n",
            "Epoch [140/150], Loss: 393.3454\n",
            "Epoch [150/150], Loss: 1233.5238\n",
            "Fold 3, RMSE: 91.82740020751953\n",
            "Epoch [10/150], Loss: 12428.8102\n",
            "Epoch [20/150], Loss: 7224.9493\n",
            "Epoch [30/150], Loss: 5125.6880\n",
            "Epoch [40/150], Loss: 3714.9372\n",
            "Epoch [50/150], Loss: 1085.9102\n",
            "Epoch [60/150], Loss: 4911.0154\n",
            "Epoch [70/150], Loss: 1348.3083\n",
            "Epoch [80/150], Loss: 1408.3835\n",
            "Epoch [90/150], Loss: 2050.7283\n",
            "Epoch [100/150], Loss: 950.6317\n",
            "Epoch [110/150], Loss: 1641.6375\n",
            "Epoch [120/150], Loss: 1857.8208\n",
            "Epoch [130/150], Loss: 1364.2421\n",
            "Epoch [140/150], Loss: 1019.8560\n",
            "Epoch [150/150], Loss: 669.1801\n",
            "Fold 4, RMSE: 40.04511260986328\n",
            "Epoch [10/150], Loss: 12228.2480\n",
            "Epoch [20/150], Loss: 7625.0658\n",
            "Epoch [30/150], Loss: 3903.9590\n",
            "Epoch [40/150], Loss: 3174.2139\n",
            "Epoch [50/150], Loss: 2111.2369\n",
            "Epoch [60/150], Loss: 1812.1992\n",
            "Epoch [70/150], Loss: 1563.7541\n",
            "Epoch [80/150], Loss: 5183.8303\n",
            "Epoch [90/150], Loss: 2149.8332\n",
            "Epoch [100/150], Loss: 1904.6366\n",
            "Epoch [110/150], Loss: 2050.7663\n",
            "Epoch [120/150], Loss: 1400.3098\n",
            "Epoch [130/150], Loss: 1739.1397\n",
            "Epoch [140/150], Loss: 852.4717\n",
            "Epoch [150/150], Loss: 3412.1304\n",
            "Fold 5, RMSE: 45.07733917236328\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 59.389800262451175\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 51755.8721\n",
            "Epoch [20/100], Loss: 11882.2693\n",
            "Epoch [30/100], Loss: 11404.2938\n",
            "Epoch [40/100], Loss: 9450.5168\n",
            "Epoch [50/100], Loss: 17119.0525\n",
            "Epoch [60/100], Loss: 3470.2551\n",
            "Epoch [70/100], Loss: 2930.2676\n",
            "Epoch [80/100], Loss: 2127.2560\n",
            "Epoch [90/100], Loss: 1768.7738\n",
            "Epoch [100/100], Loss: 1866.8661\n",
            "Fold 1, RMSE: 58.937400817871094\n",
            "Epoch [10/100], Loss: 26163.5117\n",
            "Epoch [20/100], Loss: 11565.6544\n",
            "Epoch [30/100], Loss: 10645.0601\n",
            "Epoch [40/100], Loss: 8616.6826\n",
            "Epoch [50/100], Loss: 6384.3397\n",
            "Epoch [60/100], Loss: 5890.8202\n",
            "Epoch [70/100], Loss: 2505.7053\n",
            "Epoch [80/100], Loss: 3288.6857\n",
            "Epoch [90/100], Loss: 4153.4291\n",
            "Epoch [100/100], Loss: 976.1656\n",
            "Fold 2, RMSE: 65.90242004394531\n",
            "Epoch [10/100], Loss: 53442.2988\n",
            "Epoch [20/100], Loss: 9736.5710\n",
            "Epoch [30/100], Loss: 4999.5305\n",
            "Epoch [40/100], Loss: 4426.4565\n",
            "Epoch [50/100], Loss: 3410.0142\n",
            "Epoch [60/100], Loss: 8246.6197\n",
            "Epoch [70/100], Loss: 4783.9672\n",
            "Epoch [80/100], Loss: 2721.8630\n",
            "Epoch [90/100], Loss: 1571.4209\n",
            "Epoch [100/100], Loss: 1097.2484\n",
            "Fold 3, RMSE: 96.0032730102539\n",
            "Epoch [10/100], Loss: 15744.8660\n",
            "Epoch [20/100], Loss: 14946.3687\n",
            "Epoch [30/100], Loss: 9809.2902\n",
            "Epoch [40/100], Loss: 3770.2687\n",
            "Epoch [50/100], Loss: 4997.2670\n",
            "Epoch [60/100], Loss: 3633.2411\n",
            "Epoch [70/100], Loss: 6291.5266\n",
            "Epoch [80/100], Loss: 2968.3273\n",
            "Epoch [90/100], Loss: 1834.3776\n",
            "Epoch [100/100], Loss: 2073.0900\n",
            "Fold 4, RMSE: 38.99870300292969\n",
            "Epoch [10/100], Loss: 73897.9375\n",
            "Epoch [20/100], Loss: 17682.4792\n",
            "Epoch [30/100], Loss: 7957.2928\n",
            "Epoch [40/100], Loss: 7379.8087\n",
            "Epoch [50/100], Loss: 7574.0046\n",
            "Epoch [60/100], Loss: 3503.0798\n",
            "Epoch [70/100], Loss: 1731.7082\n",
            "Epoch [80/100], Loss: 1466.7549\n",
            "Epoch [90/100], Loss: 1978.7416\n",
            "Epoch [100/100], Loss: 4901.0444\n",
            "Fold 5, RMSE: 51.87636184692383\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 62.343631744384766\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 57728.3809\n",
            "Epoch [20/150], Loss: 9543.3135\n",
            "Epoch [30/150], Loss: 6674.4470\n",
            "Epoch [40/150], Loss: 3640.5222\n",
            "Epoch [50/150], Loss: 7619.1033\n",
            "Epoch [60/150], Loss: 3068.0576\n",
            "Epoch [70/150], Loss: 3847.8010\n",
            "Epoch [80/150], Loss: 1599.2351\n",
            "Epoch [90/150], Loss: 3010.8023\n",
            "Epoch [100/150], Loss: 3046.8022\n",
            "Epoch [110/150], Loss: 1714.8542\n",
            "Epoch [120/150], Loss: 1958.1615\n",
            "Epoch [130/150], Loss: 2784.4698\n",
            "Epoch [140/150], Loss: 2081.4190\n",
            "Epoch [150/150], Loss: 1825.2584\n",
            "Fold 1, RMSE: 59.477996826171875\n",
            "Epoch [10/150], Loss: 61895.7070\n",
            "Epoch [20/150], Loss: 16808.7737\n",
            "Epoch [30/150], Loss: 10849.8529\n",
            "Epoch [40/150], Loss: 15352.5496\n",
            "Epoch [50/150], Loss: 7521.8546\n",
            "Epoch [60/150], Loss: 5581.9309\n",
            "Epoch [70/150], Loss: 5289.2401\n",
            "Epoch [80/150], Loss: 3049.8528\n",
            "Epoch [90/150], Loss: 3274.9953\n",
            "Epoch [100/150], Loss: 2835.6523\n",
            "Epoch [110/150], Loss: 1450.0294\n",
            "Epoch [120/150], Loss: 2664.1858\n",
            "Epoch [130/150], Loss: 1435.0810\n",
            "Epoch [140/150], Loss: 2211.4714\n",
            "Epoch [150/150], Loss: 1460.4382\n",
            "Fold 2, RMSE: 62.01158142089844\n",
            "Epoch [10/150], Loss: 42165.1465\n",
            "Epoch [20/150], Loss: 9778.2955\n",
            "Epoch [30/150], Loss: 10393.8269\n",
            "Epoch [40/150], Loss: 3429.0329\n",
            "Epoch [50/150], Loss: 2951.1140\n",
            "Epoch [60/150], Loss: 2791.4146\n",
            "Epoch [70/150], Loss: 2561.4305\n",
            "Epoch [80/150], Loss: 1309.4005\n",
            "Epoch [90/150], Loss: 5570.5672\n",
            "Epoch [100/150], Loss: 2090.4606\n",
            "Epoch [110/150], Loss: 964.8235\n",
            "Epoch [120/150], Loss: 1497.5533\n",
            "Epoch [130/150], Loss: 1126.5088\n",
            "Epoch [140/150], Loss: 3180.0359\n",
            "Epoch [150/150], Loss: 1989.1114\n",
            "Fold 3, RMSE: 90.05732727050781\n",
            "Epoch [10/150], Loss: 38336.9873\n",
            "Epoch [20/150], Loss: 14078.9695\n",
            "Epoch [30/150], Loss: 12580.2407\n",
            "Epoch [40/150], Loss: 9864.3215\n",
            "Epoch [50/150], Loss: 7784.8264\n",
            "Epoch [60/150], Loss: 5714.9862\n",
            "Epoch [70/150], Loss: 2661.9029\n",
            "Epoch [80/150], Loss: 5213.3792\n",
            "Epoch [90/150], Loss: 4478.0161\n",
            "Epoch [100/150], Loss: 1450.6429\n",
            "Epoch [110/150], Loss: 667.6988\n",
            "Epoch [120/150], Loss: 1791.5688\n",
            "Epoch [130/150], Loss: 1237.1772\n",
            "Epoch [140/150], Loss: 1906.6041\n",
            "Epoch [150/150], Loss: 1802.6112\n",
            "Fold 4, RMSE: 43.42945098876953\n",
            "Epoch [10/150], Loss: 33160.5935\n",
            "Epoch [20/150], Loss: 12334.6116\n",
            "Epoch [30/150], Loss: 9574.6150\n",
            "Epoch [40/150], Loss: 10383.0582\n",
            "Epoch [50/150], Loss: 6556.4181\n",
            "Epoch [60/150], Loss: 5982.0702\n",
            "Epoch [70/150], Loss: 3597.7424\n",
            "Epoch [80/150], Loss: 2491.6568\n",
            "Epoch [90/150], Loss: 3956.1212\n",
            "Epoch [100/150], Loss: 1991.7950\n",
            "Epoch [110/150], Loss: 2058.5971\n",
            "Epoch [120/150], Loss: 1484.4208\n",
            "Epoch [130/150], Loss: 3470.1888\n",
            "Epoch [140/150], Loss: 1538.8105\n",
            "Epoch [150/150], Loss: 2112.0386\n",
            "Fold 5, RMSE: 47.33073425292969\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 60.46141815185547\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 21825.5200\n",
            "Epoch [20/100], Loss: 11807.2435\n",
            "Epoch [30/100], Loss: 6888.7261\n",
            "Epoch [40/100], Loss: 3769.2332\n",
            "Epoch [50/100], Loss: 4089.9933\n",
            "Epoch [60/100], Loss: 2169.9333\n",
            "Epoch [70/100], Loss: 3760.4199\n",
            "Epoch [80/100], Loss: 1598.2347\n",
            "Epoch [90/100], Loss: 1607.7582\n",
            "Epoch [100/100], Loss: 2841.3081\n",
            "Fold 1, RMSE: 61.009098052978516\n",
            "Epoch [10/100], Loss: 23107.4824\n",
            "Epoch [20/100], Loss: 11164.5256\n",
            "Epoch [30/100], Loss: 9905.0568\n",
            "Epoch [40/100], Loss: 6504.3466\n",
            "Epoch [50/100], Loss: 6698.4257\n",
            "Epoch [60/100], Loss: 3921.2939\n",
            "Epoch [70/100], Loss: 2899.2884\n",
            "Epoch [80/100], Loss: 1862.6839\n",
            "Epoch [90/100], Loss: 3018.0360\n",
            "Epoch [100/100], Loss: 1662.0215\n",
            "Fold 2, RMSE: 61.43098449707031\n",
            "Epoch [10/100], Loss: 15954.3691\n",
            "Epoch [20/100], Loss: 6051.2322\n",
            "Epoch [30/100], Loss: 4765.2144\n",
            "Epoch [40/100], Loss: 4422.9500\n",
            "Epoch [50/100], Loss: 2290.2096\n",
            "Epoch [60/100], Loss: 2102.4703\n",
            "Epoch [70/100], Loss: 1325.3423\n",
            "Epoch [80/100], Loss: 1500.5611\n",
            "Epoch [90/100], Loss: 933.5938\n",
            "Epoch [100/100], Loss: 2985.9482\n",
            "Fold 3, RMSE: 90.98113250732422\n",
            "Epoch [10/100], Loss: 19732.7742\n",
            "Epoch [20/100], Loss: 11684.6550\n",
            "Epoch [30/100], Loss: 13695.0894\n",
            "Epoch [40/100], Loss: 3704.3270\n",
            "Epoch [50/100], Loss: 3524.2740\n",
            "Epoch [60/100], Loss: 3643.2099\n",
            "Epoch [70/100], Loss: 3000.1498\n",
            "Epoch [80/100], Loss: 1916.1863\n",
            "Epoch [90/100], Loss: 3321.8707\n",
            "Epoch [100/100], Loss: 2410.3088\n",
            "Fold 4, RMSE: 42.48975372314453\n",
            "Epoch [10/100], Loss: 13222.2834\n",
            "Epoch [20/100], Loss: 7078.8744\n",
            "Epoch [30/100], Loss: 5471.8540\n",
            "Epoch [40/100], Loss: 2634.6727\n",
            "Epoch [50/100], Loss: 5446.5646\n",
            "Epoch [60/100], Loss: 1164.3682\n",
            "Epoch [70/100], Loss: 1132.6851\n",
            "Epoch [80/100], Loss: 1805.4637\n",
            "Epoch [90/100], Loss: 1596.0806\n",
            "Epoch [100/100], Loss: 4237.1571\n",
            "Fold 5, RMSE: 48.78805923461914\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 60.939805603027345\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 11640.1600\n",
            "Epoch [20/150], Loss: 6440.4216\n",
            "Epoch [30/150], Loss: 4943.7997\n",
            "Epoch [40/150], Loss: 5803.6871\n",
            "Epoch [50/150], Loss: 2754.0906\n",
            "Epoch [60/150], Loss: 881.4196\n",
            "Epoch [70/150], Loss: 1632.2809\n",
            "Epoch [80/150], Loss: 4587.8943\n",
            "Epoch [90/150], Loss: 2102.8185\n",
            "Epoch [100/150], Loss: 2767.4852\n",
            "Epoch [110/150], Loss: 673.1137\n",
            "Epoch [120/150], Loss: 856.6298\n",
            "Epoch [130/150], Loss: 716.9720\n",
            "Epoch [140/150], Loss: 818.7381\n",
            "Epoch [150/150], Loss: 510.3386\n",
            "Fold 1, RMSE: 56.38985061645508\n",
            "Epoch [10/150], Loss: 19519.1362\n",
            "Epoch [20/150], Loss: 8116.0520\n",
            "Epoch [30/150], Loss: 7125.5737\n",
            "Epoch [40/150], Loss: 2673.8453\n",
            "Epoch [50/150], Loss: 5384.2397\n",
            "Epoch [60/150], Loss: 4043.7809\n",
            "Epoch [70/150], Loss: 3068.2191\n",
            "Epoch [80/150], Loss: 1985.9854\n",
            "Epoch [90/150], Loss: 2081.9052\n",
            "Epoch [100/150], Loss: 3879.7643\n",
            "Epoch [110/150], Loss: 2031.9222\n",
            "Epoch [120/150], Loss: 2241.3014\n",
            "Epoch [130/150], Loss: 1076.2817\n",
            "Epoch [140/150], Loss: 771.4975\n",
            "Epoch [150/150], Loss: 1412.8462\n",
            "Fold 2, RMSE: 72.53729248046875\n",
            "Epoch [10/150], Loss: 12953.3210\n",
            "Epoch [20/150], Loss: 9922.5896\n",
            "Epoch [30/150], Loss: 5402.5624\n",
            "Epoch [40/150], Loss: 3136.2546\n",
            "Epoch [50/150], Loss: 2414.7369\n",
            "Epoch [60/150], Loss: 1247.4764\n",
            "Epoch [70/150], Loss: 789.8300\n",
            "Epoch [80/150], Loss: 1987.9644\n",
            "Epoch [90/150], Loss: 944.9913\n",
            "Epoch [100/150], Loss: 1363.7310\n",
            "Epoch [110/150], Loss: 1885.3435\n",
            "Epoch [120/150], Loss: 3289.9128\n",
            "Epoch [130/150], Loss: 1267.7965\n",
            "Epoch [140/150], Loss: 2183.1013\n",
            "Epoch [150/150], Loss: 1398.7545\n",
            "Fold 3, RMSE: 91.19522094726562\n",
            "Epoch [10/150], Loss: 27169.5942\n",
            "Epoch [20/150], Loss: 13243.6482\n",
            "Epoch [30/150], Loss: 7610.4048\n",
            "Epoch [40/150], Loss: 5887.5818\n",
            "Epoch [50/150], Loss: 4932.7980\n",
            "Epoch [60/150], Loss: 2337.7567\n",
            "Epoch [70/150], Loss: 2969.3016\n",
            "Epoch [80/150], Loss: 2994.6558\n",
            "Epoch [90/150], Loss: 1666.1023\n",
            "Epoch [100/150], Loss: 1037.8061\n",
            "Epoch [110/150], Loss: 1560.6905\n",
            "Epoch [120/150], Loss: 2180.5253\n",
            "Epoch [130/150], Loss: 1398.9579\n",
            "Epoch [140/150], Loss: 907.8869\n",
            "Epoch [150/150], Loss: 1243.2184\n",
            "Fold 4, RMSE: 42.786075592041016\n",
            "Epoch [10/150], Loss: 25329.9453\n",
            "Epoch [20/150], Loss: 12289.4497\n",
            "Epoch [30/150], Loss: 9930.8378\n",
            "Epoch [40/150], Loss: 5046.0647\n",
            "Epoch [50/150], Loss: 5102.3990\n",
            "Epoch [60/150], Loss: 2812.2559\n",
            "Epoch [70/150], Loss: 2491.3033\n",
            "Epoch [80/150], Loss: 1570.0841\n",
            "Epoch [90/150], Loss: 2329.2646\n",
            "Epoch [100/150], Loss: 2648.8057\n",
            "Epoch [110/150], Loss: 2411.9946\n",
            "Epoch [120/150], Loss: 1773.0809\n",
            "Epoch [130/150], Loss: 1574.7432\n",
            "Epoch [140/150], Loss: 2215.9818\n",
            "Epoch [150/150], Loss: 1237.6306\n",
            "Fold 5, RMSE: 46.24684143066406\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 61.83105621337891\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 29921.2939\n",
            "Epoch [20/100], Loss: 10957.3396\n",
            "Epoch [30/100], Loss: 9050.2095\n",
            "Epoch [40/100], Loss: 4911.8360\n",
            "Epoch [50/100], Loss: 3140.2689\n",
            "Epoch [60/100], Loss: 3271.1132\n",
            "Epoch [70/100], Loss: 2066.1421\n",
            "Epoch [80/100], Loss: 2461.0634\n",
            "Epoch [90/100], Loss: 3525.3541\n",
            "Epoch [100/100], Loss: 1939.0619\n",
            "Fold 1, RMSE: 62.81394577026367\n",
            "Epoch [10/100], Loss: 45769.5908\n",
            "Epoch [20/100], Loss: 10875.3049\n",
            "Epoch [30/100], Loss: 7346.3700\n",
            "Epoch [40/100], Loss: 5382.6638\n",
            "Epoch [50/100], Loss: 3074.7364\n",
            "Epoch [60/100], Loss: 2949.1744\n",
            "Epoch [70/100], Loss: 2365.9031\n",
            "Epoch [80/100], Loss: 4919.7388\n",
            "Epoch [90/100], Loss: 2940.7266\n",
            "Epoch [100/100], Loss: 2497.2914\n",
            "Fold 2, RMSE: 62.389766693115234\n",
            "Epoch [10/100], Loss: 45102.5518\n",
            "Epoch [20/100], Loss: 14117.3386\n",
            "Epoch [30/100], Loss: 5522.0538\n",
            "Epoch [40/100], Loss: 5713.2087\n",
            "Epoch [50/100], Loss: 3905.0927\n",
            "Epoch [60/100], Loss: 1990.7249\n",
            "Epoch [70/100], Loss: 1768.9910\n",
            "Epoch [80/100], Loss: 1859.2649\n",
            "Epoch [90/100], Loss: 2217.7493\n",
            "Epoch [100/100], Loss: 3244.7922\n",
            "Fold 3, RMSE: 99.1683578491211\n",
            "Epoch [10/100], Loss: 25485.5747\n",
            "Epoch [20/100], Loss: 11957.9641\n",
            "Epoch [30/100], Loss: 8857.7393\n",
            "Epoch [40/100], Loss: 6984.4054\n",
            "Epoch [50/100], Loss: 6534.7370\n",
            "Epoch [60/100], Loss: 2396.5134\n",
            "Epoch [70/100], Loss: 3175.7158\n",
            "Epoch [80/100], Loss: 1697.1980\n",
            "Epoch [90/100], Loss: 2537.6619\n",
            "Epoch [100/100], Loss: 2701.1059\n",
            "Fold 4, RMSE: 40.40318298339844\n",
            "Epoch [10/100], Loss: 16317.3409\n",
            "Epoch [20/100], Loss: 10773.2026\n",
            "Epoch [30/100], Loss: 8415.3904\n",
            "Epoch [40/100], Loss: 6780.0106\n",
            "Epoch [50/100], Loss: 4026.4406\n",
            "Epoch [60/100], Loss: 3089.3368\n",
            "Epoch [70/100], Loss: 2703.3750\n",
            "Epoch [80/100], Loss: 1780.0490\n",
            "Epoch [90/100], Loss: 1786.1841\n",
            "Epoch [100/100], Loss: 6269.7825\n",
            "Fold 5, RMSE: 47.598388671875\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 62.474728393554685\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 34969.4028\n",
            "Epoch [20/150], Loss: 13415.3757\n",
            "Epoch [30/150], Loss: 10442.8729\n",
            "Epoch [40/150], Loss: 7335.9630\n",
            "Epoch [50/150], Loss: 4228.0890\n",
            "Epoch [60/150], Loss: 3804.2334\n",
            "Epoch [70/150], Loss: 2268.6040\n",
            "Epoch [80/150], Loss: 2895.6508\n",
            "Epoch [90/150], Loss: 1167.5192\n",
            "Epoch [100/150], Loss: 1447.0749\n",
            "Epoch [110/150], Loss: 2429.1822\n",
            "Epoch [120/150], Loss: 3226.9243\n",
            "Epoch [130/150], Loss: 1822.5293\n",
            "Epoch [140/150], Loss: 1379.2246\n",
            "Epoch [150/150], Loss: 2526.3007\n",
            "Fold 1, RMSE: 59.987335205078125\n",
            "Epoch [10/150], Loss: 62262.3174\n",
            "Epoch [20/150], Loss: 20332.3367\n",
            "Epoch [30/150], Loss: 11660.7148\n",
            "Epoch [40/150], Loss: 7367.1396\n",
            "Epoch [50/150], Loss: 9114.0443\n",
            "Epoch [60/150], Loss: 4165.2477\n",
            "Epoch [70/150], Loss: 2906.2968\n",
            "Epoch [80/150], Loss: 2398.6424\n",
            "Epoch [90/150], Loss: 4223.4004\n",
            "Epoch [100/150], Loss: 1731.2533\n",
            "Epoch [110/150], Loss: 2809.2838\n",
            "Epoch [120/150], Loss: 2400.9105\n",
            "Epoch [130/150], Loss: 2115.5833\n",
            "Epoch [140/150], Loss: 1131.4052\n",
            "Epoch [150/150], Loss: 1353.3154\n",
            "Fold 2, RMSE: 59.420326232910156\n",
            "Epoch [10/150], Loss: 41367.7930\n",
            "Epoch [20/150], Loss: 8669.2739\n",
            "Epoch [30/150], Loss: 6449.3144\n",
            "Epoch [40/150], Loss: 4326.0293\n",
            "Epoch [50/150], Loss: 4756.2138\n",
            "Epoch [60/150], Loss: 3039.7224\n",
            "Epoch [70/150], Loss: 2537.2741\n",
            "Epoch [80/150], Loss: 1318.1602\n",
            "Epoch [90/150], Loss: 1081.6777\n",
            "Epoch [100/150], Loss: 1671.5850\n",
            "Epoch [110/150], Loss: 1475.2382\n",
            "Epoch [120/150], Loss: 1410.9441\n",
            "Epoch [130/150], Loss: 975.9585\n",
            "Epoch [140/150], Loss: 1550.1453\n",
            "Epoch [150/150], Loss: 336.8840\n",
            "Fold 3, RMSE: 91.71485137939453\n",
            "Epoch [10/150], Loss: 58429.8320\n",
            "Epoch [20/150], Loss: 14764.8047\n",
            "Epoch [30/150], Loss: 11425.4388\n",
            "Epoch [40/150], Loss: 6692.9305\n",
            "Epoch [50/150], Loss: 2930.5667\n",
            "Epoch [60/150], Loss: 5722.1360\n",
            "Epoch [70/150], Loss: 3506.7809\n",
            "Epoch [80/150], Loss: 3656.7256\n",
            "Epoch [90/150], Loss: 4677.6696\n",
            "Epoch [100/150], Loss: 2424.5344\n",
            "Epoch [110/150], Loss: 1070.0506\n",
            "Epoch [120/150], Loss: 2467.0810\n",
            "Epoch [130/150], Loss: 2978.3135\n",
            "Epoch [140/150], Loss: 2716.3322\n",
            "Epoch [150/150], Loss: 2881.3056\n",
            "Fold 4, RMSE: 34.477474212646484\n",
            "Epoch [10/150], Loss: 64960.9863\n",
            "Epoch [20/150], Loss: 13034.3171\n",
            "Epoch [30/150], Loss: 9912.1724\n",
            "Epoch [40/150], Loss: 8579.4277\n",
            "Epoch [50/150], Loss: 3568.1437\n",
            "Epoch [60/150], Loss: 4791.1279\n",
            "Epoch [70/150], Loss: 3325.4042\n",
            "Epoch [80/150], Loss: 3466.2530\n",
            "Epoch [90/150], Loss: 5070.6433\n",
            "Epoch [100/150], Loss: 2662.7053\n",
            "Epoch [110/150], Loss: 3046.0192\n",
            "Epoch [120/150], Loss: 3540.9736\n",
            "Epoch [130/150], Loss: 3149.1594\n",
            "Epoch [140/150], Loss: 1891.9832\n",
            "Epoch [150/150], Loss: 1629.5508\n",
            "Fold 5, RMSE: 46.40665054321289\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 58.401327514648436\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16128.2759\n",
            "Epoch [20/100], Loss: 9139.3499\n",
            "Epoch [30/100], Loss: 6510.4581\n",
            "Epoch [40/100], Loss: 3002.1744\n",
            "Epoch [50/100], Loss: 2759.6332\n",
            "Epoch [60/100], Loss: 1738.6631\n",
            "Epoch [70/100], Loss: 2722.0480\n",
            "Epoch [80/100], Loss: 1268.2537\n",
            "Epoch [90/100], Loss: 1019.8407\n",
            "Epoch [100/100], Loss: 2426.3432\n",
            "Fold 1, RMSE: 59.915374755859375\n",
            "Epoch [10/100], Loss: 15661.8882\n",
            "Epoch [20/100], Loss: 9479.7603\n",
            "Epoch [30/100], Loss: 6589.6246\n",
            "Epoch [40/100], Loss: 3573.4791\n",
            "Epoch [50/100], Loss: 2007.8615\n",
            "Epoch [60/100], Loss: 3656.3045\n",
            "Epoch [70/100], Loss: 4492.4224\n",
            "Epoch [80/100], Loss: 4558.6778\n",
            "Epoch [90/100], Loss: 1788.1702\n",
            "Epoch [100/100], Loss: 1250.8189\n",
            "Fold 2, RMSE: 57.236446380615234\n",
            "Epoch [10/100], Loss: 13622.3745\n",
            "Epoch [20/100], Loss: 7773.9385\n",
            "Epoch [30/100], Loss: 4369.4955\n",
            "Epoch [40/100], Loss: 2944.2773\n",
            "Epoch [50/100], Loss: 2227.9778\n",
            "Epoch [60/100], Loss: 1874.9801\n",
            "Epoch [70/100], Loss: 1248.4906\n",
            "Epoch [80/100], Loss: 1070.4046\n",
            "Epoch [90/100], Loss: 1662.5012\n",
            "Epoch [100/100], Loss: 1136.6313\n",
            "Fold 3, RMSE: 92.06977844238281\n",
            "Epoch [10/100], Loss: 21015.9629\n",
            "Epoch [20/100], Loss: 11973.5581\n",
            "Epoch [30/100], Loss: 9111.9170\n",
            "Epoch [40/100], Loss: 8111.5608\n",
            "Epoch [50/100], Loss: 3374.7356\n",
            "Epoch [60/100], Loss: 2579.1268\n",
            "Epoch [70/100], Loss: 4700.6953\n",
            "Epoch [80/100], Loss: 1684.5418\n",
            "Epoch [90/100], Loss: 1444.6357\n",
            "Epoch [100/100], Loss: 2845.3732\n",
            "Fold 4, RMSE: 37.82011032104492\n",
            "Epoch [10/100], Loss: 21277.6826\n",
            "Epoch [20/100], Loss: 11778.9259\n",
            "Epoch [30/100], Loss: 11003.3059\n",
            "Epoch [40/100], Loss: 7155.7548\n",
            "Epoch [50/100], Loss: 5458.7217\n",
            "Epoch [60/100], Loss: 2506.9822\n",
            "Epoch [70/100], Loss: 4712.8607\n",
            "Epoch [80/100], Loss: 3103.2278\n",
            "Epoch [90/100], Loss: 1287.8773\n",
            "Epoch [100/100], Loss: 2722.2020\n",
            "Fold 5, RMSE: 46.50926971435547\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 58.71019592285156\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15868.0955\n",
            "Epoch [20/150], Loss: 15331.4651\n",
            "Epoch [30/150], Loss: 6809.5604\n",
            "Epoch [40/150], Loss: 5066.4792\n",
            "Epoch [50/150], Loss: 2557.1746\n",
            "Epoch [60/150], Loss: 5195.1408\n",
            "Epoch [70/150], Loss: 850.8461\n",
            "Epoch [80/150], Loss: 1336.3811\n",
            "Epoch [90/150], Loss: 1109.3307\n",
            "Epoch [100/150], Loss: 4371.3596\n",
            "Epoch [110/150], Loss: 3354.1097\n",
            "Epoch [120/150], Loss: 2490.2551\n",
            "Epoch [130/150], Loss: 531.3388\n",
            "Epoch [140/150], Loss: 1619.5847\n",
            "Epoch [150/150], Loss: 1037.9581\n",
            "Fold 1, RMSE: 56.5089225769043\n",
            "Epoch [10/150], Loss: 20492.9355\n",
            "Epoch [20/150], Loss: 20452.4607\n",
            "Epoch [30/150], Loss: 13696.6250\n",
            "Epoch [40/150], Loss: 8268.4012\n",
            "Epoch [50/150], Loss: 4803.1881\n",
            "Epoch [60/150], Loss: 3722.0790\n",
            "Epoch [70/150], Loss: 3982.1680\n",
            "Epoch [80/150], Loss: 3805.4636\n",
            "Epoch [90/150], Loss: 1243.1512\n",
            "Epoch [100/150], Loss: 1821.6572\n",
            "Epoch [110/150], Loss: 1843.9438\n",
            "Epoch [120/150], Loss: 1111.1508\n",
            "Epoch [130/150], Loss: 3156.2589\n",
            "Epoch [140/150], Loss: 883.0928\n",
            "Epoch [150/150], Loss: 1872.9788\n",
            "Fold 2, RMSE: 73.76959228515625\n",
            "Epoch [10/150], Loss: 11421.1165\n",
            "Epoch [20/150], Loss: 5704.2080\n",
            "Epoch [30/150], Loss: 7679.9502\n",
            "Epoch [40/150], Loss: 3557.7071\n",
            "Epoch [50/150], Loss: 2754.6788\n",
            "Epoch [60/150], Loss: 4981.3265\n",
            "Epoch [70/150], Loss: 1741.8875\n",
            "Epoch [80/150], Loss: 1126.6098\n",
            "Epoch [90/150], Loss: 2038.3579\n",
            "Epoch [100/150], Loss: 2260.8204\n",
            "Epoch [110/150], Loss: 893.8369\n",
            "Epoch [120/150], Loss: 1145.8091\n",
            "Epoch [130/150], Loss: 1424.8472\n",
            "Epoch [140/150], Loss: 848.1566\n",
            "Epoch [150/150], Loss: 1141.8396\n",
            "Fold 3, RMSE: 96.9174575805664\n",
            "Epoch [10/150], Loss: 13544.1885\n",
            "Epoch [20/150], Loss: 6968.6929\n",
            "Epoch [30/150], Loss: 5930.2818\n",
            "Epoch [40/150], Loss: 2768.3818\n",
            "Epoch [50/150], Loss: 2519.6617\n",
            "Epoch [60/150], Loss: 3866.9116\n",
            "Epoch [70/150], Loss: 2104.8593\n",
            "Epoch [80/150], Loss: 3364.0012\n",
            "Epoch [90/150], Loss: 1795.2210\n",
            "Epoch [100/150], Loss: 1431.9832\n",
            "Epoch [110/150], Loss: 757.2931\n",
            "Epoch [120/150], Loss: 2678.9763\n",
            "Epoch [130/150], Loss: 1013.0606\n",
            "Epoch [140/150], Loss: 1246.9387\n",
            "Epoch [150/150], Loss: 3050.7641\n",
            "Fold 4, RMSE: 40.466617584228516\n",
            "Epoch [10/150], Loss: 19298.5693\n",
            "Epoch [20/150], Loss: 9911.3389\n",
            "Epoch [30/150], Loss: 8328.7478\n",
            "Epoch [40/150], Loss: 5145.0417\n",
            "Epoch [50/150], Loss: 2936.1529\n",
            "Epoch [60/150], Loss: 4519.0067\n",
            "Epoch [70/150], Loss: 1854.7795\n",
            "Epoch [80/150], Loss: 3361.0783\n",
            "Epoch [90/150], Loss: 3575.1398\n",
            "Epoch [100/150], Loss: 3043.3092\n",
            "Epoch [110/150], Loss: 1075.8652\n",
            "Epoch [120/150], Loss: 4583.9638\n",
            "Epoch [130/150], Loss: 1869.2625\n",
            "Epoch [140/150], Loss: 2228.7027\n",
            "Epoch [150/150], Loss: 1487.6477\n",
            "Fold 5, RMSE: 47.48210906982422\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 63.02893981933594\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 32110.7178\n",
            "Epoch [20/100], Loss: 17349.1887\n",
            "Epoch [30/100], Loss: 7544.9001\n",
            "Epoch [40/100], Loss: 5947.7021\n",
            "Epoch [50/100], Loss: 5335.1167\n",
            "Epoch [60/100], Loss: 3618.0501\n",
            "Epoch [70/100], Loss: 4307.7239\n",
            "Epoch [80/100], Loss: 2127.5876\n",
            "Epoch [90/100], Loss: 1780.2723\n",
            "Epoch [100/100], Loss: 2177.3499\n",
            "Fold 1, RMSE: 57.20405578613281\n",
            "Epoch [10/100], Loss: 65179.6260\n",
            "Epoch [20/100], Loss: 17018.7964\n",
            "Epoch [30/100], Loss: 10963.4084\n",
            "Epoch [40/100], Loss: 9301.4426\n",
            "Epoch [50/100], Loss: 10558.3135\n",
            "Epoch [60/100], Loss: 6478.0376\n",
            "Epoch [70/100], Loss: 4175.4817\n",
            "Epoch [80/100], Loss: 2331.7435\n",
            "Epoch [90/100], Loss: 4619.9186\n",
            "Epoch [100/100], Loss: 2838.6375\n",
            "Fold 2, RMSE: 62.21405792236328\n",
            "Epoch [10/100], Loss: 40017.5801\n",
            "Epoch [20/100], Loss: 8232.3879\n",
            "Epoch [30/100], Loss: 6435.9446\n",
            "Epoch [40/100], Loss: 4870.0819\n",
            "Epoch [50/100], Loss: 3495.8558\n",
            "Epoch [60/100], Loss: 1373.7702\n",
            "Epoch [70/100], Loss: 1806.4697\n",
            "Epoch [80/100], Loss: 2667.5059\n",
            "Epoch [90/100], Loss: 1373.3596\n",
            "Epoch [100/100], Loss: 1208.0901\n",
            "Fold 3, RMSE: 89.60501861572266\n",
            "Epoch [10/100], Loss: 47002.1592\n",
            "Epoch [20/100], Loss: 20848.0459\n",
            "Epoch [30/100], Loss: 16053.9788\n",
            "Epoch [40/100], Loss: 13187.4917\n",
            "Epoch [50/100], Loss: 10682.4858\n",
            "Epoch [60/100], Loss: 6950.5895\n",
            "Epoch [70/100], Loss: 7510.2737\n",
            "Epoch [80/100], Loss: 5618.3965\n",
            "Epoch [90/100], Loss: 4525.0969\n",
            "Epoch [100/100], Loss: 4094.3996\n",
            "Fold 4, RMSE: 39.56592559814453\n",
            "Epoch [10/100], Loss: 39874.3712\n",
            "Epoch [20/100], Loss: 14890.5476\n",
            "Epoch [30/100], Loss: 11414.1008\n",
            "Epoch [40/100], Loss: 8820.4118\n",
            "Epoch [50/100], Loss: 9983.2551\n",
            "Epoch [60/100], Loss: 5994.9065\n",
            "Epoch [70/100], Loss: 3500.9831\n",
            "Epoch [80/100], Loss: 2296.5964\n",
            "Epoch [90/100], Loss: 4047.2579\n",
            "Epoch [100/100], Loss: 1830.8180\n",
            "Fold 5, RMSE: 45.60369873046875\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 58.8385513305664\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 15911.2354\n",
            "Epoch [20/150], Loss: 13625.1616\n",
            "Epoch [30/150], Loss: 8686.5177\n",
            "Epoch [40/150], Loss: 9428.2931\n",
            "Epoch [50/150], Loss: 4092.3706\n",
            "Epoch [60/150], Loss: 6179.1455\n",
            "Epoch [70/150], Loss: 3156.7523\n",
            "Epoch [80/150], Loss: 1735.1546\n",
            "Epoch [90/150], Loss: 996.1367\n",
            "Epoch [100/150], Loss: 1688.7254\n",
            "Epoch [110/150], Loss: 2616.6292\n",
            "Epoch [120/150], Loss: 1493.8345\n",
            "Epoch [130/150], Loss: 12180.8424\n",
            "Epoch [140/150], Loss: 1451.5947\n",
            "Epoch [150/150], Loss: 943.3651\n",
            "Fold 1, RMSE: 58.97266387939453\n",
            "Epoch [10/150], Loss: 62918.8711\n",
            "Epoch [20/150], Loss: 11888.3552\n",
            "Epoch [30/150], Loss: 9959.5054\n",
            "Epoch [40/150], Loss: 8006.7889\n",
            "Epoch [50/150], Loss: 6953.3373\n",
            "Epoch [60/150], Loss: 8946.8083\n",
            "Epoch [70/150], Loss: 4727.1632\n",
            "Epoch [80/150], Loss: 4155.0193\n",
            "Epoch [90/150], Loss: 4288.5476\n",
            "Epoch [100/150], Loss: 2235.4676\n",
            "Epoch [110/150], Loss: 4945.3317\n",
            "Epoch [120/150], Loss: 1800.4466\n",
            "Epoch [130/150], Loss: 1369.3126\n",
            "Epoch [140/150], Loss: 918.7889\n",
            "Epoch [150/150], Loss: 1617.4518\n",
            "Fold 2, RMSE: 63.29078674316406\n",
            "Epoch [10/150], Loss: 127052.2832\n",
            "Epoch [20/150], Loss: 12784.4033\n",
            "Epoch [30/150], Loss: 8168.1910\n",
            "Epoch [40/150], Loss: 5393.8827\n",
            "Epoch [50/150], Loss: 8535.6428\n",
            "Epoch [60/150], Loss: 2721.0689\n",
            "Epoch [70/150], Loss: 2369.0342\n",
            "Epoch [80/150], Loss: 3019.9443\n",
            "Epoch [90/150], Loss: 1663.0948\n",
            "Epoch [100/150], Loss: 1956.6705\n",
            "Epoch [110/150], Loss: 1537.2384\n",
            "Epoch [120/150], Loss: 1754.7074\n",
            "Epoch [130/150], Loss: 681.8853\n",
            "Epoch [140/150], Loss: 1075.9732\n",
            "Epoch [150/150], Loss: 2949.5603\n",
            "Fold 3, RMSE: 92.01924133300781\n",
            "Epoch [10/150], Loss: 33920.6758\n",
            "Epoch [20/150], Loss: 20502.5039\n",
            "Epoch [30/150], Loss: 13334.7485\n",
            "Epoch [40/150], Loss: 10206.1095\n",
            "Epoch [50/150], Loss: 12974.3452\n",
            "Epoch [60/150], Loss: 9542.8828\n",
            "Epoch [70/150], Loss: 10346.8684\n",
            "Epoch [80/150], Loss: 6592.7321\n",
            "Epoch [90/150], Loss: 5488.9675\n",
            "Epoch [100/150], Loss: 4598.6066\n",
            "Epoch [110/150], Loss: 2863.2812\n",
            "Epoch [120/150], Loss: 3047.8912\n",
            "Epoch [130/150], Loss: 2325.1373\n",
            "Epoch [140/150], Loss: 1088.1443\n",
            "Epoch [150/150], Loss: 3708.2116\n",
            "Fold 4, RMSE: 43.9825325012207\n",
            "Epoch [10/150], Loss: 39600.3813\n",
            "Epoch [20/150], Loss: 10625.5493\n",
            "Epoch [30/150], Loss: 10565.5599\n",
            "Epoch [40/150], Loss: 10498.9324\n",
            "Epoch [50/150], Loss: 10215.8113\n",
            "Epoch [60/150], Loss: 6406.2007\n",
            "Epoch [70/150], Loss: 3318.0271\n",
            "Epoch [80/150], Loss: 2936.7420\n",
            "Epoch [90/150], Loss: 2753.6059\n",
            "Epoch [100/150], Loss: 2120.4985\n",
            "Epoch [110/150], Loss: 3457.8264\n",
            "Epoch [120/150], Loss: 1297.4464\n",
            "Epoch [130/150], Loss: 1073.1990\n",
            "Epoch [140/150], Loss: 1903.5110\n",
            "Epoch [150/150], Loss: 2043.5543\n",
            "Fold 5, RMSE: 52.187660217285156\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 62.09057693481445\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18014.1130\n",
            "Epoch [20/100], Loss: 9827.6401\n",
            "Epoch [30/100], Loss: 6689.2745\n",
            "Epoch [40/100], Loss: 6985.5775\n",
            "Epoch [50/100], Loss: 4336.2034\n",
            "Epoch [60/100], Loss: 1825.0267\n",
            "Epoch [70/100], Loss: 1843.7764\n",
            "Epoch [80/100], Loss: 2531.3460\n",
            "Epoch [90/100], Loss: 2120.1406\n",
            "Epoch [100/100], Loss: 1542.0009\n",
            "Fold 1, RMSE: 57.26441192626953\n",
            "Epoch [10/100], Loss: 16324.6919\n",
            "Epoch [20/100], Loss: 11662.9700\n",
            "Epoch [30/100], Loss: 8994.0029\n",
            "Epoch [40/100], Loss: 6786.3206\n",
            "Epoch [50/100], Loss: 5094.5135\n",
            "Epoch [60/100], Loss: 3688.9010\n",
            "Epoch [70/100], Loss: 2710.9062\n",
            "Epoch [80/100], Loss: 1405.6044\n",
            "Epoch [90/100], Loss: 2748.1575\n",
            "Epoch [100/100], Loss: 1660.0110\n",
            "Fold 2, RMSE: 68.82042694091797\n",
            "Epoch [10/100], Loss: 16437.0327\n",
            "Epoch [20/100], Loss: 7530.3945\n",
            "Epoch [30/100], Loss: 8259.7954\n",
            "Epoch [40/100], Loss: 4927.0623\n",
            "Epoch [50/100], Loss: 3991.1754\n",
            "Epoch [60/100], Loss: 2673.5048\n",
            "Epoch [70/100], Loss: 1780.5457\n",
            "Epoch [80/100], Loss: 2076.7870\n",
            "Epoch [90/100], Loss: 1999.7299\n",
            "Epoch [100/100], Loss: 1332.2409\n",
            "Fold 3, RMSE: 90.73509979248047\n",
            "Epoch [10/100], Loss: 32553.2764\n",
            "Epoch [20/100], Loss: 11176.6947\n",
            "Epoch [30/100], Loss: 7740.9253\n",
            "Epoch [40/100], Loss: 8351.2551\n",
            "Epoch [50/100], Loss: 6258.4979\n",
            "Epoch [60/100], Loss: 3483.4525\n",
            "Epoch [70/100], Loss: 2441.2649\n",
            "Epoch [80/100], Loss: 1392.6647\n",
            "Epoch [90/100], Loss: 3653.7728\n",
            "Epoch [100/100], Loss: 2735.8419\n",
            "Fold 4, RMSE: 40.21799850463867\n",
            "Epoch [10/100], Loss: 15816.0220\n",
            "Epoch [20/100], Loss: 9406.7509\n",
            "Epoch [30/100], Loss: 8333.0182\n",
            "Epoch [40/100], Loss: 5004.9127\n",
            "Epoch [50/100], Loss: 2697.7888\n",
            "Epoch [60/100], Loss: 2533.1462\n",
            "Epoch [70/100], Loss: 2052.9255\n",
            "Epoch [80/100], Loss: 1386.4892\n",
            "Epoch [90/100], Loss: 978.6369\n",
            "Epoch [100/100], Loss: 4263.1939\n",
            "Fold 5, RMSE: 45.65192794799805\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 60.53797302246094\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 14985.9800\n",
            "Epoch [20/150], Loss: 10719.4597\n",
            "Epoch [30/150], Loss: 8581.2831\n",
            "Epoch [40/150], Loss: 4442.5182\n",
            "Epoch [50/150], Loss: 2989.6721\n",
            "Epoch [60/150], Loss: 1095.4335\n",
            "Epoch [70/150], Loss: 5489.1585\n",
            "Epoch [80/150], Loss: 921.2652\n",
            "Epoch [90/150], Loss: 4646.9490\n",
            "Epoch [100/150], Loss: 3607.1179\n",
            "Epoch [110/150], Loss: 4305.1506\n",
            "Epoch [120/150], Loss: 2225.5047\n",
            "Epoch [130/150], Loss: 1087.3899\n",
            "Epoch [140/150], Loss: 1266.2869\n",
            "Epoch [150/150], Loss: 1369.0021\n",
            "Fold 1, RMSE: 57.439208984375\n",
            "Epoch [10/150], Loss: 20240.2229\n",
            "Epoch [20/150], Loss: 15841.4297\n",
            "Epoch [30/150], Loss: 11798.4600\n",
            "Epoch [40/150], Loss: 7908.1570\n",
            "Epoch [50/150], Loss: 6632.6304\n",
            "Epoch [60/150], Loss: 6487.6664\n",
            "Epoch [70/150], Loss: 5598.6074\n",
            "Epoch [80/150], Loss: 5271.7631\n",
            "Epoch [90/150], Loss: 2883.9570\n",
            "Epoch [100/150], Loss: 1674.2310\n",
            "Epoch [110/150], Loss: 3738.1500\n",
            "Epoch [120/150], Loss: 3989.3302\n",
            "Epoch [130/150], Loss: 5659.3724\n",
            "Epoch [140/150], Loss: 1269.7995\n",
            "Epoch [150/150], Loss: 2240.6445\n",
            "Fold 2, RMSE: 65.1636962890625\n",
            "Epoch [10/150], Loss: 9414.8750\n",
            "Epoch [20/150], Loss: 9205.6715\n",
            "Epoch [30/150], Loss: 5251.2856\n",
            "Epoch [40/150], Loss: 3617.6075\n",
            "Epoch [50/150], Loss: 3860.1977\n",
            "Epoch [60/150], Loss: 3074.3693\n",
            "Epoch [70/150], Loss: 1961.4138\n",
            "Epoch [80/150], Loss: 1663.8051\n",
            "Epoch [90/150], Loss: 1151.6957\n",
            "Epoch [100/150], Loss: 1061.2765\n",
            "Epoch [110/150], Loss: 708.4515\n",
            "Epoch [120/150], Loss: 971.0316\n",
            "Epoch [130/150], Loss: 1628.5966\n",
            "Epoch [140/150], Loss: 741.4408\n",
            "Epoch [150/150], Loss: 718.1188\n",
            "Fold 3, RMSE: 93.14686584472656\n",
            "Epoch [10/150], Loss: 14975.2285\n",
            "Epoch [20/150], Loss: 10648.6062\n",
            "Epoch [30/150], Loss: 5920.2622\n",
            "Epoch [40/150], Loss: 4715.3411\n",
            "Epoch [50/150], Loss: 3443.6760\n",
            "Epoch [60/150], Loss: 2307.1305\n",
            "Epoch [70/150], Loss: 2638.9147\n",
            "Epoch [80/150], Loss: 2280.6813\n",
            "Epoch [90/150], Loss: 1733.0942\n",
            "Epoch [100/150], Loss: 808.3488\n",
            "Epoch [110/150], Loss: 2787.2888\n",
            "Epoch [120/150], Loss: 793.5331\n",
            "Epoch [130/150], Loss: 2729.5586\n",
            "Epoch [140/150], Loss: 1196.5212\n",
            "Epoch [150/150], Loss: 1391.4245\n",
            "Fold 4, RMSE: 46.57533264160156\n",
            "Epoch [10/150], Loss: 19597.9814\n",
            "Epoch [20/150], Loss: 18552.7258\n",
            "Epoch [30/150], Loss: 7667.6379\n",
            "Epoch [40/150], Loss: 11276.5371\n",
            "Epoch [50/150], Loss: 4490.6485\n",
            "Epoch [60/150], Loss: 5025.2088\n",
            "Epoch [70/150], Loss: 3507.3171\n",
            "Epoch [80/150], Loss: 2197.0493\n",
            "Epoch [90/150], Loss: 2878.8061\n",
            "Epoch [100/150], Loss: 1466.9207\n",
            "Epoch [110/150], Loss: 2643.3792\n",
            "Epoch [120/150], Loss: 935.7663\n",
            "Epoch [130/150], Loss: 1355.1547\n",
            "Epoch [140/150], Loss: 914.8446\n",
            "Epoch [150/150], Loss: 657.0243\n",
            "Fold 5, RMSE: 44.04032897949219\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 61.27308654785156\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 166065.9648\n",
            "Epoch [20/100], Loss: 52874.1250\n",
            "Epoch [30/100], Loss: 27109.6753\n",
            "Epoch [40/100], Loss: 16573.0996\n",
            "Epoch [50/100], Loss: 15541.3845\n",
            "Epoch [60/100], Loss: 12999.8418\n",
            "Epoch [70/100], Loss: 16082.9900\n",
            "Epoch [80/100], Loss: 17788.7659\n",
            "Epoch [90/100], Loss: 14727.6265\n",
            "Epoch [100/100], Loss: 11724.8315\n",
            "Fold 1, RMSE: 52.119632720947266\n",
            "Epoch [10/100], Loss: 158221.3750\n",
            "Epoch [20/100], Loss: 34323.7861\n",
            "Epoch [30/100], Loss: 29418.5918\n",
            "Epoch [40/100], Loss: 12097.2987\n",
            "Epoch [50/100], Loss: 26083.3237\n",
            "Epoch [60/100], Loss: 13952.4568\n",
            "Epoch [70/100], Loss: 10483.8169\n",
            "Epoch [80/100], Loss: 10915.3646\n",
            "Epoch [90/100], Loss: 11315.0261\n",
            "Epoch [100/100], Loss: 8349.4740\n",
            "Fold 2, RMSE: 63.76565170288086\n",
            "Epoch [10/100], Loss: 341903.3516\n",
            "Epoch [20/100], Loss: 46100.1533\n",
            "Epoch [30/100], Loss: 35325.9219\n",
            "Epoch [40/100], Loss: 23249.5654\n",
            "Epoch [50/100], Loss: 16591.1128\n",
            "Epoch [60/100], Loss: 16884.0903\n",
            "Epoch [70/100], Loss: 7137.4056\n",
            "Epoch [80/100], Loss: 22143.3560\n",
            "Epoch [90/100], Loss: 10322.3337\n",
            "Epoch [100/100], Loss: 9797.0054\n",
            "Fold 3, RMSE: 90.94251251220703\n",
            "Epoch [10/100], Loss: 231502.5547\n",
            "Epoch [20/100], Loss: 39670.2793\n",
            "Epoch [30/100], Loss: 17608.1565\n",
            "Epoch [40/100], Loss: 15689.8164\n",
            "Epoch [50/100], Loss: 13309.4595\n",
            "Epoch [60/100], Loss: 11947.6360\n",
            "Epoch [70/100], Loss: 11981.3977\n",
            "Epoch [80/100], Loss: 7819.0873\n",
            "Epoch [90/100], Loss: 7399.6265\n",
            "Epoch [100/100], Loss: 6581.5148\n",
            "Fold 4, RMSE: 40.95071029663086\n",
            "Epoch [10/100], Loss: 487162.6797\n",
            "Epoch [20/100], Loss: 43908.6592\n",
            "Epoch [30/100], Loss: 27727.9360\n",
            "Epoch [40/100], Loss: 14899.3386\n",
            "Epoch [50/100], Loss: 25357.7510\n",
            "Epoch [60/100], Loss: 16950.2861\n",
            "Epoch [70/100], Loss: 13588.5447\n",
            "Epoch [80/100], Loss: 11161.2410\n",
            "Epoch [90/100], Loss: 16633.7854\n",
            "Epoch [100/100], Loss: 12506.8542\n",
            "Fold 5, RMSE: 46.77153778076172\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 58.91000900268555\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 20340.5127\n",
            "Epoch [20/150], Loss: 20987.1587\n",
            "Epoch [30/150], Loss: 20358.4619\n",
            "Epoch [40/150], Loss: 16839.4155\n",
            "Epoch [50/150], Loss: 18597.2993\n",
            "Epoch [60/150], Loss: 17667.0215\n",
            "Epoch [70/150], Loss: 17012.3418\n",
            "Epoch [80/150], Loss: 18085.5024\n",
            "Epoch [90/150], Loss: 16217.2676\n",
            "Epoch [100/150], Loss: 15303.0333\n",
            "Epoch [110/150], Loss: 18481.3135\n",
            "Epoch [120/150], Loss: 16673.0759\n",
            "Epoch [130/150], Loss: 19487.6689\n",
            "Epoch [140/150], Loss: 20182.0088\n",
            "Epoch [150/150], Loss: 25085.9839\n",
            "Fold 1, RMSE: 66.98497009277344\n",
            "Epoch [10/150], Loss: 134305.8926\n",
            "Epoch [20/150], Loss: 54116.4873\n",
            "Epoch [30/150], Loss: 22250.9766\n",
            "Epoch [40/150], Loss: 15853.4834\n",
            "Epoch [50/150], Loss: 18857.4443\n",
            "Epoch [60/150], Loss: 12884.7734\n",
            "Epoch [70/150], Loss: 9582.7915\n",
            "Epoch [80/150], Loss: 8947.6208\n",
            "Epoch [90/150], Loss: 11481.6130\n",
            "Epoch [100/150], Loss: 4263.0676\n",
            "Epoch [110/150], Loss: 4253.1855\n",
            "Epoch [120/150], Loss: 3660.0403\n",
            "Epoch [130/150], Loss: 3152.2127\n",
            "Epoch [140/150], Loss: 1670.5236\n",
            "Epoch [150/150], Loss: 1947.2451\n",
            "Fold 2, RMSE: 58.76023864746094\n",
            "Epoch [10/150], Loss: 149690.7305\n",
            "Epoch [20/150], Loss: 30760.5332\n",
            "Epoch [30/150], Loss: 13816.3713\n",
            "Epoch [40/150], Loss: 11645.7122\n",
            "Epoch [50/150], Loss: 10498.0189\n",
            "Epoch [60/150], Loss: 7572.9146\n",
            "Epoch [70/150], Loss: 7903.8467\n",
            "Epoch [80/150], Loss: 7111.4752\n",
            "Epoch [90/150], Loss: 5761.2926\n",
            "Epoch [100/150], Loss: 6030.4871\n",
            "Epoch [110/150], Loss: 6654.1062\n",
            "Epoch [120/150], Loss: 4645.9031\n",
            "Epoch [130/150], Loss: 4526.5479\n",
            "Epoch [140/150], Loss: 2568.1261\n",
            "Epoch [150/150], Loss: 4425.0590\n",
            "Fold 3, RMSE: 88.90884399414062\n",
            "Epoch [10/150], Loss: 230575.3164\n",
            "Epoch [20/150], Loss: 104457.1914\n",
            "Epoch [30/150], Loss: 23966.7476\n",
            "Epoch [40/150], Loss: 23135.8618\n",
            "Epoch [50/150], Loss: 18089.6011\n",
            "Epoch [60/150], Loss: 13805.8381\n",
            "Epoch [70/150], Loss: 14107.3486\n",
            "Epoch [80/150], Loss: 10399.5474\n",
            "Epoch [90/150], Loss: 11560.7493\n",
            "Epoch [100/150], Loss: 5306.5936\n",
            "Epoch [110/150], Loss: 4686.7895\n",
            "Epoch [120/150], Loss: 4565.1129\n",
            "Epoch [130/150], Loss: 6927.9225\n",
            "Epoch [140/150], Loss: 8487.7400\n",
            "Epoch [150/150], Loss: 5826.6493\n",
            "Fold 4, RMSE: 37.0665283203125\n",
            "Epoch [10/150], Loss: 131398.4258\n",
            "Epoch [20/150], Loss: 20285.4187\n",
            "Epoch [30/150], Loss: 17304.3738\n",
            "Epoch [40/150], Loss: 14505.1201\n",
            "Epoch [50/150], Loss: 13254.9673\n",
            "Epoch [60/150], Loss: 13861.7222\n",
            "Epoch [70/150], Loss: 11181.9395\n",
            "Epoch [80/150], Loss: 11994.4059\n",
            "Epoch [90/150], Loss: 15439.4148\n",
            "Epoch [100/150], Loss: 13926.2346\n",
            "Epoch [110/150], Loss: 8672.9988\n",
            "Epoch [120/150], Loss: 9336.7809\n",
            "Epoch [130/150], Loss: 10830.8564\n",
            "Epoch [140/150], Loss: 12836.9225\n",
            "Epoch [150/150], Loss: 8869.2365\n",
            "Fold 5, RMSE: 50.876380920410156\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 60.51939239501953\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 21968.8091\n",
            "Epoch [20/100], Loss: 11069.5981\n",
            "Epoch [30/100], Loss: 10823.2604\n",
            "Epoch [40/100], Loss: 13599.0854\n",
            "Epoch [50/100], Loss: 6217.0541\n",
            "Epoch [60/100], Loss: 4213.3503\n",
            "Epoch [70/100], Loss: 4307.0059\n",
            "Epoch [80/100], Loss: 3490.7565\n",
            "Epoch [90/100], Loss: 2841.5315\n",
            "Epoch [100/100], Loss: 2464.7793\n",
            "Fold 1, RMSE: 56.965972900390625\n",
            "Epoch [10/100], Loss: 24512.0928\n",
            "Epoch [20/100], Loss: 12357.5212\n",
            "Epoch [30/100], Loss: 11479.8757\n",
            "Epoch [40/100], Loss: 8111.2856\n",
            "Epoch [50/100], Loss: 10261.5394\n",
            "Epoch [60/100], Loss: 17210.9741\n",
            "Epoch [70/100], Loss: 14943.0762\n",
            "Epoch [80/100], Loss: 11414.0009\n",
            "Epoch [90/100], Loss: 10026.2666\n",
            "Epoch [100/100], Loss: 8891.7371\n",
            "Fold 2, RMSE: 67.18547058105469\n",
            "Epoch [10/100], Loss: 42565.4375\n",
            "Epoch [20/100], Loss: 10174.9482\n",
            "Epoch [30/100], Loss: 7314.3369\n",
            "Epoch [40/100], Loss: 7688.7085\n",
            "Epoch [50/100], Loss: 8308.8225\n",
            "Epoch [60/100], Loss: 9743.7883\n",
            "Epoch [70/100], Loss: 9098.1321\n",
            "Epoch [80/100], Loss: 8634.3962\n",
            "Epoch [90/100], Loss: 7100.2266\n",
            "Epoch [100/100], Loss: 8593.9788\n",
            "Fold 3, RMSE: 93.80204772949219\n",
            "Epoch [10/100], Loss: 63402.2422\n",
            "Epoch [20/100], Loss: 17139.8079\n",
            "Epoch [30/100], Loss: 13943.2883\n",
            "Epoch [40/100], Loss: 15344.5913\n",
            "Epoch [50/100], Loss: 18961.3115\n",
            "Epoch [60/100], Loss: 13494.8877\n",
            "Epoch [70/100], Loss: 24690.9585\n",
            "Epoch [80/100], Loss: 12994.1392\n",
            "Epoch [90/100], Loss: 13389.2683\n",
            "Epoch [100/100], Loss: 9951.6565\n",
            "Fold 4, RMSE: 35.44720458984375\n",
            "Epoch [10/100], Loss: 49274.2998\n",
            "Epoch [20/100], Loss: 16464.0288\n",
            "Epoch [30/100], Loss: 21838.8596\n",
            "Epoch [40/100], Loss: 12001.8684\n",
            "Epoch [50/100], Loss: 11712.4565\n",
            "Epoch [60/100], Loss: 15734.9456\n",
            "Epoch [70/100], Loss: 10777.0635\n",
            "Epoch [80/100], Loss: 10967.2194\n",
            "Epoch [90/100], Loss: 9189.2289\n",
            "Epoch [100/100], Loss: 9838.0547\n",
            "Fold 5, RMSE: 47.15780258178711\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 60.11169967651367\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 49315.5098\n",
            "Epoch [20/150], Loss: 14658.9326\n",
            "Epoch [30/150], Loss: 11171.0667\n",
            "Epoch [40/150], Loss: 14510.4343\n",
            "Epoch [50/150], Loss: 14605.9277\n",
            "Epoch [60/150], Loss: 11981.6575\n",
            "Epoch [70/150], Loss: 12525.0247\n",
            "Epoch [80/150], Loss: 13020.3831\n",
            "Epoch [90/150], Loss: 9441.4257\n",
            "Epoch [100/150], Loss: 15229.0825\n",
            "Epoch [110/150], Loss: 12662.5823\n",
            "Epoch [120/150], Loss: 13630.5933\n",
            "Epoch [130/150], Loss: 12626.2532\n",
            "Epoch [140/150], Loss: 11738.7566\n",
            "Epoch [150/150], Loss: 13901.0964\n",
            "Fold 1, RMSE: 44.25567626953125\n",
            "Epoch [10/150], Loss: 57731.9121\n",
            "Epoch [20/150], Loss: 16826.6177\n",
            "Epoch [30/150], Loss: 10911.1364\n",
            "Epoch [40/150], Loss: 11413.3254\n",
            "Epoch [50/150], Loss: 14849.6705\n",
            "Epoch [60/150], Loss: 11917.6562\n",
            "Epoch [70/150], Loss: 9389.8149\n",
            "Epoch [80/150], Loss: 12289.8916\n",
            "Epoch [90/150], Loss: 9863.8519\n",
            "Epoch [100/150], Loss: 16989.5967\n",
            "Epoch [110/150], Loss: 9769.0553\n",
            "Epoch [120/150], Loss: 7781.3240\n",
            "Epoch [130/150], Loss: 18204.5071\n",
            "Epoch [140/150], Loss: 11156.8040\n",
            "Epoch [150/150], Loss: 11061.5042\n",
            "Fold 2, RMSE: 65.498046875\n",
            "Epoch [10/150], Loss: 30509.8833\n",
            "Epoch [20/150], Loss: 12292.3464\n",
            "Epoch [30/150], Loss: 7637.7544\n",
            "Epoch [40/150], Loss: 5788.0007\n",
            "Epoch [50/150], Loss: 8186.7935\n",
            "Epoch [60/150], Loss: 8358.5969\n",
            "Epoch [70/150], Loss: 5778.0261\n",
            "Epoch [80/150], Loss: 9158.6499\n",
            "Epoch [90/150], Loss: 5917.5037\n",
            "Epoch [100/150], Loss: 6672.9020\n",
            "Epoch [110/150], Loss: 7080.4053\n",
            "Epoch [120/150], Loss: 5102.2113\n",
            "Epoch [130/150], Loss: 5162.5789\n",
            "Epoch [140/150], Loss: 4906.6302\n",
            "Epoch [150/150], Loss: 3249.8542\n",
            "Fold 3, RMSE: 95.21468353271484\n",
            "Epoch [10/150], Loss: 21139.9067\n",
            "Epoch [20/150], Loss: 17549.8579\n",
            "Epoch [30/150], Loss: 13826.2129\n",
            "Epoch [40/150], Loss: 12826.9379\n",
            "Epoch [50/150], Loss: 12473.3701\n",
            "Epoch [60/150], Loss: 11408.6221\n",
            "Epoch [70/150], Loss: 11438.2404\n",
            "Epoch [80/150], Loss: 12089.3135\n",
            "Epoch [90/150], Loss: 11532.1392\n",
            "Epoch [100/150], Loss: 11580.2102\n",
            "Epoch [110/150], Loss: 10869.4136\n",
            "Epoch [120/150], Loss: 13561.6763\n",
            "Epoch [130/150], Loss: 12420.8059\n",
            "Epoch [140/150], Loss: 9786.2250\n",
            "Epoch [150/150], Loss: 16600.8101\n",
            "Fold 4, RMSE: 35.01445770263672\n",
            "Epoch [10/150], Loss: 25593.6609\n",
            "Epoch [20/150], Loss: 15672.2397\n",
            "Epoch [30/150], Loss: 16261.1711\n",
            "Epoch [40/150], Loss: 14139.2305\n",
            "Epoch [50/150], Loss: 12964.8271\n",
            "Epoch [60/150], Loss: 15993.9934\n",
            "Epoch [70/150], Loss: 13811.0610\n",
            "Epoch [80/150], Loss: 10369.9635\n",
            "Epoch [90/150], Loss: 15748.0645\n",
            "Epoch [100/150], Loss: 9877.2526\n",
            "Epoch [110/150], Loss: 12365.6274\n",
            "Epoch [120/150], Loss: 10301.7146\n",
            "Epoch [130/150], Loss: 9570.3208\n",
            "Epoch [140/150], Loss: 8710.1371\n",
            "Epoch [150/150], Loss: 11162.9219\n",
            "Fold 5, RMSE: 45.76982498168945\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 57.15053787231445\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 357317.5547\n",
            "Epoch [20/100], Loss: 57782.7617\n",
            "Epoch [30/100], Loss: 25276.8145\n",
            "Epoch [40/100], Loss: 17483.2063\n",
            "Epoch [50/100], Loss: 15952.8704\n",
            "Epoch [60/100], Loss: 13457.5887\n",
            "Epoch [70/100], Loss: 16107.7609\n",
            "Epoch [80/100], Loss: 12549.5020\n",
            "Epoch [90/100], Loss: 16042.6265\n",
            "Epoch [100/100], Loss: 11473.7927\n",
            "Fold 1, RMSE: 46.66697692871094\n",
            "Epoch [10/100], Loss: 460227.2188\n",
            "Epoch [20/100], Loss: 126467.7871\n",
            "Epoch [30/100], Loss: 63954.2969\n",
            "Epoch [40/100], Loss: 28981.6235\n",
            "Epoch [50/100], Loss: 22707.0359\n",
            "Epoch [60/100], Loss: 30704.5354\n",
            "Epoch [70/100], Loss: 25707.2400\n",
            "Epoch [80/100], Loss: 29044.8213\n",
            "Epoch [90/100], Loss: 11652.7041\n",
            "Epoch [100/100], Loss: 12172.8772\n",
            "Fold 2, RMSE: 64.47401428222656\n",
            "Epoch [10/100], Loss: 312329.0625\n",
            "Epoch [20/100], Loss: 56755.1230\n",
            "Epoch [30/100], Loss: 26399.7422\n",
            "Epoch [40/100], Loss: 23150.5190\n",
            "Epoch [50/100], Loss: 27658.2490\n",
            "Epoch [60/100], Loss: 12931.2131\n",
            "Epoch [70/100], Loss: 13233.1631\n",
            "Epoch [80/100], Loss: 10743.5269\n",
            "Epoch [90/100], Loss: 8947.9004\n",
            "Epoch [100/100], Loss: 8598.7037\n",
            "Fold 3, RMSE: 90.6238784790039\n",
            "Epoch [10/100], Loss: 768174.5781\n",
            "Epoch [20/100], Loss: 130999.4512\n",
            "Epoch [30/100], Loss: 59051.0557\n",
            "Epoch [40/100], Loss: 47033.6016\n",
            "Epoch [50/100], Loss: 35866.5381\n",
            "Epoch [60/100], Loss: 30812.6265\n",
            "Epoch [70/100], Loss: 19633.1289\n",
            "Epoch [80/100], Loss: 23146.6721\n",
            "Epoch [90/100], Loss: 14897.3506\n",
            "Epoch [100/100], Loss: 27700.5337\n",
            "Fold 4, RMSE: 36.170169830322266\n",
            "Epoch [10/100], Loss: 874484.6406\n",
            "Epoch [20/100], Loss: 83224.7246\n",
            "Epoch [30/100], Loss: 23623.6470\n",
            "Epoch [40/100], Loss: 16484.7920\n",
            "Epoch [50/100], Loss: 22755.4636\n",
            "Epoch [60/100], Loss: 18433.6091\n",
            "Epoch [70/100], Loss: 16009.6624\n",
            "Epoch [80/100], Loss: 18212.1726\n",
            "Epoch [90/100], Loss: 14407.0293\n",
            "Epoch [100/100], Loss: 10933.8988\n",
            "Fold 5, RMSE: 48.18602752685547\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 57.224213409423825\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 190896.2109\n",
            "Epoch [20/150], Loss: 85280.9111\n",
            "Epoch [30/150], Loss: 55734.1782\n",
            "Epoch [40/150], Loss: 37786.8496\n",
            "Epoch [50/150], Loss: 18514.1270\n",
            "Epoch [60/150], Loss: 17685.9471\n",
            "Epoch [70/150], Loss: 33536.2507\n",
            "Epoch [80/150], Loss: 14484.1562\n",
            "Epoch [90/150], Loss: 15649.1880\n",
            "Epoch [100/150], Loss: 11212.0651\n",
            "Epoch [110/150], Loss: 11799.7034\n",
            "Epoch [120/150], Loss: 12423.0049\n",
            "Epoch [130/150], Loss: 10802.8448\n",
            "Epoch [140/150], Loss: 12383.2224\n",
            "Epoch [150/150], Loss: 20133.3938\n",
            "Fold 1, RMSE: 46.7119140625\n",
            "Epoch [10/150], Loss: 423990.1250\n",
            "Epoch [20/150], Loss: 59761.3457\n",
            "Epoch [30/150], Loss: 28538.6509\n",
            "Epoch [40/150], Loss: 27934.9141\n",
            "Epoch [50/150], Loss: 18910.1909\n",
            "Epoch [60/150], Loss: 18390.4121\n",
            "Epoch [70/150], Loss: 13322.0349\n",
            "Epoch [80/150], Loss: 13506.8496\n",
            "Epoch [90/150], Loss: 25428.9131\n",
            "Epoch [100/150], Loss: 15159.9774\n",
            "Epoch [110/150], Loss: 14559.7380\n",
            "Epoch [120/150], Loss: 13781.1274\n",
            "Epoch [130/150], Loss: 21162.5696\n",
            "Epoch [140/150], Loss: 14106.9072\n",
            "Epoch [150/150], Loss: 11092.1271\n",
            "Fold 2, RMSE: 69.99284362792969\n",
            "Epoch [10/150], Loss: 342770.0312\n",
            "Epoch [20/150], Loss: 55418.3613\n",
            "Epoch [30/150], Loss: 22072.2275\n",
            "Epoch [40/150], Loss: 31867.5752\n",
            "Epoch [50/150], Loss: 16245.1938\n",
            "Epoch [60/150], Loss: 18909.4358\n",
            "Epoch [70/150], Loss: 7738.6552\n",
            "Epoch [80/150], Loss: 13986.0916\n",
            "Epoch [90/150], Loss: 12686.7380\n",
            "Epoch [100/150], Loss: 9033.6550\n",
            "Epoch [110/150], Loss: 9308.2483\n",
            "Epoch [120/150], Loss: 7748.8497\n",
            "Epoch [130/150], Loss: 8912.4702\n",
            "Epoch [140/150], Loss: 9614.4874\n",
            "Epoch [150/150], Loss: 10018.4613\n",
            "Fold 3, RMSE: 92.40123748779297\n",
            "Epoch [10/150], Loss: 143822.4805\n",
            "Epoch [20/150], Loss: 48610.7041\n",
            "Epoch [30/150], Loss: 21326.6357\n",
            "Epoch [40/150], Loss: 24922.0972\n",
            "Epoch [50/150], Loss: 30881.3125\n",
            "Epoch [60/150], Loss: 19053.1467\n",
            "Epoch [70/150], Loss: 14478.9989\n",
            "Epoch [80/150], Loss: 14662.4177\n",
            "Epoch [90/150], Loss: 16382.0122\n",
            "Epoch [100/150], Loss: 13216.2520\n",
            "Epoch [110/150], Loss: 13386.8534\n",
            "Epoch [120/150], Loss: 21192.7327\n",
            "Epoch [130/150], Loss: 13311.1907\n",
            "Epoch [140/150], Loss: 15827.4688\n",
            "Epoch [150/150], Loss: 13216.7593\n",
            "Fold 4, RMSE: 35.63783645629883\n",
            "Epoch [10/150], Loss: 237985.9648\n",
            "Epoch [20/150], Loss: 37405.5615\n",
            "Epoch [30/150], Loss: 23396.4575\n",
            "Epoch [40/150], Loss: 27132.6177\n",
            "Epoch [50/150], Loss: 16059.9124\n",
            "Epoch [60/150], Loss: 14895.1152\n",
            "Epoch [70/150], Loss: 13593.1558\n",
            "Epoch [80/150], Loss: 13887.4070\n",
            "Epoch [90/150], Loss: 13706.8708\n",
            "Epoch [100/150], Loss: 11427.4917\n",
            "Epoch [110/150], Loss: 10531.6970\n",
            "Epoch [120/150], Loss: 13107.4182\n",
            "Epoch [130/150], Loss: 17836.9839\n",
            "Epoch [140/150], Loss: 13626.5388\n",
            "Epoch [150/150], Loss: 12898.6282\n",
            "Fold 5, RMSE: 48.98684310913086\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 58.74613494873047\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 47539.1084\n",
            "Epoch [20/100], Loss: 16530.7017\n",
            "Epoch [30/100], Loss: 15853.8550\n",
            "Epoch [40/100], Loss: 12358.4160\n",
            "Epoch [50/100], Loss: 12157.7612\n",
            "Epoch [60/100], Loss: 10797.5700\n",
            "Epoch [70/100], Loss: 11404.0120\n",
            "Epoch [80/100], Loss: 11567.9167\n",
            "Epoch [90/100], Loss: 12585.4844\n",
            "Epoch [100/100], Loss: 10189.4271\n",
            "Fold 1, RMSE: 43.23666000366211\n",
            "Epoch [10/100], Loss: 42847.3711\n",
            "Epoch [20/100], Loss: 25916.9487\n",
            "Epoch [30/100], Loss: 15047.5005\n",
            "Epoch [40/100], Loss: 12962.4348\n",
            "Epoch [50/100], Loss: 11079.3279\n",
            "Epoch [60/100], Loss: 9366.3491\n",
            "Epoch [70/100], Loss: 10784.2117\n",
            "Epoch [80/100], Loss: 10188.6272\n",
            "Epoch [90/100], Loss: 11675.8477\n",
            "Epoch [100/100], Loss: 9923.1114\n",
            "Fold 2, RMSE: 70.35847473144531\n",
            "Epoch [10/100], Loss: 37370.4170\n",
            "Epoch [20/100], Loss: 8695.7700\n",
            "Epoch [30/100], Loss: 11678.1204\n",
            "Epoch [40/100], Loss: 9514.4907\n",
            "Epoch [50/100], Loss: 10226.4746\n",
            "Epoch [60/100], Loss: 8879.6476\n",
            "Epoch [70/100], Loss: 11809.5676\n",
            "Epoch [80/100], Loss: 11670.3022\n",
            "Epoch [90/100], Loss: 10818.0481\n",
            "Epoch [100/100], Loss: 8796.7410\n",
            "Fold 3, RMSE: 94.95978546142578\n",
            "Epoch [10/100], Loss: 23242.5547\n",
            "Epoch [20/100], Loss: 19783.8140\n",
            "Epoch [30/100], Loss: 16736.5923\n",
            "Epoch [40/100], Loss: 13450.3701\n",
            "Epoch [50/100], Loss: 11952.1909\n",
            "Epoch [60/100], Loss: 19742.7458\n",
            "Epoch [70/100], Loss: 11520.0256\n",
            "Epoch [80/100], Loss: 13812.6711\n",
            "Epoch [90/100], Loss: 11832.0197\n",
            "Epoch [100/100], Loss: 14935.7693\n",
            "Fold 4, RMSE: 35.63748550415039\n",
            "Epoch [10/100], Loss: 41664.5845\n",
            "Epoch [20/100], Loss: 15328.8599\n",
            "Epoch [30/100], Loss: 12643.3511\n",
            "Epoch [40/100], Loss: 11272.3416\n",
            "Epoch [50/100], Loss: 10791.3848\n",
            "Epoch [60/100], Loss: 13458.0266\n",
            "Epoch [70/100], Loss: 10431.1969\n",
            "Epoch [80/100], Loss: 10943.1091\n",
            "Epoch [90/100], Loss: 16275.3843\n",
            "Epoch [100/100], Loss: 10834.0588\n",
            "Fold 5, RMSE: 48.4408073425293\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 58.526642608642575\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 41373.0388\n",
            "Epoch [20/150], Loss: 14727.3984\n",
            "Epoch [30/150], Loss: 17230.0662\n",
            "Epoch [40/150], Loss: 13815.7969\n",
            "Epoch [50/150], Loss: 11350.0398\n",
            "Epoch [60/150], Loss: 12644.1174\n",
            "Epoch [70/150], Loss: 12039.0918\n",
            "Epoch [80/150], Loss: 9938.5557\n",
            "Epoch [90/150], Loss: 9809.4680\n",
            "Epoch [100/150], Loss: 7832.0317\n",
            "Epoch [110/150], Loss: 5697.3803\n",
            "Epoch [120/150], Loss: 4856.6646\n",
            "Epoch [130/150], Loss: 4031.3835\n",
            "Epoch [140/150], Loss: 6471.5247\n",
            "Epoch [150/150], Loss: 3226.3145\n",
            "Fold 1, RMSE: 57.05043411254883\n",
            "Epoch [10/150], Loss: 55794.8486\n",
            "Epoch [20/150], Loss: 14657.5190\n",
            "Epoch [30/150], Loss: 15354.9766\n",
            "Epoch [40/150], Loss: 12782.5486\n",
            "Epoch [50/150], Loss: 16749.5771\n",
            "Epoch [60/150], Loss: 12547.3547\n",
            "Epoch [70/150], Loss: 8947.4033\n",
            "Epoch [80/150], Loss: 8543.7474\n",
            "Epoch [90/150], Loss: 9707.7864\n",
            "Epoch [100/150], Loss: 12860.9187\n",
            "Epoch [110/150], Loss: 14820.4922\n",
            "Epoch [120/150], Loss: 9991.3057\n",
            "Epoch [130/150], Loss: 10395.9548\n",
            "Epoch [140/150], Loss: 8929.9119\n",
            "Epoch [150/150], Loss: 11236.9814\n",
            "Fold 2, RMSE: 67.51513671875\n",
            "Epoch [10/150], Loss: 23967.1426\n",
            "Epoch [20/150], Loss: 11907.8628\n",
            "Epoch [30/150], Loss: 10071.3384\n",
            "Epoch [40/150], Loss: 8995.4939\n",
            "Epoch [50/150], Loss: 10797.1545\n",
            "Epoch [60/150], Loss: 7971.3402\n",
            "Epoch [70/150], Loss: 10200.6619\n",
            "Epoch [80/150], Loss: 7263.2946\n",
            "Epoch [90/150], Loss: 7522.5966\n",
            "Epoch [100/150], Loss: 7757.5459\n",
            "Epoch [110/150], Loss: 7860.8059\n",
            "Epoch [120/150], Loss: 9609.2666\n",
            "Epoch [130/150], Loss: 6326.2927\n",
            "Epoch [140/150], Loss: 9676.4652\n",
            "Epoch [150/150], Loss: 10862.8074\n",
            "Fold 3, RMSE: 89.24031829833984\n",
            "Epoch [10/150], Loss: 39427.4121\n",
            "Epoch [20/150], Loss: 13093.6705\n",
            "Epoch [30/150], Loss: 10695.5188\n",
            "Epoch [40/150], Loss: 16473.5887\n",
            "Epoch [50/150], Loss: 11881.2024\n",
            "Epoch [60/150], Loss: 11057.1130\n",
            "Epoch [70/150], Loss: 12205.5083\n",
            "Epoch [80/150], Loss: 10666.3589\n",
            "Epoch [90/150], Loss: 15739.5332\n",
            "Epoch [100/150], Loss: 17931.7693\n",
            "Epoch [110/150], Loss: 16463.5723\n",
            "Epoch [120/150], Loss: 11983.5596\n",
            "Epoch [130/150], Loss: 15720.2759\n",
            "Epoch [140/150], Loss: 12002.2102\n",
            "Epoch [150/150], Loss: 11689.6003\n",
            "Fold 4, RMSE: 35.72724914550781\n",
            "Epoch [10/150], Loss: 44082.6064\n",
            "Epoch [20/150], Loss: 16545.4414\n",
            "Epoch [30/150], Loss: 15666.7192\n",
            "Epoch [40/150], Loss: 11902.0152\n",
            "Epoch [50/150], Loss: 13169.2524\n",
            "Epoch [60/150], Loss: 9797.4698\n",
            "Epoch [70/150], Loss: 12739.5327\n",
            "Epoch [80/150], Loss: 10501.4290\n",
            "Epoch [90/150], Loss: 13121.1931\n",
            "Epoch [100/150], Loss: 12483.0520\n",
            "Epoch [110/150], Loss: 12013.9930\n",
            "Epoch [120/150], Loss: 11539.5037\n",
            "Epoch [130/150], Loss: 10663.5417\n",
            "Epoch [140/150], Loss: 9334.0952\n",
            "Epoch [150/150], Loss: 14725.2256\n",
            "Fold 5, RMSE: 47.87236404418945\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 59.481100463867186\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 309285.7734\n",
            "Epoch [20/100], Loss: 24078.3931\n",
            "Epoch [30/100], Loss: 19337.4385\n",
            "Epoch [40/100], Loss: 15171.9331\n",
            "Epoch [50/100], Loss: 13458.4280\n",
            "Epoch [60/100], Loss: 13840.7356\n",
            "Epoch [70/100], Loss: 9952.3024\n",
            "Epoch [80/100], Loss: 9428.0621\n",
            "Epoch [90/100], Loss: 9664.9175\n",
            "Epoch [100/100], Loss: 10599.5082\n",
            "Fold 1, RMSE: 52.440216064453125\n",
            "Epoch [10/100], Loss: 173399.9727\n",
            "Epoch [20/100], Loss: 30873.3320\n",
            "Epoch [30/100], Loss: 17142.0989\n",
            "Epoch [40/100], Loss: 17386.5115\n",
            "Epoch [50/100], Loss: 15469.3740\n",
            "Epoch [60/100], Loss: 15736.1714\n",
            "Epoch [70/100], Loss: 13993.0715\n",
            "Epoch [80/100], Loss: 11704.1987\n",
            "Epoch [90/100], Loss: 14240.8447\n",
            "Epoch [100/100], Loss: 12707.8606\n",
            "Fold 2, RMSE: 65.54815673828125\n",
            "Epoch [10/100], Loss: 322930.0547\n",
            "Epoch [20/100], Loss: 38498.9258\n",
            "Epoch [30/100], Loss: 12563.6907\n",
            "Epoch [40/100], Loss: 12829.1365\n",
            "Epoch [50/100], Loss: 11800.0784\n",
            "Epoch [60/100], Loss: 8117.4998\n",
            "Epoch [70/100], Loss: 7538.8710\n",
            "Epoch [80/100], Loss: 6441.3329\n",
            "Epoch [90/100], Loss: 6318.6576\n",
            "Epoch [100/100], Loss: 4387.9823\n",
            "Fold 3, RMSE: 92.05988311767578\n",
            "Epoch [10/100], Loss: 719666.9375\n",
            "Epoch [20/100], Loss: 82864.6035\n",
            "Epoch [30/100], Loss: 62039.2295\n",
            "Epoch [40/100], Loss: 48052.0029\n",
            "Epoch [50/100], Loss: 26526.6157\n",
            "Epoch [60/100], Loss: 15653.6428\n",
            "Epoch [70/100], Loss: 21031.9553\n",
            "Epoch [80/100], Loss: 15297.1179\n",
            "Epoch [90/100], Loss: 11641.2153\n",
            "Epoch [100/100], Loss: 15273.6543\n",
            "Fold 4, RMSE: 35.76300811767578\n",
            "Epoch [10/100], Loss: 143188.7109\n",
            "Epoch [20/100], Loss: 45964.0078\n",
            "Epoch [30/100], Loss: 25048.7642\n",
            "Epoch [40/100], Loss: 14393.8289\n",
            "Epoch [50/100], Loss: 10548.1406\n",
            "Epoch [60/100], Loss: 10662.3199\n",
            "Epoch [70/100], Loss: 9931.7834\n",
            "Epoch [80/100], Loss: 13017.4412\n",
            "Epoch [90/100], Loss: 13272.2151\n",
            "Epoch [100/100], Loss: 16649.3604\n",
            "Fold 5, RMSE: 51.37163162231445\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 59.43657913208008\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 270986.1699\n",
            "Epoch [20/150], Loss: 60418.8828\n",
            "Epoch [30/150], Loss: 41121.3936\n",
            "Epoch [40/150], Loss: 33002.2939\n",
            "Epoch [50/150], Loss: 30135.8389\n",
            "Epoch [60/150], Loss: 21952.2378\n",
            "Epoch [70/150], Loss: 18510.7886\n",
            "Epoch [80/150], Loss: 21231.4727\n",
            "Epoch [90/150], Loss: 21423.4712\n",
            "Epoch [100/150], Loss: 15033.4082\n",
            "Epoch [110/150], Loss: 17473.2825\n",
            "Epoch [120/150], Loss: 16870.9927\n",
            "Epoch [130/150], Loss: 13090.6770\n",
            "Epoch [140/150], Loss: 12793.7004\n",
            "Epoch [150/150], Loss: 19058.6672\n",
            "Fold 1, RMSE: 45.67207717895508\n",
            "Epoch [10/150], Loss: 154610.3359\n",
            "Epoch [20/150], Loss: 31369.6484\n",
            "Epoch [30/150], Loss: 23059.3203\n",
            "Epoch [40/150], Loss: 12060.0533\n",
            "Epoch [50/150], Loss: 21300.3901\n",
            "Epoch [60/150], Loss: 14658.4031\n",
            "Epoch [70/150], Loss: 13022.0598\n",
            "Epoch [80/150], Loss: 12405.8782\n",
            "Epoch [90/150], Loss: 10033.7278\n",
            "Epoch [100/150], Loss: 9402.8073\n",
            "Epoch [110/150], Loss: 10080.9637\n",
            "Epoch [120/150], Loss: 15830.9004\n",
            "Epoch [130/150], Loss: 18086.3118\n",
            "Epoch [140/150], Loss: 10477.9673\n",
            "Epoch [150/150], Loss: 8753.8289\n",
            "Fold 2, RMSE: 66.95289611816406\n",
            "Epoch [10/150], Loss: 340907.0781\n",
            "Epoch [20/150], Loss: 45134.4541\n",
            "Epoch [30/150], Loss: 22816.2036\n",
            "Epoch [40/150], Loss: 17753.7563\n",
            "Epoch [50/150], Loss: 15777.3796\n",
            "Epoch [60/150], Loss: 10501.9293\n",
            "Epoch [70/150], Loss: 14121.3481\n",
            "Epoch [80/150], Loss: 15945.5046\n",
            "Epoch [90/150], Loss: 8817.9930\n",
            "Epoch [100/150], Loss: 9905.7961\n",
            "Epoch [110/150], Loss: 14356.2124\n",
            "Epoch [120/150], Loss: 11212.4534\n",
            "Epoch [130/150], Loss: 9727.3630\n",
            "Epoch [140/150], Loss: 7229.1504\n",
            "Epoch [150/150], Loss: 9667.4067\n",
            "Fold 3, RMSE: 89.83015441894531\n",
            "Epoch [10/150], Loss: 255873.5508\n",
            "Epoch [20/150], Loss: 51550.9229\n",
            "Epoch [30/150], Loss: 23214.1140\n",
            "Epoch [40/150], Loss: 17732.4402\n",
            "Epoch [50/150], Loss: 22748.3311\n",
            "Epoch [60/150], Loss: 18002.4878\n",
            "Epoch [70/150], Loss: 13438.1865\n",
            "Epoch [80/150], Loss: 16090.3264\n",
            "Epoch [90/150], Loss: 13615.6367\n",
            "Epoch [100/150], Loss: 14660.0874\n",
            "Epoch [110/150], Loss: 11504.6738\n",
            "Epoch [120/150], Loss: 9819.9281\n",
            "Epoch [130/150], Loss: 10717.5303\n",
            "Epoch [140/150], Loss: 16327.2032\n",
            "Epoch [150/150], Loss: 10031.8440\n",
            "Fold 4, RMSE: 35.366947174072266\n",
            "Epoch [10/150], Loss: 636371.7656\n",
            "Epoch [20/150], Loss: 69336.8604\n",
            "Epoch [30/150], Loss: 18684.4500\n",
            "Epoch [40/150], Loss: 16589.6646\n",
            "Epoch [50/150], Loss: 15462.4062\n",
            "Epoch [60/150], Loss: 18121.4414\n",
            "Epoch [70/150], Loss: 10712.2747\n",
            "Epoch [80/150], Loss: 17715.8872\n",
            "Epoch [90/150], Loss: 12599.0845\n",
            "Epoch [100/150], Loss: 16486.9211\n",
            "Epoch [110/150], Loss: 12902.9860\n",
            "Epoch [120/150], Loss: 14546.1238\n",
            "Epoch [130/150], Loss: 14966.4663\n",
            "Epoch [140/150], Loss: 10670.4406\n",
            "Epoch [150/150], Loss: 12058.2839\n",
            "Fold 5, RMSE: 49.98911666870117\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 57.56223831176758\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 28598.0225\n",
            "Epoch [20/100], Loss: 17464.4092\n",
            "Epoch [30/100], Loss: 12584.7068\n",
            "Epoch [40/100], Loss: 16997.4785\n",
            "Epoch [50/100], Loss: 13537.9485\n",
            "Epoch [60/100], Loss: 18837.2964\n",
            "Epoch [70/100], Loss: 19645.0830\n",
            "Epoch [80/100], Loss: 12045.0786\n",
            "Epoch [90/100], Loss: 13467.3948\n",
            "Epoch [100/100], Loss: 21833.1104\n",
            "Fold 1, RMSE: 50.86071014404297\n",
            "Epoch [10/100], Loss: 27357.6797\n",
            "Epoch [20/100], Loss: 14263.7031\n",
            "Epoch [30/100], Loss: 19424.3191\n",
            "Epoch [40/100], Loss: 10134.5624\n",
            "Epoch [50/100], Loss: 11115.7944\n",
            "Epoch [60/100], Loss: 10865.0278\n",
            "Epoch [70/100], Loss: 9413.9133\n",
            "Epoch [80/100], Loss: 15842.0447\n",
            "Epoch [90/100], Loss: 10502.5366\n",
            "Epoch [100/100], Loss: 9748.0131\n",
            "Fold 2, RMSE: 74.682861328125\n",
            "Epoch [10/100], Loss: 32452.6714\n",
            "Epoch [20/100], Loss: 12860.4766\n",
            "Epoch [30/100], Loss: 9716.0388\n",
            "Epoch [40/100], Loss: 6499.6941\n",
            "Epoch [50/100], Loss: 9040.2977\n",
            "Epoch [60/100], Loss: 7952.4403\n",
            "Epoch [70/100], Loss: 8494.6891\n",
            "Epoch [80/100], Loss: 6914.2031\n",
            "Epoch [90/100], Loss: 8044.2427\n",
            "Epoch [100/100], Loss: 6801.9648\n",
            "Fold 3, RMSE: 92.51329803466797\n",
            "Epoch [10/100], Loss: 47349.1333\n",
            "Epoch [20/100], Loss: 14856.6445\n",
            "Epoch [30/100], Loss: 12840.7500\n",
            "Epoch [40/100], Loss: 19763.6841\n",
            "Epoch [50/100], Loss: 17543.1138\n",
            "Epoch [60/100], Loss: 11620.4575\n",
            "Epoch [70/100], Loss: 11939.3818\n",
            "Epoch [80/100], Loss: 14668.7505\n",
            "Epoch [90/100], Loss: 11241.1270\n",
            "Epoch [100/100], Loss: 11586.6267\n",
            "Fold 4, RMSE: 36.270652770996094\n",
            "Epoch [10/100], Loss: 51374.5127\n",
            "Epoch [20/100], Loss: 15219.5061\n",
            "Epoch [30/100], Loss: 14374.2979\n",
            "Epoch [40/100], Loss: 14218.1526\n",
            "Epoch [50/100], Loss: 12811.2927\n",
            "Epoch [60/100], Loss: 13191.5016\n",
            "Epoch [70/100], Loss: 11899.1035\n",
            "Epoch [80/100], Loss: 11709.0427\n",
            "Epoch [90/100], Loss: 12066.3069\n",
            "Epoch [100/100], Loss: 10543.8108\n",
            "Fold 5, RMSE: 48.37613296508789\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 60.540731048583986\n",
            "Training with neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 37549.2842\n",
            "Epoch [20/150], Loss: 12206.7220\n",
            "Epoch [30/150], Loss: 14483.0137\n",
            "Epoch [40/150], Loss: 12849.9634\n",
            "Epoch [50/150], Loss: 12230.0408\n",
            "Epoch [60/150], Loss: 16040.6897\n",
            "Epoch [70/150], Loss: 10252.9723\n",
            "Epoch [80/150], Loss: 19975.6321\n",
            "Epoch [90/150], Loss: 14050.4763\n",
            "Epoch [100/150], Loss: 13767.9617\n",
            "Epoch [110/150], Loss: 12088.8293\n",
            "Epoch [120/150], Loss: 9758.9529\n",
            "Epoch [130/150], Loss: 8043.0527\n",
            "Epoch [140/150], Loss: 5399.0095\n",
            "Epoch [150/150], Loss: 5962.5635\n",
            "Fold 1, RMSE: 53.66489791870117\n",
            "Epoch [10/150], Loss: 41308.0906\n",
            "Epoch [20/150], Loss: 14831.0806\n",
            "Epoch [30/150], Loss: 10467.6996\n",
            "Epoch [40/150], Loss: 13726.2390\n",
            "Epoch [50/150], Loss: 11424.6672\n",
            "Epoch [60/150], Loss: 9505.1296\n",
            "Epoch [70/150], Loss: 10093.0298\n",
            "Epoch [80/150], Loss: 9778.4320\n",
            "Epoch [90/150], Loss: 9855.1182\n",
            "Epoch [100/150], Loss: 10238.1102\n",
            "Epoch [110/150], Loss: 10776.9861\n",
            "Epoch [120/150], Loss: 10632.1274\n",
            "Epoch [130/150], Loss: 19298.5164\n",
            "Epoch [140/150], Loss: 11259.7383\n",
            "Epoch [150/150], Loss: 11724.9202\n",
            "Fold 2, RMSE: 63.50339889526367\n",
            "Epoch [10/150], Loss: 25239.4795\n",
            "Epoch [20/150], Loss: 9984.2292\n",
            "Epoch [30/150], Loss: 9079.4973\n",
            "Epoch [40/150], Loss: 10463.9358\n",
            "Epoch [50/150], Loss: 8825.4624\n",
            "Epoch [60/150], Loss: 9286.9111\n",
            "Epoch [70/150], Loss: 8596.7762\n",
            "Epoch [80/150], Loss: 8029.4165\n",
            "Epoch [90/150], Loss: 5832.0221\n",
            "Epoch [100/150], Loss: 6346.1727\n",
            "Epoch [110/150], Loss: 7549.4028\n",
            "Epoch [120/150], Loss: 5917.9944\n",
            "Epoch [130/150], Loss: 8404.2718\n",
            "Epoch [140/150], Loss: 8224.4783\n",
            "Epoch [150/150], Loss: 10163.7751\n",
            "Fold 3, RMSE: 97.1587905883789\n",
            "Epoch [10/150], Loss: 29406.5361\n",
            "Epoch [20/150], Loss: 26150.9324\n",
            "Epoch [30/150], Loss: 16551.2397\n",
            "Epoch [40/150], Loss: 12643.7520\n",
            "Epoch [50/150], Loss: 13819.9648\n",
            "Epoch [60/150], Loss: 12415.8452\n",
            "Epoch [70/150], Loss: 12598.5554\n",
            "Epoch [80/150], Loss: 14654.9194\n",
            "Epoch [90/150], Loss: 10406.3680\n",
            "Epoch [100/150], Loss: 13860.3020\n",
            "Epoch [110/150], Loss: 10971.1255\n",
            "Epoch [120/150], Loss: 10408.4467\n",
            "Epoch [130/150], Loss: 15960.7700\n",
            "Epoch [140/150], Loss: 9904.7249\n",
            "Epoch [150/150], Loss: 10111.8170\n",
            "Fold 4, RMSE: 34.812801361083984\n",
            "Epoch [10/150], Loss: 77100.3359\n",
            "Epoch [20/150], Loss: 13806.7826\n",
            "Epoch [30/150], Loss: 11252.9109\n",
            "Epoch [40/150], Loss: 12494.8894\n",
            "Epoch [50/150], Loss: 13512.9901\n",
            "Epoch [60/150], Loss: 10796.7925\n",
            "Epoch [70/150], Loss: 14037.1265\n",
            "Epoch [80/150], Loss: 10557.6079\n",
            "Epoch [90/150], Loss: 12541.0422\n",
            "Epoch [100/150], Loss: 12489.2358\n",
            "Epoch [110/150], Loss: 12755.4905\n",
            "Epoch [120/150], Loss: 11690.0210\n",
            "Epoch [130/150], Loss: 10185.5801\n",
            "Epoch [140/150], Loss: 11096.0778\n",
            "Epoch [150/150], Loss: 11622.9932\n",
            "Fold 5, RMSE: 48.40620040893555\n",
            "Avg RMSE for neurons=128, dropout_rate=0.2, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 59.50921783447266\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 13106.4099\n",
            "Epoch [20/100], Loss: 16438.0084\n",
            "Epoch [30/100], Loss: 7502.9976\n",
            "Epoch [40/100], Loss: 6318.8978\n",
            "Epoch [50/100], Loss: 5031.8683\n",
            "Epoch [60/100], Loss: 5612.6482\n",
            "Epoch [70/100], Loss: 5621.1478\n",
            "Epoch [80/100], Loss: 2650.3778\n",
            "Epoch [90/100], Loss: 4633.3424\n",
            "Epoch [100/100], Loss: 1824.3685\n",
            "Fold 1, RMSE: 61.00815200805664\n",
            "Epoch [10/100], Loss: 22059.0796\n",
            "Epoch [20/100], Loss: 12397.3152\n",
            "Epoch [30/100], Loss: 7963.3323\n",
            "Epoch [40/100], Loss: 8869.2648\n",
            "Epoch [50/100], Loss: 8050.8004\n",
            "Epoch [60/100], Loss: 3259.7821\n",
            "Epoch [70/100], Loss: 3683.3618\n",
            "Epoch [80/100], Loss: 2428.1150\n",
            "Epoch [90/100], Loss: 1363.3531\n",
            "Epoch [100/100], Loss: 3449.7943\n",
            "Fold 2, RMSE: 65.88835144042969\n",
            "Epoch [10/100], Loss: 9781.2366\n",
            "Epoch [20/100], Loss: 12756.0173\n",
            "Epoch [30/100], Loss: 10419.7603\n",
            "Epoch [40/100], Loss: 4753.6898\n",
            "Epoch [50/100], Loss: 4361.9341\n",
            "Epoch [60/100], Loss: 4018.1177\n",
            "Epoch [70/100], Loss: 2379.9649\n",
            "Epoch [80/100], Loss: 2023.7321\n",
            "Epoch [90/100], Loss: 1218.4935\n",
            "Epoch [100/100], Loss: 2337.2587\n",
            "Fold 3, RMSE: 96.90861511230469\n",
            "Epoch [10/100], Loss: 16264.1985\n",
            "Epoch [20/100], Loss: 10164.4116\n",
            "Epoch [30/100], Loss: 9331.9314\n",
            "Epoch [40/100], Loss: 4716.5556\n",
            "Epoch [50/100], Loss: 5645.4308\n",
            "Epoch [60/100], Loss: 3617.1274\n",
            "Epoch [70/100], Loss: 2342.9588\n",
            "Epoch [80/100], Loss: 5981.1311\n",
            "Epoch [90/100], Loss: 3024.2145\n",
            "Epoch [100/100], Loss: 3182.9618\n",
            "Fold 4, RMSE: 45.148128509521484\n",
            "Epoch [10/100], Loss: 25024.2781\n",
            "Epoch [20/100], Loss: 13270.9761\n",
            "Epoch [30/100], Loss: 9989.0702\n",
            "Epoch [40/100], Loss: 10789.9177\n",
            "Epoch [50/100], Loss: 7061.0054\n",
            "Epoch [60/100], Loss: 5657.3606\n",
            "Epoch [70/100], Loss: 8720.6467\n",
            "Epoch [80/100], Loss: 4265.1058\n",
            "Epoch [90/100], Loss: 5632.8795\n",
            "Epoch [100/100], Loss: 3799.5488\n",
            "Fold 5, RMSE: 44.90959930419922\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 62.772569274902345\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 11002.4443\n",
            "Epoch [20/150], Loss: 8520.6438\n",
            "Epoch [30/150], Loss: 8759.8099\n",
            "Epoch [40/150], Loss: 7131.5466\n",
            "Epoch [50/150], Loss: 3975.9022\n",
            "Epoch [60/150], Loss: 4396.8447\n",
            "Epoch [70/150], Loss: 2070.9955\n",
            "Epoch [80/150], Loss: 5537.0900\n",
            "Epoch [90/150], Loss: 1051.5745\n",
            "Epoch [100/150], Loss: 2141.4236\n",
            "Epoch [110/150], Loss: 2981.8585\n",
            "Epoch [120/150], Loss: 3606.3786\n",
            "Epoch [130/150], Loss: 1474.2607\n",
            "Epoch [140/150], Loss: 2108.0570\n",
            "Epoch [150/150], Loss: 3582.8404\n",
            "Fold 1, RMSE: 59.4569206237793\n",
            "Epoch [10/150], Loss: 8805.6693\n",
            "Epoch [20/150], Loss: 10143.9692\n",
            "Epoch [30/150], Loss: 9840.3811\n",
            "Epoch [40/150], Loss: 11479.0518\n",
            "Epoch [50/150], Loss: 4406.4651\n",
            "Epoch [60/150], Loss: 3147.6957\n",
            "Epoch [70/150], Loss: 4105.7146\n",
            "Epoch [80/150], Loss: 3198.1901\n",
            "Epoch [90/150], Loss: 5233.6323\n",
            "Epoch [100/150], Loss: 5069.0663\n",
            "Epoch [110/150], Loss: 3286.2171\n",
            "Epoch [120/150], Loss: 2382.6895\n",
            "Epoch [130/150], Loss: 2545.7660\n",
            "Epoch [140/150], Loss: 2876.7999\n",
            "Epoch [150/150], Loss: 1949.7617\n",
            "Fold 2, RMSE: 65.01300811767578\n",
            "Epoch [10/150], Loss: 9153.8693\n",
            "Epoch [20/150], Loss: 10189.7273\n",
            "Epoch [30/150], Loss: 6145.6959\n",
            "Epoch [40/150], Loss: 10956.4935\n",
            "Epoch [50/150], Loss: 3197.2700\n",
            "Epoch [60/150], Loss: 2126.1139\n",
            "Epoch [70/150], Loss: 1710.9798\n",
            "Epoch [80/150], Loss: 2576.2959\n",
            "Epoch [90/150], Loss: 2403.1256\n",
            "Epoch [100/150], Loss: 7558.4129\n",
            "Epoch [110/150], Loss: 1414.1652\n",
            "Epoch [120/150], Loss: 3030.6378\n",
            "Epoch [130/150], Loss: 2822.8811\n",
            "Epoch [140/150], Loss: 1625.4177\n",
            "Epoch [150/150], Loss: 1016.6368\n",
            "Fold 3, RMSE: 93.93995666503906\n",
            "Epoch [10/150], Loss: 14120.0466\n",
            "Epoch [20/150], Loss: 11852.1075\n",
            "Epoch [30/150], Loss: 10509.1742\n",
            "Epoch [40/150], Loss: 5044.7101\n",
            "Epoch [50/150], Loss: 7071.4014\n",
            "Epoch [60/150], Loss: 8209.8394\n",
            "Epoch [70/150], Loss: 5829.0717\n",
            "Epoch [80/150], Loss: 2975.7788\n",
            "Epoch [90/150], Loss: 3044.9163\n",
            "Epoch [100/150], Loss: 3211.8142\n",
            "Epoch [110/150], Loss: 2135.0795\n",
            "Epoch [120/150], Loss: 5931.4089\n",
            "Epoch [130/150], Loss: 5432.8704\n",
            "Epoch [140/150], Loss: 1119.8010\n",
            "Epoch [150/150], Loss: 2826.1319\n",
            "Fold 4, RMSE: 42.10377502441406\n",
            "Epoch [10/150], Loss: 19555.4561\n",
            "Epoch [20/150], Loss: 11770.7498\n",
            "Epoch [30/150], Loss: 9162.8870\n",
            "Epoch [40/150], Loss: 6595.4946\n",
            "Epoch [50/150], Loss: 4833.8181\n",
            "Epoch [60/150], Loss: 4260.4739\n",
            "Epoch [70/150], Loss: 2238.8023\n",
            "Epoch [80/150], Loss: 1673.7777\n",
            "Epoch [90/150], Loss: 1731.6440\n",
            "Epoch [100/150], Loss: 2405.0521\n",
            "Epoch [110/150], Loss: 3818.4443\n",
            "Epoch [120/150], Loss: 2531.5010\n",
            "Epoch [130/150], Loss: 5316.9506\n",
            "Epoch [140/150], Loss: 2316.6037\n",
            "Epoch [150/150], Loss: 3345.1168\n",
            "Fold 5, RMSE: 46.66170883178711\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 61.43507385253906\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 11219.7004\n",
            "Epoch [20/100], Loss: 8895.4731\n",
            "Epoch [30/100], Loss: 8983.2104\n",
            "Epoch [40/100], Loss: 6404.4614\n",
            "Epoch [50/100], Loss: 3658.0294\n",
            "Epoch [60/100], Loss: 4386.2105\n",
            "Epoch [70/100], Loss: 1853.6844\n",
            "Epoch [80/100], Loss: 1685.8201\n",
            "Epoch [90/100], Loss: 2841.0975\n",
            "Epoch [100/100], Loss: 1820.8984\n",
            "Fold 1, RMSE: 57.08656692504883\n",
            "Epoch [10/100], Loss: 12082.4790\n",
            "Epoch [20/100], Loss: 8489.2142\n",
            "Epoch [30/100], Loss: 11162.1279\n",
            "Epoch [40/100], Loss: 3722.8071\n",
            "Epoch [50/100], Loss: 1882.1232\n",
            "Epoch [60/100], Loss: 3605.6815\n",
            "Epoch [70/100], Loss: 3765.6693\n",
            "Epoch [80/100], Loss: 2210.6461\n",
            "Epoch [90/100], Loss: 3934.1137\n",
            "Epoch [100/100], Loss: 5903.1654\n",
            "Fold 2, RMSE: 71.60633850097656\n",
            "Epoch [10/100], Loss: 9750.3340\n",
            "Epoch [20/100], Loss: 6093.4502\n",
            "Epoch [30/100], Loss: 3691.7242\n",
            "Epoch [40/100], Loss: 2439.4771\n",
            "Epoch [50/100], Loss: 3174.5638\n",
            "Epoch [60/100], Loss: 1953.9899\n",
            "Epoch [70/100], Loss: 2291.0251\n",
            "Epoch [80/100], Loss: 2469.6685\n",
            "Epoch [90/100], Loss: 2042.5769\n",
            "Epoch [100/100], Loss: 1151.8199\n",
            "Fold 3, RMSE: 96.43336486816406\n",
            "Epoch [10/100], Loss: 12364.0719\n",
            "Epoch [20/100], Loss: 12623.9497\n",
            "Epoch [30/100], Loss: 4423.8529\n",
            "Epoch [40/100], Loss: 3021.7695\n",
            "Epoch [50/100], Loss: 4919.0372\n",
            "Epoch [60/100], Loss: 2921.1694\n",
            "Epoch [70/100], Loss: 5637.7613\n",
            "Epoch [80/100], Loss: 2631.5722\n",
            "Epoch [90/100], Loss: 1187.7018\n",
            "Epoch [100/100], Loss: 1779.6140\n",
            "Fold 4, RMSE: 40.58473205566406\n",
            "Epoch [10/100], Loss: 16716.6240\n",
            "Epoch [20/100], Loss: 11431.4966\n",
            "Epoch [30/100], Loss: 12552.4817\n",
            "Epoch [40/100], Loss: 4899.6110\n",
            "Epoch [50/100], Loss: 5279.6720\n",
            "Epoch [60/100], Loss: 3788.5264\n",
            "Epoch [70/100], Loss: 7632.1115\n",
            "Epoch [80/100], Loss: 4922.8879\n",
            "Epoch [90/100], Loss: 3865.8582\n",
            "Epoch [100/100], Loss: 2311.7434\n",
            "Fold 5, RMSE: 44.736061096191406\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 62.089412689208984\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 12444.7703\n",
            "Epoch [20/150], Loss: 7213.6873\n",
            "Epoch [30/150], Loss: 8227.3593\n",
            "Epoch [40/150], Loss: 3736.7431\n",
            "Epoch [50/150], Loss: 3060.1426\n",
            "Epoch [60/150], Loss: 6806.4048\n",
            "Epoch [70/150], Loss: 6919.1320\n",
            "Epoch [80/150], Loss: 1985.5218\n",
            "Epoch [90/150], Loss: 1469.8336\n",
            "Epoch [100/150], Loss: 2759.8602\n",
            "Epoch [110/150], Loss: 2241.4547\n",
            "Epoch [120/150], Loss: 2840.6633\n",
            "Epoch [130/150], Loss: 1061.6662\n",
            "Epoch [140/150], Loss: 2605.8160\n",
            "Epoch [150/150], Loss: 2137.2503\n",
            "Fold 1, RMSE: 56.082000732421875\n",
            "Epoch [10/150], Loss: 14905.6179\n",
            "Epoch [20/150], Loss: 7748.3993\n",
            "Epoch [30/150], Loss: 6889.7052\n",
            "Epoch [40/150], Loss: 9356.5093\n",
            "Epoch [50/150], Loss: 4307.7178\n",
            "Epoch [60/150], Loss: 4358.0808\n",
            "Epoch [70/150], Loss: 3067.4302\n",
            "Epoch [80/150], Loss: 2231.5792\n",
            "Epoch [90/150], Loss: 3599.5571\n",
            "Epoch [100/150], Loss: 4370.2646\n",
            "Epoch [110/150], Loss: 1397.9445\n",
            "Epoch [120/150], Loss: 2710.8377\n",
            "Epoch [130/150], Loss: 1275.3163\n",
            "Epoch [140/150], Loss: 1461.5966\n",
            "Epoch [150/150], Loss: 1120.3200\n",
            "Fold 2, RMSE: 65.4859390258789\n",
            "Epoch [10/150], Loss: 10000.2527\n",
            "Epoch [20/150], Loss: 6614.3734\n",
            "Epoch [30/150], Loss: 5995.0604\n",
            "Epoch [40/150], Loss: 4709.0147\n",
            "Epoch [50/150], Loss: 3977.2277\n",
            "Epoch [60/150], Loss: 1413.1133\n",
            "Epoch [70/150], Loss: 3928.1306\n",
            "Epoch [80/150], Loss: 3673.0650\n",
            "Epoch [90/150], Loss: 6628.4502\n",
            "Epoch [100/150], Loss: 1708.3713\n",
            "Epoch [110/150], Loss: 2554.8495\n",
            "Epoch [120/150], Loss: 897.1355\n",
            "Epoch [130/150], Loss: 721.3317\n",
            "Epoch [140/150], Loss: 3309.8730\n",
            "Epoch [150/150], Loss: 1534.7027\n",
            "Fold 3, RMSE: 96.88970947265625\n",
            "Epoch [10/150], Loss: 12109.1833\n",
            "Epoch [20/150], Loss: 8648.5669\n",
            "Epoch [30/150], Loss: 4712.6461\n",
            "Epoch [40/150], Loss: 3920.9323\n",
            "Epoch [50/150], Loss: 4375.6884\n",
            "Epoch [60/150], Loss: 2643.8573\n",
            "Epoch [70/150], Loss: 2125.4250\n",
            "Epoch [80/150], Loss: 3741.3713\n",
            "Epoch [90/150], Loss: 3015.1816\n",
            "Epoch [100/150], Loss: 2830.2335\n",
            "Epoch [110/150], Loss: 2342.3320\n",
            "Epoch [120/150], Loss: 1972.2779\n",
            "Epoch [130/150], Loss: 2946.4474\n",
            "Epoch [140/150], Loss: 1942.4200\n",
            "Epoch [150/150], Loss: 1442.9058\n",
            "Fold 4, RMSE: 44.083106994628906\n",
            "Epoch [10/150], Loss: 11747.6343\n",
            "Epoch [20/150], Loss: 10597.6074\n",
            "Epoch [30/150], Loss: 6933.3300\n",
            "Epoch [40/150], Loss: 6642.3101\n",
            "Epoch [50/150], Loss: 3634.3427\n",
            "Epoch [60/150], Loss: 3433.4130\n",
            "Epoch [70/150], Loss: 5734.3019\n",
            "Epoch [80/150], Loss: 2789.8945\n",
            "Epoch [90/150], Loss: 3778.3148\n",
            "Epoch [100/150], Loss: 1953.5766\n",
            "Epoch [110/150], Loss: 3071.1635\n",
            "Epoch [120/150], Loss: 2036.7208\n",
            "Epoch [130/150], Loss: 1133.4858\n",
            "Epoch [140/150], Loss: 1546.4080\n",
            "Epoch [150/150], Loss: 593.5560\n",
            "Fold 5, RMSE: 44.214881896972656\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 61.35112762451172\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14934.9143\n",
            "Epoch [20/100], Loss: 13132.9247\n",
            "Epoch [30/100], Loss: 8800.7462\n",
            "Epoch [40/100], Loss: 5208.9562\n",
            "Epoch [50/100], Loss: 4666.7173\n",
            "Epoch [60/100], Loss: 4753.0905\n",
            "Epoch [70/100], Loss: 2391.2880\n",
            "Epoch [80/100], Loss: 3311.1162\n",
            "Epoch [90/100], Loss: 3116.1303\n",
            "Epoch [100/100], Loss: 4862.9188\n",
            "Fold 1, RMSE: 59.07535171508789\n",
            "Epoch [10/100], Loss: 22181.8013\n",
            "Epoch [20/100], Loss: 10738.1615\n",
            "Epoch [30/100], Loss: 9154.7296\n",
            "Epoch [40/100], Loss: 6230.6539\n",
            "Epoch [50/100], Loss: 3759.1884\n",
            "Epoch [60/100], Loss: 9606.8654\n",
            "Epoch [70/100], Loss: 1479.8111\n",
            "Epoch [80/100], Loss: 2940.5815\n",
            "Epoch [90/100], Loss: 1286.0308\n",
            "Epoch [100/100], Loss: 2297.1790\n",
            "Fold 2, RMSE: 60.78616714477539\n",
            "Epoch [10/100], Loss: 9901.7380\n",
            "Epoch [20/100], Loss: 7294.3634\n",
            "Epoch [30/100], Loss: 4364.9136\n",
            "Epoch [40/100], Loss: 7140.3916\n",
            "Epoch [50/100], Loss: 5197.2932\n",
            "Epoch [60/100], Loss: 3029.6634\n",
            "Epoch [70/100], Loss: 3022.9626\n",
            "Epoch [80/100], Loss: 1867.2610\n",
            "Epoch [90/100], Loss: 2958.0939\n",
            "Epoch [100/100], Loss: 4614.1872\n",
            "Fold 3, RMSE: 89.60987091064453\n",
            "Epoch [10/100], Loss: 13281.5222\n",
            "Epoch [20/100], Loss: 20993.3770\n",
            "Epoch [30/100], Loss: 6064.4505\n",
            "Epoch [40/100], Loss: 3846.6021\n",
            "Epoch [50/100], Loss: 2343.9150\n",
            "Epoch [60/100], Loss: 2926.6584\n",
            "Epoch [70/100], Loss: 4040.7598\n",
            "Epoch [80/100], Loss: 1750.0632\n",
            "Epoch [90/100], Loss: 1723.7152\n",
            "Epoch [100/100], Loss: 3266.7930\n",
            "Fold 4, RMSE: 46.611793518066406\n",
            "Epoch [10/100], Loss: 15392.2749\n",
            "Epoch [20/100], Loss: 11691.8840\n",
            "Epoch [30/100], Loss: 8555.9662\n",
            "Epoch [40/100], Loss: 3303.7067\n",
            "Epoch [50/100], Loss: 5655.3135\n",
            "Epoch [60/100], Loss: 6938.2875\n",
            "Epoch [70/100], Loss: 4133.5225\n",
            "Epoch [80/100], Loss: 3058.4625\n",
            "Epoch [90/100], Loss: 4168.0551\n",
            "Epoch [100/100], Loss: 2201.6292\n",
            "Fold 5, RMSE: 47.30332565307617\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 60.67730178833008\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 21508.4688\n",
            "Epoch [20/150], Loss: 9877.1917\n",
            "Epoch [30/150], Loss: 3734.4070\n",
            "Epoch [40/150], Loss: 9938.9283\n",
            "Epoch [50/150], Loss: 8223.0281\n",
            "Epoch [60/150], Loss: 7995.3611\n",
            "Epoch [70/150], Loss: 4238.3221\n",
            "Epoch [80/150], Loss: 4071.6281\n",
            "Epoch [90/150], Loss: 4839.2984\n",
            "Epoch [100/150], Loss: 5002.8626\n",
            "Epoch [110/150], Loss: 3299.4268\n",
            "Epoch [120/150], Loss: 5434.7174\n",
            "Epoch [130/150], Loss: 5669.1760\n",
            "Epoch [140/150], Loss: 3032.8185\n",
            "Epoch [150/150], Loss: 1939.3511\n",
            "Fold 1, RMSE: 57.1854362487793\n",
            "Epoch [10/150], Loss: 12341.5645\n",
            "Epoch [20/150], Loss: 8389.6383\n",
            "Epoch [30/150], Loss: 8314.1378\n",
            "Epoch [40/150], Loss: 7314.4432\n",
            "Epoch [50/150], Loss: 8963.8794\n",
            "Epoch [60/150], Loss: 4621.8046\n",
            "Epoch [70/150], Loss: 3337.2312\n",
            "Epoch [80/150], Loss: 8179.9187\n",
            "Epoch [90/150], Loss: 2451.6536\n",
            "Epoch [100/150], Loss: 5000.8792\n",
            "Epoch [110/150], Loss: 3236.0711\n",
            "Epoch [120/150], Loss: 2730.9781\n",
            "Epoch [130/150], Loss: 2470.5256\n",
            "Epoch [140/150], Loss: 5126.8849\n",
            "Epoch [150/150], Loss: 2049.6013\n",
            "Fold 2, RMSE: 68.8914566040039\n",
            "Epoch [10/150], Loss: 10709.1959\n",
            "Epoch [20/150], Loss: 8327.2515\n",
            "Epoch [30/150], Loss: 7700.6698\n",
            "Epoch [40/150], Loss: 5545.3186\n",
            "Epoch [50/150], Loss: 5500.6890\n",
            "Epoch [60/150], Loss: 3625.5342\n",
            "Epoch [70/150], Loss: 3114.8474\n",
            "Epoch [80/150], Loss: 2087.2016\n",
            "Epoch [90/150], Loss: 2942.5697\n",
            "Epoch [100/150], Loss: 5262.0402\n",
            "Epoch [110/150], Loss: 5044.1832\n",
            "Epoch [120/150], Loss: 7731.4415\n",
            "Epoch [130/150], Loss: 6508.1221\n",
            "Epoch [140/150], Loss: 8135.1467\n",
            "Epoch [150/150], Loss: 7958.5186\n",
            "Fold 3, RMSE: 92.9400863647461\n",
            "Epoch [10/150], Loss: 19264.0166\n",
            "Epoch [20/150], Loss: 12317.7850\n",
            "Epoch [30/150], Loss: 9825.1631\n",
            "Epoch [40/150], Loss: 6690.3235\n",
            "Epoch [50/150], Loss: 9983.9668\n",
            "Epoch [60/150], Loss: 5708.2230\n",
            "Epoch [70/150], Loss: 4717.8492\n",
            "Epoch [80/150], Loss: 2214.0939\n",
            "Epoch [90/150], Loss: 4167.4649\n",
            "Epoch [100/150], Loss: 5240.8461\n",
            "Epoch [110/150], Loss: 5523.6368\n",
            "Epoch [120/150], Loss: 5334.1157\n",
            "Epoch [130/150], Loss: 854.5907\n",
            "Epoch [140/150], Loss: 2560.0091\n",
            "Epoch [150/150], Loss: 3652.8526\n",
            "Fold 4, RMSE: 45.69734191894531\n",
            "Epoch [10/150], Loss: 20897.7520\n",
            "Epoch [20/150], Loss: 11848.4104\n",
            "Epoch [30/150], Loss: 9375.1604\n",
            "Epoch [40/150], Loss: 6072.6663\n",
            "Epoch [50/150], Loss: 3600.4803\n",
            "Epoch [60/150], Loss: 5220.9664\n",
            "Epoch [70/150], Loss: 4499.9777\n",
            "Epoch [80/150], Loss: 3356.7220\n",
            "Epoch [90/150], Loss: 2361.2119\n",
            "Epoch [100/150], Loss: 3804.6647\n",
            "Epoch [110/150], Loss: 2555.6161\n",
            "Epoch [120/150], Loss: 3134.8799\n",
            "Epoch [130/150], Loss: 4013.4166\n",
            "Epoch [140/150], Loss: 4177.3544\n",
            "Epoch [150/150], Loss: 3772.9463\n",
            "Fold 5, RMSE: 46.478485107421875\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 62.2385612487793\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 8424.2882\n",
            "Epoch [20/100], Loss: 9861.6437\n",
            "Epoch [30/100], Loss: 4906.7274\n",
            "Epoch [40/100], Loss: 2263.5848\n",
            "Epoch [50/100], Loss: 3044.5406\n",
            "Epoch [60/100], Loss: 3421.2244\n",
            "Epoch [70/100], Loss: 2099.1323\n",
            "Epoch [80/100], Loss: 2152.1048\n",
            "Epoch [90/100], Loss: 1772.8340\n",
            "Epoch [100/100], Loss: 1086.4281\n",
            "Fold 1, RMSE: 56.577030181884766\n",
            "Epoch [10/100], Loss: 8864.5586\n",
            "Epoch [20/100], Loss: 4652.8800\n",
            "Epoch [30/100], Loss: 8715.9746\n",
            "Epoch [40/100], Loss: 2294.8349\n",
            "Epoch [50/100], Loss: 3820.6072\n",
            "Epoch [60/100], Loss: 2294.8975\n",
            "Epoch [70/100], Loss: 3565.2241\n",
            "Epoch [80/100], Loss: 2690.5464\n",
            "Epoch [90/100], Loss: 2000.9285\n",
            "Epoch [100/100], Loss: 3455.2128\n",
            "Fold 2, RMSE: 63.27588653564453\n",
            "Epoch [10/100], Loss: 8553.6460\n",
            "Epoch [20/100], Loss: 6632.9692\n",
            "Epoch [30/100], Loss: 5560.5242\n",
            "Epoch [40/100], Loss: 5078.8481\n",
            "Epoch [50/100], Loss: 2364.9469\n",
            "Epoch [60/100], Loss: 2240.5376\n",
            "Epoch [70/100], Loss: 1228.9676\n",
            "Epoch [80/100], Loss: 2918.9143\n",
            "Epoch [90/100], Loss: 868.0019\n",
            "Epoch [100/100], Loss: 1040.6301\n",
            "Fold 3, RMSE: 94.27050018310547\n",
            "Epoch [10/100], Loss: 14759.6436\n",
            "Epoch [20/100], Loss: 5759.5272\n",
            "Epoch [30/100], Loss: 6947.8600\n",
            "Epoch [40/100], Loss: 9578.7404\n",
            "Epoch [50/100], Loss: 1938.0215\n",
            "Epoch [60/100], Loss: 2603.4739\n",
            "Epoch [70/100], Loss: 3207.2897\n",
            "Epoch [80/100], Loss: 3726.5603\n",
            "Epoch [90/100], Loss: 2336.8041\n",
            "Epoch [100/100], Loss: 4718.5938\n",
            "Fold 4, RMSE: 41.85223388671875\n",
            "Epoch [10/100], Loss: 12391.3323\n",
            "Epoch [20/100], Loss: 8723.1086\n",
            "Epoch [30/100], Loss: 4795.6050\n",
            "Epoch [40/100], Loss: 2937.6001\n",
            "Epoch [50/100], Loss: 3098.9811\n",
            "Epoch [60/100], Loss: 1618.3433\n",
            "Epoch [70/100], Loss: 10212.9429\n",
            "Epoch [80/100], Loss: 2097.6151\n",
            "Epoch [90/100], Loss: 3017.3268\n",
            "Epoch [100/100], Loss: 3081.7319\n",
            "Fold 5, RMSE: 47.33290481567383\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 60.66171112060547\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 20186.8138\n",
            "Epoch [20/150], Loss: 8509.7896\n",
            "Epoch [30/150], Loss: 5704.6309\n",
            "Epoch [40/150], Loss: 2373.1707\n",
            "Epoch [50/150], Loss: 2857.3437\n",
            "Epoch [60/150], Loss: 2614.6672\n",
            "Epoch [70/150], Loss: 4710.1203\n",
            "Epoch [80/150], Loss: 4094.2241\n",
            "Epoch [90/150], Loss: 3231.7745\n",
            "Epoch [100/150], Loss: 14646.6552\n",
            "Epoch [110/150], Loss: 3352.1839\n",
            "Epoch [120/150], Loss: 1359.2453\n",
            "Epoch [130/150], Loss: 2111.6995\n",
            "Epoch [140/150], Loss: 5680.7199\n",
            "Epoch [150/150], Loss: 2008.9095\n",
            "Fold 1, RMSE: 57.2242546081543\n",
            "Epoch [10/150], Loss: 12063.9775\n",
            "Epoch [20/150], Loss: 13545.2502\n",
            "Epoch [30/150], Loss: 7492.8474\n",
            "Epoch [40/150], Loss: 2405.1038\n",
            "Epoch [50/150], Loss: 3901.4002\n",
            "Epoch [60/150], Loss: 3728.9121\n",
            "Epoch [70/150], Loss: 2854.7748\n",
            "Epoch [80/150], Loss: 4297.8506\n",
            "Epoch [90/150], Loss: 1996.4404\n",
            "Epoch [100/150], Loss: 1381.6169\n",
            "Epoch [110/150], Loss: 1322.0072\n",
            "Epoch [120/150], Loss: 1318.0506\n",
            "Epoch [130/150], Loss: 976.8963\n",
            "Epoch [140/150], Loss: 832.3705\n",
            "Epoch [150/150], Loss: 3343.9550\n",
            "Fold 2, RMSE: 60.57107162475586\n",
            "Epoch [10/150], Loss: 9929.7811\n",
            "Epoch [20/150], Loss: 8754.5970\n",
            "Epoch [30/150], Loss: 3903.4015\n",
            "Epoch [40/150], Loss: 3288.4730\n",
            "Epoch [50/150], Loss: 2488.7905\n",
            "Epoch [60/150], Loss: 3368.4576\n",
            "Epoch [70/150], Loss: 1490.6969\n",
            "Epoch [80/150], Loss: 2536.4183\n",
            "Epoch [90/150], Loss: 3832.6905\n",
            "Epoch [100/150], Loss: 1107.7222\n",
            "Epoch [110/150], Loss: 1569.2505\n",
            "Epoch [120/150], Loss: 1254.2755\n",
            "Epoch [130/150], Loss: 2140.5343\n",
            "Epoch [140/150], Loss: 1346.8829\n",
            "Epoch [150/150], Loss: 1489.7285\n",
            "Fold 3, RMSE: 94.74024963378906\n",
            "Epoch [10/150], Loss: 11931.7430\n",
            "Epoch [20/150], Loss: 8816.9707\n",
            "Epoch [30/150], Loss: 8249.3763\n",
            "Epoch [40/150], Loss: 4393.4243\n",
            "Epoch [50/150], Loss: 5570.3783\n",
            "Epoch [60/150], Loss: 2660.5657\n",
            "Epoch [70/150], Loss: 3064.3915\n",
            "Epoch [80/150], Loss: 1521.5323\n",
            "Epoch [90/150], Loss: 2986.0012\n",
            "Epoch [100/150], Loss: 2025.3152\n",
            "Epoch [110/150], Loss: 3591.4272\n",
            "Epoch [120/150], Loss: 3438.0885\n",
            "Epoch [130/150], Loss: 1746.4875\n",
            "Epoch [140/150], Loss: 1533.6456\n",
            "Epoch [150/150], Loss: 2757.6942\n",
            "Fold 4, RMSE: 39.474639892578125\n",
            "Epoch [10/150], Loss: 15967.9148\n",
            "Epoch [20/150], Loss: 12826.9485\n",
            "Epoch [30/150], Loss: 6735.7560\n",
            "Epoch [40/150], Loss: 8151.4087\n",
            "Epoch [50/150], Loss: 3236.9764\n",
            "Epoch [60/150], Loss: 4508.6487\n",
            "Epoch [70/150], Loss: 3713.2209\n",
            "Epoch [80/150], Loss: 2846.6108\n",
            "Epoch [90/150], Loss: 2113.2354\n",
            "Epoch [100/150], Loss: 2565.5109\n",
            "Epoch [110/150], Loss: 2165.3135\n",
            "Epoch [120/150], Loss: 2750.3257\n",
            "Epoch [130/150], Loss: 3422.3107\n",
            "Epoch [140/150], Loss: 4085.9831\n",
            "Epoch [150/150], Loss: 712.9190\n",
            "Fold 5, RMSE: 45.75815200805664\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 59.553673553466794\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 27042.4258\n",
            "Epoch [20/100], Loss: 11308.9547\n",
            "Epoch [30/100], Loss: 11729.8877\n",
            "Epoch [40/100], Loss: 8009.4741\n",
            "Epoch [50/100], Loss: 6715.2130\n",
            "Epoch [60/100], Loss: 7588.6783\n",
            "Epoch [70/100], Loss: 3561.0052\n",
            "Epoch [80/100], Loss: 2505.0583\n",
            "Epoch [90/100], Loss: 4421.5438\n",
            "Epoch [100/100], Loss: 1530.8554\n",
            "Fold 1, RMSE: 58.48320770263672\n",
            "Epoch [10/100], Loss: 23899.7178\n",
            "Epoch [20/100], Loss: 10179.3250\n",
            "Epoch [30/100], Loss: 9804.4109\n",
            "Epoch [40/100], Loss: 7586.2771\n",
            "Epoch [50/100], Loss: 6244.8851\n",
            "Epoch [60/100], Loss: 3140.6799\n",
            "Epoch [70/100], Loss: 2341.2383\n",
            "Epoch [80/100], Loss: 2854.0541\n",
            "Epoch [90/100], Loss: 4015.0073\n",
            "Epoch [100/100], Loss: 2525.5294\n",
            "Fold 2, RMSE: 67.58426666259766\n",
            "Epoch [10/100], Loss: 17018.9351\n",
            "Epoch [20/100], Loss: 9680.0012\n",
            "Epoch [30/100], Loss: 5516.9176\n",
            "Epoch [40/100], Loss: 4634.4236\n",
            "Epoch [50/100], Loss: 3840.1848\n",
            "Epoch [60/100], Loss: 4684.0783\n",
            "Epoch [70/100], Loss: 2729.5695\n",
            "Epoch [80/100], Loss: 2261.4338\n",
            "Epoch [90/100], Loss: 1667.6257\n",
            "Epoch [100/100], Loss: 2142.7979\n",
            "Fold 3, RMSE: 90.86846923828125\n",
            "Epoch [10/100], Loss: 13556.1160\n",
            "Epoch [20/100], Loss: 11513.8882\n",
            "Epoch [30/100], Loss: 14141.6963\n",
            "Epoch [40/100], Loss: 4583.2900\n",
            "Epoch [50/100], Loss: 5777.6093\n",
            "Epoch [60/100], Loss: 3453.8158\n",
            "Epoch [70/100], Loss: 9246.4991\n",
            "Epoch [80/100], Loss: 4276.3342\n",
            "Epoch [90/100], Loss: 3325.4880\n",
            "Epoch [100/100], Loss: 6672.6008\n",
            "Fold 4, RMSE: 41.695308685302734\n",
            "Epoch [10/100], Loss: 18996.5132\n",
            "Epoch [20/100], Loss: 14410.2642\n",
            "Epoch [30/100], Loss: 7690.5747\n",
            "Epoch [40/100], Loss: 6952.0332\n",
            "Epoch [50/100], Loss: 5362.2672\n",
            "Epoch [60/100], Loss: 5646.7694\n",
            "Epoch [70/100], Loss: 3500.0482\n",
            "Epoch [80/100], Loss: 2758.4708\n",
            "Epoch [90/100], Loss: 3752.4248\n",
            "Epoch [100/100], Loss: 1726.3965\n",
            "Fold 5, RMSE: 46.26274108886719\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 60.97879867553711\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 16275.5474\n",
            "Epoch [20/150], Loss: 9168.5509\n",
            "Epoch [30/150], Loss: 5521.1202\n",
            "Epoch [40/150], Loss: 15454.4758\n",
            "Epoch [50/150], Loss: 4019.0090\n",
            "Epoch [60/150], Loss: 5524.7029\n",
            "Epoch [70/150], Loss: 3312.5288\n",
            "Epoch [80/150], Loss: 3796.5790\n",
            "Epoch [90/150], Loss: 1603.4400\n",
            "Epoch [100/150], Loss: 6876.5106\n",
            "Epoch [110/150], Loss: 3079.4437\n",
            "Epoch [120/150], Loss: 3512.1600\n",
            "Epoch [130/150], Loss: 1670.5449\n",
            "Epoch [140/150], Loss: 1031.7847\n",
            "Epoch [150/150], Loss: 3400.4049\n",
            "Fold 1, RMSE: 59.60951232910156\n",
            "Epoch [10/150], Loss: 9387.0874\n",
            "Epoch [20/150], Loss: 9607.7202\n",
            "Epoch [30/150], Loss: 10233.9287\n",
            "Epoch [40/150], Loss: 7943.8001\n",
            "Epoch [50/150], Loss: 9244.8171\n",
            "Epoch [60/150], Loss: 14617.7225\n",
            "Epoch [70/150], Loss: 7058.0835\n",
            "Epoch [80/150], Loss: 2554.7585\n",
            "Epoch [90/150], Loss: 8978.7122\n",
            "Epoch [100/150], Loss: 3186.9268\n",
            "Epoch [110/150], Loss: 3383.6945\n",
            "Epoch [120/150], Loss: 6873.1507\n",
            "Epoch [130/150], Loss: 4130.3795\n",
            "Epoch [140/150], Loss: 6141.3782\n",
            "Epoch [150/150], Loss: 8716.6301\n",
            "Fold 2, RMSE: 70.18328094482422\n",
            "Epoch [10/150], Loss: 15718.6758\n",
            "Epoch [20/150], Loss: 10822.9857\n",
            "Epoch [30/150], Loss: 6958.9064\n",
            "Epoch [40/150], Loss: 4112.0204\n",
            "Epoch [50/150], Loss: 2966.8981\n",
            "Epoch [60/150], Loss: 3674.6369\n",
            "Epoch [70/150], Loss: 2651.2251\n",
            "Epoch [80/150], Loss: 1366.8911\n",
            "Epoch [90/150], Loss: 2536.3214\n",
            "Epoch [100/150], Loss: 2217.3814\n",
            "Epoch [110/150], Loss: 1366.3790\n",
            "Epoch [120/150], Loss: 2169.5305\n",
            "Epoch [130/150], Loss: 2443.3835\n",
            "Epoch [140/150], Loss: 1505.7910\n",
            "Epoch [150/150], Loss: 1741.9879\n",
            "Fold 3, RMSE: 92.3227767944336\n",
            "Epoch [10/150], Loss: 14019.5576\n",
            "Epoch [20/150], Loss: 10891.1286\n",
            "Epoch [30/150], Loss: 5129.0784\n",
            "Epoch [40/150], Loss: 6107.3321\n",
            "Epoch [50/150], Loss: 3206.2930\n",
            "Epoch [60/150], Loss: 4776.6887\n",
            "Epoch [70/150], Loss: 3126.5374\n",
            "Epoch [80/150], Loss: 5860.9177\n",
            "Epoch [90/150], Loss: 1499.2662\n",
            "Epoch [100/150], Loss: 3269.0413\n",
            "Epoch [110/150], Loss: 1703.8364\n",
            "Epoch [120/150], Loss: 3238.5863\n",
            "Epoch [130/150], Loss: 1039.7868\n",
            "Epoch [140/150], Loss: 1463.6690\n",
            "Epoch [150/150], Loss: 1680.5929\n",
            "Fold 4, RMSE: 40.58303451538086\n",
            "Epoch [10/150], Loss: 15660.2285\n",
            "Epoch [20/150], Loss: 10181.8955\n",
            "Epoch [30/150], Loss: 7681.7571\n",
            "Epoch [40/150], Loss: 7742.4933\n",
            "Epoch [50/150], Loss: 6151.2662\n",
            "Epoch [60/150], Loss: 4373.7097\n",
            "Epoch [70/150], Loss: 3050.6846\n",
            "Epoch [80/150], Loss: 6732.3122\n",
            "Epoch [90/150], Loss: 3649.2889\n",
            "Epoch [100/150], Loss: 4456.7061\n",
            "Epoch [110/150], Loss: 2972.4247\n",
            "Epoch [120/150], Loss: 4818.3005\n",
            "Epoch [130/150], Loss: 7336.8767\n",
            "Epoch [140/150], Loss: 3363.1765\n",
            "Epoch [150/150], Loss: 7051.0144\n",
            "Fold 5, RMSE: 45.041446685791016\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 61.54801025390625\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 13737.1836\n",
            "Epoch [20/100], Loss: 7028.3961\n",
            "Epoch [30/100], Loss: 5065.2642\n",
            "Epoch [40/100], Loss: 3005.4661\n",
            "Epoch [50/100], Loss: 3143.3303\n",
            "Epoch [60/100], Loss: 2623.6313\n",
            "Epoch [70/100], Loss: 2890.4023\n",
            "Epoch [80/100], Loss: 1724.7319\n",
            "Epoch [90/100], Loss: 2993.4174\n",
            "Epoch [100/100], Loss: 2755.4679\n",
            "Fold 1, RMSE: 56.049049377441406\n",
            "Epoch [10/100], Loss: 10320.2673\n",
            "Epoch [20/100], Loss: 10409.0363\n",
            "Epoch [30/100], Loss: 4901.2417\n",
            "Epoch [40/100], Loss: 5372.5564\n",
            "Epoch [50/100], Loss: 3138.1695\n",
            "Epoch [60/100], Loss: 3277.6763\n",
            "Epoch [70/100], Loss: 5740.8502\n",
            "Epoch [80/100], Loss: 2719.5040\n",
            "Epoch [90/100], Loss: 1676.4887\n",
            "Epoch [100/100], Loss: 4100.9872\n",
            "Fold 2, RMSE: 64.12960815429688\n",
            "Epoch [10/100], Loss: 7264.6216\n",
            "Epoch [20/100], Loss: 7938.3259\n",
            "Epoch [30/100], Loss: 3967.2111\n",
            "Epoch [40/100], Loss: 3735.4468\n",
            "Epoch [50/100], Loss: 4964.0875\n",
            "Epoch [60/100], Loss: 7402.9289\n",
            "Epoch [70/100], Loss: 2694.7068\n",
            "Epoch [80/100], Loss: 4063.6937\n",
            "Epoch [90/100], Loss: 1892.9249\n",
            "Epoch [100/100], Loss: 3317.4907\n",
            "Fold 3, RMSE: 99.67850494384766\n",
            "Epoch [10/100], Loss: 12837.0945\n",
            "Epoch [20/100], Loss: 7858.2798\n",
            "Epoch [30/100], Loss: 6340.9747\n",
            "Epoch [40/100], Loss: 6758.6045\n",
            "Epoch [50/100], Loss: 7532.0671\n",
            "Epoch [60/100], Loss: 2616.9433\n",
            "Epoch [70/100], Loss: 2743.4286\n",
            "Epoch [80/100], Loss: 2678.3554\n",
            "Epoch [90/100], Loss: 3809.9904\n",
            "Epoch [100/100], Loss: 3987.5947\n",
            "Fold 4, RMSE: 43.829044342041016\n",
            "Epoch [10/100], Loss: 10758.9316\n",
            "Epoch [20/100], Loss: 8326.5859\n",
            "Epoch [30/100], Loss: 4432.4007\n",
            "Epoch [40/100], Loss: 9793.1351\n",
            "Epoch [50/100], Loss: 5700.2355\n",
            "Epoch [60/100], Loss: 5421.8621\n",
            "Epoch [70/100], Loss: 2885.1499\n",
            "Epoch [80/100], Loss: 4073.2477\n",
            "Epoch [90/100], Loss: 2563.0612\n",
            "Epoch [100/100], Loss: 4668.7491\n",
            "Fold 5, RMSE: 45.45486068725586\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 61.82821350097656\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 11715.4980\n",
            "Epoch [20/150], Loss: 5728.2454\n",
            "Epoch [30/150], Loss: 2100.1721\n",
            "Epoch [40/150], Loss: 5611.4303\n",
            "Epoch [50/150], Loss: 3273.0560\n",
            "Epoch [60/150], Loss: 4809.9331\n",
            "Epoch [70/150], Loss: 2317.5436\n",
            "Epoch [80/150], Loss: 2383.8185\n",
            "Epoch [90/150], Loss: 2019.9153\n",
            "Epoch [100/150], Loss: 6000.3448\n",
            "Epoch [110/150], Loss: 1585.6122\n",
            "Epoch [120/150], Loss: 1758.4656\n",
            "Epoch [130/150], Loss: 2084.4298\n",
            "Epoch [140/150], Loss: 2642.2373\n",
            "Epoch [150/150], Loss: 3242.3314\n",
            "Fold 1, RMSE: 56.497840881347656\n",
            "Epoch [10/150], Loss: 18774.8344\n",
            "Epoch [20/150], Loss: 11294.9883\n",
            "Epoch [30/150], Loss: 8181.9606\n",
            "Epoch [40/150], Loss: 4826.7804\n",
            "Epoch [50/150], Loss: 4827.3387\n",
            "Epoch [60/150], Loss: 2068.7733\n",
            "Epoch [70/150], Loss: 1964.3311\n",
            "Epoch [80/150], Loss: 4017.4263\n",
            "Epoch [90/150], Loss: 4746.9465\n",
            "Epoch [100/150], Loss: 4180.1509\n",
            "Epoch [110/150], Loss: 3657.5717\n",
            "Epoch [120/150], Loss: 2146.0800\n",
            "Epoch [130/150], Loss: 1220.9194\n",
            "Epoch [140/150], Loss: 2623.8841\n",
            "Epoch [150/150], Loss: 946.6247\n",
            "Fold 2, RMSE: 66.3282699584961\n",
            "Epoch [10/150], Loss: 8934.2045\n",
            "Epoch [20/150], Loss: 5692.3688\n",
            "Epoch [30/150], Loss: 3686.8403\n",
            "Epoch [40/150], Loss: 3794.1451\n",
            "Epoch [50/150], Loss: 4398.5028\n",
            "Epoch [60/150], Loss: 3582.0139\n",
            "Epoch [70/150], Loss: 1852.6786\n",
            "Epoch [80/150], Loss: 3148.6822\n",
            "Epoch [90/150], Loss: 3188.7021\n",
            "Epoch [100/150], Loss: 2955.4511\n",
            "Epoch [110/150], Loss: 1414.4778\n",
            "Epoch [120/150], Loss: 2547.9356\n",
            "Epoch [130/150], Loss: 2018.7523\n",
            "Epoch [140/150], Loss: 2667.7985\n",
            "Epoch [150/150], Loss: 1022.9606\n",
            "Fold 3, RMSE: 95.93290710449219\n",
            "Epoch [10/150], Loss: 12312.4307\n",
            "Epoch [20/150], Loss: 7074.8733\n",
            "Epoch [30/150], Loss: 4465.8397\n",
            "Epoch [40/150], Loss: 5363.5039\n",
            "Epoch [50/150], Loss: 3328.6443\n",
            "Epoch [60/150], Loss: 10212.1273\n",
            "Epoch [70/150], Loss: 3178.6811\n",
            "Epoch [80/150], Loss: 3250.1065\n",
            "Epoch [90/150], Loss: 1682.6146\n",
            "Epoch [100/150], Loss: 2757.4924\n",
            "Epoch [110/150], Loss: 2019.1074\n",
            "Epoch [120/150], Loss: 1732.7172\n",
            "Epoch [130/150], Loss: 2360.9641\n",
            "Epoch [140/150], Loss: 1201.1263\n",
            "Epoch [150/150], Loss: 2370.7577\n",
            "Fold 4, RMSE: 41.03028869628906\n",
            "Epoch [10/150], Loss: 13634.3057\n",
            "Epoch [20/150], Loss: 11095.9717\n",
            "Epoch [30/150], Loss: 7152.5551\n",
            "Epoch [40/150], Loss: 3994.3724\n",
            "Epoch [50/150], Loss: 2416.1689\n",
            "Epoch [60/150], Loss: 2977.7164\n",
            "Epoch [70/150], Loss: 1888.7362\n",
            "Epoch [80/150], Loss: 2184.3299\n",
            "Epoch [90/150], Loss: 1669.4966\n",
            "Epoch [100/150], Loss: 4134.4872\n",
            "Epoch [110/150], Loss: 1772.7117\n",
            "Epoch [120/150], Loss: 2759.4173\n",
            "Epoch [130/150], Loss: 1173.0946\n",
            "Epoch [140/150], Loss: 2188.7600\n",
            "Epoch [150/150], Loss: 1462.2783\n",
            "Fold 5, RMSE: 43.837890625\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 60.725439453125\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 37754.1748\n",
            "Epoch [20/100], Loss: 16227.6782\n",
            "Epoch [30/100], Loss: 13197.0762\n",
            "Epoch [40/100], Loss: 10598.3975\n",
            "Epoch [50/100], Loss: 8039.5548\n",
            "Epoch [60/100], Loss: 5249.5232\n",
            "Epoch [70/100], Loss: 3570.8680\n",
            "Epoch [80/100], Loss: 6245.3474\n",
            "Epoch [90/100], Loss: 3278.9573\n",
            "Epoch [100/100], Loss: 2643.9469\n",
            "Fold 1, RMSE: 56.06333923339844\n",
            "Epoch [10/100], Loss: 15829.0969\n",
            "Epoch [20/100], Loss: 12078.2664\n",
            "Epoch [30/100], Loss: 8677.9407\n",
            "Epoch [40/100], Loss: 9892.3091\n",
            "Epoch [50/100], Loss: 5515.7241\n",
            "Epoch [60/100], Loss: 3973.3116\n",
            "Epoch [70/100], Loss: 1811.6816\n",
            "Epoch [80/100], Loss: 1893.2383\n",
            "Epoch [90/100], Loss: 1960.5416\n",
            "Epoch [100/100], Loss: 2751.4692\n",
            "Fold 2, RMSE: 60.7213249206543\n",
            "Epoch [10/100], Loss: 22605.3479\n",
            "Epoch [20/100], Loss: 8862.9651\n",
            "Epoch [30/100], Loss: 10488.0950\n",
            "Epoch [40/100], Loss: 8922.8195\n",
            "Epoch [50/100], Loss: 7166.5197\n",
            "Epoch [60/100], Loss: 4756.5750\n",
            "Epoch [70/100], Loss: 2350.3287\n",
            "Epoch [80/100], Loss: 2718.3385\n",
            "Epoch [90/100], Loss: 3560.7640\n",
            "Epoch [100/100], Loss: 4463.4389\n",
            "Fold 3, RMSE: 83.34420776367188\n",
            "Epoch [10/100], Loss: 31306.1738\n",
            "Epoch [20/100], Loss: 14715.9258\n",
            "Epoch [30/100], Loss: 12388.9722\n",
            "Epoch [40/100], Loss: 11109.9044\n",
            "Epoch [50/100], Loss: 8201.4055\n",
            "Epoch [60/100], Loss: 7266.6089\n",
            "Epoch [70/100], Loss: 5477.0148\n",
            "Epoch [80/100], Loss: 5043.3524\n",
            "Epoch [90/100], Loss: 4872.9902\n",
            "Epoch [100/100], Loss: 5856.3354\n",
            "Fold 4, RMSE: 40.73588943481445\n",
            "Epoch [10/100], Loss: 61557.3281\n",
            "Epoch [20/100], Loss: 11736.7637\n",
            "Epoch [30/100], Loss: 15362.3145\n",
            "Epoch [40/100], Loss: 10704.1777\n",
            "Epoch [50/100], Loss: 5346.2935\n",
            "Epoch [60/100], Loss: 8169.9043\n",
            "Epoch [70/100], Loss: 6258.2593\n",
            "Epoch [80/100], Loss: 3243.5549\n",
            "Epoch [90/100], Loss: 4691.5151\n",
            "Epoch [100/100], Loss: 4596.8215\n",
            "Fold 5, RMSE: 45.51763916015625\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 57.27648010253906\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 22478.6470\n",
            "Epoch [20/150], Loss: 12617.6221\n",
            "Epoch [30/150], Loss: 9649.9702\n",
            "Epoch [40/150], Loss: 7496.7291\n",
            "Epoch [50/150], Loss: 7576.6594\n",
            "Epoch [60/150], Loss: 3697.6036\n",
            "Epoch [70/150], Loss: 4399.6292\n",
            "Epoch [80/150], Loss: 9130.5321\n",
            "Epoch [90/150], Loss: 5525.8541\n",
            "Epoch [100/150], Loss: 3647.4829\n",
            "Epoch [110/150], Loss: 1360.2567\n",
            "Epoch [120/150], Loss: 1907.1680\n",
            "Epoch [130/150], Loss: 1353.4863\n",
            "Epoch [140/150], Loss: 2191.8540\n",
            "Epoch [150/150], Loss: 2484.8568\n",
            "Fold 1, RMSE: 58.26967239379883\n",
            "Epoch [10/150], Loss: 16143.7854\n",
            "Epoch [20/150], Loss: 12502.3943\n",
            "Epoch [30/150], Loss: 8828.9453\n",
            "Epoch [40/150], Loss: 6941.4440\n",
            "Epoch [50/150], Loss: 3519.3344\n",
            "Epoch [60/150], Loss: 5664.5208\n",
            "Epoch [70/150], Loss: 13649.4047\n",
            "Epoch [80/150], Loss: 9924.3794\n",
            "Epoch [90/150], Loss: 9120.0229\n",
            "Epoch [100/150], Loss: 5758.8650\n",
            "Epoch [110/150], Loss: 11901.2668\n",
            "Epoch [120/150], Loss: 6910.5235\n",
            "Epoch [130/150], Loss: 7482.3068\n",
            "Epoch [140/150], Loss: 4607.9615\n",
            "Epoch [150/150], Loss: 5456.0568\n",
            "Fold 2, RMSE: 66.6763687133789\n",
            "Epoch [10/150], Loss: 20691.9648\n",
            "Epoch [20/150], Loss: 11135.3411\n",
            "Epoch [30/150], Loss: 8454.2665\n",
            "Epoch [40/150], Loss: 7956.0916\n",
            "Epoch [50/150], Loss: 6369.9631\n",
            "Epoch [60/150], Loss: 6434.4419\n",
            "Epoch [70/150], Loss: 5798.2687\n",
            "Epoch [80/150], Loss: 4030.8263\n",
            "Epoch [90/150], Loss: 3207.3899\n",
            "Epoch [100/150], Loss: 2082.2144\n",
            "Epoch [110/150], Loss: 2776.2762\n",
            "Epoch [120/150], Loss: 5577.2335\n",
            "Epoch [130/150], Loss: 4460.3734\n",
            "Epoch [140/150], Loss: 3128.0245\n",
            "Epoch [150/150], Loss: 3469.7836\n",
            "Fold 3, RMSE: 92.01728820800781\n",
            "Epoch [10/150], Loss: 28595.2974\n",
            "Epoch [20/150], Loss: 13456.8120\n",
            "Epoch [30/150], Loss: 14975.5918\n",
            "Epoch [40/150], Loss: 9398.6364\n",
            "Epoch [50/150], Loss: 5434.9512\n",
            "Epoch [60/150], Loss: 8955.6589\n",
            "Epoch [70/150], Loss: 4455.5520\n",
            "Epoch [80/150], Loss: 7019.0081\n",
            "Epoch [90/150], Loss: 5196.2247\n",
            "Epoch [100/150], Loss: 5803.9067\n",
            "Epoch [110/150], Loss: 2567.7675\n",
            "Epoch [120/150], Loss: 3892.8859\n",
            "Epoch [130/150], Loss: 2536.6206\n",
            "Epoch [140/150], Loss: 3353.7477\n",
            "Epoch [150/150], Loss: 1477.7191\n",
            "Fold 4, RMSE: 41.131866455078125\n",
            "Epoch [10/150], Loss: 31172.0049\n",
            "Epoch [20/150], Loss: 12573.9740\n",
            "Epoch [30/150], Loss: 18228.1948\n",
            "Epoch [40/150], Loss: 8780.6321\n",
            "Epoch [50/150], Loss: 8267.0881\n",
            "Epoch [60/150], Loss: 7648.9211\n",
            "Epoch [70/150], Loss: 7259.9939\n",
            "Epoch [80/150], Loss: 6486.8234\n",
            "Epoch [90/150], Loss: 4736.6193\n",
            "Epoch [100/150], Loss: 3736.3702\n",
            "Epoch [110/150], Loss: 3855.6099\n",
            "Epoch [120/150], Loss: 2354.4224\n",
            "Epoch [130/150], Loss: 2911.2134\n",
            "Epoch [140/150], Loss: 4586.8938\n",
            "Epoch [150/150], Loss: 3739.0941\n",
            "Fold 5, RMSE: 44.48263168334961\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 60.51556549072266\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 25463.1753\n",
            "Epoch [20/100], Loss: 10355.9951\n",
            "Epoch [30/100], Loss: 9886.1423\n",
            "Epoch [40/100], Loss: 2905.5087\n",
            "Epoch [50/100], Loss: 4260.2231\n",
            "Epoch [60/100], Loss: 2626.6423\n",
            "Epoch [70/100], Loss: 3892.0914\n",
            "Epoch [80/100], Loss: 2241.5233\n",
            "Epoch [90/100], Loss: 1902.0087\n",
            "Epoch [100/100], Loss: 5344.4890\n",
            "Fold 1, RMSE: 62.19084548950195\n",
            "Epoch [10/100], Loss: 11020.6760\n",
            "Epoch [20/100], Loss: 9568.9178\n",
            "Epoch [30/100], Loss: 8360.1290\n",
            "Epoch [40/100], Loss: 4886.5100\n",
            "Epoch [50/100], Loss: 6363.0217\n",
            "Epoch [60/100], Loss: 5924.5055\n",
            "Epoch [70/100], Loss: 3897.4204\n",
            "Epoch [80/100], Loss: 3540.7523\n",
            "Epoch [90/100], Loss: 8118.1710\n",
            "Epoch [100/100], Loss: 7021.9431\n",
            "Fold 2, RMSE: 71.0640869140625\n",
            "Epoch [10/100], Loss: 10216.7620\n",
            "Epoch [20/100], Loss: 9150.8550\n",
            "Epoch [30/100], Loss: 7923.5839\n",
            "Epoch [40/100], Loss: 8801.9072\n",
            "Epoch [50/100], Loss: 2768.5547\n",
            "Epoch [60/100], Loss: 2673.5205\n",
            "Epoch [70/100], Loss: 2858.1429\n",
            "Epoch [80/100], Loss: 3366.2070\n",
            "Epoch [90/100], Loss: 1787.7394\n",
            "Epoch [100/100], Loss: 2303.7334\n",
            "Fold 3, RMSE: 100.64978790283203\n",
            "Epoch [10/100], Loss: 15915.2251\n",
            "Epoch [20/100], Loss: 9762.1433\n",
            "Epoch [30/100], Loss: 5430.2711\n",
            "Epoch [40/100], Loss: 6381.4524\n",
            "Epoch [50/100], Loss: 4521.2378\n",
            "Epoch [60/100], Loss: 2907.2988\n",
            "Epoch [70/100], Loss: 8748.1724\n",
            "Epoch [80/100], Loss: 5971.4489\n",
            "Epoch [90/100], Loss: 7283.3465\n",
            "Epoch [100/100], Loss: 5068.3563\n",
            "Fold 4, RMSE: 45.06385803222656\n",
            "Epoch [10/100], Loss: 10399.5979\n",
            "Epoch [20/100], Loss: 6648.4869\n",
            "Epoch [30/100], Loss: 9140.7712\n",
            "Epoch [40/100], Loss: 6222.9851\n",
            "Epoch [50/100], Loss: 10477.7059\n",
            "Epoch [60/100], Loss: 1726.0098\n",
            "Epoch [70/100], Loss: 3102.0230\n",
            "Epoch [80/100], Loss: 4565.2909\n",
            "Epoch [90/100], Loss: 6253.6907\n",
            "Epoch [100/100], Loss: 2902.5811\n",
            "Fold 5, RMSE: 45.661460876464844\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 64.92600784301757\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15739.6802\n",
            "Epoch [20/150], Loss: 12773.0776\n",
            "Epoch [30/150], Loss: 6078.2218\n",
            "Epoch [40/150], Loss: 5010.5031\n",
            "Epoch [50/150], Loss: 5187.0557\n",
            "Epoch [60/150], Loss: 4246.8350\n",
            "Epoch [70/150], Loss: 3270.9664\n",
            "Epoch [80/150], Loss: 4850.5208\n",
            "Epoch [90/150], Loss: 1878.0019\n",
            "Epoch [100/150], Loss: 4024.7961\n",
            "Epoch [110/150], Loss: 3686.6357\n",
            "Epoch [120/150], Loss: 3295.7299\n",
            "Epoch [130/150], Loss: 1963.2565\n",
            "Epoch [140/150], Loss: 1436.9243\n",
            "Epoch [150/150], Loss: 2312.9840\n",
            "Fold 1, RMSE: 59.12023162841797\n",
            "Epoch [10/150], Loss: 28637.5127\n",
            "Epoch [20/150], Loss: 9043.4681\n",
            "Epoch [30/150], Loss: 10391.4149\n",
            "Epoch [40/150], Loss: 9841.8962\n",
            "Epoch [50/150], Loss: 5731.2556\n",
            "Epoch [60/150], Loss: 4431.9266\n",
            "Epoch [70/150], Loss: 2581.0749\n",
            "Epoch [80/150], Loss: 5057.7446\n",
            "Epoch [90/150], Loss: 4012.4240\n",
            "Epoch [100/150], Loss: 1489.8588\n",
            "Epoch [110/150], Loss: 2986.2718\n",
            "Epoch [120/150], Loss: 3660.6489\n",
            "Epoch [130/150], Loss: 7990.7932\n",
            "Epoch [140/150], Loss: 3187.6689\n",
            "Epoch [150/150], Loss: 5319.1760\n",
            "Fold 2, RMSE: 67.56928253173828\n",
            "Epoch [10/150], Loss: 11599.7729\n",
            "Epoch [20/150], Loss: 7139.8361\n",
            "Epoch [30/150], Loss: 4348.8803\n",
            "Epoch [40/150], Loss: 3753.8586\n",
            "Epoch [50/150], Loss: 4579.1126\n",
            "Epoch [60/150], Loss: 3078.9391\n",
            "Epoch [70/150], Loss: 3162.8066\n",
            "Epoch [80/150], Loss: 5328.7189\n",
            "Epoch [90/150], Loss: 2399.6785\n",
            "Epoch [100/150], Loss: 1252.8156\n",
            "Epoch [110/150], Loss: 2237.4005\n",
            "Epoch [120/150], Loss: 1019.7006\n",
            "Epoch [130/150], Loss: 2007.5864\n",
            "Epoch [140/150], Loss: 1389.3235\n",
            "Epoch [150/150], Loss: 1767.5507\n",
            "Fold 3, RMSE: 93.56388092041016\n",
            "Epoch [10/150], Loss: 18509.3091\n",
            "Epoch [20/150], Loss: 11456.2166\n",
            "Epoch [30/150], Loss: 8868.8107\n",
            "Epoch [40/150], Loss: 6237.2029\n",
            "Epoch [50/150], Loss: 3633.1902\n",
            "Epoch [60/150], Loss: 3470.4910\n",
            "Epoch [70/150], Loss: 4095.8321\n",
            "Epoch [80/150], Loss: 2342.7145\n",
            "Epoch [90/150], Loss: 3203.0044\n",
            "Epoch [100/150], Loss: 2126.3135\n",
            "Epoch [110/150], Loss: 3285.7991\n",
            "Epoch [120/150], Loss: 4573.7261\n",
            "Epoch [130/150], Loss: 2186.8219\n",
            "Epoch [140/150], Loss: 1555.0839\n",
            "Epoch [150/150], Loss: 2155.9930\n",
            "Fold 4, RMSE: 44.38557815551758\n",
            "Epoch [10/150], Loss: 13080.3923\n",
            "Epoch [20/150], Loss: 9481.9153\n",
            "Epoch [30/150], Loss: 6183.5836\n",
            "Epoch [40/150], Loss: 5448.9823\n",
            "Epoch [50/150], Loss: 3984.2362\n",
            "Epoch [60/150], Loss: 6256.3393\n",
            "Epoch [70/150], Loss: 7726.6562\n",
            "Epoch [80/150], Loss: 13181.0823\n",
            "Epoch [90/150], Loss: 1874.1514\n",
            "Epoch [100/150], Loss: 8177.4000\n",
            "Epoch [110/150], Loss: 2376.1519\n",
            "Epoch [120/150], Loss: 3129.0827\n",
            "Epoch [130/150], Loss: 1079.7145\n",
            "Epoch [140/150], Loss: 4206.5619\n",
            "Epoch [150/150], Loss: 3864.3306\n",
            "Fold 5, RMSE: 44.47272491455078\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 61.822339630126955\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 20071.2124\n",
            "Epoch [20/100], Loss: 10278.9551\n",
            "Epoch [30/100], Loss: 6629.4050\n",
            "Epoch [40/100], Loss: 7178.7372\n",
            "Epoch [50/100], Loss: 6523.4552\n",
            "Epoch [60/100], Loss: 4135.3627\n",
            "Epoch [70/100], Loss: 3401.4200\n",
            "Epoch [80/100], Loss: 5164.1775\n",
            "Epoch [90/100], Loss: 6639.1364\n",
            "Epoch [100/100], Loss: 2868.9031\n",
            "Fold 1, RMSE: 56.411277770996094\n",
            "Epoch [10/100], Loss: 30938.4272\n",
            "Epoch [20/100], Loss: 10691.1621\n",
            "Epoch [30/100], Loss: 10042.6139\n",
            "Epoch [40/100], Loss: 10086.1097\n",
            "Epoch [50/100], Loss: 9716.5151\n",
            "Epoch [60/100], Loss: 8880.4331\n",
            "Epoch [70/100], Loss: 11046.8621\n",
            "Epoch [80/100], Loss: 8156.3091\n",
            "Epoch [90/100], Loss: 4219.6922\n",
            "Epoch [100/100], Loss: 3553.4523\n",
            "Fold 2, RMSE: 66.84015655517578\n",
            "Epoch [10/100], Loss: 13770.1703\n",
            "Epoch [20/100], Loss: 8107.2805\n",
            "Epoch [30/100], Loss: 7726.8254\n",
            "Epoch [40/100], Loss: 6925.1349\n",
            "Epoch [50/100], Loss: 5834.3948\n",
            "Epoch [60/100], Loss: 5113.9317\n",
            "Epoch [70/100], Loss: 4406.5781\n",
            "Epoch [80/100], Loss: 12597.6818\n",
            "Epoch [90/100], Loss: 3414.7096\n",
            "Epoch [100/100], Loss: 10643.7556\n",
            "Fold 3, RMSE: 109.77808380126953\n",
            "Epoch [10/100], Loss: 25712.0688\n",
            "Epoch [20/100], Loss: 13770.0725\n",
            "Epoch [30/100], Loss: 13797.3679\n",
            "Epoch [40/100], Loss: 13281.2397\n",
            "Epoch [50/100], Loss: 9971.2607\n",
            "Epoch [60/100], Loss: 9409.0317\n",
            "Epoch [70/100], Loss: 9615.6158\n",
            "Epoch [80/100], Loss: 5822.3890\n",
            "Epoch [90/100], Loss: 8889.1776\n",
            "Epoch [100/100], Loss: 5062.0293\n",
            "Fold 4, RMSE: 42.19206237792969\n",
            "Epoch [10/100], Loss: 28942.9399\n",
            "Epoch [20/100], Loss: 12156.4050\n",
            "Epoch [30/100], Loss: 8866.4396\n",
            "Epoch [40/100], Loss: 8447.8269\n",
            "Epoch [50/100], Loss: 7808.7657\n",
            "Epoch [60/100], Loss: 4629.2192\n",
            "Epoch [70/100], Loss: 6379.1066\n",
            "Epoch [80/100], Loss: 6456.8109\n",
            "Epoch [90/100], Loss: 6253.0832\n",
            "Epoch [100/100], Loss: 5489.7189\n",
            "Fold 5, RMSE: 44.90191650390625\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 64.02469940185547\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 37100.5547\n",
            "Epoch [20/150], Loss: 13647.9951\n",
            "Epoch [30/150], Loss: 7232.0566\n",
            "Epoch [40/150], Loss: 3720.8143\n",
            "Epoch [50/150], Loss: 8697.6727\n",
            "Epoch [60/150], Loss: 7904.4255\n",
            "Epoch [70/150], Loss: 2856.8649\n",
            "Epoch [80/150], Loss: 6736.2003\n",
            "Epoch [90/150], Loss: 2935.5015\n",
            "Epoch [100/150], Loss: 4455.9473\n",
            "Epoch [110/150], Loss: 3237.3192\n",
            "Epoch [120/150], Loss: 2183.3470\n",
            "Epoch [130/150], Loss: 3575.0037\n",
            "Epoch [140/150], Loss: 3763.8177\n",
            "Epoch [150/150], Loss: 2718.6967\n",
            "Fold 1, RMSE: 56.880950927734375\n",
            "Epoch [10/150], Loss: 13641.3486\n",
            "Epoch [20/150], Loss: 11739.9292\n",
            "Epoch [30/150], Loss: 8700.7532\n",
            "Epoch [40/150], Loss: 7680.2580\n",
            "Epoch [50/150], Loss: 4604.5433\n",
            "Epoch [60/150], Loss: 5004.7885\n",
            "Epoch [70/150], Loss: 4558.4040\n",
            "Epoch [80/150], Loss: 5207.3727\n",
            "Epoch [90/150], Loss: 1815.9629\n",
            "Epoch [100/150], Loss: 4278.0345\n",
            "Epoch [110/150], Loss: 2826.8376\n",
            "Epoch [120/150], Loss: 1096.0327\n",
            "Epoch [130/150], Loss: 5489.9365\n",
            "Epoch [140/150], Loss: 6055.9563\n",
            "Epoch [150/150], Loss: 4463.4320\n",
            "Fold 2, RMSE: 61.00101089477539\n",
            "Epoch [10/150], Loss: 88210.2314\n",
            "Epoch [20/150], Loss: 15641.6658\n",
            "Epoch [30/150], Loss: 8105.3073\n",
            "Epoch [40/150], Loss: 6101.0120\n",
            "Epoch [50/150], Loss: 6532.7070\n",
            "Epoch [60/150], Loss: 5628.2214\n",
            "Epoch [70/150], Loss: 6348.4783\n",
            "Epoch [80/150], Loss: 4327.1798\n",
            "Epoch [90/150], Loss: 2647.5847\n",
            "Epoch [100/150], Loss: 3553.8289\n",
            "Epoch [110/150], Loss: 2817.6240\n",
            "Epoch [120/150], Loss: 3319.6181\n",
            "Epoch [130/150], Loss: 3343.4705\n",
            "Epoch [140/150], Loss: 1616.4037\n",
            "Epoch [150/150], Loss: 920.5031\n",
            "Fold 3, RMSE: 94.09578704833984\n",
            "Epoch [10/150], Loss: 33787.3789\n",
            "Epoch [20/150], Loss: 12232.6155\n",
            "Epoch [30/150], Loss: 7788.8502\n",
            "Epoch [40/150], Loss: 9696.2180\n",
            "Epoch [50/150], Loss: 5566.4211\n",
            "Epoch [60/150], Loss: 7681.9366\n",
            "Epoch [70/150], Loss: 5453.6106\n",
            "Epoch [80/150], Loss: 3180.9785\n",
            "Epoch [90/150], Loss: 2843.1609\n",
            "Epoch [100/150], Loss: 4194.6888\n",
            "Epoch [110/150], Loss: 2291.3733\n",
            "Epoch [120/150], Loss: 3978.3210\n",
            "Epoch [130/150], Loss: 3240.9735\n",
            "Epoch [140/150], Loss: 4000.4702\n",
            "Epoch [150/150], Loss: 2173.7227\n",
            "Fold 4, RMSE: 48.56221008300781\n",
            "Epoch [10/150], Loss: 16186.1211\n",
            "Epoch [20/150], Loss: 13908.7979\n",
            "Epoch [30/150], Loss: 8489.7411\n",
            "Epoch [40/150], Loss: 10206.2598\n",
            "Epoch [50/150], Loss: 7061.8296\n",
            "Epoch [60/150], Loss: 7321.4722\n",
            "Epoch [70/150], Loss: 5956.2191\n",
            "Epoch [80/150], Loss: 7386.8212\n",
            "Epoch [90/150], Loss: 1295.7673\n",
            "Epoch [100/150], Loss: 1895.6590\n",
            "Epoch [110/150], Loss: 2706.1263\n",
            "Epoch [120/150], Loss: 3054.3947\n",
            "Epoch [130/150], Loss: 2229.9250\n",
            "Epoch [140/150], Loss: 3698.5303\n",
            "Epoch [150/150], Loss: 3260.2603\n",
            "Fold 5, RMSE: 43.91563034057617\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 60.891117858886716\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 14309.1245\n",
            "Epoch [20/100], Loss: 10230.4624\n",
            "Epoch [30/100], Loss: 7915.3969\n",
            "Epoch [40/100], Loss: 5918.5767\n",
            "Epoch [50/100], Loss: 4399.8806\n",
            "Epoch [60/100], Loss: 4045.8660\n",
            "Epoch [70/100], Loss: 3467.2914\n",
            "Epoch [80/100], Loss: 3611.1580\n",
            "Epoch [90/100], Loss: 3263.1561\n",
            "Epoch [100/100], Loss: 2577.7697\n",
            "Fold 1, RMSE: 57.229103088378906\n",
            "Epoch [10/100], Loss: 28786.0547\n",
            "Epoch [20/100], Loss: 15105.5889\n",
            "Epoch [30/100], Loss: 7608.8846\n",
            "Epoch [40/100], Loss: 11312.2766\n",
            "Epoch [50/100], Loss: 5241.2143\n",
            "Epoch [60/100], Loss: 3210.0172\n",
            "Epoch [70/100], Loss: 4936.8359\n",
            "Epoch [80/100], Loss: 2916.0408\n",
            "Epoch [90/100], Loss: 2420.1123\n",
            "Epoch [100/100], Loss: 2721.0790\n",
            "Fold 2, RMSE: 70.6015853881836\n",
            "Epoch [10/100], Loss: 13248.4004\n",
            "Epoch [20/100], Loss: 7939.6086\n",
            "Epoch [30/100], Loss: 7757.2542\n",
            "Epoch [40/100], Loss: 6935.0873\n",
            "Epoch [50/100], Loss: 4800.9341\n",
            "Epoch [60/100], Loss: 2620.3143\n",
            "Epoch [70/100], Loss: 3389.6687\n",
            "Epoch [80/100], Loss: 3212.2333\n",
            "Epoch [90/100], Loss: 1963.7868\n",
            "Epoch [100/100], Loss: 1925.8018\n",
            "Fold 3, RMSE: 92.2055435180664\n",
            "Epoch [10/100], Loss: 15511.1704\n",
            "Epoch [20/100], Loss: 11264.0576\n",
            "Epoch [30/100], Loss: 8570.0177\n",
            "Epoch [40/100], Loss: 8306.9644\n",
            "Epoch [50/100], Loss: 4323.3627\n",
            "Epoch [60/100], Loss: 9593.8254\n",
            "Epoch [70/100], Loss: 2529.2196\n",
            "Epoch [80/100], Loss: 3887.7519\n",
            "Epoch [90/100], Loss: 5297.0396\n",
            "Epoch [100/100], Loss: 4206.7297\n",
            "Fold 4, RMSE: 37.49501419067383\n",
            "Epoch [10/100], Loss: 19027.7776\n",
            "Epoch [20/100], Loss: 11780.3813\n",
            "Epoch [30/100], Loss: 6436.0856\n",
            "Epoch [40/100], Loss: 3237.2847\n",
            "Epoch [50/100], Loss: 4673.0361\n",
            "Epoch [60/100], Loss: 5360.5879\n",
            "Epoch [70/100], Loss: 3551.7262\n",
            "Epoch [80/100], Loss: 3055.7938\n",
            "Epoch [90/100], Loss: 3170.9410\n",
            "Epoch [100/100], Loss: 4264.6921\n",
            "Fold 5, RMSE: 43.868125915527344\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 60.27987442016602\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 20054.1797\n",
            "Epoch [20/150], Loss: 11059.7799\n",
            "Epoch [30/150], Loss: 7767.5171\n",
            "Epoch [40/150], Loss: 8230.3669\n",
            "Epoch [50/150], Loss: 5049.3544\n",
            "Epoch [60/150], Loss: 2249.0418\n",
            "Epoch [70/150], Loss: 6070.6152\n",
            "Epoch [80/150], Loss: 1929.9305\n",
            "Epoch [90/150], Loss: 1430.5361\n",
            "Epoch [100/150], Loss: 4791.3593\n",
            "Epoch [110/150], Loss: 3600.4972\n",
            "Epoch [120/150], Loss: 3826.3922\n",
            "Epoch [130/150], Loss: 1620.0739\n",
            "Epoch [140/150], Loss: 2009.1253\n",
            "Epoch [150/150], Loss: 2953.7081\n",
            "Fold 1, RMSE: 52.0230827331543\n",
            "Epoch [10/150], Loss: 12642.1145\n",
            "Epoch [20/150], Loss: 13003.2168\n",
            "Epoch [30/150], Loss: 5904.0906\n",
            "Epoch [40/150], Loss: 4344.7387\n",
            "Epoch [50/150], Loss: 6443.0292\n",
            "Epoch [60/150], Loss: 2642.6946\n",
            "Epoch [70/150], Loss: 1359.4808\n",
            "Epoch [80/150], Loss: 3463.8456\n",
            "Epoch [90/150], Loss: 3517.0686\n",
            "Epoch [100/150], Loss: 1958.9754\n",
            "Epoch [110/150], Loss: 2343.1886\n",
            "Epoch [120/150], Loss: 2371.5251\n",
            "Epoch [130/150], Loss: 1705.5100\n",
            "Epoch [140/150], Loss: 5984.0094\n",
            "Epoch [150/150], Loss: 2278.8317\n",
            "Fold 2, RMSE: 71.62178039550781\n",
            "Epoch [10/150], Loss: 9450.7205\n",
            "Epoch [20/150], Loss: 7569.6123\n",
            "Epoch [30/150], Loss: 7476.9104\n",
            "Epoch [40/150], Loss: 5190.8007\n",
            "Epoch [50/150], Loss: 2829.1345\n",
            "Epoch [60/150], Loss: 4466.1510\n",
            "Epoch [70/150], Loss: 3104.7297\n",
            "Epoch [80/150], Loss: 3094.2492\n",
            "Epoch [90/150], Loss: 4982.2021\n",
            "Epoch [100/150], Loss: 3644.9190\n",
            "Epoch [110/150], Loss: 1674.1426\n",
            "Epoch [120/150], Loss: 1094.6385\n",
            "Epoch [130/150], Loss: 7330.4844\n",
            "Epoch [140/150], Loss: 2566.1774\n",
            "Epoch [150/150], Loss: 1330.6786\n",
            "Fold 3, RMSE: 94.16239166259766\n",
            "Epoch [10/150], Loss: 15546.0088\n",
            "Epoch [20/150], Loss: 13275.2373\n",
            "Epoch [30/150], Loss: 8596.9426\n",
            "Epoch [40/150], Loss: 8558.7349\n",
            "Epoch [50/150], Loss: 7592.4301\n",
            "Epoch [60/150], Loss: 5273.9841\n",
            "Epoch [70/150], Loss: 2538.8137\n",
            "Epoch [80/150], Loss: 3328.5144\n",
            "Epoch [90/150], Loss: 4831.2150\n",
            "Epoch [100/150], Loss: 7852.6857\n",
            "Epoch [110/150], Loss: 3202.7372\n",
            "Epoch [120/150], Loss: 7328.0536\n",
            "Epoch [130/150], Loss: 3653.5114\n",
            "Epoch [140/150], Loss: 2501.4173\n",
            "Epoch [150/150], Loss: 2601.4391\n",
            "Fold 4, RMSE: 44.69783401489258\n",
            "Epoch [10/150], Loss: 12825.0183\n",
            "Epoch [20/150], Loss: 10947.2627\n",
            "Epoch [30/150], Loss: 7052.3665\n",
            "Epoch [40/150], Loss: 4397.9586\n",
            "Epoch [50/150], Loss: 3099.6884\n",
            "Epoch [60/150], Loss: 4691.2359\n",
            "Epoch [70/150], Loss: 3192.8868\n",
            "Epoch [80/150], Loss: 3833.8404\n",
            "Epoch [90/150], Loss: 3602.3890\n",
            "Epoch [100/150], Loss: 4511.5465\n",
            "Epoch [110/150], Loss: 3708.9028\n",
            "Epoch [120/150], Loss: 3523.0424\n",
            "Epoch [130/150], Loss: 1916.4467\n",
            "Epoch [140/150], Loss: 4327.8311\n",
            "Epoch [150/150], Loss: 2100.9006\n",
            "Fold 5, RMSE: 44.363922119140625\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 61.373802185058594\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 31499.5327\n",
            "Epoch [20/100], Loss: 11503.9807\n",
            "Epoch [30/100], Loss: 11298.2742\n",
            "Epoch [40/100], Loss: 8575.7566\n",
            "Epoch [50/100], Loss: 6790.1088\n",
            "Epoch [60/100], Loss: 2476.1651\n",
            "Epoch [70/100], Loss: 2112.9242\n",
            "Epoch [80/100], Loss: 6551.6840\n",
            "Epoch [90/100], Loss: 1929.7348\n",
            "Epoch [100/100], Loss: 2924.1924\n",
            "Fold 1, RMSE: 60.55900192260742\n",
            "Epoch [10/100], Loss: 89653.7266\n",
            "Epoch [20/100], Loss: 16584.5234\n",
            "Epoch [30/100], Loss: 12272.0410\n",
            "Epoch [40/100], Loss: 16996.6794\n",
            "Epoch [50/100], Loss: 11408.7764\n",
            "Epoch [60/100], Loss: 9969.2561\n",
            "Epoch [70/100], Loss: 8589.6298\n",
            "Epoch [80/100], Loss: 5164.5253\n",
            "Epoch [90/100], Loss: 8712.8230\n",
            "Epoch [100/100], Loss: 5231.6287\n",
            "Fold 2, RMSE: 65.37936401367188\n",
            "Epoch [10/100], Loss: 45588.6514\n",
            "Epoch [20/100], Loss: 18058.7920\n",
            "Epoch [30/100], Loss: 9187.8809\n",
            "Epoch [40/100], Loss: 7342.0487\n",
            "Epoch [50/100], Loss: 6117.1437\n",
            "Epoch [60/100], Loss: 6181.4697\n",
            "Epoch [70/100], Loss: 2585.9427\n",
            "Epoch [80/100], Loss: 2439.1074\n",
            "Epoch [90/100], Loss: 3088.2560\n",
            "Epoch [100/100], Loss: 2719.8783\n",
            "Fold 3, RMSE: 94.30516815185547\n",
            "Epoch [10/100], Loss: 25030.4150\n",
            "Epoch [20/100], Loss: 18670.2648\n",
            "Epoch [30/100], Loss: 14916.6084\n",
            "Epoch [40/100], Loss: 8694.5210\n",
            "Epoch [50/100], Loss: 5921.9564\n",
            "Epoch [60/100], Loss: 2646.2603\n",
            "Epoch [70/100], Loss: 2507.2805\n",
            "Epoch [80/100], Loss: 5418.8704\n",
            "Epoch [90/100], Loss: 2693.0722\n",
            "Epoch [100/100], Loss: 3859.4128\n",
            "Fold 4, RMSE: 41.61610794067383\n",
            "Epoch [10/100], Loss: 23838.3291\n",
            "Epoch [20/100], Loss: 13053.6089\n",
            "Epoch [30/100], Loss: 6675.8000\n",
            "Epoch [40/100], Loss: 10460.4941\n",
            "Epoch [50/100], Loss: 4827.0062\n",
            "Epoch [60/100], Loss: 1945.2853\n",
            "Epoch [70/100], Loss: 9111.3873\n",
            "Epoch [80/100], Loss: 12832.8695\n",
            "Epoch [90/100], Loss: 6556.3260\n",
            "Epoch [100/100], Loss: 5497.7057\n",
            "Fold 5, RMSE: 45.682106018066406\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 61.508349609375\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 28779.5518\n",
            "Epoch [20/150], Loss: 12012.4454\n",
            "Epoch [30/150], Loss: 8278.8635\n",
            "Epoch [40/150], Loss: 7763.4478\n",
            "Epoch [50/150], Loss: 5242.6798\n",
            "Epoch [60/150], Loss: 3892.6843\n",
            "Epoch [70/150], Loss: 4230.5994\n",
            "Epoch [80/150], Loss: 8425.6541\n",
            "Epoch [90/150], Loss: 3615.2362\n",
            "Epoch [100/150], Loss: 4088.3806\n",
            "Epoch [110/150], Loss: 5379.3275\n",
            "Epoch [120/150], Loss: 2815.1902\n",
            "Epoch [130/150], Loss: 4870.5906\n",
            "Epoch [140/150], Loss: 2175.5998\n",
            "Epoch [150/150], Loss: 2933.1242\n",
            "Fold 1, RMSE: 57.82627487182617\n",
            "Epoch [10/150], Loss: 32168.3452\n",
            "Epoch [20/150], Loss: 16800.1692\n",
            "Epoch [30/150], Loss: 5400.8945\n",
            "Epoch [40/150], Loss: 11078.8398\n",
            "Epoch [50/150], Loss: 5317.0371\n",
            "Epoch [60/150], Loss: 3713.4891\n",
            "Epoch [70/150], Loss: 4038.5872\n",
            "Epoch [80/150], Loss: 4948.8248\n",
            "Epoch [90/150], Loss: 2340.9843\n",
            "Epoch [100/150], Loss: 2676.6798\n",
            "Epoch [110/150], Loss: 5428.3392\n",
            "Epoch [120/150], Loss: 4486.1218\n",
            "Epoch [130/150], Loss: 14441.1529\n",
            "Epoch [140/150], Loss: 5832.2298\n",
            "Epoch [150/150], Loss: 8663.0885\n",
            "Fold 2, RMSE: 69.70698547363281\n",
            "Epoch [10/150], Loss: 7823.8242\n",
            "Epoch [20/150], Loss: 4703.7295\n",
            "Epoch [30/150], Loss: 3385.5503\n",
            "Epoch [40/150], Loss: 3726.5087\n",
            "Epoch [50/150], Loss: 6217.0261\n",
            "Epoch [60/150], Loss: 3312.1329\n",
            "Epoch [70/150], Loss: 3392.0779\n",
            "Epoch [80/150], Loss: 4585.8099\n",
            "Epoch [90/150], Loss: 1968.4672\n",
            "Epoch [100/150], Loss: 3072.7869\n",
            "Epoch [110/150], Loss: 4723.6165\n",
            "Epoch [120/150], Loss: 1044.9170\n",
            "Epoch [130/150], Loss: 1947.4624\n",
            "Epoch [140/150], Loss: 1350.3654\n",
            "Epoch [150/150], Loss: 700.5100\n",
            "Fold 3, RMSE: 95.33023834228516\n",
            "Epoch [10/150], Loss: 14451.8882\n",
            "Epoch [20/150], Loss: 7403.1844\n",
            "Epoch [30/150], Loss: 7957.9052\n",
            "Epoch [40/150], Loss: 5658.4087\n",
            "Epoch [50/150], Loss: 5635.4052\n",
            "Epoch [60/150], Loss: 9541.4685\n",
            "Epoch [70/150], Loss: 2792.0474\n",
            "Epoch [80/150], Loss: 6229.7605\n",
            "Epoch [90/150], Loss: 7916.4172\n",
            "Epoch [100/150], Loss: 5295.1411\n",
            "Epoch [110/150], Loss: 6753.3790\n",
            "Epoch [120/150], Loss: 7333.7822\n",
            "Epoch [130/150], Loss: 7701.6147\n",
            "Epoch [140/150], Loss: 2797.7863\n",
            "Epoch [150/150], Loss: 4316.6709\n",
            "Fold 4, RMSE: 45.44175720214844\n",
            "Epoch [10/150], Loss: 18906.9833\n",
            "Epoch [20/150], Loss: 13797.7739\n",
            "Epoch [30/150], Loss: 9157.4288\n",
            "Epoch [40/150], Loss: 9370.1829\n",
            "Epoch [50/150], Loss: 7276.8846\n",
            "Epoch [60/150], Loss: 5538.1920\n",
            "Epoch [70/150], Loss: 6815.5087\n",
            "Epoch [80/150], Loss: 6168.2284\n",
            "Epoch [90/150], Loss: 2979.0666\n",
            "Epoch [100/150], Loss: 5548.5118\n",
            "Epoch [110/150], Loss: 13112.2830\n",
            "Epoch [120/150], Loss: 3430.4598\n",
            "Epoch [130/150], Loss: 3393.5792\n",
            "Epoch [140/150], Loss: 4847.7804\n",
            "Epoch [150/150], Loss: 3757.0552\n",
            "Fold 5, RMSE: 46.810585021972656\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 63.02316818237305\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 15858.3599\n",
            "Epoch [20/100], Loss: 9395.9280\n",
            "Epoch [30/100], Loss: 11300.0142\n",
            "Epoch [40/100], Loss: 3653.3295\n",
            "Epoch [50/100], Loss: 5281.4598\n",
            "Epoch [60/100], Loss: 2888.3457\n",
            "Epoch [70/100], Loss: 2726.3428\n",
            "Epoch [80/100], Loss: 5733.0474\n",
            "Epoch [90/100], Loss: 1926.6613\n",
            "Epoch [100/100], Loss: 2098.1707\n",
            "Fold 1, RMSE: 52.97677230834961\n",
            "Epoch [10/100], Loss: 11775.0144\n",
            "Epoch [20/100], Loss: 9490.2102\n",
            "Epoch [30/100], Loss: 3366.0145\n",
            "Epoch [40/100], Loss: 4045.9243\n",
            "Epoch [50/100], Loss: 5407.0461\n",
            "Epoch [60/100], Loss: 2382.1437\n",
            "Epoch [70/100], Loss: 7075.2444\n",
            "Epoch [80/100], Loss: 3073.5962\n",
            "Epoch [90/100], Loss: 5128.0086\n",
            "Epoch [100/100], Loss: 3014.2642\n",
            "Fold 2, RMSE: 57.19501876831055\n",
            "Epoch [10/100], Loss: 14577.1565\n",
            "Epoch [20/100], Loss: 6454.2292\n",
            "Epoch [30/100], Loss: 4147.0002\n",
            "Epoch [40/100], Loss: 3389.8918\n",
            "Epoch [50/100], Loss: 3957.7379\n",
            "Epoch [60/100], Loss: 2531.7635\n",
            "Epoch [70/100], Loss: 2769.1758\n",
            "Epoch [80/100], Loss: 1310.2310\n",
            "Epoch [90/100], Loss: 2146.3054\n",
            "Epoch [100/100], Loss: 3797.3481\n",
            "Fold 3, RMSE: 95.93647766113281\n",
            "Epoch [10/100], Loss: 16237.8278\n",
            "Epoch [20/100], Loss: 11747.6896\n",
            "Epoch [30/100], Loss: 5486.1019\n",
            "Epoch [40/100], Loss: 4925.8829\n",
            "Epoch [50/100], Loss: 3333.7089\n",
            "Epoch [60/100], Loss: 7578.6970\n",
            "Epoch [70/100], Loss: 8796.3282\n",
            "Epoch [80/100], Loss: 4046.9734\n",
            "Epoch [90/100], Loss: 2176.2346\n",
            "Epoch [100/100], Loss: 3165.9791\n",
            "Fold 4, RMSE: 41.876312255859375\n",
            "Epoch [10/100], Loss: 14789.0767\n",
            "Epoch [20/100], Loss: 12580.0186\n",
            "Epoch [30/100], Loss: 11563.4904\n",
            "Epoch [40/100], Loss: 8466.6639\n",
            "Epoch [50/100], Loss: 4942.9437\n",
            "Epoch [60/100], Loss: 4901.1750\n",
            "Epoch [70/100], Loss: 3749.3179\n",
            "Epoch [80/100], Loss: 2271.8658\n",
            "Epoch [90/100], Loss: 6129.6096\n",
            "Epoch [100/100], Loss: 4304.7242\n",
            "Fold 5, RMSE: 45.0479850769043\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 58.60651321411133\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15388.7213\n",
            "Epoch [20/150], Loss: 13493.4252\n",
            "Epoch [30/150], Loss: 9190.5840\n",
            "Epoch [40/150], Loss: 7673.7177\n",
            "Epoch [50/150], Loss: 8012.1863\n",
            "Epoch [60/150], Loss: 3926.7339\n",
            "Epoch [70/150], Loss: 6172.9996\n",
            "Epoch [80/150], Loss: 8332.5232\n",
            "Epoch [90/150], Loss: 2632.8311\n",
            "Epoch [100/150], Loss: 1848.6007\n",
            "Epoch [110/150], Loss: 2568.7817\n",
            "Epoch [120/150], Loss: 1797.5758\n",
            "Epoch [130/150], Loss: 2395.0980\n",
            "Epoch [140/150], Loss: 908.3838\n",
            "Epoch [150/150], Loss: 2328.0104\n",
            "Fold 1, RMSE: 51.4428596496582\n",
            "Epoch [10/150], Loss: 13680.4219\n",
            "Epoch [20/150], Loss: 10553.9956\n",
            "Epoch [30/150], Loss: 9339.8169\n",
            "Epoch [40/150], Loss: 6406.8442\n",
            "Epoch [50/150], Loss: 5649.9818\n",
            "Epoch [60/150], Loss: 5511.7525\n",
            "Epoch [70/150], Loss: 2734.2615\n",
            "Epoch [80/150], Loss: 2041.7952\n",
            "Epoch [90/150], Loss: 4452.7472\n",
            "Epoch [100/150], Loss: 2409.0246\n",
            "Epoch [110/150], Loss: 1599.0565\n",
            "Epoch [120/150], Loss: 8015.1781\n",
            "Epoch [130/150], Loss: 2337.1215\n",
            "Epoch [140/150], Loss: 2761.7460\n",
            "Epoch [150/150], Loss: 9195.2980\n",
            "Fold 2, RMSE: 66.32465362548828\n",
            "Epoch [10/150], Loss: 12758.3071\n",
            "Epoch [20/150], Loss: 5917.7438\n",
            "Epoch [30/150], Loss: 4154.5905\n",
            "Epoch [40/150], Loss: 3845.1985\n",
            "Epoch [50/150], Loss: 6285.7583\n",
            "Epoch [60/150], Loss: 5560.7391\n",
            "Epoch [70/150], Loss: 2566.2091\n",
            "Epoch [80/150], Loss: 3777.1453\n",
            "Epoch [90/150], Loss: 1550.9416\n",
            "Epoch [100/150], Loss: 7558.3405\n",
            "Epoch [110/150], Loss: 2206.0250\n",
            "Epoch [120/150], Loss: 2162.6900\n",
            "Epoch [130/150], Loss: 2111.4774\n",
            "Epoch [140/150], Loss: 2122.4957\n",
            "Epoch [150/150], Loss: 1453.1876\n",
            "Fold 3, RMSE: 93.69728088378906\n",
            "Epoch [10/150], Loss: 15931.8782\n",
            "Epoch [20/150], Loss: 10511.8595\n",
            "Epoch [30/150], Loss: 10639.3278\n",
            "Epoch [40/150], Loss: 7899.5674\n",
            "Epoch [50/150], Loss: 7118.2825\n",
            "Epoch [60/150], Loss: 5553.1832\n",
            "Epoch [70/150], Loss: 11089.6497\n",
            "Epoch [80/150], Loss: 3244.6924\n",
            "Epoch [90/150], Loss: 5914.6816\n",
            "Epoch [100/150], Loss: 3531.9780\n",
            "Epoch [110/150], Loss: 2495.0402\n",
            "Epoch [120/150], Loss: 1624.4430\n",
            "Epoch [130/150], Loss: 4127.0151\n",
            "Epoch [140/150], Loss: 2177.3416\n",
            "Epoch [150/150], Loss: 1804.4698\n",
            "Fold 4, RMSE: 42.430423736572266\n",
            "Epoch [10/150], Loss: 29145.0059\n",
            "Epoch [20/150], Loss: 12161.6333\n",
            "Epoch [30/150], Loss: 10475.5984\n",
            "Epoch [40/150], Loss: 8164.5273\n",
            "Epoch [50/150], Loss: 5006.1971\n",
            "Epoch [60/150], Loss: 3713.4255\n",
            "Epoch [70/150], Loss: 6536.6558\n",
            "Epoch [80/150], Loss: 1478.4013\n",
            "Epoch [90/150], Loss: 2512.7977\n",
            "Epoch [100/150], Loss: 2272.1052\n",
            "Epoch [110/150], Loss: 2312.7905\n",
            "Epoch [120/150], Loss: 5221.7164\n",
            "Epoch [130/150], Loss: 2897.5553\n",
            "Epoch [140/150], Loss: 8610.5726\n",
            "Epoch [150/150], Loss: 1978.2208\n",
            "Fold 5, RMSE: 46.276206970214844\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 60.03428497314453\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 130756.3086\n",
            "Epoch [20/100], Loss: 19394.0200\n",
            "Epoch [30/100], Loss: 14855.7354\n",
            "Epoch [40/100], Loss: 13690.8872\n",
            "Epoch [50/100], Loss: 8980.0600\n",
            "Epoch [60/100], Loss: 13356.3914\n",
            "Epoch [70/100], Loss: 8890.2856\n",
            "Epoch [80/100], Loss: 10132.1433\n",
            "Epoch [90/100], Loss: 7040.8781\n",
            "Epoch [100/100], Loss: 9522.9540\n",
            "Fold 1, RMSE: 57.94449996948242\n",
            "Epoch [10/100], Loss: 32120.8711\n",
            "Epoch [20/100], Loss: 17247.6333\n",
            "Epoch [30/100], Loss: 22267.2498\n",
            "Epoch [40/100], Loss: 17411.2151\n",
            "Epoch [50/100], Loss: 24725.4968\n",
            "Epoch [60/100], Loss: 10111.8684\n",
            "Epoch [70/100], Loss: 12402.8699\n",
            "Epoch [80/100], Loss: 11129.5854\n",
            "Epoch [90/100], Loss: 5600.9069\n",
            "Epoch [100/100], Loss: 7117.6178\n",
            "Fold 2, RMSE: 68.49132537841797\n",
            "Epoch [10/100], Loss: 185684.1758\n",
            "Epoch [20/100], Loss: 35971.0786\n",
            "Epoch [30/100], Loss: 24031.1519\n",
            "Epoch [40/100], Loss: 14854.0176\n",
            "Epoch [50/100], Loss: 16287.1890\n",
            "Epoch [60/100], Loss: 13407.7329\n",
            "Epoch [70/100], Loss: 8614.0109\n",
            "Epoch [80/100], Loss: 11737.7463\n",
            "Epoch [90/100], Loss: 8202.8766\n",
            "Epoch [100/100], Loss: 15186.8032\n",
            "Fold 3, RMSE: 93.07034301757812\n",
            "Epoch [10/100], Loss: 106751.5254\n",
            "Epoch [20/100], Loss: 16437.1992\n",
            "Epoch [30/100], Loss: 16250.8491\n",
            "Epoch [40/100], Loss: 15012.9646\n",
            "Epoch [50/100], Loss: 14476.8577\n",
            "Epoch [60/100], Loss: 15153.1936\n",
            "Epoch [70/100], Loss: 9544.0498\n",
            "Epoch [80/100], Loss: 9636.1091\n",
            "Epoch [90/100], Loss: 6616.0946\n",
            "Epoch [100/100], Loss: 12354.8176\n",
            "Fold 4, RMSE: 40.5285530090332\n",
            "Epoch [10/100], Loss: 81659.3066\n",
            "Epoch [20/100], Loss: 18052.9351\n",
            "Epoch [30/100], Loss: 18782.6860\n",
            "Epoch [40/100], Loss: 11781.3187\n",
            "Epoch [50/100], Loss: 12604.7400\n",
            "Epoch [60/100], Loss: 10449.4504\n",
            "Epoch [70/100], Loss: 12523.5844\n",
            "Epoch [80/100], Loss: 12217.7551\n",
            "Epoch [90/100], Loss: 5283.5963\n",
            "Epoch [100/100], Loss: 8737.0002\n",
            "Fold 5, RMSE: 47.20801544189453\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 61.44854736328125\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 191125.0352\n",
            "Epoch [20/150], Loss: 36303.8447\n",
            "Epoch [30/150], Loss: 23775.4800\n",
            "Epoch [40/150], Loss: 26803.2842\n",
            "Epoch [50/150], Loss: 12672.3668\n",
            "Epoch [60/150], Loss: 24818.1489\n",
            "Epoch [70/150], Loss: 12115.3025\n",
            "Epoch [80/150], Loss: 13705.7107\n",
            "Epoch [90/150], Loss: 13129.7046\n",
            "Epoch [100/150], Loss: 11470.6978\n",
            "Epoch [110/150], Loss: 19415.1497\n",
            "Epoch [120/150], Loss: 8065.2621\n",
            "Epoch [130/150], Loss: 17518.0137\n",
            "Epoch [140/150], Loss: 11131.0873\n",
            "Epoch [150/150], Loss: 19581.5159\n",
            "Fold 1, RMSE: 50.28573226928711\n",
            "Epoch [10/150], Loss: 173589.1768\n",
            "Epoch [20/150], Loss: 37528.2939\n",
            "Epoch [30/150], Loss: 18533.7378\n",
            "Epoch [40/150], Loss: 15843.7642\n",
            "Epoch [50/150], Loss: 12131.4189\n",
            "Epoch [60/150], Loss: 13787.9663\n",
            "Epoch [70/150], Loss: 10248.5942\n",
            "Epoch [80/150], Loss: 9037.4575\n",
            "Epoch [90/150], Loss: 7152.0154\n",
            "Epoch [100/150], Loss: 6653.0350\n",
            "Epoch [110/150], Loss: 16974.5341\n",
            "Epoch [120/150], Loss: 8280.8958\n",
            "Epoch [130/150], Loss: 6880.1849\n",
            "Epoch [140/150], Loss: 8261.6577\n",
            "Epoch [150/150], Loss: 3750.0521\n",
            "Fold 2, RMSE: 65.33026123046875\n",
            "Epoch [10/150], Loss: 15325.7324\n",
            "Epoch [20/150], Loss: 14579.2922\n",
            "Epoch [30/150], Loss: 14088.6073\n",
            "Epoch [40/150], Loss: 12372.6443\n",
            "Epoch [50/150], Loss: 14772.6785\n",
            "Epoch [60/150], Loss: 13217.9226\n",
            "Epoch [70/150], Loss: 11457.7831\n",
            "Epoch [80/150], Loss: 13131.7839\n",
            "Epoch [90/150], Loss: 14964.8032\n",
            "Epoch [100/150], Loss: 16691.0376\n",
            "Epoch [110/150], Loss: 12911.2595\n",
            "Epoch [120/150], Loss: 15746.9229\n",
            "Epoch [130/150], Loss: 12403.9216\n",
            "Epoch [140/150], Loss: 12636.0667\n",
            "Epoch [150/150], Loss: 13384.5205\n",
            "Fold 3, RMSE: 109.61861419677734\n",
            "Epoch [10/150], Loss: 141667.7383\n",
            "Epoch [20/150], Loss: 18642.9520\n",
            "Epoch [30/150], Loss: 20111.0312\n",
            "Epoch [40/150], Loss: 14392.7510\n",
            "Epoch [50/150], Loss: 12027.0518\n",
            "Epoch [60/150], Loss: 12809.7512\n",
            "Epoch [70/150], Loss: 24226.5763\n",
            "Epoch [80/150], Loss: 11608.8162\n",
            "Epoch [90/150], Loss: 10290.0511\n",
            "Epoch [100/150], Loss: 18691.1687\n",
            "Epoch [110/150], Loss: 11853.2009\n",
            "Epoch [120/150], Loss: 13032.6606\n",
            "Epoch [130/150], Loss: 9237.4208\n",
            "Epoch [140/150], Loss: 8499.7744\n",
            "Epoch [150/150], Loss: 10791.7251\n",
            "Fold 4, RMSE: 39.95667266845703\n",
            "Epoch [10/150], Loss: 17301.8594\n",
            "Epoch [20/150], Loss: 17521.5327\n",
            "Epoch [30/150], Loss: 18538.5220\n",
            "Epoch [40/150], Loss: 19514.4878\n",
            "Epoch [50/150], Loss: 13832.2434\n",
            "Epoch [60/150], Loss: 14165.3254\n",
            "Epoch [70/150], Loss: 9858.8748\n",
            "Epoch [80/150], Loss: 14911.1011\n",
            "Epoch [90/150], Loss: 9321.8136\n",
            "Epoch [100/150], Loss: 11219.5250\n",
            "Epoch [110/150], Loss: 9237.8096\n",
            "Epoch [120/150], Loss: 13889.0881\n",
            "Epoch [130/150], Loss: 9318.7317\n",
            "Epoch [140/150], Loss: 7162.5826\n",
            "Epoch [150/150], Loss: 10358.2565\n",
            "Fold 5, RMSE: 45.1497917175293\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 62.068214416503906\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 33113.1021\n",
            "Epoch [20/100], Loss: 18642.8750\n",
            "Epoch [30/100], Loss: 12532.9172\n",
            "Epoch [40/100], Loss: 14233.5518\n",
            "Epoch [50/100], Loss: 16116.4150\n",
            "Epoch [60/100], Loss: 11522.2382\n",
            "Epoch [70/100], Loss: 9957.7980\n",
            "Epoch [80/100], Loss: 16648.7676\n",
            "Epoch [90/100], Loss: 14073.1016\n",
            "Epoch [100/100], Loss: 10067.6025\n",
            "Fold 1, RMSE: 47.07091522216797\n",
            "Epoch [10/100], Loss: 30610.0776\n",
            "Epoch [20/100], Loss: 13215.8809\n",
            "Epoch [30/100], Loss: 11292.4087\n",
            "Epoch [40/100], Loss: 10399.2982\n",
            "Epoch [50/100], Loss: 10402.2429\n",
            "Epoch [60/100], Loss: 10519.8248\n",
            "Epoch [70/100], Loss: 16160.3219\n",
            "Epoch [80/100], Loss: 12960.4810\n",
            "Epoch [90/100], Loss: 8357.1948\n",
            "Epoch [100/100], Loss: 9679.8557\n",
            "Fold 2, RMSE: 69.11427307128906\n",
            "Epoch [10/100], Loss: 82298.0234\n",
            "Epoch [20/100], Loss: 14741.1760\n",
            "Epoch [30/100], Loss: 10867.5623\n",
            "Epoch [40/100], Loss: 8909.1215\n",
            "Epoch [50/100], Loss: 8193.8676\n",
            "Epoch [60/100], Loss: 7491.7896\n",
            "Epoch [70/100], Loss: 8280.2253\n",
            "Epoch [80/100], Loss: 6330.3146\n",
            "Epoch [90/100], Loss: 10811.2026\n",
            "Epoch [100/100], Loss: 11032.4403\n",
            "Fold 3, RMSE: 95.71509552001953\n",
            "Epoch [10/100], Loss: 31957.3447\n",
            "Epoch [20/100], Loss: 13791.3843\n",
            "Epoch [30/100], Loss: 15208.6904\n",
            "Epoch [40/100], Loss: 11102.2591\n",
            "Epoch [50/100], Loss: 12967.3403\n",
            "Epoch [60/100], Loss: 12131.9641\n",
            "Epoch [70/100], Loss: 10660.4179\n",
            "Epoch [80/100], Loss: 11450.4724\n",
            "Epoch [90/100], Loss: 9464.0942\n",
            "Epoch [100/100], Loss: 7435.4391\n",
            "Fold 4, RMSE: 36.42140579223633\n",
            "Epoch [10/100], Loss: 71908.3057\n",
            "Epoch [20/100], Loss: 23644.4238\n",
            "Epoch [30/100], Loss: 17511.3945\n",
            "Epoch [40/100], Loss: 17517.5276\n",
            "Epoch [50/100], Loss: 12105.4194\n",
            "Epoch [60/100], Loss: 11937.3406\n",
            "Epoch [70/100], Loss: 13122.9886\n",
            "Epoch [80/100], Loss: 12845.6216\n",
            "Epoch [90/100], Loss: 12080.1688\n",
            "Epoch [100/100], Loss: 11802.9653\n",
            "Fold 5, RMSE: 49.19331359863281\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 59.50300064086914\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 21320.3530\n",
            "Epoch [20/150], Loss: 17439.3032\n",
            "Epoch [30/150], Loss: 15196.9995\n",
            "Epoch [40/150], Loss: 14618.5061\n",
            "Epoch [50/150], Loss: 15961.0559\n",
            "Epoch [60/150], Loss: 16448.9514\n",
            "Epoch [70/150], Loss: 20690.5010\n",
            "Epoch [80/150], Loss: 17772.5974\n",
            "Epoch [90/150], Loss: 12941.0510\n",
            "Epoch [100/150], Loss: 12160.9274\n",
            "Epoch [110/150], Loss: 14333.8738\n",
            "Epoch [120/150], Loss: 18224.9364\n",
            "Epoch [130/150], Loss: 12596.8630\n",
            "Epoch [140/150], Loss: 10021.4109\n",
            "Epoch [150/150], Loss: 10151.3499\n",
            "Fold 1, RMSE: 53.6008186340332\n",
            "Epoch [10/150], Loss: 59178.6367\n",
            "Epoch [20/150], Loss: 22053.1323\n",
            "Epoch [30/150], Loss: 15416.1436\n",
            "Epoch [40/150], Loss: 19664.6538\n",
            "Epoch [50/150], Loss: 14666.1685\n",
            "Epoch [60/150], Loss: 13282.5374\n",
            "Epoch [70/150], Loss: 11198.0947\n",
            "Epoch [80/150], Loss: 12089.1143\n",
            "Epoch [90/150], Loss: 10932.8965\n",
            "Epoch [100/150], Loss: 9636.3959\n",
            "Epoch [110/150], Loss: 13750.7810\n",
            "Epoch [120/150], Loss: 10279.0552\n",
            "Epoch [130/150], Loss: 11881.8835\n",
            "Epoch [140/150], Loss: 10144.3088\n",
            "Epoch [150/150], Loss: 9843.9113\n",
            "Fold 2, RMSE: 72.90445709228516\n",
            "Epoch [10/150], Loss: 33202.0474\n",
            "Epoch [20/150], Loss: 6329.3408\n",
            "Epoch [30/150], Loss: 8923.0577\n",
            "Epoch [40/150], Loss: 7599.2778\n",
            "Epoch [50/150], Loss: 8400.8656\n",
            "Epoch [60/150], Loss: 8908.2983\n",
            "Epoch [70/150], Loss: 9537.3350\n",
            "Epoch [80/150], Loss: 8622.0916\n",
            "Epoch [90/150], Loss: 9269.4006\n",
            "Epoch [100/150], Loss: 8473.8198\n",
            "Epoch [110/150], Loss: 9067.4425\n",
            "Epoch [120/150], Loss: 6898.9807\n",
            "Epoch [130/150], Loss: 6846.2511\n",
            "Epoch [140/150], Loss: 7923.0227\n",
            "Epoch [150/150], Loss: 11038.6455\n",
            "Fold 3, RMSE: 95.79065704345703\n",
            "Epoch [10/150], Loss: 97722.8105\n",
            "Epoch [20/150], Loss: 17786.1921\n",
            "Epoch [30/150], Loss: 12809.8945\n",
            "Epoch [40/150], Loss: 15630.5029\n",
            "Epoch [50/150], Loss: 17290.9321\n",
            "Epoch [60/150], Loss: 19886.0188\n",
            "Epoch [70/150], Loss: 12212.3922\n",
            "Epoch [80/150], Loss: 12831.4839\n",
            "Epoch [90/150], Loss: 13405.1902\n",
            "Epoch [100/150], Loss: 10552.1696\n",
            "Epoch [110/150], Loss: 10440.4268\n",
            "Epoch [120/150], Loss: 11914.4592\n",
            "Epoch [130/150], Loss: 12807.2334\n",
            "Epoch [140/150], Loss: 15716.5679\n",
            "Epoch [150/150], Loss: 12216.7329\n",
            "Fold 4, RMSE: 34.13602828979492\n",
            "Epoch [10/150], Loss: 53270.5352\n",
            "Epoch [20/150], Loss: 18493.4080\n",
            "Epoch [30/150], Loss: 19431.6523\n",
            "Epoch [40/150], Loss: 15718.0393\n",
            "Epoch [50/150], Loss: 16243.2476\n",
            "Epoch [60/150], Loss: 14381.3391\n",
            "Epoch [70/150], Loss: 15090.7495\n",
            "Epoch [80/150], Loss: 13669.7537\n",
            "Epoch [90/150], Loss: 12250.8313\n",
            "Epoch [100/150], Loss: 13635.2317\n",
            "Epoch [110/150], Loss: 10627.7205\n",
            "Epoch [120/150], Loss: 11338.1802\n",
            "Epoch [130/150], Loss: 12506.0907\n",
            "Epoch [140/150], Loss: 9780.1758\n",
            "Epoch [150/150], Loss: 8215.7000\n",
            "Fold 5, RMSE: 47.93307113647461\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 60.873006439208986\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 312488.8867\n",
            "Epoch [20/100], Loss: 81958.2461\n",
            "Epoch [30/100], Loss: 29803.5859\n",
            "Epoch [40/100], Loss: 21727.5161\n",
            "Epoch [50/100], Loss: 12762.7898\n",
            "Epoch [60/100], Loss: 14046.0438\n",
            "Epoch [70/100], Loss: 17486.5430\n",
            "Epoch [80/100], Loss: 17430.9150\n",
            "Epoch [90/100], Loss: 24420.7878\n",
            "Epoch [100/100], Loss: 14925.9946\n",
            "Fold 1, RMSE: 47.21144485473633\n",
            "Epoch [10/100], Loss: 220414.5273\n",
            "Epoch [20/100], Loss: 52788.9492\n",
            "Epoch [30/100], Loss: 37332.3975\n",
            "Epoch [40/100], Loss: 38983.0732\n",
            "Epoch [50/100], Loss: 16041.9048\n",
            "Epoch [60/100], Loss: 19742.8989\n",
            "Epoch [70/100], Loss: 23123.3275\n",
            "Epoch [80/100], Loss: 15710.5811\n",
            "Epoch [90/100], Loss: 18532.5767\n",
            "Epoch [100/100], Loss: 14304.9331\n",
            "Fold 2, RMSE: 73.38728332519531\n",
            "Epoch [10/100], Loss: 313277.9375\n",
            "Epoch [20/100], Loss: 63312.6270\n",
            "Epoch [30/100], Loss: 33416.2334\n",
            "Epoch [40/100], Loss: 17357.0908\n",
            "Epoch [50/100], Loss: 21042.9326\n",
            "Epoch [60/100], Loss: 15947.8386\n",
            "Epoch [70/100], Loss: 29940.1323\n",
            "Epoch [80/100], Loss: 9692.0308\n",
            "Epoch [90/100], Loss: 10757.9089\n",
            "Epoch [100/100], Loss: 8358.6177\n",
            "Fold 3, RMSE: 92.897216796875\n",
            "Epoch [10/100], Loss: 635477.7812\n",
            "Epoch [20/100], Loss: 41449.7705\n",
            "Epoch [30/100], Loss: 38388.0645\n",
            "Epoch [40/100], Loss: 25035.1152\n",
            "Epoch [50/100], Loss: 26967.2041\n",
            "Epoch [60/100], Loss: 15321.9507\n",
            "Epoch [70/100], Loss: 15635.2388\n",
            "Epoch [80/100], Loss: 28313.7715\n",
            "Epoch [90/100], Loss: 15401.4702\n",
            "Epoch [100/100], Loss: 13516.4131\n",
            "Fold 4, RMSE: 41.12223434448242\n",
            "Epoch [10/100], Loss: 151837.5664\n",
            "Epoch [20/100], Loss: 28894.6885\n",
            "Epoch [30/100], Loss: 24642.9194\n",
            "Epoch [40/100], Loss: 19595.2991\n",
            "Epoch [50/100], Loss: 14994.7864\n",
            "Epoch [60/100], Loss: 15221.8411\n",
            "Epoch [70/100], Loss: 22755.7644\n",
            "Epoch [80/100], Loss: 16040.0090\n",
            "Epoch [90/100], Loss: 16204.8687\n",
            "Epoch [100/100], Loss: 12642.5083\n",
            "Fold 5, RMSE: 47.89990997314453\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 60.50361785888672\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 342266.5234\n",
            "Epoch [20/150], Loss: 44892.2671\n",
            "Epoch [30/150], Loss: 33575.4434\n",
            "Epoch [40/150], Loss: 47045.0859\n",
            "Epoch [50/150], Loss: 24165.1272\n",
            "Epoch [60/150], Loss: 21712.9531\n",
            "Epoch [70/150], Loss: 20977.0645\n",
            "Epoch [80/150], Loss: 14384.5671\n",
            "Epoch [90/150], Loss: 13594.6558\n",
            "Epoch [100/150], Loss: 24280.7700\n",
            "Epoch [110/150], Loss: 13621.1133\n",
            "Epoch [120/150], Loss: 19122.5801\n",
            "Epoch [130/150], Loss: 15779.9177\n",
            "Epoch [140/150], Loss: 14745.0771\n",
            "Epoch [150/150], Loss: 13007.2274\n",
            "Fold 1, RMSE: 47.04172134399414\n",
            "Epoch [10/150], Loss: 337972.9844\n",
            "Epoch [20/150], Loss: 56012.7656\n",
            "Epoch [30/150], Loss: 23668.6482\n",
            "Epoch [40/150], Loss: 15481.3445\n",
            "Epoch [50/150], Loss: 35118.6030\n",
            "Epoch [60/150], Loss: 14820.0293\n",
            "Epoch [70/150], Loss: 14116.8750\n",
            "Epoch [80/150], Loss: 19499.2759\n",
            "Epoch [90/150], Loss: 12355.1748\n",
            "Epoch [100/150], Loss: 10435.5400\n",
            "Epoch [110/150], Loss: 11353.5197\n",
            "Epoch [120/150], Loss: 15458.5125\n",
            "Epoch [130/150], Loss: 13188.5352\n",
            "Epoch [140/150], Loss: 15253.9626\n",
            "Epoch [150/150], Loss: 11253.1875\n",
            "Fold 2, RMSE: 70.76512908935547\n",
            "Epoch [10/150], Loss: 767030.1719\n",
            "Epoch [20/150], Loss: 95411.8525\n",
            "Epoch [30/150], Loss: 40687.0366\n",
            "Epoch [40/150], Loss: 27306.3286\n",
            "Epoch [50/150], Loss: 14626.2483\n",
            "Epoch [60/150], Loss: 16790.7749\n",
            "Epoch [70/150], Loss: 14705.2500\n",
            "Epoch [80/150], Loss: 10260.5515\n",
            "Epoch [90/150], Loss: 16296.8579\n",
            "Epoch [100/150], Loss: 8873.0590\n",
            "Epoch [110/150], Loss: 9708.2920\n",
            "Epoch [120/150], Loss: 10641.7334\n",
            "Epoch [130/150], Loss: 8454.1453\n",
            "Epoch [140/150], Loss: 8447.7709\n",
            "Epoch [150/150], Loss: 6814.0217\n",
            "Fold 3, RMSE: 93.50080871582031\n",
            "Epoch [10/150], Loss: 375688.3125\n",
            "Epoch [20/150], Loss: 34189.8145\n",
            "Epoch [30/150], Loss: 26197.3984\n",
            "Epoch [40/150], Loss: 15896.0117\n",
            "Epoch [50/150], Loss: 26198.1650\n",
            "Epoch [60/150], Loss: 19892.7705\n",
            "Epoch [70/150], Loss: 16079.4702\n",
            "Epoch [80/150], Loss: 25862.8018\n",
            "Epoch [90/150], Loss: 12195.8035\n",
            "Epoch [100/150], Loss: 11249.2291\n",
            "Epoch [110/150], Loss: 13633.0576\n",
            "Epoch [120/150], Loss: 11238.5327\n",
            "Epoch [130/150], Loss: 15296.9578\n",
            "Epoch [140/150], Loss: 11376.7650\n",
            "Epoch [150/150], Loss: 11227.6418\n",
            "Fold 4, RMSE: 35.8607063293457\n",
            "Epoch [10/150], Loss: 520216.3438\n",
            "Epoch [20/150], Loss: 91167.2363\n",
            "Epoch [30/150], Loss: 31010.1812\n",
            "Epoch [40/150], Loss: 39451.7070\n",
            "Epoch [50/150], Loss: 26384.9600\n",
            "Epoch [60/150], Loss: 21658.1538\n",
            "Epoch [70/150], Loss: 21855.7139\n",
            "Epoch [80/150], Loss: 16003.0173\n",
            "Epoch [90/150], Loss: 15273.3379\n",
            "Epoch [100/150], Loss: 17919.9810\n",
            "Epoch [110/150], Loss: 23898.5732\n",
            "Epoch [120/150], Loss: 15339.6582\n",
            "Epoch [130/150], Loss: 13061.9941\n",
            "Epoch [140/150], Loss: 14315.5696\n",
            "Epoch [150/150], Loss: 12520.4104\n",
            "Fold 5, RMSE: 48.74028396606445\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 59.18172988891602\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 70153.4941\n",
            "Epoch [20/100], Loss: 24244.2321\n",
            "Epoch [30/100], Loss: 15198.8857\n",
            "Epoch [40/100], Loss: 13853.5852\n",
            "Epoch [50/100], Loss: 15263.2249\n",
            "Epoch [60/100], Loss: 10711.7736\n",
            "Epoch [70/100], Loss: 10470.2157\n",
            "Epoch [80/100], Loss: 10075.9408\n",
            "Epoch [90/100], Loss: 20893.9233\n",
            "Epoch [100/100], Loss: 17086.9717\n",
            "Fold 1, RMSE: 51.69445037841797\n",
            "Epoch [10/100], Loss: 82450.7539\n",
            "Epoch [20/100], Loss: 21388.1064\n",
            "Epoch [30/100], Loss: 11438.5547\n",
            "Epoch [40/100], Loss: 13573.8467\n",
            "Epoch [50/100], Loss: 15617.2695\n",
            "Epoch [60/100], Loss: 11220.9243\n",
            "Epoch [70/100], Loss: 12636.6509\n",
            "Epoch [80/100], Loss: 10241.6274\n",
            "Epoch [90/100], Loss: 8207.0690\n",
            "Epoch [100/100], Loss: 9314.6536\n",
            "Fold 2, RMSE: 70.20856475830078\n",
            "Epoch [10/100], Loss: 65000.5176\n",
            "Epoch [20/100], Loss: 14215.3921\n",
            "Epoch [30/100], Loss: 13251.8374\n",
            "Epoch [40/100], Loss: 7243.0601\n",
            "Epoch [50/100], Loss: 9450.3289\n",
            "Epoch [60/100], Loss: 12152.0557\n",
            "Epoch [70/100], Loss: 10267.3080\n",
            "Epoch [80/100], Loss: 7101.9540\n",
            "Epoch [90/100], Loss: 8544.5304\n",
            "Epoch [100/100], Loss: 9413.1387\n",
            "Fold 3, RMSE: 92.8050765991211\n",
            "Epoch [10/100], Loss: 30481.7437\n",
            "Epoch [20/100], Loss: 14689.7157\n",
            "Epoch [30/100], Loss: 10623.9995\n",
            "Epoch [40/100], Loss: 12300.3008\n",
            "Epoch [50/100], Loss: 15447.8103\n",
            "Epoch [60/100], Loss: 12681.7473\n",
            "Epoch [70/100], Loss: 12445.2996\n",
            "Epoch [80/100], Loss: 17774.4829\n",
            "Epoch [90/100], Loss: 12604.2578\n",
            "Epoch [100/100], Loss: 12359.4285\n",
            "Fold 4, RMSE: 36.57936096191406\n",
            "Epoch [10/100], Loss: 117645.2773\n",
            "Epoch [20/100], Loss: 22532.7041\n",
            "Epoch [30/100], Loss: 17804.6021\n",
            "Epoch [40/100], Loss: 19021.7522\n",
            "Epoch [50/100], Loss: 10023.6372\n",
            "Epoch [60/100], Loss: 12498.0779\n",
            "Epoch [70/100], Loss: 9781.8636\n",
            "Epoch [80/100], Loss: 9584.1147\n",
            "Epoch [90/100], Loss: 15237.0679\n",
            "Epoch [100/100], Loss: 12682.8794\n",
            "Fold 5, RMSE: 48.061222076416016\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 59.86973495483399\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 54669.6592\n",
            "Epoch [20/150], Loss: 21775.6047\n",
            "Epoch [30/150], Loss: 14758.9265\n",
            "Epoch [40/150], Loss: 10673.4821\n",
            "Epoch [50/150], Loss: 17861.9907\n",
            "Epoch [60/150], Loss: 18363.7373\n",
            "Epoch [70/150], Loss: 12369.0847\n",
            "Epoch [80/150], Loss: 18972.4790\n",
            "Epoch [90/150], Loss: 13601.0686\n",
            "Epoch [100/150], Loss: 19307.4373\n",
            "Epoch [110/150], Loss: 19159.2668\n",
            "Epoch [120/150], Loss: 10959.4657\n",
            "Epoch [130/150], Loss: 10555.4330\n",
            "Epoch [140/150], Loss: 10214.8306\n",
            "Epoch [150/150], Loss: 12524.0667\n",
            "Fold 1, RMSE: 51.076683044433594\n",
            "Epoch [10/150], Loss: 61791.8242\n",
            "Epoch [20/150], Loss: 18999.5366\n",
            "Epoch [30/150], Loss: 11058.9622\n",
            "Epoch [40/150], Loss: 11376.1951\n",
            "Epoch [50/150], Loss: 16022.6292\n",
            "Epoch [60/150], Loss: 11452.8379\n",
            "Epoch [70/150], Loss: 20514.2385\n",
            "Epoch [80/150], Loss: 14299.0208\n",
            "Epoch [90/150], Loss: 17531.1616\n",
            "Epoch [100/150], Loss: 9286.2170\n",
            "Epoch [110/150], Loss: 10600.0671\n",
            "Epoch [120/150], Loss: 8811.7346\n",
            "Epoch [130/150], Loss: 9882.5332\n",
            "Epoch [140/150], Loss: 11170.1445\n",
            "Epoch [150/150], Loss: 9951.0522\n",
            "Fold 2, RMSE: 73.18730163574219\n",
            "Epoch [10/150], Loss: 32158.6958\n",
            "Epoch [20/150], Loss: 11253.0283\n",
            "Epoch [30/150], Loss: 7751.5740\n",
            "Epoch [40/150], Loss: 8808.2554\n",
            "Epoch [50/150], Loss: 8898.6914\n",
            "Epoch [60/150], Loss: 7614.7756\n",
            "Epoch [70/150], Loss: 8125.1595\n",
            "Epoch [80/150], Loss: 7421.6139\n",
            "Epoch [90/150], Loss: 9835.0276\n",
            "Epoch [100/150], Loss: 7162.3315\n",
            "Epoch [110/150], Loss: 5963.2865\n",
            "Epoch [120/150], Loss: 10123.9646\n",
            "Epoch [130/150], Loss: 8536.5587\n",
            "Epoch [140/150], Loss: 6420.4717\n",
            "Epoch [150/150], Loss: 8191.7009\n",
            "Fold 3, RMSE: 95.55562591552734\n",
            "Epoch [10/150], Loss: 40029.5098\n",
            "Epoch [20/150], Loss: 14464.6777\n",
            "Epoch [30/150], Loss: 13206.7017\n",
            "Epoch [40/150], Loss: 13319.7429\n",
            "Epoch [50/150], Loss: 12063.6687\n",
            "Epoch [60/150], Loss: 13469.0251\n",
            "Epoch [70/150], Loss: 11122.1896\n",
            "Epoch [80/150], Loss: 10995.2209\n",
            "Epoch [90/150], Loss: 12746.9294\n",
            "Epoch [100/150], Loss: 10950.2639\n",
            "Epoch [110/150], Loss: 12906.1057\n",
            "Epoch [120/150], Loss: 16194.5166\n",
            "Epoch [130/150], Loss: 14964.1313\n",
            "Epoch [140/150], Loss: 15756.0505\n",
            "Epoch [150/150], Loss: 10692.6794\n",
            "Fold 4, RMSE: 35.116214752197266\n",
            "Epoch [10/150], Loss: 29012.0996\n",
            "Epoch [20/150], Loss: 16313.7759\n",
            "Epoch [30/150], Loss: 11795.9275\n",
            "Epoch [40/150], Loss: 12107.3419\n",
            "Epoch [50/150], Loss: 12151.0916\n",
            "Epoch [60/150], Loss: 10131.0664\n",
            "Epoch [70/150], Loss: 10647.3875\n",
            "Epoch [80/150], Loss: 16199.1757\n",
            "Epoch [90/150], Loss: 13036.8679\n",
            "Epoch [100/150], Loss: 16231.0186\n",
            "Epoch [110/150], Loss: 12918.5193\n",
            "Epoch [120/150], Loss: 11117.0999\n",
            "Epoch [130/150], Loss: 9486.1182\n",
            "Epoch [140/150], Loss: 11106.9075\n",
            "Epoch [150/150], Loss: 12890.9919\n",
            "Fold 5, RMSE: 46.8525276184082\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 60.357670593261716\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 289978.4453\n",
            "Epoch [20/100], Loss: 30361.3164\n",
            "Epoch [30/100], Loss: 15131.2646\n",
            "Epoch [40/100], Loss: 14192.8156\n",
            "Epoch [50/100], Loss: 14858.2495\n",
            "Epoch [60/100], Loss: 15675.8223\n",
            "Epoch [70/100], Loss: 16425.6899\n",
            "Epoch [80/100], Loss: 11908.7595\n",
            "Epoch [90/100], Loss: 9985.0588\n",
            "Epoch [100/100], Loss: 7952.2349\n",
            "Fold 1, RMSE: 50.294734954833984\n",
            "Epoch [10/100], Loss: 262570.9180\n",
            "Epoch [20/100], Loss: 28376.4326\n",
            "Epoch [30/100], Loss: 21599.7754\n",
            "Epoch [40/100], Loss: 14027.8748\n",
            "Epoch [50/100], Loss: 31808.2075\n",
            "Epoch [60/100], Loss: 10131.6835\n",
            "Epoch [70/100], Loss: 10928.0020\n",
            "Epoch [80/100], Loss: 12570.1379\n",
            "Epoch [90/100], Loss: 10615.2581\n",
            "Epoch [100/100], Loss: 10428.1069\n",
            "Fold 2, RMSE: 70.96261596679688\n",
            "Epoch [10/100], Loss: 401177.1641\n",
            "Epoch [20/100], Loss: 55847.1465\n",
            "Epoch [30/100], Loss: 35307.1260\n",
            "Epoch [40/100], Loss: 18575.3057\n",
            "Epoch [50/100], Loss: 11874.4548\n",
            "Epoch [60/100], Loss: 17070.8076\n",
            "Epoch [70/100], Loss: 14908.4656\n",
            "Epoch [80/100], Loss: 16357.2500\n",
            "Epoch [90/100], Loss: 11670.6460\n",
            "Epoch [100/100], Loss: 10680.0950\n",
            "Fold 3, RMSE: 91.45036315917969\n",
            "Epoch [10/100], Loss: 359019.6992\n",
            "Epoch [20/100], Loss: 51746.7480\n",
            "Epoch [30/100], Loss: 27941.1050\n",
            "Epoch [40/100], Loss: 18771.3804\n",
            "Epoch [50/100], Loss: 13379.4717\n",
            "Epoch [60/100], Loss: 15675.7000\n",
            "Epoch [70/100], Loss: 15334.5002\n",
            "Epoch [80/100], Loss: 14068.5024\n",
            "Epoch [90/100], Loss: 12904.7896\n",
            "Epoch [100/100], Loss: 13087.0603\n",
            "Fold 4, RMSE: 33.345462799072266\n",
            "Epoch [10/100], Loss: 608591.1250\n",
            "Epoch [20/100], Loss: 55253.0098\n",
            "Epoch [30/100], Loss: 41114.6011\n",
            "Epoch [40/100], Loss: 48288.6045\n",
            "Epoch [50/100], Loss: 14986.5199\n",
            "Epoch [60/100], Loss: 18652.2158\n",
            "Epoch [70/100], Loss: 23656.1340\n",
            "Epoch [80/100], Loss: 14979.5024\n",
            "Epoch [90/100], Loss: 18395.6245\n",
            "Epoch [100/100], Loss: 9793.7618\n",
            "Fold 5, RMSE: 47.88591003417969\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 58.7878173828125\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 537336.0312\n",
            "Epoch [20/150], Loss: 87456.0137\n",
            "Epoch [30/150], Loss: 33278.2231\n",
            "Epoch [40/150], Loss: 27323.5234\n",
            "Epoch [50/150], Loss: 21229.1619\n",
            "Epoch [60/150], Loss: 22948.2476\n",
            "Epoch [70/150], Loss: 13789.4456\n",
            "Epoch [80/150], Loss: 14557.4226\n",
            "Epoch [90/150], Loss: 19187.7632\n",
            "Epoch [100/150], Loss: 14805.7837\n",
            "Epoch [110/150], Loss: 13040.8979\n",
            "Epoch [120/150], Loss: 16151.8140\n",
            "Epoch [130/150], Loss: 18266.5310\n",
            "Epoch [140/150], Loss: 10756.8477\n",
            "Epoch [150/150], Loss: 13775.8669\n",
            "Fold 1, RMSE: 45.422603607177734\n",
            "Epoch [10/150], Loss: 1014615.6719\n",
            "Epoch [20/150], Loss: 116845.6523\n",
            "Epoch [30/150], Loss: 60955.7461\n",
            "Epoch [40/150], Loss: 47799.7461\n",
            "Epoch [50/150], Loss: 34253.6807\n",
            "Epoch [60/150], Loss: 39411.6357\n",
            "Epoch [70/150], Loss: 18355.5120\n",
            "Epoch [80/150], Loss: 17470.0732\n",
            "Epoch [90/150], Loss: 16543.4993\n",
            "Epoch [100/150], Loss: 19568.5723\n",
            "Epoch [110/150], Loss: 19941.2935\n",
            "Epoch [120/150], Loss: 16325.8054\n",
            "Epoch [130/150], Loss: 11539.1387\n",
            "Epoch [140/150], Loss: 12265.7117\n",
            "Epoch [150/150], Loss: 12666.2727\n",
            "Fold 2, RMSE: 69.3343734741211\n",
            "Epoch [10/150], Loss: 295420.8359\n",
            "Epoch [20/150], Loss: 60453.6562\n",
            "Epoch [30/150], Loss: 26146.3113\n",
            "Epoch [40/150], Loss: 18097.0383\n",
            "Epoch [50/150], Loss: 19707.2061\n",
            "Epoch [60/150], Loss: 12106.2351\n",
            "Epoch [70/150], Loss: 11993.8042\n",
            "Epoch [80/150], Loss: 8018.7648\n",
            "Epoch [90/150], Loss: 12029.8674\n",
            "Epoch [100/150], Loss: 10312.7031\n",
            "Epoch [110/150], Loss: 6424.0413\n",
            "Epoch [120/150], Loss: 8064.3948\n",
            "Epoch [130/150], Loss: 11793.2083\n",
            "Epoch [140/150], Loss: 9135.2126\n",
            "Epoch [150/150], Loss: 7562.0430\n",
            "Fold 3, RMSE: 94.01907348632812\n",
            "Epoch [10/150], Loss: 275581.1641\n",
            "Epoch [20/150], Loss: 108432.8730\n",
            "Epoch [30/150], Loss: 23000.7969\n",
            "Epoch [40/150], Loss: 25870.5679\n",
            "Epoch [50/150], Loss: 14068.2026\n",
            "Epoch [60/150], Loss: 11533.7579\n",
            "Epoch [70/150], Loss: 20218.8472\n",
            "Epoch [80/150], Loss: 16629.3301\n",
            "Epoch [90/150], Loss: 15131.3381\n",
            "Epoch [100/150], Loss: 18456.4456\n",
            "Epoch [110/150], Loss: 12485.3772\n",
            "Epoch [120/150], Loss: 19689.8049\n",
            "Epoch [130/150], Loss: 15885.3701\n",
            "Epoch [140/150], Loss: 12210.3894\n",
            "Epoch [150/150], Loss: 13246.6882\n",
            "Fold 4, RMSE: 36.71385955810547\n",
            "Epoch [10/150], Loss: 547150.2812\n",
            "Epoch [20/150], Loss: 71240.8945\n",
            "Epoch [30/150], Loss: 42454.2158\n",
            "Epoch [40/150], Loss: 27955.3943\n",
            "Epoch [50/150], Loss: 19788.9988\n",
            "Epoch [60/150], Loss: 15943.4653\n",
            "Epoch [70/150], Loss: 20370.5825\n",
            "Epoch [80/150], Loss: 16122.2615\n",
            "Epoch [90/150], Loss: 18536.8857\n",
            "Epoch [100/150], Loss: 14927.9414\n",
            "Epoch [110/150], Loss: 16996.8633\n",
            "Epoch [120/150], Loss: 11431.7471\n",
            "Epoch [130/150], Loss: 16000.9087\n",
            "Epoch [140/150], Loss: 16901.9644\n",
            "Epoch [150/150], Loss: 12213.5270\n",
            "Fold 5, RMSE: 47.53871154785156\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 58.6057243347168\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 69554.6768\n",
            "Epoch [20/100], Loss: 23270.9614\n",
            "Epoch [30/100], Loss: 15010.2712\n",
            "Epoch [40/100], Loss: 12976.2529\n",
            "Epoch [50/100], Loss: 15126.0508\n",
            "Epoch [60/100], Loss: 18458.4153\n",
            "Epoch [70/100], Loss: 13993.2944\n",
            "Epoch [80/100], Loss: 15623.2271\n",
            "Epoch [90/100], Loss: 14703.1187\n",
            "Epoch [100/100], Loss: 12773.3885\n",
            "Fold 1, RMSE: 47.78479766845703\n",
            "Epoch [10/100], Loss: 55526.5688\n",
            "Epoch [20/100], Loss: 14055.8743\n",
            "Epoch [30/100], Loss: 13723.1953\n",
            "Epoch [40/100], Loss: 14197.4900\n",
            "Epoch [50/100], Loss: 12552.6379\n",
            "Epoch [60/100], Loss: 11362.5012\n",
            "Epoch [70/100], Loss: 10331.6649\n",
            "Epoch [80/100], Loss: 10979.1445\n",
            "Epoch [90/100], Loss: 9537.0142\n",
            "Epoch [100/100], Loss: 11219.4937\n",
            "Fold 2, RMSE: 71.95760345458984\n",
            "Epoch [10/100], Loss: 46288.3184\n",
            "Epoch [20/100], Loss: 16807.6868\n",
            "Epoch [30/100], Loss: 12293.5312\n",
            "Epoch [40/100], Loss: 7827.2888\n",
            "Epoch [50/100], Loss: 10738.3054\n",
            "Epoch [60/100], Loss: 10310.5928\n",
            "Epoch [70/100], Loss: 11032.1963\n",
            "Epoch [80/100], Loss: 6747.0109\n",
            "Epoch [90/100], Loss: 9610.2903\n",
            "Epoch [100/100], Loss: 8170.4243\n",
            "Fold 3, RMSE: 92.15570831298828\n",
            "Epoch [10/100], Loss: 56502.5938\n",
            "Epoch [20/100], Loss: 15794.4214\n",
            "Epoch [30/100], Loss: 14667.9038\n",
            "Epoch [40/100], Loss: 21340.1074\n",
            "Epoch [50/100], Loss: 17187.5591\n",
            "Epoch [60/100], Loss: 13383.7610\n",
            "Epoch [70/100], Loss: 11581.3159\n",
            "Epoch [80/100], Loss: 15879.6011\n",
            "Epoch [90/100], Loss: 19528.0422\n",
            "Epoch [100/100], Loss: 10415.8713\n",
            "Fold 4, RMSE: 35.482418060302734\n",
            "Epoch [10/100], Loss: 49633.4307\n",
            "Epoch [20/100], Loss: 17584.4111\n",
            "Epoch [30/100], Loss: 12699.3313\n",
            "Epoch [40/100], Loss: 12981.1938\n",
            "Epoch [50/100], Loss: 10555.6405\n",
            "Epoch [60/100], Loss: 15696.8812\n",
            "Epoch [70/100], Loss: 15955.4038\n",
            "Epoch [80/100], Loss: 10978.2990\n",
            "Epoch [90/100], Loss: 13131.3110\n",
            "Epoch [100/100], Loss: 16370.1851\n",
            "Fold 5, RMSE: 48.0244140625\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 59.08098831176758\n",
            "Training with neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 59469.6562\n",
            "Epoch [20/150], Loss: 19147.6255\n",
            "Epoch [30/150], Loss: 16487.0571\n",
            "Epoch [40/150], Loss: 21712.3564\n",
            "Epoch [50/150], Loss: 14642.9229\n",
            "Epoch [60/150], Loss: 17291.8355\n",
            "Epoch [70/150], Loss: 16013.8894\n",
            "Epoch [80/150], Loss: 14487.9365\n",
            "Epoch [90/150], Loss: 12303.5601\n",
            "Epoch [100/150], Loss: 13807.4260\n",
            "Epoch [110/150], Loss: 18884.9197\n",
            "Epoch [120/150], Loss: 16315.8167\n",
            "Epoch [130/150], Loss: 10917.9152\n",
            "Epoch [140/150], Loss: 12296.9778\n",
            "Epoch [150/150], Loss: 13955.6543\n",
            "Fold 1, RMSE: 55.19583511352539\n",
            "Epoch [10/150], Loss: 47350.5186\n",
            "Epoch [20/150], Loss: 18540.4819\n",
            "Epoch [30/150], Loss: 17988.7466\n",
            "Epoch [40/150], Loss: 11485.8002\n",
            "Epoch [50/150], Loss: 22192.9519\n",
            "Epoch [60/150], Loss: 12160.9417\n",
            "Epoch [70/150], Loss: 11155.5339\n",
            "Epoch [80/150], Loss: 10606.4968\n",
            "Epoch [90/150], Loss: 9516.0214\n",
            "Epoch [100/150], Loss: 10766.3635\n",
            "Epoch [110/150], Loss: 10297.0483\n",
            "Epoch [120/150], Loss: 11555.5852\n",
            "Epoch [130/150], Loss: 11660.6274\n",
            "Epoch [140/150], Loss: 11570.6313\n",
            "Epoch [150/150], Loss: 10219.2974\n",
            "Fold 2, RMSE: 69.96271514892578\n",
            "Epoch [10/150], Loss: 57417.4727\n",
            "Epoch [20/150], Loss: 17655.2866\n",
            "Epoch [30/150], Loss: 9823.7124\n",
            "Epoch [40/150], Loss: 9037.4578\n",
            "Epoch [50/150], Loss: 11117.5537\n",
            "Epoch [60/150], Loss: 8784.3510\n",
            "Epoch [70/150], Loss: 6585.3762\n",
            "Epoch [80/150], Loss: 7263.9703\n",
            "Epoch [90/150], Loss: 7460.0619\n",
            "Epoch [100/150], Loss: 10330.6606\n",
            "Epoch [110/150], Loss: 8297.1606\n",
            "Epoch [120/150], Loss: 8119.4865\n",
            "Epoch [130/150], Loss: 8033.7026\n",
            "Epoch [140/150], Loss: 7469.3700\n",
            "Epoch [150/150], Loss: 8736.8521\n",
            "Fold 3, RMSE: 87.72210693359375\n",
            "Epoch [10/150], Loss: 81450.3496\n",
            "Epoch [20/150], Loss: 18634.0330\n",
            "Epoch [30/150], Loss: 16788.6516\n",
            "Epoch [40/150], Loss: 19472.7085\n",
            "Epoch [50/150], Loss: 15241.1355\n",
            "Epoch [60/150], Loss: 17499.9800\n",
            "Epoch [70/150], Loss: 16215.2349\n",
            "Epoch [80/150], Loss: 17088.3425\n",
            "Epoch [90/150], Loss: 13428.8184\n",
            "Epoch [100/150], Loss: 17563.2930\n",
            "Epoch [110/150], Loss: 13329.9768\n",
            "Epoch [120/150], Loss: 10210.4257\n",
            "Epoch [130/150], Loss: 9917.4622\n",
            "Epoch [140/150], Loss: 13142.6907\n",
            "Epoch [150/150], Loss: 12189.2175\n",
            "Fold 4, RMSE: 39.67031478881836\n",
            "Epoch [10/150], Loss: 109150.5859\n",
            "Epoch [20/150], Loss: 31343.3726\n",
            "Epoch [30/150], Loss: 17765.0513\n",
            "Epoch [40/150], Loss: 11419.8882\n",
            "Epoch [50/150], Loss: 21470.4092\n",
            "Epoch [60/150], Loss: 12646.4735\n",
            "Epoch [70/150], Loss: 13519.6917\n",
            "Epoch [80/150], Loss: 13410.7097\n",
            "Epoch [90/150], Loss: 11847.5066\n",
            "Epoch [100/150], Loss: 10957.1562\n",
            "Epoch [110/150], Loss: 13410.4238\n",
            "Epoch [120/150], Loss: 11479.7993\n",
            "Epoch [130/150], Loss: 14541.4900\n",
            "Epoch [140/150], Loss: 12741.2576\n",
            "Epoch [150/150], Loss: 10607.1125\n",
            "Fold 5, RMSE: 48.446571350097656\n",
            "Avg RMSE for neurons=128, dropout_rate=0.3, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 60.19950866699219\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 24265.2334\n",
            "Epoch [20/100], Loss: 15823.9460\n",
            "Epoch [30/100], Loss: 11455.4038\n",
            "Epoch [40/100], Loss: 9713.7188\n",
            "Epoch [50/100], Loss: 9863.7156\n",
            "Epoch [60/100], Loss: 13218.0264\n",
            "Epoch [70/100], Loss: 4539.3982\n",
            "Epoch [80/100], Loss: 9302.5137\n",
            "Epoch [90/100], Loss: 12918.0354\n",
            "Epoch [100/100], Loss: 5427.1226\n",
            "Fold 1, RMSE: 56.066978454589844\n",
            "Epoch [10/100], Loss: 14736.8962\n",
            "Epoch [20/100], Loss: 11328.2012\n",
            "Epoch [30/100], Loss: 12691.3604\n",
            "Epoch [40/100], Loss: 10809.1893\n",
            "Epoch [50/100], Loss: 6977.3732\n",
            "Epoch [60/100], Loss: 8303.3704\n",
            "Epoch [70/100], Loss: 6029.8339\n",
            "Epoch [80/100], Loss: 6210.1870\n",
            "Epoch [90/100], Loss: 14858.7097\n",
            "Epoch [100/100], Loss: 7882.7780\n",
            "Fold 2, RMSE: 59.601200103759766\n",
            "Epoch [10/100], Loss: 10002.6791\n",
            "Epoch [20/100], Loss: 12990.1387\n",
            "Epoch [30/100], Loss: 14357.7178\n",
            "Epoch [40/100], Loss: 12580.9688\n",
            "Epoch [50/100], Loss: 12076.9840\n",
            "Epoch [60/100], Loss: 12245.1472\n",
            "Epoch [70/100], Loss: 15491.7097\n",
            "Epoch [80/100], Loss: 10988.9559\n",
            "Epoch [90/100], Loss: 11939.2332\n",
            "Epoch [100/100], Loss: 13648.4827\n",
            "Fold 3, RMSE: 109.79403686523438\n",
            "Epoch [10/100], Loss: 28694.8855\n",
            "Epoch [20/100], Loss: 17515.0461\n",
            "Epoch [30/100], Loss: 24102.3962\n",
            "Epoch [40/100], Loss: 12416.9352\n",
            "Epoch [50/100], Loss: 18127.8286\n",
            "Epoch [60/100], Loss: 18915.2734\n",
            "Epoch [70/100], Loss: 15918.0107\n",
            "Epoch [80/100], Loss: 7703.6649\n",
            "Epoch [90/100], Loss: 14619.3391\n",
            "Epoch [100/100], Loss: 13138.3210\n",
            "Fold 4, RMSE: 46.829776763916016\n",
            "Epoch [10/100], Loss: 15743.1917\n",
            "Epoch [20/100], Loss: 12005.5708\n",
            "Epoch [30/100], Loss: 13313.5405\n",
            "Epoch [40/100], Loss: 10285.1272\n",
            "Epoch [50/100], Loss: 11509.3749\n",
            "Epoch [60/100], Loss: 8617.4244\n",
            "Epoch [70/100], Loss: 12587.4121\n",
            "Epoch [80/100], Loss: 12330.2348\n",
            "Epoch [90/100], Loss: 10102.5303\n",
            "Epoch [100/100], Loss: 8252.8710\n",
            "Fold 5, RMSE: 45.962738037109375\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=100: 63.650946044921874\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 13520.3396\n",
            "Epoch [20/150], Loss: 20140.5491\n",
            "Epoch [30/150], Loss: 11109.2743\n",
            "Epoch [40/150], Loss: 11326.4810\n",
            "Epoch [50/150], Loss: 8268.8314\n",
            "Epoch [60/150], Loss: 7373.3530\n",
            "Epoch [70/150], Loss: 11940.1382\n",
            "Epoch [80/150], Loss: 5744.6829\n",
            "Epoch [90/150], Loss: 4995.3932\n",
            "Epoch [100/150], Loss: 19482.2666\n",
            "Epoch [110/150], Loss: 11385.3300\n",
            "Epoch [120/150], Loss: 10182.4282\n",
            "Epoch [130/150], Loss: 15551.4880\n",
            "Epoch [140/150], Loss: 4658.2233\n",
            "Epoch [150/150], Loss: 9563.4935\n",
            "Fold 1, RMSE: 55.15806579589844\n",
            "Epoch [10/150], Loss: 9262.5524\n",
            "Epoch [20/150], Loss: 13682.7485\n",
            "Epoch [30/150], Loss: 15878.2603\n",
            "Epoch [40/150], Loss: 17184.1018\n",
            "Epoch [50/150], Loss: 10038.6005\n",
            "Epoch [60/150], Loss: 4295.4482\n",
            "Epoch [70/150], Loss: 7226.6382\n",
            "Epoch [80/150], Loss: 7383.3337\n",
            "Epoch [90/150], Loss: 9098.0519\n",
            "Epoch [100/150], Loss: 9419.6257\n",
            "Epoch [110/150], Loss: 7004.2164\n",
            "Epoch [120/150], Loss: 3181.7494\n",
            "Epoch [130/150], Loss: 4185.2800\n",
            "Epoch [140/150], Loss: 3642.8153\n",
            "Epoch [150/150], Loss: 6832.7316\n",
            "Fold 2, RMSE: 72.52247619628906\n",
            "Epoch [10/150], Loss: 13078.0935\n",
            "Epoch [20/150], Loss: 6922.0510\n",
            "Epoch [30/150], Loss: 7029.6860\n",
            "Epoch [40/150], Loss: 10674.8552\n",
            "Epoch [50/150], Loss: 11832.6371\n",
            "Epoch [60/150], Loss: 5453.4442\n",
            "Epoch [70/150], Loss: 7745.5220\n",
            "Epoch [80/150], Loss: 3559.6526\n",
            "Epoch [90/150], Loss: 4262.4747\n",
            "Epoch [100/150], Loss: 9993.6918\n",
            "Epoch [110/150], Loss: 12829.1155\n",
            "Epoch [120/150], Loss: 13418.8110\n",
            "Epoch [130/150], Loss: 6802.0437\n",
            "Epoch [140/150], Loss: 6413.9386\n",
            "Epoch [150/150], Loss: 9583.9808\n",
            "Fold 3, RMSE: 99.45462036132812\n",
            "Epoch [10/150], Loss: 28608.0120\n",
            "Epoch [20/150], Loss: 22183.6687\n",
            "Epoch [30/150], Loss: 15657.6191\n",
            "Epoch [40/150], Loss: 12670.0919\n",
            "Epoch [50/150], Loss: 12173.0645\n",
            "Epoch [60/150], Loss: 10730.1389\n",
            "Epoch [70/150], Loss: 9698.0374\n",
            "Epoch [80/150], Loss: 10590.0137\n",
            "Epoch [90/150], Loss: 5159.7874\n",
            "Epoch [100/150], Loss: 15682.5146\n",
            "Epoch [110/150], Loss: 6878.6744\n",
            "Epoch [120/150], Loss: 9519.8569\n",
            "Epoch [130/150], Loss: 9790.4792\n",
            "Epoch [140/150], Loss: 21331.0146\n",
            "Epoch [150/150], Loss: 10432.7783\n",
            "Fold 4, RMSE: 42.92302703857422\n",
            "Epoch [10/150], Loss: 17833.1782\n",
            "Epoch [20/150], Loss: 21151.6235\n",
            "Epoch [30/150], Loss: 9430.1482\n",
            "Epoch [40/150], Loss: 9246.3993\n",
            "Epoch [50/150], Loss: 6006.1689\n",
            "Epoch [60/150], Loss: 10905.1465\n",
            "Epoch [70/150], Loss: 8608.9788\n",
            "Epoch [80/150], Loss: 8590.8030\n",
            "Epoch [90/150], Loss: 8420.6981\n",
            "Epoch [100/150], Loss: 17925.2607\n",
            "Epoch [110/150], Loss: 6058.3687\n",
            "Epoch [120/150], Loss: 12383.6389\n",
            "Epoch [130/150], Loss: 6819.5988\n",
            "Epoch [140/150], Loss: 7825.6865\n",
            "Epoch [150/150], Loss: 4808.7366\n",
            "Fold 5, RMSE: 45.19316482543945\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=2, epochs=150: 63.05027084350586\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 26250.3735\n",
            "Epoch [20/100], Loss: 15010.4509\n",
            "Epoch [30/100], Loss: 11521.1128\n",
            "Epoch [40/100], Loss: 7658.9545\n",
            "Epoch [50/100], Loss: 11022.2363\n",
            "Epoch [60/100], Loss: 11041.4219\n",
            "Epoch [70/100], Loss: 12996.4236\n",
            "Epoch [80/100], Loss: 12146.4426\n",
            "Epoch [90/100], Loss: 6010.1734\n",
            "Epoch [100/100], Loss: 7520.3715\n",
            "Fold 1, RMSE: 63.16613006591797\n",
            "Epoch [10/100], Loss: 12686.1338\n",
            "Epoch [20/100], Loss: 11182.4368\n",
            "Epoch [30/100], Loss: 17088.4381\n",
            "Epoch [40/100], Loss: 11832.6736\n",
            "Epoch [50/100], Loss: 10123.3987\n",
            "Epoch [60/100], Loss: 11486.0632\n",
            "Epoch [70/100], Loss: 7723.9646\n",
            "Epoch [80/100], Loss: 14682.4492\n",
            "Epoch [90/100], Loss: 11047.0869\n",
            "Epoch [100/100], Loss: 16467.5920\n",
            "Fold 2, RMSE: 71.3235092163086\n",
            "Epoch [10/100], Loss: 8346.5093\n",
            "Epoch [20/100], Loss: 8933.8934\n",
            "Epoch [30/100], Loss: 6527.0669\n",
            "Epoch [40/100], Loss: 5304.1996\n",
            "Epoch [50/100], Loss: 7990.8213\n",
            "Epoch [60/100], Loss: 10730.6299\n",
            "Epoch [70/100], Loss: 6050.4124\n",
            "Epoch [80/100], Loss: 9519.3568\n",
            "Epoch [90/100], Loss: 7077.3428\n",
            "Epoch [100/100], Loss: 7903.1321\n",
            "Fold 3, RMSE: 108.6816177368164\n",
            "Epoch [10/100], Loss: 16007.6531\n",
            "Epoch [20/100], Loss: 14717.1575\n",
            "Epoch [30/100], Loss: 20957.7083\n",
            "Epoch [40/100], Loss: 14191.7432\n",
            "Epoch [50/100], Loss: 9008.8821\n",
            "Epoch [60/100], Loss: 6901.3679\n",
            "Epoch [70/100], Loss: 9483.9192\n",
            "Epoch [80/100], Loss: 14759.5674\n",
            "Epoch [90/100], Loss: 15097.0869\n",
            "Epoch [100/100], Loss: 17002.5895\n",
            "Fold 4, RMSE: 42.428646087646484\n",
            "Epoch [10/100], Loss: 16276.5540\n",
            "Epoch [20/100], Loss: 11875.3896\n",
            "Epoch [30/100], Loss: 9900.2290\n",
            "Epoch [40/100], Loss: 12073.6141\n",
            "Epoch [50/100], Loss: 4984.2527\n",
            "Epoch [60/100], Loss: 12347.1023\n",
            "Epoch [70/100], Loss: 8959.0439\n",
            "Epoch [80/100], Loss: 5858.6470\n",
            "Epoch [90/100], Loss: 5732.7805\n",
            "Epoch [100/100], Loss: 8128.7300\n",
            "Fold 5, RMSE: 45.70047378540039\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=100: 66.26007537841797\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 19658.2039\n",
            "Epoch [20/150], Loss: 11652.0062\n",
            "Epoch [30/150], Loss: 13085.6797\n",
            "Epoch [40/150], Loss: 10989.8712\n",
            "Epoch [50/150], Loss: 8427.8937\n",
            "Epoch [60/150], Loss: 13882.6178\n",
            "Epoch [70/150], Loss: 7731.3542\n",
            "Epoch [80/150], Loss: 7006.7454\n",
            "Epoch [90/150], Loss: 4938.9401\n",
            "Epoch [100/150], Loss: 7958.4808\n",
            "Epoch [110/150], Loss: 21512.5747\n",
            "Epoch [120/150], Loss: 6970.2325\n",
            "Epoch [130/150], Loss: 5770.2253\n",
            "Epoch [140/150], Loss: 8982.5113\n",
            "Epoch [150/150], Loss: 5603.6415\n",
            "Fold 1, RMSE: 58.8160285949707\n",
            "Epoch [10/150], Loss: 17072.1201\n",
            "Epoch [20/150], Loss: 14233.8318\n",
            "Epoch [30/150], Loss: 10364.9858\n",
            "Epoch [40/150], Loss: 9317.1612\n",
            "Epoch [50/150], Loss: 9205.6842\n",
            "Epoch [60/150], Loss: 5296.1899\n",
            "Epoch [70/150], Loss: 8666.2283\n",
            "Epoch [80/150], Loss: 5137.9150\n",
            "Epoch [90/150], Loss: 4342.8447\n",
            "Epoch [100/150], Loss: 4126.2842\n",
            "Epoch [110/150], Loss: 5061.0133\n",
            "Epoch [120/150], Loss: 7571.0486\n",
            "Epoch [130/150], Loss: 6258.3491\n",
            "Epoch [140/150], Loss: 8737.5607\n",
            "Epoch [150/150], Loss: 7054.7112\n",
            "Fold 2, RMSE: 75.3106689453125\n",
            "Epoch [10/150], Loss: 12146.5735\n",
            "Epoch [20/150], Loss: 8293.2626\n",
            "Epoch [30/150], Loss: 9327.5925\n",
            "Epoch [40/150], Loss: 7569.6388\n",
            "Epoch [50/150], Loss: 6544.0522\n",
            "Epoch [60/150], Loss: 6886.9991\n",
            "Epoch [70/150], Loss: 5029.1132\n",
            "Epoch [80/150], Loss: 5298.1989\n",
            "Epoch [90/150], Loss: 8982.9880\n",
            "Epoch [100/150], Loss: 8360.2021\n",
            "Epoch [110/150], Loss: 4319.4081\n",
            "Epoch [120/150], Loss: 4990.4926\n",
            "Epoch [130/150], Loss: 5912.3671\n",
            "Epoch [140/150], Loss: 3105.4977\n",
            "Epoch [150/150], Loss: 3944.8883\n",
            "Fold 3, RMSE: 104.33011627197266\n",
            "Epoch [10/150], Loss: 15299.6458\n",
            "Epoch [20/150], Loss: 11481.7412\n",
            "Epoch [30/150], Loss: 12559.0151\n",
            "Epoch [40/150], Loss: 9632.2622\n",
            "Epoch [50/150], Loss: 10780.1309\n",
            "Epoch [60/150], Loss: 6027.2297\n",
            "Epoch [70/150], Loss: 9040.3115\n",
            "Epoch [80/150], Loss: 5741.5639\n",
            "Epoch [90/150], Loss: 7472.0081\n",
            "Epoch [100/150], Loss: 11825.1609\n",
            "Epoch [110/150], Loss: 8534.0042\n",
            "Epoch [120/150], Loss: 12266.8721\n",
            "Epoch [130/150], Loss: 6939.5277\n",
            "Epoch [140/150], Loss: 4534.7421\n",
            "Epoch [150/150], Loss: 6559.6119\n",
            "Fold 4, RMSE: 39.085750579833984\n",
            "Epoch [10/150], Loss: 20982.6782\n",
            "Epoch [20/150], Loss: 14705.3650\n",
            "Epoch [30/150], Loss: 8381.9153\n",
            "Epoch [40/150], Loss: 11563.6934\n",
            "Epoch [50/150], Loss: 6931.9237\n",
            "Epoch [60/150], Loss: 5057.8878\n",
            "Epoch [70/150], Loss: 7480.2651\n",
            "Epoch [80/150], Loss: 7367.3596\n",
            "Epoch [90/150], Loss: 18113.9678\n",
            "Epoch [100/150], Loss: 6221.4092\n",
            "Epoch [110/150], Loss: 7242.6771\n",
            "Epoch [120/150], Loss: 4734.5929\n",
            "Epoch [130/150], Loss: 6712.6868\n",
            "Epoch [140/150], Loss: 5245.2288\n",
            "Epoch [150/150], Loss: 15760.0443\n",
            "Fold 5, RMSE: 46.82850646972656\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0, num_layers=3, epochs=150: 64.87421417236328\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 28457.7847\n",
            "Epoch [20/100], Loss: 16455.0435\n",
            "Epoch [30/100], Loss: 19353.0938\n",
            "Epoch [40/100], Loss: 21176.5718\n",
            "Epoch [50/100], Loss: 17084.2339\n",
            "Epoch [60/100], Loss: 17575.4785\n",
            "Epoch [70/100], Loss: 16751.8931\n",
            "Epoch [80/100], Loss: 19118.3779\n",
            "Epoch [90/100], Loss: 16089.9426\n",
            "Epoch [100/100], Loss: 17953.3706\n",
            "Fold 1, RMSE: 67.5233154296875\n",
            "Epoch [10/100], Loss: 13092.5811\n",
            "Epoch [20/100], Loss: 9009.9125\n",
            "Epoch [30/100], Loss: 9913.5349\n",
            "Epoch [40/100], Loss: 10923.9436\n",
            "Epoch [50/100], Loss: 7989.7667\n",
            "Epoch [60/100], Loss: 10897.1150\n",
            "Epoch [70/100], Loss: 4303.0116\n",
            "Epoch [80/100], Loss: 5489.5570\n",
            "Epoch [90/100], Loss: 10125.5057\n",
            "Epoch [100/100], Loss: 16847.3744\n",
            "Fold 2, RMSE: 71.19659423828125\n",
            "Epoch [10/100], Loss: 14173.7773\n",
            "Epoch [20/100], Loss: 9787.7224\n",
            "Epoch [30/100], Loss: 8437.5571\n",
            "Epoch [40/100], Loss: 12723.2153\n",
            "Epoch [50/100], Loss: 4878.7344\n",
            "Epoch [60/100], Loss: 4835.3260\n",
            "Epoch [70/100], Loss: 6847.3097\n",
            "Epoch [80/100], Loss: 8875.7210\n",
            "Epoch [90/100], Loss: 7757.8033\n",
            "Epoch [100/100], Loss: 5138.1617\n",
            "Fold 3, RMSE: 101.3475112915039\n",
            "Epoch [10/100], Loss: 24146.4663\n",
            "Epoch [20/100], Loss: 15397.7683\n",
            "Epoch [30/100], Loss: 15735.5195\n",
            "Epoch [40/100], Loss: 16337.8369\n",
            "Epoch [50/100], Loss: 11101.8250\n",
            "Epoch [60/100], Loss: 9928.0030\n",
            "Epoch [70/100], Loss: 9452.9458\n",
            "Epoch [80/100], Loss: 9206.3538\n",
            "Epoch [90/100], Loss: 10792.4646\n",
            "Epoch [100/100], Loss: 14086.6270\n",
            "Fold 4, RMSE: 52.31028366088867\n",
            "Epoch [10/100], Loss: 23473.4497\n",
            "Epoch [20/100], Loss: 9662.1233\n",
            "Epoch [30/100], Loss: 13723.8445\n",
            "Epoch [40/100], Loss: 12760.1130\n",
            "Epoch [50/100], Loss: 10049.4192\n",
            "Epoch [60/100], Loss: 9529.5854\n",
            "Epoch [70/100], Loss: 14973.2917\n",
            "Epoch [80/100], Loss: 7590.3521\n",
            "Epoch [90/100], Loss: 14792.9697\n",
            "Epoch [100/100], Loss: 7479.4141\n",
            "Fold 5, RMSE: 44.490234375\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=100: 67.37358779907227\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 19393.9351\n",
            "Epoch [20/150], Loss: 14951.7418\n",
            "Epoch [30/150], Loss: 10880.4587\n",
            "Epoch [40/150], Loss: 7601.3679\n",
            "Epoch [50/150], Loss: 5582.5355\n",
            "Epoch [60/150], Loss: 12511.7075\n",
            "Epoch [70/150], Loss: 9025.5664\n",
            "Epoch [80/150], Loss: 7946.8043\n",
            "Epoch [90/150], Loss: 9785.6016\n",
            "Epoch [100/150], Loss: 6699.9744\n",
            "Epoch [110/150], Loss: 11188.0312\n",
            "Epoch [120/150], Loss: 14991.9446\n",
            "Epoch [130/150], Loss: 7946.9921\n",
            "Epoch [140/150], Loss: 11661.8107\n",
            "Epoch [150/150], Loss: 14667.5222\n",
            "Fold 1, RMSE: 61.04041290283203\n",
            "Epoch [10/150], Loss: 19743.0825\n",
            "Epoch [20/150], Loss: 11403.7454\n",
            "Epoch [30/150], Loss: 8946.3151\n",
            "Epoch [40/150], Loss: 11352.5008\n",
            "Epoch [50/150], Loss: 11194.0020\n",
            "Epoch [60/150], Loss: 21096.2830\n",
            "Epoch [70/150], Loss: 14970.7644\n",
            "Epoch [80/150], Loss: 5830.6921\n",
            "Epoch [90/150], Loss: 18858.6348\n",
            "Epoch [100/150], Loss: 7090.7479\n",
            "Epoch [110/150], Loss: 11475.0654\n",
            "Epoch [120/150], Loss: 6984.0710\n",
            "Epoch [130/150], Loss: 6848.3525\n",
            "Epoch [140/150], Loss: 12749.6360\n",
            "Epoch [150/150], Loss: 8250.1508\n",
            "Fold 2, RMSE: 73.47425842285156\n",
            "Epoch [10/150], Loss: 14449.2100\n",
            "Epoch [20/150], Loss: 8511.0789\n",
            "Epoch [30/150], Loss: 5930.9976\n",
            "Epoch [40/150], Loss: 5427.9978\n",
            "Epoch [50/150], Loss: 6200.5301\n",
            "Epoch [60/150], Loss: 5290.8882\n",
            "Epoch [70/150], Loss: 6070.1426\n",
            "Epoch [80/150], Loss: 6642.3094\n",
            "Epoch [90/150], Loss: 11157.8772\n",
            "Epoch [100/150], Loss: 11310.5669\n",
            "Epoch [110/150], Loss: 7866.4448\n",
            "Epoch [120/150], Loss: 7559.9503\n",
            "Epoch [130/150], Loss: 5218.1594\n",
            "Epoch [140/150], Loss: 10647.5488\n",
            "Epoch [150/150], Loss: 9909.7437\n",
            "Fold 3, RMSE: 109.38760375976562\n",
            "Epoch [10/150], Loss: 19898.5010\n",
            "Epoch [20/150], Loss: 13878.3905\n",
            "Epoch [30/150], Loss: 16156.5449\n",
            "Epoch [40/150], Loss: 14969.2974\n",
            "Epoch [50/150], Loss: 9185.9116\n",
            "Epoch [60/150], Loss: 7737.3234\n",
            "Epoch [70/150], Loss: 8439.0332\n",
            "Epoch [80/150], Loss: 8756.8972\n",
            "Epoch [90/150], Loss: 14261.9702\n",
            "Epoch [100/150], Loss: 9177.8528\n",
            "Epoch [110/150], Loss: 11670.3599\n",
            "Epoch [120/150], Loss: 11081.0408\n",
            "Epoch [130/150], Loss: 13736.5569\n",
            "Epoch [140/150], Loss: 11074.1485\n",
            "Epoch [150/150], Loss: 8200.7338\n",
            "Fold 4, RMSE: 40.7767448425293\n",
            "Epoch [10/150], Loss: 13956.4266\n",
            "Epoch [20/150], Loss: 24592.6118\n",
            "Epoch [30/150], Loss: 18221.4705\n",
            "Epoch [40/150], Loss: 16712.2506\n",
            "Epoch [50/150], Loss: 17106.3816\n",
            "Epoch [60/150], Loss: 18904.4634\n",
            "Epoch [70/150], Loss: 20418.1924\n",
            "Epoch [80/150], Loss: 21263.3955\n",
            "Epoch [90/150], Loss: 23902.8848\n",
            "Epoch [100/150], Loss: 23263.8062\n",
            "Epoch [110/150], Loss: 20813.7847\n",
            "Epoch [120/150], Loss: 24084.5332\n",
            "Epoch [130/150], Loss: 18644.9717\n",
            "Epoch [140/150], Loss: 17020.1066\n",
            "Epoch [150/150], Loss: 18882.8818\n",
            "Fold 5, RMSE: 57.803470611572266\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=2, epochs=150: 68.49649810791016\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 13506.9757\n",
            "Epoch [20/100], Loss: 12650.3027\n",
            "Epoch [30/100], Loss: 10421.9707\n",
            "Epoch [40/100], Loss: 11361.6112\n",
            "Epoch [50/100], Loss: 10991.1945\n",
            "Epoch [60/100], Loss: 4805.6788\n",
            "Epoch [70/100], Loss: 8837.5057\n",
            "Epoch [80/100], Loss: 5400.6753\n",
            "Epoch [90/100], Loss: 5752.4652\n",
            "Epoch [100/100], Loss: 12807.0449\n",
            "Fold 1, RMSE: 65.98684692382812\n",
            "Epoch [10/100], Loss: 22832.9932\n",
            "Epoch [20/100], Loss: 9016.4439\n",
            "Epoch [30/100], Loss: 9105.3450\n",
            "Epoch [40/100], Loss: 20014.9214\n",
            "Epoch [50/100], Loss: 6302.1550\n",
            "Epoch [60/100], Loss: 6332.0485\n",
            "Epoch [70/100], Loss: 17101.6763\n",
            "Epoch [80/100], Loss: 5765.9987\n",
            "Epoch [90/100], Loss: 5633.3199\n",
            "Epoch [100/100], Loss: 8162.2727\n",
            "Fold 2, RMSE: 77.16403198242188\n",
            "Epoch [10/100], Loss: 11506.5034\n",
            "Epoch [20/100], Loss: 7715.3729\n",
            "Epoch [30/100], Loss: 9452.6683\n",
            "Epoch [40/100], Loss: 8058.4367\n",
            "Epoch [50/100], Loss: 9149.0062\n",
            "Epoch [60/100], Loss: 17417.4272\n",
            "Epoch [70/100], Loss: 7125.8519\n",
            "Epoch [80/100], Loss: 5193.3414\n",
            "Epoch [90/100], Loss: 4111.5263\n",
            "Epoch [100/100], Loss: 9447.8865\n",
            "Fold 3, RMSE: 103.32160186767578\n",
            "Epoch [10/100], Loss: 20280.0393\n",
            "Epoch [20/100], Loss: 14890.5789\n",
            "Epoch [30/100], Loss: 16331.4338\n",
            "Epoch [40/100], Loss: 7343.7946\n",
            "Epoch [50/100], Loss: 8026.8319\n",
            "Epoch [60/100], Loss: 8062.3519\n",
            "Epoch [70/100], Loss: 8417.4414\n",
            "Epoch [80/100], Loss: 12861.8367\n",
            "Epoch [90/100], Loss: 8259.4988\n",
            "Epoch [100/100], Loss: 6114.0463\n",
            "Fold 4, RMSE: 44.84552764892578\n",
            "Epoch [10/100], Loss: 25606.9709\n",
            "Epoch [20/100], Loss: 19116.7095\n",
            "Epoch [30/100], Loss: 10994.2610\n",
            "Epoch [40/100], Loss: 8521.9766\n",
            "Epoch [50/100], Loss: 12379.3730\n",
            "Epoch [60/100], Loss: 14161.9497\n",
            "Epoch [70/100], Loss: 8997.9568\n",
            "Epoch [80/100], Loss: 5125.0072\n",
            "Epoch [90/100], Loss: 4271.0996\n",
            "Epoch [100/100], Loss: 6144.9076\n",
            "Fold 5, RMSE: 44.300193786621094\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=100: 67.12364044189454\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 12786.4761\n",
            "Epoch [20/150], Loss: 10514.1206\n",
            "Epoch [30/150], Loss: 10421.2034\n",
            "Epoch [40/150], Loss: 7033.4365\n",
            "Epoch [50/150], Loss: 3615.6190\n",
            "Epoch [60/150], Loss: 4197.4447\n",
            "Epoch [70/150], Loss: 7530.3785\n",
            "Epoch [80/150], Loss: 4965.7425\n",
            "Epoch [90/150], Loss: 6817.4612\n",
            "Epoch [100/150], Loss: 5250.6416\n",
            "Epoch [110/150], Loss: 6902.8484\n",
            "Epoch [120/150], Loss: 11959.4968\n",
            "Epoch [130/150], Loss: 13065.1702\n",
            "Epoch [140/150], Loss: 8138.6956\n",
            "Epoch [150/150], Loss: 6059.7885\n",
            "Fold 1, RMSE: 58.84241485595703\n",
            "Epoch [10/150], Loss: 14151.9165\n",
            "Epoch [20/150], Loss: 13903.9180\n",
            "Epoch [30/150], Loss: 13400.3271\n",
            "Epoch [40/150], Loss: 12774.9976\n",
            "Epoch [50/150], Loss: 12550.6858\n",
            "Epoch [60/150], Loss: 11130.7444\n",
            "Epoch [70/150], Loss: 20885.2446\n",
            "Epoch [80/150], Loss: 9522.3080\n",
            "Epoch [90/150], Loss: 9445.7443\n",
            "Epoch [100/150], Loss: 9260.9277\n",
            "Epoch [110/150], Loss: 6190.8497\n",
            "Epoch [120/150], Loss: 21317.3040\n",
            "Epoch [130/150], Loss: 13501.3462\n",
            "Epoch [140/150], Loss: 8866.7073\n",
            "Epoch [150/150], Loss: 9723.0496\n",
            "Fold 2, RMSE: 77.15068817138672\n",
            "Epoch [10/150], Loss: 11058.3987\n",
            "Epoch [20/150], Loss: 9869.6124\n",
            "Epoch [30/150], Loss: 10046.0183\n",
            "Epoch [40/150], Loss: 8584.1224\n",
            "Epoch [50/150], Loss: 5762.5316\n",
            "Epoch [60/150], Loss: 4531.1878\n",
            "Epoch [70/150], Loss: 2706.1358\n",
            "Epoch [80/150], Loss: 6180.9620\n",
            "Epoch [90/150], Loss: 4967.4265\n",
            "Epoch [100/150], Loss: 4151.7772\n",
            "Epoch [110/150], Loss: 3903.0403\n",
            "Epoch [120/150], Loss: 2922.3589\n",
            "Epoch [130/150], Loss: 4616.2691\n",
            "Epoch [140/150], Loss: 1686.0617\n",
            "Epoch [150/150], Loss: 4915.0617\n",
            "Fold 3, RMSE: 96.86206817626953\n",
            "Epoch [10/150], Loss: 18520.5669\n",
            "Epoch [20/150], Loss: 22761.2412\n",
            "Epoch [30/150], Loss: 11572.2793\n",
            "Epoch [40/150], Loss: 11430.7103\n",
            "Epoch [50/150], Loss: 7740.4795\n",
            "Epoch [60/150], Loss: 6744.6321\n",
            "Epoch [70/150], Loss: 11415.7354\n",
            "Epoch [80/150], Loss: 5175.3525\n",
            "Epoch [90/150], Loss: 5717.3694\n",
            "Epoch [100/150], Loss: 3801.0941\n",
            "Epoch [110/150], Loss: 4685.6452\n",
            "Epoch [120/150], Loss: 4972.1804\n",
            "Epoch [130/150], Loss: 10850.8473\n",
            "Epoch [140/150], Loss: 5438.4419\n",
            "Epoch [150/150], Loss: 4783.2860\n",
            "Fold 4, RMSE: 39.59980773925781\n",
            "Epoch [10/150], Loss: 19445.9688\n",
            "Epoch [20/150], Loss: 12657.5757\n",
            "Epoch [30/150], Loss: 10493.5215\n",
            "Epoch [40/150], Loss: 12563.6692\n",
            "Epoch [50/150], Loss: 14207.0190\n",
            "Epoch [60/150], Loss: 12956.5752\n",
            "Epoch [70/150], Loss: 7494.5532\n",
            "Epoch [80/150], Loss: 8632.9656\n",
            "Epoch [90/150], Loss: 7052.3218\n",
            "Epoch [100/150], Loss: 9740.6515\n",
            "Epoch [110/150], Loss: 12286.0386\n",
            "Epoch [120/150], Loss: 16120.2991\n",
            "Epoch [130/150], Loss: 9170.3320\n",
            "Epoch [140/150], Loss: 3508.6918\n",
            "Epoch [150/150], Loss: 5369.0685\n",
            "Fold 5, RMSE: 45.875083923339844\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=0.0001, num_layers=3, epochs=150: 63.666012573242185\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 15388.2527\n",
            "Epoch [20/100], Loss: 12666.6693\n",
            "Epoch [30/100], Loss: 9830.8120\n",
            "Epoch [40/100], Loss: 7904.8065\n",
            "Epoch [50/100], Loss: 8216.4071\n",
            "Epoch [60/100], Loss: 4874.2480\n",
            "Epoch [70/100], Loss: 6916.4048\n",
            "Epoch [80/100], Loss: 11616.2738\n",
            "Epoch [90/100], Loss: 7743.0480\n",
            "Epoch [100/100], Loss: 18266.5961\n",
            "Fold 1, RMSE: 57.59734344482422\n",
            "Epoch [10/100], Loss: 15147.2732\n",
            "Epoch [20/100], Loss: 8220.8140\n",
            "Epoch [30/100], Loss: 9969.7413\n",
            "Epoch [40/100], Loss: 10391.2902\n",
            "Epoch [50/100], Loss: 8919.2469\n",
            "Epoch [60/100], Loss: 15772.8340\n",
            "Epoch [70/100], Loss: 11345.4680\n",
            "Epoch [80/100], Loss: 14164.2710\n",
            "Epoch [90/100], Loss: 9170.4985\n",
            "Epoch [100/100], Loss: 10534.1503\n",
            "Fold 2, RMSE: 76.14532470703125\n",
            "Epoch [10/100], Loss: 11807.1008\n",
            "Epoch [20/100], Loss: 10544.8066\n",
            "Epoch [30/100], Loss: 8594.1493\n",
            "Epoch [40/100], Loss: 7633.1242\n",
            "Epoch [50/100], Loss: 5474.0785\n",
            "Epoch [60/100], Loss: 3847.4650\n",
            "Epoch [70/100], Loss: 6731.4702\n",
            "Epoch [80/100], Loss: 4533.8722\n",
            "Epoch [90/100], Loss: 4218.1266\n",
            "Epoch [100/100], Loss: 4528.6987\n",
            "Fold 3, RMSE: 94.68315124511719\n",
            "Epoch [10/100], Loss: 18824.9717\n",
            "Epoch [20/100], Loss: 15369.8516\n",
            "Epoch [30/100], Loss: 13624.8748\n",
            "Epoch [40/100], Loss: 15301.5415\n",
            "Epoch [50/100], Loss: 7997.7529\n",
            "Epoch [60/100], Loss: 7194.2235\n",
            "Epoch [70/100], Loss: 12729.2753\n",
            "Epoch [80/100], Loss: 9732.5907\n",
            "Epoch [90/100], Loss: 5683.7445\n",
            "Epoch [100/100], Loss: 7271.8806\n",
            "Fold 4, RMSE: 45.99760818481445\n",
            "Epoch [10/100], Loss: 22527.3716\n",
            "Epoch [20/100], Loss: 16839.8572\n",
            "Epoch [30/100], Loss: 15300.8174\n",
            "Epoch [40/100], Loss: 9841.3616\n",
            "Epoch [50/100], Loss: 10700.2993\n",
            "Epoch [60/100], Loss: 8646.1824\n",
            "Epoch [70/100], Loss: 6260.0630\n",
            "Epoch [80/100], Loss: 7017.1848\n",
            "Epoch [90/100], Loss: 7853.4534\n",
            "Epoch [100/100], Loss: 7772.9641\n",
            "Fold 5, RMSE: 44.70304870605469\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=100: 63.82529525756836\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 23100.2959\n",
            "Epoch [20/150], Loss: 21553.6350\n",
            "Epoch [30/150], Loss: 10385.4518\n",
            "Epoch [40/150], Loss: 15178.5576\n",
            "Epoch [50/150], Loss: 10360.2498\n",
            "Epoch [60/150], Loss: 8931.2148\n",
            "Epoch [70/150], Loss: 14228.9746\n",
            "Epoch [80/150], Loss: 6736.9618\n",
            "Epoch [90/150], Loss: 4946.7394\n",
            "Epoch [100/150], Loss: 4322.6951\n",
            "Epoch [110/150], Loss: 8848.5276\n",
            "Epoch [120/150], Loss: 7469.6622\n",
            "Epoch [130/150], Loss: 6507.4491\n",
            "Epoch [140/150], Loss: 7780.8407\n",
            "Epoch [150/150], Loss: 5719.2845\n",
            "Fold 1, RMSE: 58.83224868774414\n",
            "Epoch [10/150], Loss: 14107.0303\n",
            "Epoch [20/150], Loss: 9155.7087\n",
            "Epoch [30/150], Loss: 24121.4861\n",
            "Epoch [40/150], Loss: 19618.9949\n",
            "Epoch [50/150], Loss: 8200.0725\n",
            "Epoch [60/150], Loss: 6333.6755\n",
            "Epoch [70/150], Loss: 5802.6204\n",
            "Epoch [80/150], Loss: 9115.6509\n",
            "Epoch [90/150], Loss: 16227.4246\n",
            "Epoch [100/150], Loss: 13880.5930\n",
            "Epoch [110/150], Loss: 12412.7498\n",
            "Epoch [120/150], Loss: 11396.9722\n",
            "Epoch [130/150], Loss: 12262.8611\n",
            "Epoch [140/150], Loss: 13159.4614\n",
            "Epoch [150/150], Loss: 11115.1542\n",
            "Fold 2, RMSE: 73.7119140625\n",
            "Epoch [10/150], Loss: 7890.3553\n",
            "Epoch [20/150], Loss: 10130.6489\n",
            "Epoch [30/150], Loss: 9477.7305\n",
            "Epoch [40/150], Loss: 5196.3553\n",
            "Epoch [50/150], Loss: 5322.6179\n",
            "Epoch [60/150], Loss: 5903.7546\n",
            "Epoch [70/150], Loss: 8128.0859\n",
            "Epoch [80/150], Loss: 4892.5831\n",
            "Epoch [90/150], Loss: 4834.0845\n",
            "Epoch [100/150], Loss: 3133.3393\n",
            "Epoch [110/150], Loss: 7087.8472\n",
            "Epoch [120/150], Loss: 5644.5067\n",
            "Epoch [130/150], Loss: 2603.4439\n",
            "Epoch [140/150], Loss: 4295.6360\n",
            "Epoch [150/150], Loss: 5979.2582\n",
            "Fold 3, RMSE: 94.20909881591797\n",
            "Epoch [10/150], Loss: 21194.5508\n",
            "Epoch [20/150], Loss: 21447.9238\n",
            "Epoch [30/150], Loss: 12954.0159\n",
            "Epoch [40/150], Loss: 19109.0952\n",
            "Epoch [50/150], Loss: 14826.6707\n",
            "Epoch [60/150], Loss: 10379.1885\n",
            "Epoch [70/150], Loss: 8576.9146\n",
            "Epoch [80/150], Loss: 16027.3058\n",
            "Epoch [90/150], Loss: 13773.3870\n",
            "Epoch [100/150], Loss: 10368.0833\n",
            "Epoch [110/150], Loss: 15804.2538\n",
            "Epoch [120/150], Loss: 14186.9808\n",
            "Epoch [130/150], Loss: 11423.0464\n",
            "Epoch [140/150], Loss: 7916.3279\n",
            "Epoch [150/150], Loss: 6343.9159\n",
            "Fold 4, RMSE: 47.331993103027344\n",
            "Epoch [10/150], Loss: 17844.9480\n",
            "Epoch [20/150], Loss: 16100.8816\n",
            "Epoch [30/150], Loss: 15152.5333\n",
            "Epoch [40/150], Loss: 16082.8022\n",
            "Epoch [50/150], Loss: 14298.9548\n",
            "Epoch [60/150], Loss: 11667.2373\n",
            "Epoch [70/150], Loss: 17353.7559\n",
            "Epoch [80/150], Loss: 15839.2014\n",
            "Epoch [90/150], Loss: 11773.9773\n",
            "Epoch [100/150], Loss: 11736.2908\n",
            "Epoch [110/150], Loss: 8800.7278\n",
            "Epoch [120/150], Loss: 9515.1033\n",
            "Epoch [130/150], Loss: 15052.4954\n",
            "Epoch [140/150], Loss: 12211.6406\n",
            "Epoch [150/150], Loss: 11375.0911\n",
            "Fold 5, RMSE: 44.0382080078125\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=2, epochs=150: 63.62469253540039\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 16865.2214\n",
            "Epoch [20/100], Loss: 11515.6633\n",
            "Epoch [30/100], Loss: 8013.5820\n",
            "Epoch [40/100], Loss: 9450.1013\n",
            "Epoch [50/100], Loss: 6681.0032\n",
            "Epoch [60/100], Loss: 10200.7714\n",
            "Epoch [70/100], Loss: 9125.8262\n",
            "Epoch [80/100], Loss: 9983.4470\n",
            "Epoch [90/100], Loss: 5777.9292\n",
            "Epoch [100/100], Loss: 7969.0195\n",
            "Fold 1, RMSE: 58.951995849609375\n",
            "Epoch [10/100], Loss: 19166.9871\n",
            "Epoch [20/100], Loss: 10266.9100\n",
            "Epoch [30/100], Loss: 11939.1960\n",
            "Epoch [40/100], Loss: 6676.6619\n",
            "Epoch [50/100], Loss: 8987.5685\n",
            "Epoch [60/100], Loss: 4682.2703\n",
            "Epoch [70/100], Loss: 6220.1481\n",
            "Epoch [80/100], Loss: 6290.4108\n",
            "Epoch [90/100], Loss: 4491.4650\n",
            "Epoch [100/100], Loss: 6925.0065\n",
            "Fold 2, RMSE: 70.93635559082031\n",
            "Epoch [10/100], Loss: 14558.9192\n",
            "Epoch [20/100], Loss: 7010.4015\n",
            "Epoch [30/100], Loss: 9954.7324\n",
            "Epoch [40/100], Loss: 8394.8354\n",
            "Epoch [50/100], Loss: 7907.9325\n",
            "Epoch [60/100], Loss: 8693.9951\n",
            "Epoch [70/100], Loss: 11179.0430\n",
            "Epoch [80/100], Loss: 11262.4666\n",
            "Epoch [90/100], Loss: 9459.5071\n",
            "Epoch [100/100], Loss: 10938.6541\n",
            "Fold 3, RMSE: 93.91104125976562\n",
            "Epoch [10/100], Loss: 17986.9590\n",
            "Epoch [20/100], Loss: 14923.2329\n",
            "Epoch [30/100], Loss: 6173.1049\n",
            "Epoch [40/100], Loss: 10343.0634\n",
            "Epoch [50/100], Loss: 11779.8391\n",
            "Epoch [60/100], Loss: 13395.5093\n",
            "Epoch [70/100], Loss: 10622.6726\n",
            "Epoch [80/100], Loss: 11187.4443\n",
            "Epoch [90/100], Loss: 7111.3151\n",
            "Epoch [100/100], Loss: 9957.1886\n",
            "Fold 4, RMSE: 43.03270721435547\n",
            "Epoch [10/100], Loss: 16044.4067\n",
            "Epoch [20/100], Loss: 12620.3484\n",
            "Epoch [30/100], Loss: 6582.4197\n",
            "Epoch [40/100], Loss: 11632.4609\n",
            "Epoch [50/100], Loss: 16131.5266\n",
            "Epoch [60/100], Loss: 7025.6442\n",
            "Epoch [70/100], Loss: 11485.5286\n",
            "Epoch [80/100], Loss: 3281.1218\n",
            "Epoch [90/100], Loss: 10268.3901\n",
            "Epoch [100/100], Loss: 9471.1565\n",
            "Fold 5, RMSE: 44.46315002441406\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=100: 62.25904998779297\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15320.1082\n",
            "Epoch [20/150], Loss: 15789.6990\n",
            "Epoch [30/150], Loss: 12545.5880\n",
            "Epoch [40/150], Loss: 10006.0386\n",
            "Epoch [50/150], Loss: 8213.7728\n",
            "Epoch [60/150], Loss: 9173.0100\n",
            "Epoch [70/150], Loss: 7286.6694\n",
            "Epoch [80/150], Loss: 9314.2717\n",
            "Epoch [90/150], Loss: 7378.3423\n",
            "Epoch [100/150], Loss: 9354.5955\n",
            "Epoch [110/150], Loss: 6750.2040\n",
            "Epoch [120/150], Loss: 7683.5587\n",
            "Epoch [130/150], Loss: 6990.4210\n",
            "Epoch [140/150], Loss: 2552.0379\n",
            "Epoch [150/150], Loss: 4021.4233\n",
            "Fold 1, RMSE: 55.68577575683594\n",
            "Epoch [10/150], Loss: 14653.1426\n",
            "Epoch [20/150], Loss: 11015.7233\n",
            "Epoch [30/150], Loss: 16626.8875\n",
            "Epoch [40/150], Loss: 11589.5987\n",
            "Epoch [50/150], Loss: 8182.5488\n",
            "Epoch [60/150], Loss: 13243.6345\n",
            "Epoch [70/150], Loss: 11356.7090\n",
            "Epoch [80/150], Loss: 10020.4253\n",
            "Epoch [90/150], Loss: 15952.2037\n",
            "Epoch [100/150], Loss: 7614.6339\n",
            "Epoch [110/150], Loss: 7978.6731\n",
            "Epoch [120/150], Loss: 13668.5234\n",
            "Epoch [130/150], Loss: 13633.4097\n",
            "Epoch [140/150], Loss: 20017.1100\n",
            "Epoch [150/150], Loss: 20854.2327\n",
            "Fold 2, RMSE: 74.44631958007812\n",
            "Epoch [10/150], Loss: 9539.4893\n",
            "Epoch [20/150], Loss: 14029.4993\n",
            "Epoch [30/150], Loss: 7874.4053\n",
            "Epoch [40/150], Loss: 4929.5496\n",
            "Epoch [50/150], Loss: 3740.4791\n",
            "Epoch [60/150], Loss: 9260.5935\n",
            "Epoch [70/150], Loss: 4045.5332\n",
            "Epoch [80/150], Loss: 6650.6113\n",
            "Epoch [90/150], Loss: 7172.8191\n",
            "Epoch [100/150], Loss: 3476.5502\n",
            "Epoch [110/150], Loss: 4657.8322\n",
            "Epoch [120/150], Loss: 5042.8019\n",
            "Epoch [130/150], Loss: 3965.8452\n",
            "Epoch [140/150], Loss: 3923.5391\n",
            "Epoch [150/150], Loss: 3864.2015\n",
            "Fold 3, RMSE: 98.79725646972656\n",
            "Epoch [10/150], Loss: 20899.9033\n",
            "Epoch [20/150], Loss: 17381.5579\n",
            "Epoch [30/150], Loss: 10640.9187\n",
            "Epoch [40/150], Loss: 10048.1793\n",
            "Epoch [50/150], Loss: 12261.5847\n",
            "Epoch [60/150], Loss: 7809.6602\n",
            "Epoch [70/150], Loss: 10340.0684\n",
            "Epoch [80/150], Loss: 8627.6809\n",
            "Epoch [90/150], Loss: 6639.1381\n",
            "Epoch [100/150], Loss: 6248.3340\n",
            "Epoch [110/150], Loss: 6503.6698\n",
            "Epoch [120/150], Loss: 5033.9740\n",
            "Epoch [130/150], Loss: 5245.6370\n",
            "Epoch [140/150], Loss: 5525.1592\n",
            "Epoch [150/150], Loss: 7694.0826\n",
            "Fold 4, RMSE: 47.08979797363281\n",
            "Epoch [10/150], Loss: 14876.2847\n",
            "Epoch [20/150], Loss: 12047.5615\n",
            "Epoch [30/150], Loss: 15955.4963\n",
            "Epoch [40/150], Loss: 11720.7010\n",
            "Epoch [50/150], Loss: 9333.8644\n",
            "Epoch [60/150], Loss: 17242.0178\n",
            "Epoch [70/150], Loss: 13026.9326\n",
            "Epoch [80/150], Loss: 10265.9553\n",
            "Epoch [90/150], Loss: 18199.4385\n",
            "Epoch [100/150], Loss: 13150.4045\n",
            "Epoch [110/150], Loss: 10633.5756\n",
            "Epoch [120/150], Loss: 11717.5540\n",
            "Epoch [130/150], Loss: 13784.7021\n",
            "Epoch [140/150], Loss: 10438.1556\n",
            "Epoch [150/150], Loss: 16693.3843\n",
            "Fold 5, RMSE: 45.03586959838867\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.0005, weight_decay=1e-05, num_layers=3, epochs=150: 64.21100387573242\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 15908.1089\n",
            "Epoch [20/100], Loss: 17222.2153\n",
            "Epoch [30/100], Loss: 12732.9478\n",
            "Epoch [40/100], Loss: 11930.5337\n",
            "Epoch [50/100], Loss: 13087.6182\n",
            "Epoch [60/100], Loss: 18288.7360\n",
            "Epoch [70/100], Loss: 7944.7181\n",
            "Epoch [80/100], Loss: 8631.8081\n",
            "Epoch [90/100], Loss: 9164.4227\n",
            "Epoch [100/100], Loss: 8294.4995\n",
            "Fold 1, RMSE: 63.54840850830078\n",
            "Epoch [10/100], Loss: 18046.4097\n",
            "Epoch [20/100], Loss: 18119.6199\n",
            "Epoch [30/100], Loss: 15579.8442\n",
            "Epoch [40/100], Loss: 18547.4248\n",
            "Epoch [50/100], Loss: 24883.4717\n",
            "Epoch [60/100], Loss: 18896.6279\n",
            "Epoch [70/100], Loss: 16196.7476\n",
            "Epoch [80/100], Loss: 15245.5056\n",
            "Epoch [90/100], Loss: 17069.3018\n",
            "Epoch [100/100], Loss: 17795.9031\n",
            "Fold 2, RMSE: 87.28526306152344\n",
            "Epoch [10/100], Loss: 13988.2625\n",
            "Epoch [20/100], Loss: 17538.5308\n",
            "Epoch [30/100], Loss: 13911.8110\n",
            "Epoch [40/100], Loss: 19577.3132\n",
            "Epoch [50/100], Loss: 14465.2632\n",
            "Epoch [60/100], Loss: 15444.1152\n",
            "Epoch [70/100], Loss: 17768.2214\n",
            "Epoch [80/100], Loss: 11255.7936\n",
            "Epoch [90/100], Loss: 13769.1343\n",
            "Epoch [100/100], Loss: 11570.6584\n",
            "Fold 3, RMSE: 109.59302520751953\n",
            "Epoch [10/100], Loss: 19255.8379\n",
            "Epoch [20/100], Loss: 21344.8276\n",
            "Epoch [30/100], Loss: 24679.7466\n",
            "Epoch [40/100], Loss: 19414.8335\n",
            "Epoch [50/100], Loss: 21972.6646\n",
            "Epoch [60/100], Loss: 17427.5835\n",
            "Epoch [70/100], Loss: 16363.9612\n",
            "Epoch [80/100], Loss: 18793.3569\n",
            "Epoch [90/100], Loss: 18268.3340\n",
            "Epoch [100/100], Loss: 18355.4163\n",
            "Fold 4, RMSE: 54.46305465698242\n",
            "Epoch [10/100], Loss: 19794.7358\n",
            "Epoch [20/100], Loss: 13525.9208\n",
            "Epoch [30/100], Loss: 11403.2383\n",
            "Epoch [40/100], Loss: 9801.2300\n",
            "Epoch [50/100], Loss: 7439.4781\n",
            "Epoch [60/100], Loss: 6628.9717\n",
            "Epoch [70/100], Loss: 10423.1040\n",
            "Epoch [80/100], Loss: 9787.8953\n",
            "Epoch [90/100], Loss: 11979.7970\n",
            "Epoch [100/100], Loss: 12158.8232\n",
            "Fold 5, RMSE: 43.454959869384766\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=100: 71.66894226074218\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 72302.5703\n",
            "Epoch [20/150], Loss: 17880.5317\n",
            "Epoch [30/150], Loss: 14305.3425\n",
            "Epoch [40/150], Loss: 15720.1637\n",
            "Epoch [50/150], Loss: 13505.3320\n",
            "Epoch [60/150], Loss: 15337.3693\n",
            "Epoch [70/150], Loss: 14670.0127\n",
            "Epoch [80/150], Loss: 17910.6519\n",
            "Epoch [90/150], Loss: 11716.8115\n",
            "Epoch [100/150], Loss: 11243.5168\n",
            "Epoch [110/150], Loss: 7683.8431\n",
            "Epoch [120/150], Loss: 13393.5852\n",
            "Epoch [130/150], Loss: 12776.3550\n",
            "Epoch [140/150], Loss: 8737.6282\n",
            "Epoch [150/150], Loss: 9566.0862\n",
            "Fold 1, RMSE: 61.30997848510742\n",
            "Epoch [10/150], Loss: 16050.1575\n",
            "Epoch [20/150], Loss: 18147.9053\n",
            "Epoch [30/150], Loss: 16125.5400\n",
            "Epoch [40/150], Loss: 15626.3855\n",
            "Epoch [50/150], Loss: 14647.2218\n",
            "Epoch [60/150], Loss: 16810.2661\n",
            "Epoch [70/150], Loss: 15758.9099\n",
            "Epoch [80/150], Loss: 19176.8770\n",
            "Epoch [90/150], Loss: 14164.3220\n",
            "Epoch [100/150], Loss: 24703.9321\n",
            "Epoch [110/150], Loss: 26568.0388\n",
            "Epoch [120/150], Loss: 17902.2964\n",
            "Epoch [130/150], Loss: 15150.1616\n",
            "Epoch [140/150], Loss: 13648.0129\n",
            "Epoch [150/150], Loss: 18354.4619\n",
            "Fold 2, RMSE: 86.8795166015625\n",
            "Epoch [10/150], Loss: 15937.2751\n",
            "Epoch [20/150], Loss: 13259.9641\n",
            "Epoch [30/150], Loss: 11488.6915\n",
            "Epoch [40/150], Loss: 17975.6279\n",
            "Epoch [50/150], Loss: 9680.1926\n",
            "Epoch [60/150], Loss: 10319.0476\n",
            "Epoch [70/150], Loss: 7699.4520\n",
            "Epoch [80/150], Loss: 9172.6288\n",
            "Epoch [90/150], Loss: 14193.8457\n",
            "Epoch [100/150], Loss: 16131.8655\n",
            "Epoch [110/150], Loss: 11127.3950\n",
            "Epoch [120/150], Loss: 10972.9290\n",
            "Epoch [130/150], Loss: 6781.1545\n",
            "Epoch [140/150], Loss: 13429.9504\n",
            "Epoch [150/150], Loss: 6853.3574\n",
            "Fold 3, RMSE: 100.08317565917969\n",
            "Epoch [10/150], Loss: 18322.9856\n",
            "Epoch [20/150], Loss: 20140.1880\n",
            "Epoch [30/150], Loss: 18368.9980\n",
            "Epoch [40/150], Loss: 16095.4736\n",
            "Epoch [50/150], Loss: 15089.6018\n",
            "Epoch [60/150], Loss: 11012.7498\n",
            "Epoch [70/150], Loss: 12686.4595\n",
            "Epoch [80/150], Loss: 12479.7139\n",
            "Epoch [90/150], Loss: 21407.6082\n",
            "Epoch [100/150], Loss: 8534.8606\n",
            "Epoch [110/150], Loss: 14573.0901\n",
            "Epoch [120/150], Loss: 11788.7937\n",
            "Epoch [130/150], Loss: 13739.4792\n",
            "Epoch [140/150], Loss: 19986.0087\n",
            "Epoch [150/150], Loss: 9711.1335\n",
            "Fold 4, RMSE: 48.34181213378906\n",
            "Epoch [10/150], Loss: 20970.2212\n",
            "Epoch [20/150], Loss: 17765.6667\n",
            "Epoch [30/150], Loss: 22051.6860\n",
            "Epoch [40/150], Loss: 19557.9634\n",
            "Epoch [50/150], Loss: 21690.2412\n",
            "Epoch [60/150], Loss: 19018.4258\n",
            "Epoch [70/150], Loss: 18637.4163\n",
            "Epoch [80/150], Loss: 17428.0493\n",
            "Epoch [90/150], Loss: 18888.3462\n",
            "Epoch [100/150], Loss: 18665.1787\n",
            "Epoch [110/150], Loss: 19034.1406\n",
            "Epoch [120/150], Loss: 18996.8916\n",
            "Epoch [130/150], Loss: 18344.4077\n",
            "Epoch [140/150], Loss: 18077.7910\n",
            "Epoch [150/150], Loss: 17476.8806\n",
            "Fold 5, RMSE: 57.843597412109375\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=2, epochs=150: 70.89161605834961\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 18813.4854\n",
            "Epoch [20/100], Loss: 17942.3062\n",
            "Epoch [30/100], Loss: 16096.1697\n",
            "Epoch [40/100], Loss: 12967.5574\n",
            "Epoch [50/100], Loss: 16142.9385\n",
            "Epoch [60/100], Loss: 7547.0905\n",
            "Epoch [70/100], Loss: 11761.8228\n",
            "Epoch [80/100], Loss: 14237.9631\n",
            "Epoch [90/100], Loss: 11264.7601\n",
            "Epoch [100/100], Loss: 13469.2771\n",
            "Fold 1, RMSE: 66.23453521728516\n",
            "Epoch [10/100], Loss: 13959.0332\n",
            "Epoch [20/100], Loss: 20937.5547\n",
            "Epoch [30/100], Loss: 12482.6382\n",
            "Epoch [40/100], Loss: 9677.9519\n",
            "Epoch [50/100], Loss: 11971.9238\n",
            "Epoch [60/100], Loss: 3975.6825\n",
            "Epoch [70/100], Loss: 8987.4323\n",
            "Epoch [80/100], Loss: 11049.9790\n",
            "Epoch [90/100], Loss: 14916.3125\n",
            "Epoch [100/100], Loss: 18600.8391\n",
            "Fold 2, RMSE: 86.93299102783203\n",
            "Epoch [10/100], Loss: 13146.4949\n",
            "Epoch [20/100], Loss: 11504.1208\n",
            "Epoch [30/100], Loss: 6626.5533\n",
            "Epoch [40/100], Loss: 9674.6458\n",
            "Epoch [50/100], Loss: 6254.2216\n",
            "Epoch [60/100], Loss: 9371.3051\n",
            "Epoch [70/100], Loss: 5283.3022\n",
            "Epoch [80/100], Loss: 5429.9861\n",
            "Epoch [90/100], Loss: 4720.7690\n",
            "Epoch [100/100], Loss: 6514.4922\n",
            "Fold 3, RMSE: 105.5123062133789\n",
            "Epoch [10/100], Loss: 18373.1968\n",
            "Epoch [20/100], Loss: 16022.1519\n",
            "Epoch [30/100], Loss: 9801.7310\n",
            "Epoch [40/100], Loss: 12236.8770\n",
            "Epoch [50/100], Loss: 11899.1086\n",
            "Epoch [60/100], Loss: 16451.6936\n",
            "Epoch [70/100], Loss: 7007.5150\n",
            "Epoch [80/100], Loss: 11452.7097\n",
            "Epoch [90/100], Loss: 11006.0286\n",
            "Epoch [100/100], Loss: 8451.9294\n",
            "Fold 4, RMSE: 37.23295211791992\n",
            "Epoch [10/100], Loss: 16758.4204\n",
            "Epoch [20/100], Loss: 20207.0981\n",
            "Epoch [30/100], Loss: 16147.9004\n",
            "Epoch [40/100], Loss: 17703.3826\n",
            "Epoch [50/100], Loss: 14508.8602\n",
            "Epoch [60/100], Loss: 15173.3364\n",
            "Epoch [70/100], Loss: 11097.6414\n",
            "Epoch [80/100], Loss: 19546.7478\n",
            "Epoch [90/100], Loss: 13477.1147\n",
            "Epoch [100/100], Loss: 10752.9036\n",
            "Fold 5, RMSE: 49.71238708496094\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=100: 69.12503433227539\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 25290.0659\n",
            "Epoch [20/150], Loss: 11500.9478\n",
            "Epoch [30/150], Loss: 8736.1311\n",
            "Epoch [40/150], Loss: 11891.4521\n",
            "Epoch [50/150], Loss: 16113.1755\n",
            "Epoch [60/150], Loss: 21931.1650\n",
            "Epoch [70/150], Loss: 10533.2761\n",
            "Epoch [80/150], Loss: 9680.8262\n",
            "Epoch [90/150], Loss: 8484.4412\n",
            "Epoch [100/150], Loss: 14442.9834\n",
            "Epoch [110/150], Loss: 11948.5056\n",
            "Epoch [120/150], Loss: 9220.5754\n",
            "Epoch [130/150], Loss: 8592.4425\n",
            "Epoch [140/150], Loss: 10725.4972\n",
            "Epoch [150/150], Loss: 5493.6163\n",
            "Fold 1, RMSE: 59.05253219604492\n",
            "Epoch [10/150], Loss: 15240.2371\n",
            "Epoch [20/150], Loss: 10849.8691\n",
            "Epoch [30/150], Loss: 9324.3486\n",
            "Epoch [40/150], Loss: 13706.5059\n",
            "Epoch [50/150], Loss: 11770.1310\n",
            "Epoch [60/150], Loss: 22524.4885\n",
            "Epoch [70/150], Loss: 9054.4583\n",
            "Epoch [80/150], Loss: 9308.4785\n",
            "Epoch [90/150], Loss: 19738.9017\n",
            "Epoch [100/150], Loss: 10822.3361\n",
            "Epoch [110/150], Loss: 14520.5400\n",
            "Epoch [120/150], Loss: 9757.3094\n",
            "Epoch [130/150], Loss: 8986.8516\n",
            "Epoch [140/150], Loss: 14273.6797\n",
            "Epoch [150/150], Loss: 10810.5233\n",
            "Fold 2, RMSE: 78.42445373535156\n",
            "Epoch [10/150], Loss: 18546.3616\n",
            "Epoch [20/150], Loss: 12836.3635\n",
            "Epoch [30/150], Loss: 10969.5576\n",
            "Epoch [40/150], Loss: 7115.8341\n",
            "Epoch [50/150], Loss: 11283.3864\n",
            "Epoch [60/150], Loss: 8313.1576\n",
            "Epoch [70/150], Loss: 7340.5576\n",
            "Epoch [80/150], Loss: 13470.2195\n",
            "Epoch [90/150], Loss: 8056.4231\n",
            "Epoch [100/150], Loss: 8288.4763\n",
            "Epoch [110/150], Loss: 12803.8778\n",
            "Epoch [120/150], Loss: 8243.7983\n",
            "Epoch [130/150], Loss: 6637.8491\n",
            "Epoch [140/150], Loss: 8098.0012\n",
            "Epoch [150/150], Loss: 6265.8069\n",
            "Fold 3, RMSE: 100.90438079833984\n",
            "Epoch [10/150], Loss: 29174.7568\n",
            "Epoch [20/150], Loss: 26997.2554\n",
            "Epoch [30/150], Loss: 21915.4917\n",
            "Epoch [40/150], Loss: 23004.0264\n",
            "Epoch [50/150], Loss: 20443.0933\n",
            "Epoch [60/150], Loss: 17625.2566\n",
            "Epoch [70/150], Loss: 23455.7837\n",
            "Epoch [80/150], Loss: 19351.0278\n",
            "Epoch [90/150], Loss: 18187.7544\n",
            "Epoch [100/150], Loss: 25242.4778\n",
            "Epoch [110/150], Loss: 24603.3767\n",
            "Epoch [120/150], Loss: 18649.8071\n",
            "Epoch [130/150], Loss: 18327.9678\n",
            "Epoch [140/150], Loss: 25688.0840\n",
            "Epoch [150/150], Loss: 21142.3281\n",
            "Fold 4, RMSE: 53.03131103515625\n",
            "Epoch [10/150], Loss: 13681.7103\n",
            "Epoch [20/150], Loss: 11674.9221\n",
            "Epoch [30/150], Loss: 22787.1584\n",
            "Epoch [40/150], Loss: 11153.8044\n",
            "Epoch [50/150], Loss: 7905.7746\n",
            "Epoch [60/150], Loss: 10817.8192\n",
            "Epoch [70/150], Loss: 10035.9923\n",
            "Epoch [80/150], Loss: 19324.3779\n",
            "Epoch [90/150], Loss: 8765.3656\n",
            "Epoch [100/150], Loss: 10592.1187\n",
            "Epoch [110/150], Loss: 10791.9906\n",
            "Epoch [120/150], Loss: 10581.4849\n",
            "Epoch [130/150], Loss: 10083.8307\n",
            "Epoch [140/150], Loss: 7387.7856\n",
            "Epoch [150/150], Loss: 10890.4426\n",
            "Fold 5, RMSE: 47.050872802734375\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0, num_layers=3, epochs=150: 67.6927101135254\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 18697.3867\n",
            "Epoch [20/100], Loss: 16486.7137\n",
            "Epoch [30/100], Loss: 16952.7739\n",
            "Epoch [40/100], Loss: 10665.9172\n",
            "Epoch [50/100], Loss: 19396.0405\n",
            "Epoch [60/100], Loss: 12768.4517\n",
            "Epoch [70/100], Loss: 12836.7290\n",
            "Epoch [80/100], Loss: 13788.5300\n",
            "Epoch [90/100], Loss: 20044.7959\n",
            "Epoch [100/100], Loss: 11140.8578\n",
            "Fold 1, RMSE: 58.25765609741211\n",
            "Epoch [10/100], Loss: 17433.9104\n",
            "Epoch [20/100], Loss: 15110.1655\n",
            "Epoch [30/100], Loss: 24190.4031\n",
            "Epoch [40/100], Loss: 17767.6733\n",
            "Epoch [50/100], Loss: 15037.3364\n",
            "Epoch [60/100], Loss: 16744.8950\n",
            "Epoch [70/100], Loss: 15007.7261\n",
            "Epoch [80/100], Loss: 15058.5520\n",
            "Epoch [90/100], Loss: 18224.4370\n",
            "Epoch [100/100], Loss: 18819.1785\n",
            "Fold 2, RMSE: 87.24665069580078\n",
            "Epoch [10/100], Loss: 16128.3972\n",
            "Epoch [20/100], Loss: 10658.1082\n",
            "Epoch [30/100], Loss: 13408.2124\n",
            "Epoch [40/100], Loss: 9815.2113\n",
            "Epoch [50/100], Loss: 7856.8994\n",
            "Epoch [60/100], Loss: 8853.5367\n",
            "Epoch [70/100], Loss: 7873.2034\n",
            "Epoch [80/100], Loss: 7030.8912\n",
            "Epoch [90/100], Loss: 11736.2922\n",
            "Epoch [100/100], Loss: 16562.1079\n",
            "Fold 3, RMSE: 109.74996185302734\n",
            "Epoch [10/100], Loss: 15598.2539\n",
            "Epoch [20/100], Loss: 22177.4578\n",
            "Epoch [30/100], Loss: 15106.3472\n",
            "Epoch [40/100], Loss: 11324.9148\n",
            "Epoch [50/100], Loss: 14237.4231\n",
            "Epoch [60/100], Loss: 14533.5127\n",
            "Epoch [70/100], Loss: 10983.5664\n",
            "Epoch [80/100], Loss: 15898.8817\n",
            "Epoch [90/100], Loss: 12033.9639\n",
            "Epoch [100/100], Loss: 11756.8452\n",
            "Fold 4, RMSE: 44.59833908081055\n",
            "Epoch [10/100], Loss: 19884.3662\n",
            "Epoch [20/100], Loss: 16925.3735\n",
            "Epoch [30/100], Loss: 18894.5811\n",
            "Epoch [40/100], Loss: 19343.1650\n",
            "Epoch [50/100], Loss: 19340.0894\n",
            "Epoch [60/100], Loss: 18601.3362\n",
            "Epoch [70/100], Loss: 13709.4556\n",
            "Epoch [80/100], Loss: 15641.6689\n",
            "Epoch [90/100], Loss: 9864.6460\n",
            "Epoch [100/100], Loss: 11598.7200\n",
            "Fold 5, RMSE: 49.99583053588867\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=100: 69.9696876525879\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 27948.6064\n",
            "Epoch [20/150], Loss: 15896.6061\n",
            "Epoch [30/150], Loss: 16617.2064\n",
            "Epoch [40/150], Loss: 22088.2881\n",
            "Epoch [50/150], Loss: 17692.4778\n",
            "Epoch [60/150], Loss: 27825.7239\n",
            "Epoch [70/150], Loss: 15962.8772\n",
            "Epoch [80/150], Loss: 29176.3555\n",
            "Epoch [90/150], Loss: 16354.1749\n",
            "Epoch [100/150], Loss: 17676.1104\n",
            "Epoch [110/150], Loss: 17550.5413\n",
            "Epoch [120/150], Loss: 19724.2646\n",
            "Epoch [130/150], Loss: 20409.9648\n",
            "Epoch [140/150], Loss: 17542.4126\n",
            "Epoch [150/150], Loss: 20752.6919\n",
            "Fold 1, RMSE: 67.2431869506836\n",
            "Epoch [10/150], Loss: 15950.7566\n",
            "Epoch [20/150], Loss: 20646.4741\n",
            "Epoch [30/150], Loss: 14788.8511\n",
            "Epoch [40/150], Loss: 25008.7407\n",
            "Epoch [50/150], Loss: 11104.2959\n",
            "Epoch [60/150], Loss: 13625.5088\n",
            "Epoch [70/150], Loss: 8638.5647\n",
            "Epoch [80/150], Loss: 9113.9495\n",
            "Epoch [90/150], Loss: 8688.8113\n",
            "Epoch [100/150], Loss: 7469.0745\n",
            "Epoch [110/150], Loss: 9397.6603\n",
            "Epoch [120/150], Loss: 10239.7468\n",
            "Epoch [130/150], Loss: 10647.1021\n",
            "Epoch [140/150], Loss: 13246.6340\n",
            "Epoch [150/150], Loss: 15008.6133\n",
            "Fold 2, RMSE: 84.9605712890625\n",
            "Epoch [10/150], Loss: 19533.5845\n",
            "Epoch [20/150], Loss: 12359.8801\n",
            "Epoch [30/150], Loss: 9293.0964\n",
            "Epoch [40/150], Loss: 12530.1484\n",
            "Epoch [50/150], Loss: 9442.1372\n",
            "Epoch [60/150], Loss: 10798.2024\n",
            "Epoch [70/150], Loss: 11434.2061\n",
            "Epoch [80/150], Loss: 6165.9716\n",
            "Epoch [90/150], Loss: 5056.6066\n",
            "Epoch [100/150], Loss: 8546.4536\n",
            "Epoch [110/150], Loss: 4982.6591\n",
            "Epoch [120/150], Loss: 5390.6973\n",
            "Epoch [130/150], Loss: 10971.5674\n",
            "Epoch [140/150], Loss: 8718.8658\n",
            "Epoch [150/150], Loss: 4817.1660\n",
            "Fold 3, RMSE: 97.78730010986328\n",
            "Epoch [10/150], Loss: 36878.3354\n",
            "Epoch [20/150], Loss: 14980.9219\n",
            "Epoch [30/150], Loss: 14774.2236\n",
            "Epoch [40/150], Loss: 15649.7585\n",
            "Epoch [50/150], Loss: 15548.1906\n",
            "Epoch [60/150], Loss: 9591.7523\n",
            "Epoch [70/150], Loss: 11916.2073\n",
            "Epoch [80/150], Loss: 11973.5081\n",
            "Epoch [90/150], Loss: 13872.5176\n",
            "Epoch [100/150], Loss: 11490.7014\n",
            "Epoch [110/150], Loss: 22130.7534\n",
            "Epoch [120/150], Loss: 10315.4220\n",
            "Epoch [130/150], Loss: 12840.4288\n",
            "Epoch [140/150], Loss: 10234.9055\n",
            "Epoch [150/150], Loss: 8215.6456\n",
            "Fold 4, RMSE: 47.88602066040039\n",
            "Epoch [10/150], Loss: 36004.1123\n",
            "Epoch [20/150], Loss: 26128.9717\n",
            "Epoch [30/150], Loss: 21238.0566\n",
            "Epoch [40/150], Loss: 19987.5742\n",
            "Epoch [50/150], Loss: 18180.0120\n",
            "Epoch [60/150], Loss: 23341.7295\n",
            "Epoch [70/150], Loss: 20232.3271\n",
            "Epoch [80/150], Loss: 20612.8457\n",
            "Epoch [90/150], Loss: 17692.0820\n",
            "Epoch [100/150], Loss: 23188.2432\n",
            "Epoch [110/150], Loss: 18582.0112\n",
            "Epoch [120/150], Loss: 17463.4487\n",
            "Epoch [130/150], Loss: 19164.2461\n",
            "Epoch [140/150], Loss: 18428.7148\n",
            "Epoch [150/150], Loss: 20543.5957\n",
            "Fold 5, RMSE: 57.81249237060547\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=2, epochs=150: 71.13791427612304\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 19291.4512\n",
            "Epoch [20/100], Loss: 16715.7417\n",
            "Epoch [30/100], Loss: 16815.6816\n",
            "Epoch [40/100], Loss: 14658.8923\n",
            "Epoch [50/100], Loss: 14203.9512\n",
            "Epoch [60/100], Loss: 13219.3530\n",
            "Epoch [70/100], Loss: 18430.2285\n",
            "Epoch [80/100], Loss: 9008.9583\n",
            "Epoch [90/100], Loss: 7719.0236\n",
            "Epoch [100/100], Loss: 10727.7795\n",
            "Fold 1, RMSE: 62.6535758972168\n",
            "Epoch [10/100], Loss: 22133.7988\n",
            "Epoch [20/100], Loss: 13453.6360\n",
            "Epoch [30/100], Loss: 12067.0435\n",
            "Epoch [40/100], Loss: 12383.4609\n",
            "Epoch [50/100], Loss: 11263.8002\n",
            "Epoch [60/100], Loss: 12160.4370\n",
            "Epoch [70/100], Loss: 9464.2386\n",
            "Epoch [80/100], Loss: 6250.2604\n",
            "Epoch [90/100], Loss: 8571.3643\n",
            "Epoch [100/100], Loss: 6716.8978\n",
            "Fold 2, RMSE: 81.75132751464844\n",
            "Epoch [10/100], Loss: 15677.4482\n",
            "Epoch [20/100], Loss: 12619.8157\n",
            "Epoch [30/100], Loss: 8314.8517\n",
            "Epoch [40/100], Loss: 8931.9551\n",
            "Epoch [50/100], Loss: 9218.9695\n",
            "Epoch [60/100], Loss: 6276.9618\n",
            "Epoch [70/100], Loss: 8269.4348\n",
            "Epoch [80/100], Loss: 9412.0345\n",
            "Epoch [90/100], Loss: 4096.1360\n",
            "Epoch [100/100], Loss: 4820.8917\n",
            "Fold 3, RMSE: 94.73284149169922\n",
            "Epoch [10/100], Loss: 19817.2793\n",
            "Epoch [20/100], Loss: 13382.7524\n",
            "Epoch [30/100], Loss: 11283.1123\n",
            "Epoch [40/100], Loss: 8819.5740\n",
            "Epoch [50/100], Loss: 14565.9370\n",
            "Epoch [60/100], Loss: 7590.4890\n",
            "Epoch [70/100], Loss: 6310.3837\n",
            "Epoch [80/100], Loss: 17522.9908\n",
            "Epoch [90/100], Loss: 18504.1740\n",
            "Epoch [100/100], Loss: 6089.6252\n",
            "Fold 4, RMSE: 43.20369338989258\n",
            "Epoch [10/100], Loss: 25073.3198\n",
            "Epoch [20/100], Loss: 16174.6660\n",
            "Epoch [30/100], Loss: 18268.3857\n",
            "Epoch [40/100], Loss: 13083.3383\n",
            "Epoch [50/100], Loss: 21268.4568\n",
            "Epoch [60/100], Loss: 14182.7778\n",
            "Epoch [70/100], Loss: 14172.6594\n",
            "Epoch [80/100], Loss: 11966.9167\n",
            "Epoch [90/100], Loss: 12308.6468\n",
            "Epoch [100/100], Loss: 7652.3717\n",
            "Fold 5, RMSE: 44.7815055847168\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=100: 65.42458877563476\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 26298.0847\n",
            "Epoch [20/150], Loss: 13300.1360\n",
            "Epoch [30/150], Loss: 8028.5779\n",
            "Epoch [40/150], Loss: 8576.1735\n",
            "Epoch [50/150], Loss: 7726.9065\n",
            "Epoch [60/150], Loss: 11205.4843\n",
            "Epoch [70/150], Loss: 8374.6228\n",
            "Epoch [80/150], Loss: 11925.9946\n",
            "Epoch [90/150], Loss: 11929.6413\n",
            "Epoch [100/150], Loss: 10302.0789\n",
            "Epoch [110/150], Loss: 8402.3953\n",
            "Epoch [120/150], Loss: 15118.4059\n",
            "Epoch [130/150], Loss: 12141.2083\n",
            "Epoch [140/150], Loss: 8986.5891\n",
            "Epoch [150/150], Loss: 10914.5959\n",
            "Fold 1, RMSE: 59.08989334106445\n",
            "Epoch [10/150], Loss: 17091.2310\n",
            "Epoch [20/150], Loss: 16516.7017\n",
            "Epoch [30/150], Loss: 17572.2930\n",
            "Epoch [40/150], Loss: 15198.1904\n",
            "Epoch [50/150], Loss: 20287.5464\n",
            "Epoch [60/150], Loss: 17758.2168\n",
            "Epoch [70/150], Loss: 15662.8210\n",
            "Epoch [80/150], Loss: 16600.3418\n",
            "Epoch [90/150], Loss: 14035.0325\n",
            "Epoch [100/150], Loss: 14305.5330\n",
            "Epoch [110/150], Loss: 23527.7219\n",
            "Epoch [120/150], Loss: 15236.9272\n",
            "Epoch [130/150], Loss: 17625.4971\n",
            "Epoch [140/150], Loss: 16653.4744\n",
            "Epoch [150/150], Loss: 13987.1689\n",
            "Fold 2, RMSE: 86.49652862548828\n",
            "Epoch [10/150], Loss: 15073.5293\n",
            "Epoch [20/150], Loss: 12060.4388\n",
            "Epoch [30/150], Loss: 14486.7615\n",
            "Epoch [40/150], Loss: 16402.2010\n",
            "Epoch [50/150], Loss: 14179.9543\n",
            "Epoch [60/150], Loss: 13345.6707\n",
            "Epoch [70/150], Loss: 9788.6592\n",
            "Epoch [80/150], Loss: 9389.1219\n",
            "Epoch [90/150], Loss: 12306.6035\n",
            "Epoch [100/150], Loss: 8416.9423\n",
            "Epoch [110/150], Loss: 8615.5679\n",
            "Epoch [120/150], Loss: 9121.3157\n",
            "Epoch [130/150], Loss: 5392.9387\n",
            "Epoch [140/150], Loss: 7437.7648\n",
            "Epoch [150/150], Loss: 8515.8711\n",
            "Fold 3, RMSE: 101.15137481689453\n",
            "Epoch [10/150], Loss: 24551.0498\n",
            "Epoch [20/150], Loss: 14616.0691\n",
            "Epoch [30/150], Loss: 10067.3539\n",
            "Epoch [40/150], Loss: 13963.5388\n",
            "Epoch [50/150], Loss: 8256.3013\n",
            "Epoch [60/150], Loss: 20801.8347\n",
            "Epoch [70/150], Loss: 5156.4327\n",
            "Epoch [80/150], Loss: 14434.7314\n",
            "Epoch [90/150], Loss: 7144.6724\n",
            "Epoch [100/150], Loss: 10581.7231\n",
            "Epoch [110/150], Loss: 8854.9758\n",
            "Epoch [120/150], Loss: 9749.4128\n",
            "Epoch [130/150], Loss: 9539.6047\n",
            "Epoch [140/150], Loss: 12184.7081\n",
            "Epoch [150/150], Loss: 11337.4361\n",
            "Fold 4, RMSE: 50.13554000854492\n",
            "Epoch [10/150], Loss: 18220.5815\n",
            "Epoch [20/150], Loss: 13850.3755\n",
            "Epoch [30/150], Loss: 12545.8658\n",
            "Epoch [40/150], Loss: 11401.4174\n",
            "Epoch [50/150], Loss: 18483.6853\n",
            "Epoch [60/150], Loss: 10052.9874\n",
            "Epoch [70/150], Loss: 7232.7014\n",
            "Epoch [80/150], Loss: 16502.5480\n",
            "Epoch [90/150], Loss: 7267.3364\n",
            "Epoch [100/150], Loss: 8182.3835\n",
            "Epoch [110/150], Loss: 8412.2607\n",
            "Epoch [120/150], Loss: 7456.3840\n",
            "Epoch [130/150], Loss: 6353.6547\n",
            "Epoch [140/150], Loss: 6050.8273\n",
            "Epoch [150/150], Loss: 6959.0499\n",
            "Fold 5, RMSE: 46.75061798095703\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=0.0001, num_layers=3, epochs=150: 68.72479095458985\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 14281.6130\n",
            "Epoch [20/100], Loss: 11504.7197\n",
            "Epoch [30/100], Loss: 12532.6472\n",
            "Epoch [40/100], Loss: 9434.6147\n",
            "Epoch [50/100], Loss: 12773.1536\n",
            "Epoch [60/100], Loss: 18064.6787\n",
            "Epoch [70/100], Loss: 5474.0134\n",
            "Epoch [80/100], Loss: 8336.8119\n",
            "Epoch [90/100], Loss: 9175.5426\n",
            "Epoch [100/100], Loss: 7591.0710\n",
            "Fold 1, RMSE: 64.71744537353516\n",
            "Epoch [10/100], Loss: 14289.6277\n",
            "Epoch [20/100], Loss: 12568.7468\n",
            "Epoch [30/100], Loss: 14175.8015\n",
            "Epoch [40/100], Loss: 14717.8262\n",
            "Epoch [50/100], Loss: 14034.3914\n",
            "Epoch [60/100], Loss: 11553.0830\n",
            "Epoch [70/100], Loss: 21488.7944\n",
            "Epoch [80/100], Loss: 13585.9722\n",
            "Epoch [90/100], Loss: 12598.2562\n",
            "Epoch [100/100], Loss: 8769.2114\n",
            "Fold 2, RMSE: 84.49088287353516\n",
            "Epoch [10/100], Loss: 31209.3442\n",
            "Epoch [20/100], Loss: 14690.9048\n",
            "Epoch [30/100], Loss: 15951.3730\n",
            "Epoch [40/100], Loss: 13873.4944\n",
            "Epoch [50/100], Loss: 12175.4321\n",
            "Epoch [60/100], Loss: 8770.0149\n",
            "Epoch [70/100], Loss: 5681.7159\n",
            "Epoch [80/100], Loss: 11348.2817\n",
            "Epoch [90/100], Loss: 7519.2184\n",
            "Epoch [100/100], Loss: 11481.5750\n",
            "Fold 3, RMSE: 109.29151153564453\n",
            "Epoch [10/100], Loss: 27601.5156\n",
            "Epoch [20/100], Loss: 14440.9518\n",
            "Epoch [30/100], Loss: 12972.8443\n",
            "Epoch [40/100], Loss: 19770.4287\n",
            "Epoch [50/100], Loss: 18136.2976\n",
            "Epoch [60/100], Loss: 17523.4390\n",
            "Epoch [70/100], Loss: 19429.1499\n",
            "Epoch [80/100], Loss: 28801.1279\n",
            "Epoch [90/100], Loss: 20214.7119\n",
            "Epoch [100/100], Loss: 20146.8511\n",
            "Fold 4, RMSE: 54.5579719543457\n",
            "Epoch [10/100], Loss: 25916.3164\n",
            "Epoch [20/100], Loss: 19978.9612\n",
            "Epoch [30/100], Loss: 17755.7587\n",
            "Epoch [40/100], Loss: 16028.9587\n",
            "Epoch [50/100], Loss: 14836.4768\n",
            "Epoch [60/100], Loss: 17137.7983\n",
            "Epoch [70/100], Loss: 8636.1980\n",
            "Epoch [80/100], Loss: 13235.9861\n",
            "Epoch [90/100], Loss: 13922.7371\n",
            "Epoch [100/100], Loss: 9423.5703\n",
            "Fold 5, RMSE: 44.65652847290039\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=100: 71.5428680419922\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 15378.9017\n",
            "Epoch [20/150], Loss: 23520.4104\n",
            "Epoch [30/150], Loss: 17879.8860\n",
            "Epoch [40/150], Loss: 16945.8057\n",
            "Epoch [50/150], Loss: 23645.3242\n",
            "Epoch [60/150], Loss: 11071.4360\n",
            "Epoch [70/150], Loss: 8764.4509\n",
            "Epoch [80/150], Loss: 14101.5549\n",
            "Epoch [90/150], Loss: 5829.2191\n",
            "Epoch [100/150], Loss: 12996.3840\n",
            "Epoch [110/150], Loss: 16219.5100\n",
            "Epoch [120/150], Loss: 11069.0778\n",
            "Epoch [130/150], Loss: 11978.9039\n",
            "Epoch [140/150], Loss: 13165.5541\n",
            "Epoch [150/150], Loss: 8365.4226\n",
            "Fold 1, RMSE: 58.36821365356445\n",
            "Epoch [10/150], Loss: 21193.8135\n",
            "Epoch [20/150], Loss: 26732.3630\n",
            "Epoch [30/150], Loss: 12810.4509\n",
            "Epoch [40/150], Loss: 12084.5911\n",
            "Epoch [50/150], Loss: 14553.7893\n",
            "Epoch [60/150], Loss: 11858.4602\n",
            "Epoch [70/150], Loss: 14618.2473\n",
            "Epoch [80/150], Loss: 9437.6553\n",
            "Epoch [90/150], Loss: 14837.9521\n",
            "Epoch [100/150], Loss: 11369.9475\n",
            "Epoch [110/150], Loss: 19185.6213\n",
            "Epoch [120/150], Loss: 14161.3523\n",
            "Epoch [130/150], Loss: 6744.6261\n",
            "Epoch [140/150], Loss: 11167.7847\n",
            "Epoch [150/150], Loss: 7936.1221\n",
            "Fold 2, RMSE: 70.76551818847656\n",
            "Epoch [10/150], Loss: 16230.5034\n",
            "Epoch [20/150], Loss: 13776.6895\n",
            "Epoch [30/150], Loss: 11684.9429\n",
            "Epoch [40/150], Loss: 10766.4902\n",
            "Epoch [50/150], Loss: 8980.8962\n",
            "Epoch [60/150], Loss: 10352.2937\n",
            "Epoch [70/150], Loss: 8173.7503\n",
            "Epoch [80/150], Loss: 7968.1206\n",
            "Epoch [90/150], Loss: 13422.9243\n",
            "Epoch [100/150], Loss: 7835.1456\n",
            "Epoch [110/150], Loss: 7485.0410\n",
            "Epoch [120/150], Loss: 7418.3446\n",
            "Epoch [130/150], Loss: 9594.0771\n",
            "Epoch [140/150], Loss: 7313.6629\n",
            "Epoch [150/150], Loss: 10590.6765\n",
            "Fold 3, RMSE: 102.10011291503906\n",
            "Epoch [10/150], Loss: 20821.6489\n",
            "Epoch [20/150], Loss: 17944.8306\n",
            "Epoch [30/150], Loss: 19790.5786\n",
            "Epoch [40/150], Loss: 21553.8494\n",
            "Epoch [50/150], Loss: 19166.9810\n",
            "Epoch [60/150], Loss: 20326.7178\n",
            "Epoch [70/150], Loss: 27258.3589\n",
            "Epoch [80/150], Loss: 25981.7192\n",
            "Epoch [90/150], Loss: 19714.2368\n",
            "Epoch [100/150], Loss: 17476.6222\n",
            "Epoch [110/150], Loss: 27854.1458\n",
            "Epoch [120/150], Loss: 18707.7766\n",
            "Epoch [130/150], Loss: 19039.0332\n",
            "Epoch [140/150], Loss: 18035.3591\n",
            "Epoch [150/150], Loss: 18072.2881\n",
            "Fold 4, RMSE: 54.134971618652344\n",
            "Epoch [10/150], Loss: 28749.8579\n",
            "Epoch [20/150], Loss: 21575.5820\n",
            "Epoch [30/150], Loss: 20173.8081\n",
            "Epoch [40/150], Loss: 19842.3018\n",
            "Epoch [50/150], Loss: 20210.8972\n",
            "Epoch [60/150], Loss: 14765.8909\n",
            "Epoch [70/150], Loss: 17576.1450\n",
            "Epoch [80/150], Loss: 10805.2095\n",
            "Epoch [90/150], Loss: 14900.6613\n",
            "Epoch [100/150], Loss: 12096.2148\n",
            "Epoch [110/150], Loss: 13573.2277\n",
            "Epoch [120/150], Loss: 14993.7997\n",
            "Epoch [130/150], Loss: 9660.9387\n",
            "Epoch [140/150], Loss: 11482.6248\n",
            "Epoch [150/150], Loss: 9266.9465\n",
            "Fold 5, RMSE: 44.394187927246094\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=2, epochs=150: 65.95260086059571\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 11306.1066\n",
            "Epoch [20/100], Loss: 10477.3416\n",
            "Epoch [30/100], Loss: 8635.2224\n",
            "Epoch [40/100], Loss: 7712.5864\n",
            "Epoch [50/100], Loss: 7454.4891\n",
            "Epoch [60/100], Loss: 8090.5493\n",
            "Epoch [70/100], Loss: 14742.5688\n",
            "Epoch [80/100], Loss: 19218.5181\n",
            "Epoch [90/100], Loss: 12812.1719\n",
            "Epoch [100/100], Loss: 15822.4585\n",
            "Fold 1, RMSE: 67.15819549560547\n",
            "Epoch [10/100], Loss: 14119.9780\n",
            "Epoch [20/100], Loss: 12632.9709\n",
            "Epoch [30/100], Loss: 10082.6582\n",
            "Epoch [40/100], Loss: 14501.9216\n",
            "Epoch [50/100], Loss: 12964.3311\n",
            "Epoch [60/100], Loss: 9052.0590\n",
            "Epoch [70/100], Loss: 12116.8499\n",
            "Epoch [80/100], Loss: 12902.8726\n",
            "Epoch [90/100], Loss: 15149.7727\n",
            "Epoch [100/100], Loss: 6557.8669\n",
            "Fold 2, RMSE: 72.84547424316406\n",
            "Epoch [10/100], Loss: 10788.0447\n",
            "Epoch [20/100], Loss: 7813.2034\n",
            "Epoch [30/100], Loss: 10198.3625\n",
            "Epoch [40/100], Loss: 7736.7932\n",
            "Epoch [50/100], Loss: 11393.7983\n",
            "Epoch [60/100], Loss: 13970.7142\n",
            "Epoch [70/100], Loss: 10833.7422\n",
            "Epoch [80/100], Loss: 12040.9297\n",
            "Epoch [90/100], Loss: 9910.2493\n",
            "Epoch [100/100], Loss: 12818.0615\n",
            "Fold 3, RMSE: 104.07856750488281\n",
            "Epoch [10/100], Loss: 19320.9902\n",
            "Epoch [20/100], Loss: 12874.4373\n",
            "Epoch [30/100], Loss: 14028.8196\n",
            "Epoch [40/100], Loss: 8794.4283\n",
            "Epoch [50/100], Loss: 14282.4214\n",
            "Epoch [60/100], Loss: 8853.2015\n",
            "Epoch [70/100], Loss: 14330.7441\n",
            "Epoch [80/100], Loss: 10251.0504\n",
            "Epoch [90/100], Loss: 7330.1558\n",
            "Epoch [100/100], Loss: 15184.5151\n",
            "Fold 4, RMSE: 44.60066223144531\n",
            "Epoch [10/100], Loss: 17170.8748\n",
            "Epoch [20/100], Loss: 14473.4888\n",
            "Epoch [30/100], Loss: 14246.8533\n",
            "Epoch [40/100], Loss: 10152.2944\n",
            "Epoch [50/100], Loss: 8771.6501\n",
            "Epoch [60/100], Loss: 8332.8381\n",
            "Epoch [70/100], Loss: 12364.2495\n",
            "Epoch [80/100], Loss: 5592.5154\n",
            "Epoch [90/100], Loss: 4584.0595\n",
            "Epoch [100/100], Loss: 5997.2212\n",
            "Fold 5, RMSE: 49.232460021972656\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=100: 67.58307189941407\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 15541.2477\n",
            "Epoch [20/150], Loss: 14990.1221\n",
            "Epoch [30/150], Loss: 12120.2887\n",
            "Epoch [40/150], Loss: 11757.3528\n",
            "Epoch [50/150], Loss: 6338.9841\n",
            "Epoch [60/150], Loss: 7050.9618\n",
            "Epoch [70/150], Loss: 14309.2583\n",
            "Epoch [80/150], Loss: 10512.5524\n",
            "Epoch [90/150], Loss: 6565.7855\n",
            "Epoch [100/150], Loss: 6461.1140\n",
            "Epoch [110/150], Loss: 4357.7477\n",
            "Epoch [120/150], Loss: 8458.5254\n",
            "Epoch [130/150], Loss: 11546.4695\n",
            "Epoch [140/150], Loss: 8885.1848\n",
            "Epoch [150/150], Loss: 8205.2642\n",
            "Fold 1, RMSE: 57.97294616699219\n",
            "Epoch [10/150], Loss: 16900.6943\n",
            "Epoch [20/150], Loss: 11279.1987\n",
            "Epoch [30/150], Loss: 10725.8889\n",
            "Epoch [40/150], Loss: 19587.7876\n",
            "Epoch [50/150], Loss: 10041.9270\n",
            "Epoch [60/150], Loss: 9791.6210\n",
            "Epoch [70/150], Loss: 10389.6980\n",
            "Epoch [80/150], Loss: 11756.8794\n",
            "Epoch [90/150], Loss: 11027.7095\n",
            "Epoch [100/150], Loss: 15034.9663\n",
            "Epoch [110/150], Loss: 6214.3951\n",
            "Epoch [120/150], Loss: 8111.9921\n",
            "Epoch [130/150], Loss: 5297.7798\n",
            "Epoch [140/150], Loss: 8353.9338\n",
            "Epoch [150/150], Loss: 9885.4738\n",
            "Fold 2, RMSE: 75.43839263916016\n",
            "Epoch [10/150], Loss: 10905.3595\n",
            "Epoch [20/150], Loss: 8619.9253\n",
            "Epoch [30/150], Loss: 6710.9696\n",
            "Epoch [40/150], Loss: 4832.5559\n",
            "Epoch [50/150], Loss: 8202.2023\n",
            "Epoch [60/150], Loss: 5899.7643\n",
            "Epoch [70/150], Loss: 8557.5679\n",
            "Epoch [80/150], Loss: 6258.0920\n",
            "Epoch [90/150], Loss: 4614.2610\n",
            "Epoch [100/150], Loss: 7215.8712\n",
            "Epoch [110/150], Loss: 5727.9242\n",
            "Epoch [120/150], Loss: 3714.9963\n",
            "Epoch [130/150], Loss: 6059.8539\n",
            "Epoch [140/150], Loss: 6962.4525\n",
            "Epoch [150/150], Loss: 7270.5850\n",
            "Fold 3, RMSE: 101.4210205078125\n",
            "Epoch [10/150], Loss: 17566.5518\n",
            "Epoch [20/150], Loss: 12400.8651\n",
            "Epoch [30/150], Loss: 8945.2882\n",
            "Epoch [40/150], Loss: 9951.8572\n",
            "Epoch [50/150], Loss: 7810.1360\n",
            "Epoch [60/150], Loss: 5279.2763\n",
            "Epoch [70/150], Loss: 5979.2514\n",
            "Epoch [80/150], Loss: 10361.5186\n",
            "Epoch [90/150], Loss: 8633.7229\n",
            "Epoch [100/150], Loss: 6109.7579\n",
            "Epoch [110/150], Loss: 11849.0219\n",
            "Epoch [120/150], Loss: 12331.1477\n",
            "Epoch [130/150], Loss: 6252.1757\n",
            "Epoch [140/150], Loss: 5325.9076\n",
            "Epoch [150/150], Loss: 6678.4380\n",
            "Fold 4, RMSE: 40.57347869873047\n",
            "Epoch [10/150], Loss: 17983.4917\n",
            "Epoch [20/150], Loss: 12280.0232\n",
            "Epoch [30/150], Loss: 16419.3662\n",
            "Epoch [40/150], Loss: 16878.7402\n",
            "Epoch [50/150], Loss: 21863.8882\n",
            "Epoch [60/150], Loss: 12049.1592\n",
            "Epoch [70/150], Loss: 8578.0920\n",
            "Epoch [80/150], Loss: 12351.4802\n",
            "Epoch [90/150], Loss: 11651.4053\n",
            "Epoch [100/150], Loss: 8659.2396\n",
            "Epoch [110/150], Loss: 15833.7268\n",
            "Epoch [120/150], Loss: 8631.8679\n",
            "Epoch [130/150], Loss: 11606.1006\n",
            "Epoch [140/150], Loss: 9686.1865\n",
            "Epoch [150/150], Loss: 10601.0203\n",
            "Fold 5, RMSE: 47.65569305419922\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.001, weight_decay=1e-05, num_layers=3, epochs=150: 64.6123062133789\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 96318.2383\n",
            "Epoch [20/100], Loss: 23778.7466\n",
            "Epoch [30/100], Loss: 21860.1904\n",
            "Epoch [40/100], Loss: 21480.6682\n",
            "Epoch [50/100], Loss: 16572.2236\n",
            "Epoch [60/100], Loss: 17259.8423\n",
            "Epoch [70/100], Loss: 16570.4148\n",
            "Epoch [80/100], Loss: 21675.3308\n",
            "Epoch [90/100], Loss: 17435.3145\n",
            "Epoch [100/100], Loss: 18214.0183\n",
            "Fold 1, RMSE: 67.4307861328125\n",
            "Epoch [10/100], Loss: 245044.1328\n",
            "Epoch [20/100], Loss: 32157.8340\n",
            "Epoch [30/100], Loss: 21030.5669\n",
            "Epoch [40/100], Loss: 18728.8828\n",
            "Epoch [50/100], Loss: 14677.8335\n",
            "Epoch [60/100], Loss: 15782.3091\n",
            "Epoch [70/100], Loss: 13998.6816\n",
            "Epoch [80/100], Loss: 13320.4666\n",
            "Epoch [90/100], Loss: 13391.5833\n",
            "Epoch [100/100], Loss: 11202.0299\n",
            "Fold 2, RMSE: 71.59175109863281\n",
            "Epoch [10/100], Loss: 14318.8384\n",
            "Epoch [20/100], Loss: 12501.0710\n",
            "Epoch [30/100], Loss: 14330.3848\n",
            "Epoch [40/100], Loss: 11979.7250\n",
            "Epoch [50/100], Loss: 14545.2402\n",
            "Epoch [60/100], Loss: 16245.7087\n",
            "Epoch [70/100], Loss: 12858.4568\n",
            "Epoch [80/100], Loss: 11828.3745\n",
            "Epoch [90/100], Loss: 16089.8503\n",
            "Epoch [100/100], Loss: 12039.4077\n",
            "Fold 3, RMSE: 109.85955810546875\n",
            "Epoch [10/100], Loss: 361260.6562\n",
            "Epoch [20/100], Loss: 51401.7490\n",
            "Epoch [30/100], Loss: 25164.3149\n",
            "Epoch [40/100], Loss: 20683.8359\n",
            "Epoch [50/100], Loss: 21532.2280\n",
            "Epoch [60/100], Loss: 19324.8569\n",
            "Epoch [70/100], Loss: 21280.4810\n",
            "Epoch [80/100], Loss: 14713.0179\n",
            "Epoch [90/100], Loss: 17245.6040\n",
            "Epoch [100/100], Loss: 11537.6724\n",
            "Fold 4, RMSE: 39.52806854248047\n",
            "Epoch [10/100], Loss: 192519.1445\n",
            "Epoch [20/100], Loss: 20017.4807\n",
            "Epoch [30/100], Loss: 15662.7798\n",
            "Epoch [40/100], Loss: 15915.7336\n",
            "Epoch [50/100], Loss: 16457.7319\n",
            "Epoch [60/100], Loss: 15755.4121\n",
            "Epoch [70/100], Loss: 20183.0186\n",
            "Epoch [80/100], Loss: 15855.5187\n",
            "Epoch [90/100], Loss: 18067.4409\n",
            "Epoch [100/100], Loss: 24706.1497\n",
            "Fold 5, RMSE: 53.9531364440918\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=100: 68.47266006469727\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 22892.2607\n",
            "Epoch [20/150], Loss: 24658.6450\n",
            "Epoch [30/150], Loss: 18818.2173\n",
            "Epoch [40/150], Loss: 16799.0659\n",
            "Epoch [50/150], Loss: 20769.1445\n",
            "Epoch [60/150], Loss: 17553.1907\n",
            "Epoch [70/150], Loss: 21883.7197\n",
            "Epoch [80/150], Loss: 20041.6846\n",
            "Epoch [90/150], Loss: 26440.0662\n",
            "Epoch [100/150], Loss: 22847.8408\n",
            "Epoch [110/150], Loss: 20019.3838\n",
            "Epoch [120/150], Loss: 24447.1943\n",
            "Epoch [130/150], Loss: 20215.0093\n",
            "Epoch [140/150], Loss: 20490.1777\n",
            "Epoch [150/150], Loss: 18816.2192\n",
            "Fold 1, RMSE: 67.53617858886719\n",
            "Epoch [10/150], Loss: 25061.1016\n",
            "Epoch [20/150], Loss: 17398.2761\n",
            "Epoch [30/150], Loss: 15393.0537\n",
            "Epoch [40/150], Loss: 15113.8223\n",
            "Epoch [50/150], Loss: 17653.6472\n",
            "Epoch [60/150], Loss: 15389.0322\n",
            "Epoch [70/150], Loss: 17154.1436\n",
            "Epoch [80/150], Loss: 14643.0194\n",
            "Epoch [90/150], Loss: 15113.8521\n",
            "Epoch [100/150], Loss: 27729.8357\n",
            "Epoch [110/150], Loss: 16820.6660\n",
            "Epoch [120/150], Loss: 13766.1077\n",
            "Epoch [130/150], Loss: 16644.0200\n",
            "Epoch [140/150], Loss: 22617.5803\n",
            "Epoch [150/150], Loss: 16419.1509\n",
            "Fold 2, RMSE: 87.34259033203125\n",
            "Epoch [10/150], Loss: 14671.3040\n",
            "Epoch [20/150], Loss: 11530.7224\n",
            "Epoch [30/150], Loss: 17210.0493\n",
            "Epoch [40/150], Loss: 11344.9910\n",
            "Epoch [50/150], Loss: 12470.6982\n",
            "Epoch [60/150], Loss: 12290.0171\n",
            "Epoch [70/150], Loss: 13265.5593\n",
            "Epoch [80/150], Loss: 15204.2876\n",
            "Epoch [90/150], Loss: 12296.5754\n",
            "Epoch [100/150], Loss: 12442.7854\n",
            "Epoch [110/150], Loss: 16392.5132\n",
            "Epoch [120/150], Loss: 14923.4037\n",
            "Epoch [130/150], Loss: 11698.1702\n",
            "Epoch [140/150], Loss: 13167.1289\n",
            "Epoch [150/150], Loss: 12684.7109\n",
            "Fold 3, RMSE: 109.65579223632812\n",
            "Epoch [10/150], Loss: 599551.3047\n",
            "Epoch [20/150], Loss: 42199.2129\n",
            "Epoch [30/150], Loss: 40038.3174\n",
            "Epoch [40/150], Loss: 26366.9995\n",
            "Epoch [50/150], Loss: 22835.0386\n",
            "Epoch [60/150], Loss: 17883.0596\n",
            "Epoch [70/150], Loss: 13831.3606\n",
            "Epoch [80/150], Loss: 15324.2073\n",
            "Epoch [90/150], Loss: 18509.3438\n",
            "Epoch [100/150], Loss: 16903.9468\n",
            "Epoch [110/150], Loss: 12802.5206\n",
            "Epoch [120/150], Loss: 15659.0986\n",
            "Epoch [130/150], Loss: 15980.1653\n",
            "Epoch [140/150], Loss: 15797.9419\n",
            "Epoch [150/150], Loss: 15709.7952\n",
            "Fold 4, RMSE: 40.41923141479492\n",
            "Epoch [10/150], Loss: 221465.9180\n",
            "Epoch [20/150], Loss: 37805.7314\n",
            "Epoch [30/150], Loss: 15993.1539\n",
            "Epoch [40/150], Loss: 19234.3379\n",
            "Epoch [50/150], Loss: 27486.1226\n",
            "Epoch [60/150], Loss: 22638.0735\n",
            "Epoch [70/150], Loss: 13103.7515\n",
            "Epoch [80/150], Loss: 10093.3013\n",
            "Epoch [90/150], Loss: 11677.8550\n",
            "Epoch [100/150], Loss: 10891.7939\n",
            "Epoch [110/150], Loss: 11041.7439\n",
            "Epoch [120/150], Loss: 13136.5781\n",
            "Epoch [130/150], Loss: 15651.2114\n",
            "Epoch [140/150], Loss: 6802.8802\n",
            "Epoch [150/150], Loss: 11575.2395\n",
            "Fold 5, RMSE: 46.03488540649414\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=2, epochs=150: 70.19773559570312\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 42985.2666\n",
            "Epoch [20/100], Loss: 13894.7617\n",
            "Epoch [30/100], Loss: 16183.4956\n",
            "Epoch [40/100], Loss: 13231.5081\n",
            "Epoch [50/100], Loss: 14064.8843\n",
            "Epoch [60/100], Loss: 14528.8845\n",
            "Epoch [70/100], Loss: 14505.7485\n",
            "Epoch [80/100], Loss: 9990.6755\n",
            "Epoch [90/100], Loss: 12227.9138\n",
            "Epoch [100/100], Loss: 14209.6348\n",
            "Fold 1, RMSE: 59.32212448120117\n",
            "Epoch [10/100], Loss: 30102.8677\n",
            "Epoch [20/100], Loss: 14424.2751\n",
            "Epoch [30/100], Loss: 14331.5549\n",
            "Epoch [40/100], Loss: 14731.1709\n",
            "Epoch [50/100], Loss: 12625.0990\n",
            "Epoch [60/100], Loss: 19187.7902\n",
            "Epoch [70/100], Loss: 11409.9604\n",
            "Epoch [80/100], Loss: 12174.2812\n",
            "Epoch [90/100], Loss: 19983.6824\n",
            "Epoch [100/100], Loss: 12096.0726\n",
            "Fold 2, RMSE: 78.10126495361328\n",
            "Epoch [10/100], Loss: 29006.1885\n",
            "Epoch [20/100], Loss: 20366.4019\n",
            "Epoch [30/100], Loss: 13477.7798\n",
            "Epoch [40/100], Loss: 9044.8262\n",
            "Epoch [50/100], Loss: 12137.1567\n",
            "Epoch [60/100], Loss: 11179.9749\n",
            "Epoch [70/100], Loss: 10408.0830\n",
            "Epoch [80/100], Loss: 10642.3447\n",
            "Epoch [90/100], Loss: 9650.0154\n",
            "Epoch [100/100], Loss: 9739.0938\n",
            "Fold 3, RMSE: 99.64991760253906\n",
            "Epoch [10/100], Loss: 21536.4170\n",
            "Epoch [20/100], Loss: 17210.8127\n",
            "Epoch [30/100], Loss: 20047.6025\n",
            "Epoch [40/100], Loss: 20509.9565\n",
            "Epoch [50/100], Loss: 25956.2456\n",
            "Epoch [60/100], Loss: 19596.1001\n",
            "Epoch [70/100], Loss: 18397.6382\n",
            "Epoch [80/100], Loss: 18000.3782\n",
            "Epoch [90/100], Loss: 19205.1704\n",
            "Epoch [100/100], Loss: 26640.8711\n",
            "Fold 4, RMSE: 52.07047653198242\n",
            "Epoch [10/100], Loss: 18634.3838\n",
            "Epoch [20/100], Loss: 25449.2930\n",
            "Epoch [30/100], Loss: 16539.7603\n",
            "Epoch [40/100], Loss: 19728.1704\n",
            "Epoch [50/100], Loss: 19077.4600\n",
            "Epoch [60/100], Loss: 15142.0906\n",
            "Epoch [70/100], Loss: 19152.2505\n",
            "Epoch [80/100], Loss: 19780.8511\n",
            "Epoch [90/100], Loss: 23664.8208\n",
            "Epoch [100/100], Loss: 12708.4985\n",
            "Fold 5, RMSE: 55.217227935791016\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=100: 68.8722023010254\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 111152.0840\n",
            "Epoch [20/150], Loss: 16544.4146\n",
            "Epoch [30/150], Loss: 16839.6765\n",
            "Epoch [40/150], Loss: 22274.6760\n",
            "Epoch [50/150], Loss: 15219.1577\n",
            "Epoch [60/150], Loss: 14386.3649\n",
            "Epoch [70/150], Loss: 17734.9775\n",
            "Epoch [80/150], Loss: 12937.6665\n",
            "Epoch [90/150], Loss: 18263.8638\n",
            "Epoch [100/150], Loss: 10948.9427\n",
            "Epoch [110/150], Loss: 12191.6738\n",
            "Epoch [120/150], Loss: 11927.5916\n",
            "Epoch [130/150], Loss: 12656.9678\n",
            "Epoch [140/150], Loss: 11961.6365\n",
            "Epoch [150/150], Loss: 12463.4253\n",
            "Fold 1, RMSE: 60.80512237548828\n",
            "Epoch [10/150], Loss: 118256.3281\n",
            "Epoch [20/150], Loss: 26000.7383\n",
            "Epoch [30/150], Loss: 18314.4399\n",
            "Epoch [40/150], Loss: 13313.7805\n",
            "Epoch [50/150], Loss: 18412.5540\n",
            "Epoch [60/150], Loss: 24695.7856\n",
            "Epoch [70/150], Loss: 16761.5801\n",
            "Epoch [80/150], Loss: 15069.7227\n",
            "Epoch [90/150], Loss: 13717.9490\n",
            "Epoch [100/150], Loss: 12059.6448\n",
            "Epoch [110/150], Loss: 16466.3845\n",
            "Epoch [120/150], Loss: 10261.7940\n",
            "Epoch [130/150], Loss: 10627.9592\n",
            "Epoch [140/150], Loss: 10217.5795\n",
            "Epoch [150/150], Loss: 15467.1252\n",
            "Fold 2, RMSE: 77.3409194946289\n",
            "Epoch [10/150], Loss: 73022.3262\n",
            "Epoch [20/150], Loss: 41736.9609\n",
            "Epoch [30/150], Loss: 12246.4966\n",
            "Epoch [40/150], Loss: 11629.1472\n",
            "Epoch [50/150], Loss: 9859.6084\n",
            "Epoch [60/150], Loss: 15671.7256\n",
            "Epoch [70/150], Loss: 11753.9119\n",
            "Epoch [80/150], Loss: 15592.1162\n",
            "Epoch [90/150], Loss: 9363.4016\n",
            "Epoch [100/150], Loss: 13723.5867\n",
            "Epoch [110/150], Loss: 8784.4580\n",
            "Epoch [120/150], Loss: 13996.9673\n",
            "Epoch [130/150], Loss: 14040.1398\n",
            "Epoch [140/150], Loss: 10632.8242\n",
            "Epoch [150/150], Loss: 10305.3049\n",
            "Fold 3, RMSE: 102.27655029296875\n",
            "Epoch [10/150], Loss: 14691.8441\n",
            "Epoch [20/150], Loss: 16559.1665\n",
            "Epoch [30/150], Loss: 15806.4180\n",
            "Epoch [40/150], Loss: 14269.6475\n",
            "Epoch [50/150], Loss: 12963.6257\n",
            "Epoch [60/150], Loss: 16498.9675\n",
            "Epoch [70/150], Loss: 16034.0920\n",
            "Epoch [80/150], Loss: 14674.1199\n",
            "Epoch [90/150], Loss: 15504.8192\n",
            "Epoch [100/150], Loss: 13714.1858\n",
            "Epoch [110/150], Loss: 14932.1238\n",
            "Epoch [120/150], Loss: 10457.8309\n",
            "Epoch [130/150], Loss: 13697.7737\n",
            "Epoch [140/150], Loss: 6670.4738\n",
            "Epoch [150/150], Loss: 13046.1211\n",
            "Fold 4, RMSE: 45.10270309448242\n",
            "Epoch [10/150], Loss: 37683.4233\n",
            "Epoch [20/150], Loss: 12720.6001\n",
            "Epoch [30/150], Loss: 16822.9722\n",
            "Epoch [40/150], Loss: 19144.9172\n",
            "Epoch [50/150], Loss: 19783.0220\n",
            "Epoch [60/150], Loss: 12326.0952\n",
            "Epoch [70/150], Loss: 14483.0364\n",
            "Epoch [80/150], Loss: 10942.3489\n",
            "Epoch [90/150], Loss: 18903.2222\n",
            "Epoch [100/150], Loss: 9226.4214\n",
            "Epoch [110/150], Loss: 10497.8665\n",
            "Epoch [120/150], Loss: 12318.1018\n",
            "Epoch [130/150], Loss: 8420.3322\n",
            "Epoch [140/150], Loss: 9701.7487\n",
            "Epoch [150/150], Loss: 10722.2952\n",
            "Fold 5, RMSE: 48.678077697753906\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0, num_layers=3, epochs=150: 66.84067459106446\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 98631.5742\n",
            "Epoch [20/100], Loss: 15934.3276\n",
            "Epoch [30/100], Loss: 16148.0510\n",
            "Epoch [40/100], Loss: 18420.7261\n",
            "Epoch [50/100], Loss: 12844.1636\n",
            "Epoch [60/100], Loss: 14919.0322\n",
            "Epoch [70/100], Loss: 17799.8181\n",
            "Epoch [80/100], Loss: 12439.4956\n",
            "Epoch [90/100], Loss: 10551.1055\n",
            "Epoch [100/100], Loss: 11733.9988\n",
            "Fold 1, RMSE: 56.83012008666992\n",
            "Epoch [10/100], Loss: 15146.7219\n",
            "Epoch [20/100], Loss: 14334.9773\n",
            "Epoch [30/100], Loss: 17970.3450\n",
            "Epoch [40/100], Loss: 16133.2651\n",
            "Epoch [50/100], Loss: 24468.6233\n",
            "Epoch [60/100], Loss: 16347.6768\n",
            "Epoch [70/100], Loss: 15766.4348\n",
            "Epoch [80/100], Loss: 14918.2017\n",
            "Epoch [90/100], Loss: 17641.8838\n",
            "Epoch [100/100], Loss: 16790.6685\n",
            "Fold 2, RMSE: 89.85717010498047\n",
            "Epoch [10/100], Loss: 713302.8750\n",
            "Epoch [20/100], Loss: 88880.0312\n",
            "Epoch [30/100], Loss: 56459.4023\n",
            "Epoch [40/100], Loss: 41322.9814\n",
            "Epoch [50/100], Loss: 36218.2104\n",
            "Epoch [60/100], Loss: 21431.9585\n",
            "Epoch [70/100], Loss: 14698.7413\n",
            "Epoch [80/100], Loss: 20849.8804\n",
            "Epoch [90/100], Loss: 17818.4600\n",
            "Epoch [100/100], Loss: 12644.0593\n",
            "Fold 3, RMSE: 101.94942474365234\n",
            "Epoch [10/100], Loss: 138984.4023\n",
            "Epoch [20/100], Loss: 20004.4937\n",
            "Epoch [30/100], Loss: 13943.8954\n",
            "Epoch [40/100], Loss: 17489.3047\n",
            "Epoch [50/100], Loss: 14203.9214\n",
            "Epoch [60/100], Loss: 13562.5605\n",
            "Epoch [70/100], Loss: 14110.3037\n",
            "Epoch [80/100], Loss: 19374.5728\n",
            "Epoch [90/100], Loss: 11658.9927\n",
            "Epoch [100/100], Loss: 18152.6250\n",
            "Fold 4, RMSE: 39.18163299560547\n",
            "Epoch [10/100], Loss: 657291.8594\n",
            "Epoch [20/100], Loss: 110665.0957\n",
            "Epoch [30/100], Loss: 58751.9775\n",
            "Epoch [40/100], Loss: 28476.5522\n",
            "Epoch [50/100], Loss: 21234.8162\n",
            "Epoch [60/100], Loss: 61149.8521\n",
            "Epoch [70/100], Loss: 24243.6279\n",
            "Epoch [80/100], Loss: 20321.3462\n",
            "Epoch [90/100], Loss: 16316.8872\n",
            "Epoch [100/100], Loss: 16965.7891\n",
            "Fold 5, RMSE: 53.37696075439453\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=100: 68.23906173706055\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 739171.5156\n",
            "Epoch [20/150], Loss: 60011.0513\n",
            "Epoch [30/150], Loss: 46749.8730\n",
            "Epoch [40/150], Loss: 35435.3252\n",
            "Epoch [50/150], Loss: 36506.6074\n",
            "Epoch [60/150], Loss: 27592.9980\n",
            "Epoch [70/150], Loss: 15966.8882\n",
            "Epoch [80/150], Loss: 18096.3325\n",
            "Epoch [90/150], Loss: 23995.2964\n",
            "Epoch [100/150], Loss: 22341.8958\n",
            "Epoch [110/150], Loss: 17289.4155\n",
            "Epoch [120/150], Loss: 20567.0288\n",
            "Epoch [130/150], Loss: 16942.9839\n",
            "Epoch [140/150], Loss: 16378.4424\n",
            "Epoch [150/150], Loss: 20809.0515\n",
            "Fold 1, RMSE: 53.08913803100586\n",
            "Epoch [10/150], Loss: 655918.3125\n",
            "Epoch [20/150], Loss: 72041.4473\n",
            "Epoch [30/150], Loss: 36550.8564\n",
            "Epoch [40/150], Loss: 21549.9529\n",
            "Epoch [50/150], Loss: 19157.8916\n",
            "Epoch [60/150], Loss: 26477.9751\n",
            "Epoch [70/150], Loss: 17209.2974\n",
            "Epoch [80/150], Loss: 12393.8501\n",
            "Epoch [90/150], Loss: 11594.8777\n",
            "Epoch [100/150], Loss: 16260.5713\n",
            "Epoch [110/150], Loss: 13152.7625\n",
            "Epoch [120/150], Loss: 14476.5576\n",
            "Epoch [130/150], Loss: 9302.3145\n",
            "Epoch [140/150], Loss: 16418.0581\n",
            "Epoch [150/150], Loss: 16891.6689\n",
            "Fold 2, RMSE: 75.77555084228516\n",
            "Epoch [10/150], Loss: 452014.1641\n",
            "Epoch [20/150], Loss: 38042.3188\n",
            "Epoch [30/150], Loss: 32778.8223\n",
            "Epoch [40/150], Loss: 14815.2500\n",
            "Epoch [50/150], Loss: 16516.8894\n",
            "Epoch [60/150], Loss: 11956.5815\n",
            "Epoch [70/150], Loss: 14142.0615\n",
            "Epoch [80/150], Loss: 12434.0505\n",
            "Epoch [90/150], Loss: 11498.9814\n",
            "Epoch [100/150], Loss: 10610.6675\n",
            "Epoch [110/150], Loss: 11387.8000\n",
            "Epoch [120/150], Loss: 9562.8400\n",
            "Epoch [130/150], Loss: 9216.7075\n",
            "Epoch [140/150], Loss: 9076.8459\n",
            "Epoch [150/150], Loss: 10711.7036\n",
            "Fold 3, RMSE: 97.97007751464844\n",
            "Epoch [10/150], Loss: 497175.1016\n",
            "Epoch [20/150], Loss: 99333.5723\n",
            "Epoch [30/150], Loss: 38683.8916\n",
            "Epoch [40/150], Loss: 27502.6387\n",
            "Epoch [50/150], Loss: 22728.7075\n",
            "Epoch [60/150], Loss: 28768.0225\n",
            "Epoch [70/150], Loss: 24482.0205\n",
            "Epoch [80/150], Loss: 24307.7178\n",
            "Epoch [90/150], Loss: 20918.9329\n",
            "Epoch [100/150], Loss: 14620.2393\n",
            "Epoch [110/150], Loss: 22767.3423\n",
            "Epoch [120/150], Loss: 15908.7083\n",
            "Epoch [130/150], Loss: 12461.2412\n",
            "Epoch [140/150], Loss: 11224.9255\n",
            "Epoch [150/150], Loss: 9228.3223\n",
            "Fold 4, RMSE: 42.55834197998047\n",
            "Epoch [10/150], Loss: 264374.9844\n",
            "Epoch [20/150], Loss: 28778.4761\n",
            "Epoch [30/150], Loss: 24363.2886\n",
            "Epoch [40/150], Loss: 19728.5935\n",
            "Epoch [50/150], Loss: 16739.7458\n",
            "Epoch [60/150], Loss: 19179.0601\n",
            "Epoch [70/150], Loss: 24555.8032\n",
            "Epoch [80/150], Loss: 15821.5864\n",
            "Epoch [90/150], Loss: 15917.4824\n",
            "Epoch [100/150], Loss: 18321.2246\n",
            "Epoch [110/150], Loss: 20769.4404\n",
            "Epoch [120/150], Loss: 18615.1418\n",
            "Epoch [130/150], Loss: 18040.2881\n",
            "Epoch [140/150], Loss: 17537.1226\n",
            "Epoch [150/150], Loss: 22256.8281\n",
            "Fold 5, RMSE: 46.37966537475586\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=2, epochs=150: 63.154554748535155\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 115264.8867\n",
            "Epoch [20/100], Loss: 38346.1904\n",
            "Epoch [30/100], Loss: 21328.7375\n",
            "Epoch [40/100], Loss: 15093.3044\n",
            "Epoch [50/100], Loss: 13676.6711\n",
            "Epoch [60/100], Loss: 14749.1680\n",
            "Epoch [70/100], Loss: 24143.3154\n",
            "Epoch [80/100], Loss: 19336.3958\n",
            "Epoch [90/100], Loss: 14808.2808\n",
            "Epoch [100/100], Loss: 15118.4131\n",
            "Fold 1, RMSE: 56.58867645263672\n",
            "Epoch [10/100], Loss: 70419.4473\n",
            "Epoch [20/100], Loss: 13801.5303\n",
            "Epoch [30/100], Loss: 18778.6990\n",
            "Epoch [40/100], Loss: 14438.2690\n",
            "Epoch [50/100], Loss: 14690.6560\n",
            "Epoch [60/100], Loss: 20372.8464\n",
            "Epoch [70/100], Loss: 10923.4938\n",
            "Epoch [80/100], Loss: 13898.8000\n",
            "Epoch [90/100], Loss: 11607.0786\n",
            "Epoch [100/100], Loss: 18885.9216\n",
            "Fold 2, RMSE: 77.6409912109375\n",
            "Epoch [10/100], Loss: 66742.8857\n",
            "Epoch [20/100], Loss: 11239.4143\n",
            "Epoch [30/100], Loss: 19146.8481\n",
            "Epoch [40/100], Loss: 11271.4558\n",
            "Epoch [50/100], Loss: 11678.5623\n",
            "Epoch [60/100], Loss: 14511.9844\n",
            "Epoch [70/100], Loss: 21567.6875\n",
            "Epoch [80/100], Loss: 8410.3904\n",
            "Epoch [90/100], Loss: 8790.5623\n",
            "Epoch [100/100], Loss: 10276.4402\n",
            "Fold 3, RMSE: 102.22997283935547\n",
            "Epoch [10/100], Loss: 274791.8828\n",
            "Epoch [20/100], Loss: 26826.9517\n",
            "Epoch [30/100], Loss: 26350.0615\n",
            "Epoch [40/100], Loss: 18509.9712\n",
            "Epoch [50/100], Loss: 25442.0630\n",
            "Epoch [60/100], Loss: 17453.8943\n",
            "Epoch [70/100], Loss: 14412.6094\n",
            "Epoch [80/100], Loss: 15515.3395\n",
            "Epoch [90/100], Loss: 13363.6592\n",
            "Epoch [100/100], Loss: 22816.8574\n",
            "Fold 4, RMSE: 40.917076110839844\n",
            "Epoch [10/100], Loss: 109714.0488\n",
            "Epoch [20/100], Loss: 30216.2832\n",
            "Epoch [30/100], Loss: 24566.6357\n",
            "Epoch [40/100], Loss: 26764.7754\n",
            "Epoch [50/100], Loss: 22988.5698\n",
            "Epoch [60/100], Loss: 17994.4158\n",
            "Epoch [70/100], Loss: 16999.5334\n",
            "Epoch [80/100], Loss: 16291.4495\n",
            "Epoch [90/100], Loss: 17488.7061\n",
            "Epoch [100/100], Loss: 21107.7217\n",
            "Fold 5, RMSE: 50.035213470458984\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=100: 65.48238601684571\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 133618.3535\n",
            "Epoch [20/150], Loss: 27717.3359\n",
            "Epoch [30/150], Loss: 20346.9019\n",
            "Epoch [40/150], Loss: 16946.0767\n",
            "Epoch [50/150], Loss: 18648.8364\n",
            "Epoch [60/150], Loss: 18876.5615\n",
            "Epoch [70/150], Loss: 13433.5171\n",
            "Epoch [80/150], Loss: 13424.0784\n",
            "Epoch [90/150], Loss: 12670.8748\n",
            "Epoch [100/150], Loss: 15860.0200\n",
            "Epoch [110/150], Loss: 12140.1045\n",
            "Epoch [120/150], Loss: 12863.9379\n",
            "Epoch [130/150], Loss: 12145.6714\n",
            "Epoch [140/150], Loss: 11743.2588\n",
            "Epoch [150/150], Loss: 8856.1901\n",
            "Fold 1, RMSE: 55.313446044921875\n",
            "Epoch [10/150], Loss: 759411.0469\n",
            "Epoch [20/150], Loss: 120425.0547\n",
            "Epoch [30/150], Loss: 40085.6245\n",
            "Epoch [40/150], Loss: 22931.7339\n",
            "Epoch [50/150], Loss: 27082.1060\n",
            "Epoch [60/150], Loss: 22418.2051\n",
            "Epoch [70/150], Loss: 34088.7949\n",
            "Epoch [80/150], Loss: 14128.6276\n",
            "Epoch [90/150], Loss: 23289.9102\n",
            "Epoch [100/150], Loss: 12429.6610\n",
            "Epoch [110/150], Loss: 15423.0339\n",
            "Epoch [120/150], Loss: 19940.7280\n",
            "Epoch [130/150], Loss: 13093.3563\n",
            "Epoch [140/150], Loss: 13266.9185\n",
            "Epoch [150/150], Loss: 10203.7223\n",
            "Fold 2, RMSE: 76.58690643310547\n",
            "Epoch [10/150], Loss: 157121.6680\n",
            "Epoch [20/150], Loss: 33730.2476\n",
            "Epoch [30/150], Loss: 17817.9019\n",
            "Epoch [40/150], Loss: 10836.3496\n",
            "Epoch [50/150], Loss: 11774.3240\n",
            "Epoch [60/150], Loss: 10871.4299\n",
            "Epoch [70/150], Loss: 13655.4709\n",
            "Epoch [80/150], Loss: 10456.9553\n",
            "Epoch [90/150], Loss: 8123.8745\n",
            "Epoch [100/150], Loss: 8222.5272\n",
            "Epoch [110/150], Loss: 9469.5981\n",
            "Epoch [120/150], Loss: 10796.1057\n",
            "Epoch [130/150], Loss: 6542.0812\n",
            "Epoch [140/150], Loss: 11178.1433\n",
            "Epoch [150/150], Loss: 10637.7166\n",
            "Fold 3, RMSE: 97.74005889892578\n",
            "Epoch [10/150], Loss: 315748.4648\n",
            "Epoch [20/150], Loss: 30447.9512\n",
            "Epoch [30/150], Loss: 20480.9639\n",
            "Epoch [40/150], Loss: 20010.5615\n",
            "Epoch [50/150], Loss: 17538.7329\n",
            "Epoch [60/150], Loss: 16599.6138\n",
            "Epoch [70/150], Loss: 15841.8120\n",
            "Epoch [80/150], Loss: 12276.5476\n",
            "Epoch [90/150], Loss: 14777.3093\n",
            "Epoch [100/150], Loss: 19053.5925\n",
            "Epoch [110/150], Loss: 14046.1477\n",
            "Epoch [120/150], Loss: 12765.9165\n",
            "Epoch [130/150], Loss: 18466.6323\n",
            "Epoch [140/150], Loss: 12880.2952\n",
            "Epoch [150/150], Loss: 19154.2561\n",
            "Fold 4, RMSE: 45.044464111328125\n",
            "Epoch [10/150], Loss: 246616.4922\n",
            "Epoch [20/150], Loss: 49755.2178\n",
            "Epoch [30/150], Loss: 26159.0332\n",
            "Epoch [40/150], Loss: 30788.2334\n",
            "Epoch [50/150], Loss: 19144.0396\n",
            "Epoch [60/150], Loss: 19845.4561\n",
            "Epoch [70/150], Loss: 27663.9238\n",
            "Epoch [80/150], Loss: 14285.1161\n",
            "Epoch [90/150], Loss: 14082.8429\n",
            "Epoch [100/150], Loss: 16281.4797\n",
            "Epoch [110/150], Loss: 17729.0439\n",
            "Epoch [120/150], Loss: 12511.8516\n",
            "Epoch [130/150], Loss: 18683.4165\n",
            "Epoch [140/150], Loss: 19422.3169\n",
            "Epoch [150/150], Loss: 12129.4207\n",
            "Fold 5, RMSE: 49.08256912231445\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=0.0001, num_layers=3, epochs=150: 64.75348892211915\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100\n",
            "Epoch [10/100], Loss: 481019.1875\n",
            "Epoch [20/100], Loss: 124277.6250\n",
            "Epoch [30/100], Loss: 56024.9092\n",
            "Epoch [40/100], Loss: 26618.3345\n",
            "Epoch [50/100], Loss: 17588.1839\n",
            "Epoch [60/100], Loss: 24547.2739\n",
            "Epoch [70/100], Loss: 17054.7559\n",
            "Epoch [80/100], Loss: 18586.3354\n",
            "Epoch [90/100], Loss: 16105.5083\n",
            "Epoch [100/100], Loss: 15786.0649\n",
            "Fold 1, RMSE: 51.23920822143555\n",
            "Epoch [10/100], Loss: 151515.3320\n",
            "Epoch [20/100], Loss: 26159.1084\n",
            "Epoch [30/100], Loss: 16104.4788\n",
            "Epoch [40/100], Loss: 15275.1299\n",
            "Epoch [50/100], Loss: 14392.8945\n",
            "Epoch [60/100], Loss: 17496.4934\n",
            "Epoch [70/100], Loss: 14722.9226\n",
            "Epoch [80/100], Loss: 10518.2705\n",
            "Epoch [90/100], Loss: 12091.6899\n",
            "Epoch [100/100], Loss: 10989.7070\n",
            "Fold 2, RMSE: 84.23497009277344\n",
            "Epoch [10/100], Loss: 408765.4297\n",
            "Epoch [20/100], Loss: 56812.2832\n",
            "Epoch [30/100], Loss: 15967.9851\n",
            "Epoch [40/100], Loss: 17785.2432\n",
            "Epoch [50/100], Loss: 16629.1047\n",
            "Epoch [60/100], Loss: 14172.9939\n",
            "Epoch [70/100], Loss: 12688.0620\n",
            "Epoch [80/100], Loss: 8228.1351\n",
            "Epoch [90/100], Loss: 7593.1067\n",
            "Epoch [100/100], Loss: 9391.1139\n",
            "Fold 3, RMSE: 97.85816955566406\n",
            "Epoch [10/100], Loss: 1094823.8125\n",
            "Epoch [20/100], Loss: 67280.0947\n",
            "Epoch [30/100], Loss: 31631.5083\n",
            "Epoch [40/100], Loss: 23402.3726\n",
            "Epoch [50/100], Loss: 21028.9861\n",
            "Epoch [60/100], Loss: 17122.3755\n",
            "Epoch [70/100], Loss: 20078.6011\n",
            "Epoch [80/100], Loss: 24288.9316\n",
            "Epoch [90/100], Loss: 14428.1980\n",
            "Epoch [100/100], Loss: 18807.7212\n",
            "Fold 4, RMSE: 40.45559310913086\n",
            "Epoch [10/100], Loss: 317248.1328\n",
            "Epoch [20/100], Loss: 54108.2637\n",
            "Epoch [30/100], Loss: 36321.3896\n",
            "Epoch [40/100], Loss: 24537.9634\n",
            "Epoch [50/100], Loss: 20207.1279\n",
            "Epoch [60/100], Loss: 13768.3783\n",
            "Epoch [70/100], Loss: 15945.7234\n",
            "Epoch [80/100], Loss: 15776.5647\n",
            "Epoch [90/100], Loss: 17550.8804\n",
            "Epoch [100/100], Loss: 14936.1580\n",
            "Fold 5, RMSE: 49.39437484741211\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=100: 64.6364631652832\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150\n",
            "Epoch [10/150], Loss: 406431.0000\n",
            "Epoch [20/150], Loss: 53352.7354\n",
            "Epoch [30/150], Loss: 28108.1689\n",
            "Epoch [40/150], Loss: 35263.1309\n",
            "Epoch [50/150], Loss: 17214.4209\n",
            "Epoch [60/150], Loss: 29422.5779\n",
            "Epoch [70/150], Loss: 16405.8125\n",
            "Epoch [80/150], Loss: 18714.2573\n",
            "Epoch [90/150], Loss: 15400.5454\n",
            "Epoch [100/150], Loss: 12962.5183\n",
            "Epoch [110/150], Loss: 14019.0398\n",
            "Epoch [120/150], Loss: 17431.4954\n",
            "Epoch [130/150], Loss: 12100.1885\n",
            "Epoch [140/150], Loss: 11008.5295\n",
            "Epoch [150/150], Loss: 15721.2637\n",
            "Fold 1, RMSE: 52.01097869873047\n",
            "Epoch [10/150], Loss: 63582.8057\n",
            "Epoch [20/150], Loss: 15394.6157\n",
            "Epoch [30/150], Loss: 15389.7040\n",
            "Epoch [40/150], Loss: 22990.8379\n",
            "Epoch [50/150], Loss: 16843.2048\n",
            "Epoch [60/150], Loss: 14738.2627\n",
            "Epoch [70/150], Loss: 27521.4607\n",
            "Epoch [80/150], Loss: 16017.4592\n",
            "Epoch [90/150], Loss: 16587.0989\n",
            "Epoch [100/150], Loss: 14945.2107\n",
            "Epoch [110/150], Loss: 17410.3752\n",
            "Epoch [120/150], Loss: 16853.3157\n",
            "Epoch [130/150], Loss: 15374.3625\n",
            "Epoch [140/150], Loss: 14383.1388\n",
            "Epoch [150/150], Loss: 14784.5020\n",
            "Fold 2, RMSE: 87.31156158447266\n",
            "Epoch [10/150], Loss: 427495.3516\n",
            "Epoch [20/150], Loss: 104361.4810\n",
            "Epoch [30/150], Loss: 31855.4253\n",
            "Epoch [40/150], Loss: 30672.1064\n",
            "Epoch [50/150], Loss: 20860.7002\n",
            "Epoch [60/150], Loss: 16358.3020\n",
            "Epoch [70/150], Loss: 11329.6995\n",
            "Epoch [80/150], Loss: 14916.1467\n",
            "Epoch [90/150], Loss: 10718.7192\n",
            "Epoch [100/150], Loss: 9367.7759\n",
            "Epoch [110/150], Loss: 12562.2949\n",
            "Epoch [120/150], Loss: 8944.2615\n",
            "Epoch [130/150], Loss: 9321.4707\n",
            "Epoch [140/150], Loss: 10274.1528\n",
            "Epoch [150/150], Loss: 9601.8838\n",
            "Fold 3, RMSE: 95.88198852539062\n",
            "Epoch [10/150], Loss: 150233.8672\n",
            "Epoch [20/150], Loss: 41298.3721\n",
            "Epoch [30/150], Loss: 24136.2046\n",
            "Epoch [40/150], Loss: 15753.8832\n",
            "Epoch [50/150], Loss: 21047.0791\n",
            "Epoch [60/150], Loss: 21410.2573\n",
            "Epoch [70/150], Loss: 23130.3916\n",
            "Epoch [80/150], Loss: 11698.2087\n",
            "Epoch [90/150], Loss: 23048.7024\n",
            "Epoch [100/150], Loss: 12229.4479\n",
            "Epoch [110/150], Loss: 13046.4722\n",
            "Epoch [120/150], Loss: 13092.0441\n",
            "Epoch [130/150], Loss: 15675.0593\n",
            "Epoch [140/150], Loss: 11685.5741\n",
            "Epoch [150/150], Loss: 10361.9884\n",
            "Fold 4, RMSE: 38.94932174682617\n",
            "Epoch [10/150], Loss: 605779.8594\n",
            "Epoch [20/150], Loss: 39502.7412\n",
            "Epoch [30/150], Loss: 27916.7461\n",
            "Epoch [40/150], Loss: 21882.4395\n",
            "Epoch [50/150], Loss: 14474.3584\n",
            "Epoch [60/150], Loss: 12577.0522\n",
            "Epoch [70/150], Loss: 24131.0073\n",
            "Epoch [80/150], Loss: 15984.5745\n",
            "Epoch [90/150], Loss: 15148.1111\n",
            "Epoch [100/150], Loss: 11346.9407\n",
            "Epoch [110/150], Loss: 11241.4156\n",
            "Epoch [120/150], Loss: 14472.7839\n",
            "Epoch [130/150], Loss: 14814.4966\n",
            "Epoch [140/150], Loss: 14534.8862\n",
            "Epoch [150/150], Loss: 15696.3982\n",
            "Fold 5, RMSE: 48.968082427978516\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=2, epochs=150: 64.62438659667968\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100\n",
            "Epoch [10/100], Loss: 46504.6895\n",
            "Epoch [20/100], Loss: 22276.4888\n",
            "Epoch [30/100], Loss: 20858.1877\n",
            "Epoch [40/100], Loss: 20521.2798\n",
            "Epoch [50/100], Loss: 14032.2119\n",
            "Epoch [60/100], Loss: 12392.5231\n",
            "Epoch [70/100], Loss: 13479.7214\n",
            "Epoch [80/100], Loss: 15036.7231\n",
            "Epoch [90/100], Loss: 15266.9272\n",
            "Epoch [100/100], Loss: 13324.9065\n",
            "Fold 1, RMSE: 58.97076416015625\n",
            "Epoch [10/100], Loss: 416840.6250\n",
            "Epoch [20/100], Loss: 60925.2148\n",
            "Epoch [30/100], Loss: 56542.4814\n",
            "Epoch [40/100], Loss: 20610.6665\n",
            "Epoch [50/100], Loss: 14165.7070\n",
            "Epoch [60/100], Loss: 16770.2842\n",
            "Epoch [70/100], Loss: 15129.1990\n",
            "Epoch [80/100], Loss: 14867.2275\n",
            "Epoch [90/100], Loss: 14905.8975\n",
            "Epoch [100/100], Loss: 33519.9790\n",
            "Fold 2, RMSE: 80.5835189819336\n",
            "Epoch [10/100], Loss: 196985.8555\n",
            "Epoch [20/100], Loss: 28838.1426\n",
            "Epoch [30/100], Loss: 14811.6694\n",
            "Epoch [40/100], Loss: 16958.6538\n",
            "Epoch [50/100], Loss: 13676.8398\n",
            "Epoch [60/100], Loss: 9889.1814\n",
            "Epoch [70/100], Loss: 10226.1646\n",
            "Epoch [80/100], Loss: 7819.6670\n",
            "Epoch [90/100], Loss: 7089.6707\n",
            "Epoch [100/100], Loss: 8922.6860\n",
            "Fold 3, RMSE: 101.70515441894531\n",
            "Epoch [10/100], Loss: 134805.2910\n",
            "Epoch [20/100], Loss: 27160.7065\n",
            "Epoch [30/100], Loss: 27167.2642\n",
            "Epoch [40/100], Loss: 20218.6797\n",
            "Epoch [50/100], Loss: 19667.3074\n",
            "Epoch [60/100], Loss: 21970.5432\n",
            "Epoch [70/100], Loss: 14627.4937\n",
            "Epoch [80/100], Loss: 17682.1792\n",
            "Epoch [90/100], Loss: 18270.0308\n",
            "Epoch [100/100], Loss: 15159.6055\n",
            "Fold 4, RMSE: 48.11321258544922\n",
            "Epoch [10/100], Loss: 201713.9688\n",
            "Epoch [20/100], Loss: 25914.5811\n",
            "Epoch [30/100], Loss: 14971.0466\n",
            "Epoch [40/100], Loss: 21807.5044\n",
            "Epoch [50/100], Loss: 16408.7544\n",
            "Epoch [60/100], Loss: 18670.9194\n",
            "Epoch [70/100], Loss: 14032.1870\n",
            "Epoch [80/100], Loss: 9782.7527\n",
            "Epoch [90/100], Loss: 13084.6198\n",
            "Epoch [100/100], Loss: 16836.1670\n",
            "Fold 5, RMSE: 51.99221420288086\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=100: 68.27297286987304\n",
            "Training with neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150\n",
            "Epoch [10/150], Loss: 45263.0918\n",
            "Epoch [20/150], Loss: 12703.1785\n",
            "Epoch [30/150], Loss: 19756.5728\n",
            "Epoch [40/150], Loss: 14400.5229\n",
            "Epoch [50/150], Loss: 11929.8596\n",
            "Epoch [60/150], Loss: 14874.0635\n",
            "Epoch [70/150], Loss: 10823.9862\n",
            "Epoch [80/150], Loss: 10581.3682\n",
            "Epoch [90/150], Loss: 12296.7351\n",
            "Epoch [100/150], Loss: 9386.1167\n",
            "Epoch [110/150], Loss: 11541.7544\n",
            "Epoch [120/150], Loss: 12094.1313\n",
            "Epoch [130/150], Loss: 9356.2810\n",
            "Epoch [140/150], Loss: 9324.9668\n",
            "Epoch [150/150], Loss: 10094.3647\n",
            "Fold 1, RMSE: 57.0522346496582\n",
            "Epoch [10/150], Loss: 153178.0918\n",
            "Epoch [20/150], Loss: 38005.5986\n",
            "Epoch [30/150], Loss: 19347.6611\n",
            "Epoch [40/150], Loss: 21338.5591\n",
            "Epoch [50/150], Loss: 13329.4326\n",
            "Epoch [60/150], Loss: 20587.9319\n",
            "Epoch [70/150], Loss: 12968.3508\n",
            "Epoch [80/150], Loss: 11889.7151\n",
            "Epoch [90/150], Loss: 9521.6124\n",
            "Epoch [100/150], Loss: 13138.3311\n",
            "Epoch [110/150], Loss: 13550.4192\n",
            "Epoch [120/150], Loss: 14042.9944\n",
            "Epoch [130/150], Loss: 11280.0424\n",
            "Epoch [140/150], Loss: 12333.0684\n",
            "Epoch [150/150], Loss: 10153.7120\n",
            "Fold 2, RMSE: 78.44873046875\n",
            "Epoch [10/150], Loss: 238992.4844\n",
            "Epoch [20/150], Loss: 22530.4336\n",
            "Epoch [30/150], Loss: 22780.4791\n",
            "Epoch [40/150], Loss: 12260.0145\n",
            "Epoch [50/150], Loss: 12722.6477\n",
            "Epoch [60/150], Loss: 11278.4304\n",
            "Epoch [70/150], Loss: 15118.9541\n",
            "Epoch [80/150], Loss: 9421.6309\n",
            "Epoch [90/150], Loss: 8098.5883\n",
            "Epoch [100/150], Loss: 10997.1448\n",
            "Epoch [110/150], Loss: 10649.5208\n",
            "Epoch [120/150], Loss: 10393.9238\n",
            "Epoch [130/150], Loss: 7934.0295\n",
            "Epoch [140/150], Loss: 9594.1702\n",
            "Epoch [150/150], Loss: 12062.5137\n",
            "Fold 3, RMSE: 102.0968246459961\n",
            "Epoch [10/150], Loss: 168954.7129\n",
            "Epoch [20/150], Loss: 47606.1826\n",
            "Epoch [30/150], Loss: 42478.6982\n",
            "Epoch [40/150], Loss: 23977.0850\n",
            "Epoch [50/150], Loss: 22700.0337\n",
            "Epoch [60/150], Loss: 18470.3999\n",
            "Epoch [70/150], Loss: 19040.5361\n",
            "Epoch [80/150], Loss: 15513.4375\n",
            "Epoch [90/150], Loss: 22425.2812\n",
            "Epoch [100/150], Loss: 13784.2773\n",
            "Epoch [110/150], Loss: 15926.8101\n",
            "Epoch [120/150], Loss: 14264.4551\n",
            "Epoch [130/150], Loss: 15609.3230\n",
            "Epoch [140/150], Loss: 14301.8726\n",
            "Epoch [150/150], Loss: 17920.3203\n",
            "Fold 4, RMSE: 41.7703971862793\n",
            "Epoch [10/150], Loss: 189217.9102\n",
            "Epoch [20/150], Loss: 38919.0244\n",
            "Epoch [30/150], Loss: 28395.4321\n",
            "Epoch [40/150], Loss: 14523.9659\n",
            "Epoch [50/150], Loss: 16043.1472\n",
            "Epoch [60/150], Loss: 13171.4078\n",
            "Epoch [70/150], Loss: 14558.6802\n",
            "Epoch [80/150], Loss: 13698.6367\n",
            "Epoch [90/150], Loss: 21195.9807\n",
            "Epoch [100/150], Loss: 10907.3473\n",
            "Epoch [110/150], Loss: 11178.4524\n",
            "Epoch [120/150], Loss: 18007.0601\n",
            "Epoch [130/150], Loss: 13379.0730\n",
            "Epoch [140/150], Loss: 20472.2471\n",
            "Epoch [150/150], Loss: 9307.7432\n",
            "Fold 5, RMSE: 50.40127944946289\n",
            "Avg RMSE for neurons=128, dropout_rate=0.5, learning_rate=0.005, weight_decay=1e-05, num_layers=3, epochs=150: 65.95389328002929\n",
            "Best RMSE: 55.140906524658206\n",
            "Best Params: (96, 0.3, 0.005, 1e-05, 2, 150)\n",
            "Model 4 RMSE: 55.140906524658206\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model4_rmse = model_with_tuning(X_combined, y)\n",
        "print(f\"Model 4 RMSE: {model4_rmse}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}