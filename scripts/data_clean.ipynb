{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files have been merged and saved as 'merged_output.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "df1 = pd.read_csv('output_1.csv')\n",
    "df2 = pd.read_csv('output.csv')\n",
    "\n",
    "# Merge the DataFrames on 'ID' from df1 and 'subject_id' from df2\n",
    "merged_df = pd.merge(df1, df2, left_on='ID', right_on='subject_id', how='inner')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_output.csv', index=False)\n",
    "\n",
    "print(\"The files have been merged and saved as 'merged_output.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before removal:\n",
      "ID                 0\n",
      "age                0\n",
      "sex                0\n",
      "HOC                0\n",
      "dInj               0\n",
      "dRecov            10\n",
      "subject_id         0\n",
      "file_name          0\n",
      "mean_intensity     0\n",
      "std_intensity      0\n",
      "max_intensity      0\n",
      "min_intensity      0\n",
      "shape              0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after removal:\n",
      "ID                0\n",
      "age               0\n",
      "sex               0\n",
      "HOC               0\n",
      "dInj              0\n",
      "dRecov            0\n",
      "subject_id        0\n",
      "file_name         0\n",
      "mean_intensity    0\n",
      "std_intensity     0\n",
      "max_intensity     0\n",
      "min_intensity     0\n",
      "shape             0\n",
      "dtype: int64\n",
      "Rows with missing 'dRecov' values have been removed. Cleaned data saved as 'cleaned_output.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from merged_output.csv\n",
    "df = pd.read_csv('merged_output.csv')\n",
    "\n",
    "# Display the number of missing values in each column (optional, for verification)\n",
    "print(\"Missing values before removal:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows where 'dRecov' has missing values\n",
    "df_cleaned = df.dropna(subset=['dRecov'])\n",
    "\n",
    "# Display the number of missing values in each column after removal (optional, for verification)\n",
    "print(\"\\nMissing values after removal:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "df_cleaned.to_csv('cleaned_output.csv', index=False)\n",
    "\n",
    "print(\"Rows with missing 'dRecov' values have been removed. Cleaned data saved as 'cleaned_output.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned HDF5 file saved as 'cleaned_output.h5'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# Step 1: Load the merged_output.csv and identify IDs with missing dRecov\n",
    "df = pd.read_csv('merged_output.csv')\n",
    "\n",
    "# Identify the IDs where 'dRecov' is missing\n",
    "missing_ids = df[df['dRecov'].isnull()]['ID']\n",
    "\n",
    "# Save the missing IDs to a CSV for reference (optional)\n",
    "missing_ids.to_csv('removed_ids.csv', index=False)\n",
    "\n",
    "# Step 2: Load the original output.h5 file\n",
    "with h5py.File('output.h5', 'r') as h5_file:\n",
    "    # Copy data to a new HDF5 file, excluding the rows with missing IDs\n",
    "    with h5py.File('cleaned_output.h5', 'w') as cleaned_h5:\n",
    "        for subject_id in h5_file.keys():\n",
    "            # Only copy data if the subject_id is not in the missing_ids list\n",
    "            if subject_id not in missing_ids.astype(str).values:\n",
    "                # Copy the group corresponding to this subject_id\n",
    "                h5_file.copy(subject_id, cleaned_h5)\n",
    "\n",
    "print(\"Cleaned HDF5 file saved as 'cleaned_output.h5'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlayed data saved in 'brain_overlayed_new.h5' and 'gm_overlayed_new.h5'.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Function to overlay a mask with MRI data\n",
    "def overlay_mask(mri_data, mask_data):\n",
    "    reduced_data = mri_data[mask_data == 1]\n",
    "    \n",
    "    return reduced_data\n",
    "\n",
    "# Load the brain and GM masks\n",
    "with h5py.File('brain_mask.h5', 'r') as brain_mask_file, \\\n",
    "     h5py.File('gm_mask.h5', 'r') as gm_mask_file:\n",
    "\n",
    "    brain_mask = brain_mask_file['image_array'][:]\n",
    "    gm_mask = gm_mask_file['image_array'][:]\n",
    "\n",
    "    # Load the cleaned MRI data and create new HDF5 files for overlayed data\n",
    "    with h5py.File('cleaned_output.h5', 'r') as cleaned_file, \\\n",
    "         h5py.File('brain_overlayed_new.h5', 'w') as brain_overlayed_file, \\\n",
    "         h5py.File('gm_overlayed_new.h5', 'w') as gm_overlayed_file:\n",
    "\n",
    "        for subject_id in cleaned_file.keys():\n",
    "            # Load the MRI data for the current subject\n",
    "            mri_data = cleaned_file[subject_id]['image_array'][:]\n",
    "\n",
    "            # Overlay the brain mask\n",
    "            brain_overlay = overlay_mask(mri_data, brain_mask)\n",
    "            brain_overlayed_file.create_dataset(f'{subject_id}/image_array', data=brain_overlay)\n",
    "\n",
    "            # Overlay the GM mask\n",
    "            gm_overlay = overlay_mask(mri_data, gm_mask)\n",
    "            gm_overlayed_file.create_dataset(f'{subject_id}/image_array', data=gm_overlay)\n",
    "\n",
    "print(\"Overlayed data saved in 'brain_overlayed_new.h5' and 'gm_overlayed_new.h5'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 620ms/step - loss: 34284036.0000 - val_loss: 369005.7188\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327ms/step - loss: 1120861.8750 - val_loss: 183958.7812\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319ms/step - loss: 960735.5000 - val_loss: 24012.4961\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - loss: 1053184.2500 - val_loss: 656512.1250\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - loss: 1107223.0000 - val_loss: 140808.7969\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332ms/step - loss: 1186439.6250 - val_loss: 2310.1797\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 324ms/step - loss: 1279148.1250 - val_loss: 91696.7500\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313ms/step - loss: 1378528.0000 - val_loss: 113123.8828\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - loss: 1059637.2500 - val_loss: 13042.9102\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 363ms/step - loss: 769458.5625 - val_loss: 16019.9521\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305ms/step - loss: 838130.2500 - val_loss: 97523.4141\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279ms/step - loss: 658881.4375 - val_loss: 29702.6230\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314ms/step - loss: 665048.4375 - val_loss: 3470.9485\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - loss: 541619.1875 - val_loss: 6980.7388\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368ms/step - loss: 466657.4688 - val_loss: 25082.1367\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step - loss: 555243.2500 - val_loss: 3964.8752\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331ms/step - loss: 340763.4688 - val_loss: 79900.8594\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - loss: 379425.4688 - val_loss: 143093.7656\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step - loss: 518906.5938 - val_loss: 41486.6055\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 326ms/step - loss: 270429.0625 - val_loss: 7185.5684\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - loss: 631682.4375 - val_loss: 3605.1958\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340ms/step - loss: 415875.2812 - val_loss: 12861.1152\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - loss: 456635.8438 - val_loss: 149609.8125\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339ms/step - loss: 396872.4375 - val_loss: 250581.1562\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - loss: 352726.4688 - val_loss: 75076.6719\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383ms/step - loss: 244044.5469 - val_loss: 3546.7319\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 317ms/step - loss: 296858.7812 - val_loss: 14443.2217\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288ms/step - loss: 248259.3594 - val_loss: 7202.9302\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334ms/step - loss: 328220.6875 - val_loss: 135466.4844\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 322ms/step - loss: 288929.0000 - val_loss: 253616.6875\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378ms/step - loss: 378672.9688 - val_loss: 116282.1328\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318ms/step - loss: 254996.3281 - val_loss: 2779.9814\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307ms/step - loss: 227987.0000 - val_loss: 69599.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 322ms/step - loss: 482424.3125 - val_loss: 34038.0820\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340ms/step - loss: 189035.7812 - val_loss: 5252.4897\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289ms/step - loss: 207637.5938 - val_loss: 28763.3340\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 322ms/step - loss: 272217.5312 - val_loss: 19752.2422\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289ms/step - loss: 210018.2656 - val_loss: 3399.7854\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 350ms/step - loss: 165258.9062 - val_loss: 6171.3179\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - loss: 137943.4375 - val_loss: 10364.6963\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - loss: 171206.2031 - val_loss: 5934.4707\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step - loss: 147197.8125 - val_loss: 4816.7256\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340ms/step - loss: 123818.6328 - val_loss: 2568.5710\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361ms/step - loss: 117959.6797 - val_loss: 2708.3557\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476ms/step - loss: 156380.0469 - val_loss: 6030.6196\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347ms/step - loss: 93313.1328 - val_loss: 9455.6221\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331ms/step - loss: 78168.2969 - val_loss: 5552.9795\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377ms/step - loss: 90607.1328 - val_loss: 2541.6025\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - loss: 82418.3203 - val_loss: 2548.9675\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374ms/step - loss: 90720.0469 - val_loss: 2974.1462\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2386.8052\n",
      "Test Loss (RMSE): 48.85494013691195\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load CSV Data\n",
    "df = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Define input features and target variable\n",
    "X = df[['age', 'sex', 'HOC', 'dInj']]\n",
    "y = df['dRecov']\n",
    "\n",
    "# Normalize or standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Load MRI and Mask Data from HDF5\n",
    "with h5py.File('brain_overlayed.h5', 'r') as brain_h5, \\\n",
    "     h5py.File('gm_overlayed.h5', 'r') as gm_h5:\n",
    "\n",
    "    # Example: Extract features for each subject and concatenate with X_scaled\n",
    "    mri_features = []\n",
    "    for subject_id in brain_h5.keys():\n",
    "        brain_data = brain_h5[subject_id]['image_array'][:]\n",
    "        gm_data = gm_h5[subject_id]['image_array'][:]\n",
    "        \n",
    "        # Flatten the MRI data or use a feature extractor here\n",
    "        brain_flatten = brain_data.flatten()\n",
    "        gm_flatten = gm_data.flatten()\n",
    "        \n",
    "        # Combine all features into one vector\n",
    "        combined_features = np.concatenate((brain_flatten, gm_flatten))\n",
    "        mri_features.append(combined_features)\n",
    "\n",
    "    mri_features = np.array(mri_features)\n",
    "\n",
    "# Combine the features from the CSV with the MRI features\n",
    "X_combined = np.hstack((X_scaled, mri_features))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (RMSE): {np.sqrt(test_loss)}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 485ms/step - loss: 45776900.0000 - val_loss: 1158715.7500\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307ms/step - loss: 3546350.0000 - val_loss: 2034875.7500\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step - loss: 2230781.2500 - val_loss: 890452.6875\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - loss: 2821116.7500 - val_loss: 806163.3125\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - loss: 1280334.5000 - val_loss: 128545.2188\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - loss: 807340.9375 - val_loss: 1613796.8750\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333ms/step - loss: 1590319.0000 - val_loss: 1290348.7500\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330ms/step - loss: 1537342.0000 - val_loss: 147528.8125\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377ms/step - loss: 557057.7500 - val_loss: 47604.2305\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - loss: 1072015.3750 - val_loss: 29777.2168\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383ms/step - loss: 687254.0625 - val_loss: 72895.8672\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328ms/step - loss: 443623.8750 - val_loss: 403210.1875\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320ms/step - loss: 732398.4375 - val_loss: 363298.8438\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 324ms/step - loss: 419166.3125 - val_loss: 77583.3516\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346ms/step - loss: 186735.9688 - val_loss: 4902.5503\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315ms/step - loss: 282987.0938 - val_loss: 26934.3496\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - loss: 330308.8125 - val_loss: 3560.5447\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320ms/step - loss: 163337.4844 - val_loss: 45663.6133\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - loss: 181086.5000 - val_loss: 86502.2344\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323ms/step - loss: 138069.0469 - val_loss: 56248.6484\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313ms/step - loss: 202866.7188 - val_loss: 10424.9268\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315ms/step - loss: 71637.0312 - val_loss: 7614.6479\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289ms/step - loss: 71016.7109 - val_loss: 22243.2812\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - loss: 135830.5938 - val_loss: 9757.5615\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328ms/step - loss: 111951.9531 - val_loss: 4942.4438\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step - loss: 55864.0898 - val_loss: 21220.0547\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - loss: 78893.1719 - val_loss: 23526.6621\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319ms/step - loss: 55014.6719 - val_loss: 6881.5996\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - loss: 80615.0312 - val_loss: 3502.4468\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334ms/step - loss: 133578.3438 - val_loss: 5610.3896\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - loss: 45318.0156 - val_loss: 4627.0947\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320ms/step - loss: 78232.6484 - val_loss: 2788.2021\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - loss: 43483.9844 - val_loss: 4083.6753\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - loss: 31951.9238 - val_loss: 5966.3213\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327ms/step - loss: 27944.3633 - val_loss: 9059.9170\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349ms/step - loss: 49470.9492 - val_loss: 8150.2104\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348ms/step - loss: 28181.6914 - val_loss: 2926.4756\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302ms/step - loss: 42991.6133 - val_loss: 2798.0520\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - loss: 25328.7949 - val_loss: 3237.6907\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - loss: 48197.4297 - val_loss: 3026.2610\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313ms/step - loss: 16465.8926 - val_loss: 2398.1624\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - loss: 23477.2246 - val_loss: 2865.4524\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379ms/step - loss: 18540.2070 - val_loss: 3562.3425\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - loss: 80018.8438 - val_loss: 2492.8325\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - loss: 68361.9219 - val_loss: 2496.6536\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 366ms/step - loss: 22020.0117 - val_loss: 3510.6042\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step - loss: 22654.4297 - val_loss: 4012.2908\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338ms/step - loss: 27378.9629 - val_loss: 3509.3120\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - loss: 17648.8438 - val_loss: 3379.6558\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319ms/step - loss: 82862.0469 - val_loss: 2523.2041\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2041.5986\n",
      "Test Loss (RMSE): 45.184052859526666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load CSV Data\n",
    "df = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Convert IDs to strings and pad zeros to make sure they are of length 3 (e.g., '001' instead of '1')\n",
    "csv_ids = set(df['ID'].astype(str).str.zfill(3))\n",
    "\n",
    "# Define input features and target variable\n",
    "X = df[['age', 'sex', 'HOC', 'dInj']]\n",
    "y = df['dRecov']\n",
    "\n",
    "# Normalize or standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Load MRI and Mask Data from HDF5\n",
    "mri_features = []\n",
    "\n",
    "\n",
    "with h5py.File('brain_overlayed.h5', 'r') as brain_h5, \\\n",
    "     h5py.File('gm_overlayed.h5', 'r') as gm_h5:\n",
    "\n",
    "    for subject_id in brain_h5.keys():\n",
    "        if subject_id in csv_ids:\n",
    "            # Load the MRI data for the current subject\n",
    "            brain_data = brain_h5[subject_id]['image_array'][:]\n",
    "            gm_data = gm_h5[subject_id]['image_array'][:]\n",
    "\n",
    "            # Flatten the MRI data or use a feature extractor here\n",
    "            brain_flatten = brain_data.flatten()\n",
    "            gm_flatten = gm_data.flatten()\n",
    "\n",
    "            # Combine all features into one vector\n",
    "            combined_features = np.concatenate((brain_flatten, gm_flatten))\n",
    "            mri_features.append(combined_features)\n",
    "\n",
    "    mri_features = np.array(mri_features)\n",
    "\n",
    "# Check if the dimensions match after filtering\n",
    "if X_scaled.shape[0] != mri_features.shape[0]:\n",
    "    raise ValueError(f\"Dimension mismatch: X_scaled has {X_scaled.shape[0]} rows, but mri_features has {mri_features.shape[0]} rows.\")\n",
    "\n",
    "# Combine the features from the CSV with the MRI features\n",
    "X_combined = np.hstack((X_scaled, mri_features))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (RMSE): {np.sqrt(test_loss)}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 61.9045522072842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 59.519913618783924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 77.42995826874764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 90.60838748599657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 51.27667227054253\n",
      "Mean RMSE across all folds: 68.14789677027098\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load CSV Data\n",
    "df = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Select input features and target variable\n",
    "X = df[['age', 'sex', 'HOC', 'dInj']]\n",
    "y = df['dRecov']\n",
    "\n",
    "# Normalize or standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Load MRI Data from HDF5\n",
    "mri_features = []\n",
    "\n",
    "# Extract MRI data from HDF5 and match it by subject ID\n",
    "with h5py.File('cleaned_output.h5', 'r') as h5_file:\n",
    "    for subject_id in df['ID'].astype(str).str.zfill(3):\n",
    "        if subject_id in h5_file.keys():\n",
    "            # Load MRI data for the current subject\n",
    "            mri_data = h5_file[subject_id]['image_array'][:]\n",
    "            \n",
    "            # Flatten the MRI data or use a feature extractor here\n",
    "            mri_flatten = mri_data.flatten()\n",
    "            \n",
    "            # Append the flattened MRI data to the features list\n",
    "            mri_features.append(mri_flatten)\n",
    "\n",
    "mri_features = np.array(mri_features)\n",
    "\n",
    "# Combine the features from the CSV with the MRI features\n",
    "X_combined = np.hstack((X_scaled, mri_features))\n",
    "\n",
    "# Discretize y to create bins for stratification\n",
    "y_binned = pd.qcut(y, q=5, labels=False)  # Create 5 bins for stratification\n",
    "\n",
    "# Define the Stratified K-Fold Cross-Validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for each fold\n",
    "fold_rmse = []\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "for train_index, test_index in skf.split(X_combined, y_binned):\n",
    "    X_train, X_test = X_combined[train_index], X_combined[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    rmse = np.sqrt(test_loss)\n",
    "    fold_rmse.append(rmse)\n",
    "    print(f\"Fold RMSE: {rmse}\")\n",
    "\n",
    "# Calculate the mean RMSE across all folds\n",
    "mean_rmse = np.mean(fold_rmse)\n",
    "print(f\"Mean RMSE across all folds: {mean_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gridsearchCV, repeated stratified k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Intel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Load CSV Data\n",
    "df = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Select input features and target variable\n",
    "X = df[['age', 'sex', 'HOC', 'dInj']]\n",
    "y = df['dRecov']\n",
    "\n",
    "# Normalize or standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Load MRI Data from HDF5\n",
    "mri_features = []\n",
    "\n",
    "# Extract MRI data from HDF5 and match it by subject ID\n",
    "with h5py.File('cleaned_output.h5', 'r') as h5_file:\n",
    "    for subject_id in df['ID'].astype(str).str.zfill(3):\n",
    "        if subject_id in h5_file.keys():\n",
    "            # Load MRI data for the current subject\n",
    "            mri_data = h5_file[subject_id]['image_array'][:]\n",
    "            \n",
    "            # Flatten the MRI data or use a feature extractor here\n",
    "            mri_flatten = mri_data.flatten()\n",
    "            \n",
    "            # Append the flattened MRI data to the features list\n",
    "            mri_features.append(mri_flatten)\n",
    "\n",
    "mri_features = np.array(mri_features)\n",
    "\n",
    "# Combine the features from the CSV with the MRI features\n",
    "X_combined = np.hstack((X_scaled, mri_features))\n",
    "\n",
    "# Define a function to create a Keras model for use in GridSearchCV\n",
    "def create_model(neurons=64, learning_rate=0.001, dropout_rate=0.0):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(neurons, activation='relu', input_shape=(X_combined.shape[1],)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(neurons // 2, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Wrap the model for use in scikit-learn\n",
    "model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'model__neurons': [32, 64, 128],  # Number of neurons in the first hidden layer\n",
    "    'model__learning_rate': [0.001, 0.01, 0.1],  # Learning rate for Adam optimizer\n",
    "    'model__dropout_rate': [0.0, 0.2, 0.5],  # Dropout rate\n",
    "    'batch_size': [16, 32, 64],  # Batch size for training\n",
    "    'epochs': [50, 100]  # Number of epochs\n",
    "}\n",
    "\n",
    "# Set up the stratified k-fold cross-validation\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "# Define the scoring function (RMSE)\n",
    "scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Set up GridSearchCV to search for the best hyperparameters\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, cv=rskf, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_result = grid.fit(X_combined, y)\n",
    "\n",
    "# Display the best parameters and the best RMSE score\n",
    "print(f\"Best parameters: {grid_result.best_params_}\")\n",
    "print(f\"Best RMSE: {grid_result.best_score_}\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Evaluate the best model on a hold-out test set if available, or using cross-validation scores\n",
    "print(f\"Best model score (RMSE): {grid_result.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load CSV Data\n",
    "df = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Convert IDs to strings and pad zeros to make sure they are of length 3 (e.g., '001' instead of '1')\n",
    "csv_ids = set(df['ID'].astype(str).str.zfill(3))\n",
    "\n",
    "# Select input features and target variable\n",
    "X = df[['age', 'sex', 'HOC', 'dInj']]\n",
    "y = df['dRecov']\n",
    "\n",
    "# Normalize or standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Load MRI Data from HDF5\n",
    "mri_features = []\n",
    "\n",
    "# Extract MRI data from HDF5 and match it by subject ID\n",
    "with h5py.File('cleaned_output.h5', 'r') as h5_file:\n",
    "    for subject_id in csv_ids:\n",
    "        if subject_id in h5_file.keys():\n",
    "            # Load MRI data for the current subject\n",
    "            mri_data = h5_file[subject_id]['image_array'][:]\n",
    "            \n",
    "            # Flatten the MRI data or use a feature extractor here\n",
    "            mri_flatten = mri_data.flatten()\n",
    "            \n",
    "            # Append the flattened MRI data to the features list\n",
    "            mri_features.append(mri_flatten)\n",
    "\n",
    "mri_features = np.array(mri_features)\n",
    "\n",
    "# Ensure the dimensions match after processing\n",
    "if X_scaled.shape[0] != mri_features.shape[0]:\n",
    "    raise ValueError(f\"Dimension mismatch: X_scaled has {X_scaled.shape[0]} rows, but mri_features has {mri_features.shape[0]} rows.\")\n",
    "\n",
    "# Step 3: Combine Features\n",
    "X_combined = np.hstack((X_scaled, mri_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HDF5 data has been successfully converted to 'cleaned_output.csv'.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('cleaned_output.h5', 'r') as h5_file:\n",
    "    # Create a list to store the data to be converted to CSV\n",
    "    data_list = []\n",
    "\n",
    "    # Iterate through each subject in the HDF5 file\n",
    "    for subject_id in h5_file.keys():\n",
    "        # Load MRI data for the current subject\n",
    "        mri_data = h5_file[subject_id]['image_array'][:]\n",
    "        \n",
    "        # Flatten the MRI data (or perform any desired feature extraction)\n",
    "        mri_flattened = mri_data.flatten()\n",
    "        \n",
    "        # Convert the flattened MRI data into a list\n",
    "        mri_flattened_list = mri_flattened.tolist()\n",
    "        \n",
    "        # Append the subject ID and the flattened MRI data to the data list\n",
    "        data_list.append([subject_id] + mri_flattened_list)\n",
    "\n",
    "# Convert the data list to a DataFrame\n",
    "df_mri = pd.DataFrame(data_list)\n",
    "\n",
    "# Define the column names (subject_id + MRI pixel values)\n",
    "df_mri.columns = ['subject_id'] + [f'pixel_{i}' for i in range(df_mri.shape[1] - 1)]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_mri.to_csv('cleaned_output_from_h5.csv', index=False)\n",
    "\n",
    "print(\"The HDF5 data has been successfully converted to 'cleaned_output.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common IDs across all files: {'113', '119', '044', '008', '040', '067', '075', '083', '020', '080', '043', '115', '002', '107', '110', '004', '050', '073', '069', '101', '068', '003', '047', '071', '082', '057', '064', '120', '072', '005', '009', '088', '079', '085', '097', '011', '112', '049', '055', '042', '010', '028', '127', '061', '102', '103', '118', '015', '014', '022', '033', '024', '063', '046', '123', '017', '091', '051', '018', '109', '036', '126', '125', '013', '089', '093', '062', '114', '081', '023', '006', '099', '007', '104', '034', '021', '016', '052', '031', '012', '105', '032', '076', '111', '106', '090', '039', '019', '056', '094', '092', '027', '078'}\n",
      "\n",
      "Unique IDs in brain_overlayed.h5: set()\n",
      "\n",
      "Unique IDs in gm_overlayed.h5: set()\n",
      "\n",
      "Unique IDs in cleaned_output.h5: set()\n",
      "\n",
      "Unique IDs in cleaned_output.csv: set()\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract IDs from brain_overlayed.h5\n",
    "with h5py.File('brain_overlayed.h5', 'r') as brain_h5:\n",
    "    brain_ids = set(brain_h5.keys())\n",
    "\n",
    "# Step 2: Extract IDs from gm_overlayed.h5\n",
    "with h5py.File('gm_overlayed.h5', 'r') as gm_h5:\n",
    "    gm_ids = set(gm_h5.keys())\n",
    "\n",
    "# Step 3: Extract IDs from cleaned_output.h5\n",
    "with h5py.File('cleaned_output.h5', 'r') as cleaned_h5:\n",
    "    cleaned_h5_ids = set(cleaned_h5.keys())\n",
    "\n",
    "# Step 4: Extract and format IDs from cleaned_output.csv\n",
    "df = pd.read_csv('cleaned_output.csv')\n",
    "\n",
    "# Convert IDs to strings and pad zeros to make sure they are of length 3 (e.g., '001' instead of '1')\n",
    "csv_ids = set(df['ID'].astype(str).str.zfill(3))\n",
    "\n",
    "# Step 5: Compare the IDs\n",
    "# Find common IDs\n",
    "common_ids = brain_ids & gm_ids & cleaned_h5_ids & csv_ids\n",
    "\n",
    "# Find unique IDs in each set\n",
    "unique_brain_ids = brain_ids - common_ids\n",
    "unique_gm_ids = gm_ids - common_ids\n",
    "unique_cleaned_h5_ids = cleaned_h5_ids - common_ids\n",
    "unique_csv_ids = csv_ids - common_ids\n",
    "\n",
    "# Print the results\n",
    "print(\"Common IDs across all files:\", common_ids)\n",
    "print(\"\\nUnique IDs in brain_overlayed.h5:\", unique_brain_ids)\n",
    "print(\"\\nUnique IDs in gm_overlayed.h5:\", unique_gm_ids)\n",
    "print(\"\\nUnique IDs in cleaned_output.h5:\", unique_cleaned_h5_ids)\n",
    "print(\"\\nUnique IDs in cleaned_output.csv:\", unique_csv_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified IDs have been removed and original files have been updated.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "# Define the IDs to be removed\n",
    "ids_to_remove = {'086', '035', '060', '066', '084'}\n",
    "\n",
    "# Function to remove IDs from an HDF5 file and overwrite the original file\n",
    "def remove_ids_from_h5(file_path, ids_to_remove):\n",
    "    # Open the existing HDF5 file\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        # Create a temporary file to store cleaned data\n",
    "        temp_file_path = 'temp_' + file_path\n",
    "        with h5py.File(temp_file_path, 'w') as cleaned_h5_file:\n",
    "            # Copy groups that are not in ids_to_remove\n",
    "            for subject_id in h5_file.keys():\n",
    "                if subject_id not in ids_to_remove:\n",
    "                    h5_file.copy(subject_id, cleaned_h5_file)\n",
    "    \n",
    "    # Replace the original file with the cleaned temporary file\n",
    "    os.remove(file_path)\n",
    "    os.rename(temp_file_path, file_path)\n",
    "\n",
    "# Remove IDs from brain_overlayed.h5 and overwrite the file\n",
    "remove_ids_from_h5('brain_overlayed.h5', ids_to_remove)\n",
    "\n",
    "# Remove IDs from gm_overlayed.h5 and overwrite the file\n",
    "remove_ids_from_h5('gm_overlayed.h5', ids_to_remove)\n",
    "\n",
    "# Remove IDs from cleaned_output.h5 and overwrite the file\n",
    "remove_ids_from_h5('cleaned_output.h5', ids_to_remove)\n",
    "\n",
    "print(\"Specified IDs have been removed and original files have been updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
